{"posts":[{"title":"API Gateway와 EC2 연결하기.","text":"API Gateway를 활용하여, EC2 인스턴스에 프록시로서 연결하는 방법에 대해 알아보겠다. 보통 AWS Lambda의 API를 만들때 API Gateway를 활용하곤 한다. 그런데, 몇몇 경우에는 EC2에 Proxy를 만들어서 사용해야하는 경우가 있다. EC2 인스턴스에 FastAPI 서버 하나를 돌리고 있으니, “http://x.x.x.x:5000/“ 라는 서버에 API Gateway를 연결해보도록 하겠다. API Gateway - 제공 API 유형API Gateway에서 제공하는 API는 대표적으로 3종류가 있다. HTTP API : API 프록시 기능정도만 필요할 때 적합. 단순 / 저렴하고 빠르다. REST API : API 관리 기능, 요청/응답에 대한 제어가 필요할 경우 적합, 복잡 / 비싸고 느리다. WebSocket API : 웹소켓 용도. 실시간 애플리케이션에서 주로 사용한다. 출처: https://inpa.tistory.com/entry/AWS-📚-API-Gateway-개념-기본-사용법-정리#rest_api [Inpa Dev 👨‍💻:티스토리] HTTP API HTTP를 통신 방식으로 사용하는 API를 HTTP API라고 한다. HTTP API는 Endpoint를 API gateway로 활용하여 HTTP 요청을 통해서 서버에 접근할 수 있도록 만들어준다. HTTP API는 데이터만 주고 받고 UI 화면이 필요하면 클라이언트가 별도로 처리한다. 대게 앱/웹/서버 to 서버에서 사용된다. 대부분의 Web API가 HTTP API로 이루어지고 있다. REST API REST API는 HTTP API에 여러가지 제약 조건이 추가된 형태이다. 자원의 식별 메시지를 통한 리소스 조작 자기서술적 메세지 애플리케이션의 상태에 대한 엔진으로서 하이퍼미디어 REST는 웹 서비스의 구조를 만드는데 활용되는 패턴이며 위의 4가지 제약조건을 만족해야 RESTFUL 하다라고 말할 수 있다. 대표적으로 CRUD 메서드 동작을 일컫는다. CREATE(post), READ(get), UPDATE(put), DELETE(delete) 그런데 이런 부분을 완벽하게 지키면서 개발하는 것은 현실적으로 어렵고, 또 추가 개발 비용대비 효과가 있는 것도 아니어서, 이미 많은 사람들이 해당 조건을 지키지 않아도 REST API라고 하기 때문에, HTTP API나 REST API를 거의 같은 의미로 사용하고 있는 현실이다. (물론 엄격하게는 다르다) WEBSOCKET API 요청을 받고 응답하는 REST API와 달리 WebSocket API는 클라이언트 앱과 백엔드 간의 양방향 통신을 지원한다. 웹 소켓은 사용자의 브라우저와 서버 사이의 인터액티브 통신 세션을 설정할 수 있게 하는 고급 기술 이다. 채팅 앱 및 스트리밍 대시보드와 같은 실시간 양방향 통신 애플리케이션을 구축하여 백엔드 서비스와 클라이언트 간의 메시지 전송을 처리하기위해 지속적인 연결을 유지한다. API GatewayAPI Gateway는 람다와 같이 서버리스 서비스이며, 수신한 API 호출에 대해서만 지불한다.다만 HTTP API / REST API / WEBSOCKET API 각각 모두 요금대가 다르다. API Gateway 중 REST API 사용REST API 게이트웨이 생성API Gateway 콘솔에서 API 생성 &gt; REST API 생성을 클릭한다. API 세부정보 새 API 엔드포인트 유형 - 지역 엔드포인트 유형 지역 : 특정 리전 안에서 사용 최적화된 에지 : CloudFront 사용(일반적인 인터넷 상) 프라이빗 : AWS내 VPC에서만 접근 가능 REST API 게이트웨이 경로 설정API를 생성하면 HTTP API와는 달리 REST API는 구성 화면이 약간 복잡하게 되어있다. (HTTP API보다 제공 기능이 많아서 그렇다) 리소스 메뉴를 눌러 리소스 생성을 한다. 리소스는 실제 api를 호출하는 api url을 정의한다라고 보면 된다. 리소스 이름과 리소스 경로를 설정해준다. 리소스 경로 쪽에 {userid}와 같이 중괄호로 묶어주면 경로 파라미터로서 사용할수 있다. 경로 파라미터란, 예를들어 아래 URL 경로와 같이 http://localhost/hello/12http://localhost/hello/667http://localhost/hello/55 서비스 특성상 뒤의 경로가 고정되어있지 않고, 여러개의 경로값을 사용할경우 이들을 묶는 일종의 변수 역할이라고 보면 된다. 두개의 경로를 설정해줬으니 이제 요청할 메서드 설정을 해준다. 통합 유형으로 HTTP를 선택하고, 엔드포인트 URL에는 요청보낼 목적지를 설정한다. https://api.github.com/users/{userid} 위 url은 깃헙에서 무료로 제공하는 api로서, {userid} 부분에 깃헙 프로필 닉네임을 적고 요청하면 해당 깃헙 프로필 정보를 json으로 반환해준다. 메서드 유형 : HTTP 엔드포인트 URL : 콘텐츠 처리 : 패스스루 REST API 게이트웨이 테스트 및 배포메서드 등록이 완료되면, 메서드 실행 환경으로 접근된다. {useri} 경로 파라미터에 닉네임 아무거나 적어서 테스트를 한다. 테스트까지 완료되면 완성된 REST API를 API Gateway에 정식 배포한다. 작업 &gt; API 배포를 선택하여 API를 스테이지에 등록한다. EC2의 네트워크 이슈위 방법대로 api gateway를 배포했는데 504 에러가 난다. ec2 끼리 ping 테스트도 안되는것을 보니 네트워크 문제가 있는듯하다. 보안 그룹 설정 확인인바운드 규칙: 두 인스턴스의 보안 그룹에서 ICMP 프로토콜(핑 요청)을 허용하고 있는지 확인하세요. ICMP를 허용하려면:AWS 콘솔에서 보안 그룹으로 이동하여 두 인스턴스에 적용된 보안 그룹을 선택합니다.인바운드 규칙에서 Custom ICMP(사용자 지정 ICMP - IPv4) - Echo Request(에코 응답) 유형을 추가하고, 소스에 허용할 IP 범위(예: 0.0.0.0/0 또는 특정 CIDR 블록, Anywhere-IPv4)를 지정합니다. 또는 HTTP를 허용해줘야 한다. 아웃바운드 규칙: 대부분의 경우 기본적으로 모든 아웃바운드 트래픽이 허용되지만, 이를 확인하여 ICMP 프로토콜이 아웃바운드 트래픽에서도 허용되는지 점검합니다. 네트워크 ACL 설정 확인네트워크 ACL: 인스턴스가 위치한 서브넷의 네트워크 ACL이 ICMP 트래픽을 허용하는지 확인합니다.네트워크 ACL에서 인바운드 및 아웃바운드 트래픽 모두에 대해 ICMP(코드 0, 유형 8) 규칙이 설정되어 있는지 확인합니다.네트워크 ACL은 상태 비저장(Stateless)이므로, 인바운드와 아웃바운드 규칙이 모두 필요합니다. 위의 내용들을 참고하여 설정해주면 같은 서브넷 내의 EC2 인스턴스끼리는 통신이 된다. (private IP로) 그런데 API Gateway에서는 EC2 호출이 되지 않고, 2번 EC2에서 1번 EC2 호출 시 public IP로는 실패한다. 하지만 퍼블릭 IPv4 DNS로 접근하면 된다… 아마 퍼블릭 IPv4 주소의 문제이지 않을까 싶다. EC2끼리 통신은 해결했으니 이제 API Gateway -&gt; EC2 통신이 잘 되도록 해결해야한다. 서브넷의 라우팅테이블 확인EC2가 속해있는 VPC 서브넷의 라우팅 테이블이 0.0.0.0/0 -&gt; IGW가 아니라 0.0.0.0/0 -&gt; Transit Gateway 인것으로 보아 해당 서브넷이 퍼블릭 서브넷이 아님을 의미하며, 모든 트래픽이 Transit Gateway를 통해 라우팅된다는 것을 나타낸다. Transit Gateway가 대상일 때의 의미: Transit Gateway 사용: Transit Gateway는 여러 VPC, 온프레미스 데이터 센터, 그리고 AWS의 다른 리전 간의 네트워크 연결을 중앙집중식으로 관리하는 서비스입니다. 이 설정은 모든 트래픽(특히 인터넷 트래픽이 아닌 트래픽)이 Transit Gateway를 통해 라우팅되어 다른 네트워크로 전달된다는 것을 의미합니다. 퍼블릭 서브넷이 아님: 인터넷 게이트웨이(Internet Gateway)가 대상이 아니기 때문에, 이 서브넷은 퍼블릭 서브넷이 아닙니다. 즉, 이 서브넷의 인스턴스들은 직접적으로 인터넷에 접근할 수 없습니다. 대신, 트래픽은 Transit Gateway를 통해 다른 네트워크(예: 다른 VPC, 온프레미스 네트워크)로 전달됩니다. 프라이빗 서브넷의 특성: 이 설정은 주로 프라이빗 서브넷에서 사용되며, 특정 리소스(예: 데이터베이스 서버, 내부 서비스 등)가 외부 인터넷이 아닌, 내부 네트워크나 다른 VPC로만 접근이 필요할 때 유용합니다. 실질적인 사용 사례: 기업 네트워크 통합: 여러 VPC나 온프레미스 네트워크를 AWS Transit Gateway를 통해 연결하고, 중앙집중식으로 네트워크를 관리하고자 할 때. 보안 강화: 인터넷에 직접 노출되지 않도록 하고, 모든 트래픽을 보안 정책이 적용된 내부 네트워크로만 전달하고자 할 때. 결론:해당 서브넷은 퍼블릭 서브넷이 아니며, 외부 인터넷과의 통신은 불가능합니다. 모든 트래픽은 Transit Gateway를 통해 다른 VPC나 온프레미스 네트워크로 전달됩니다. 따라서, 인터넷을 통해 외부와 직접 통신하기 위해서는 별도의 퍼블릭 서브넷과 인터넷 게이트웨이를 사용하는 것이 필요합니다. API Gateway가 VPC 리소스(EC2 인스턴스)와 상호작용하도록 VPC Link 설정하기API Gateway는 일반적으로 AWS의 전역 서비스로 동작하며, 특정 VPC에 속하지 않습니다. 대신, API Gateway가 VPC 리소스(예: EC2 인스턴스)와 상호작용하도록 설정할 수 있는 방법이 있습니다. 이 설정은 **VPC 링크(VPC Link)**를 통해 이루어지며, 이 경우 API Gateway는 특정 VPC와 연결됩니다. API Gateway를 통해 VPC 내의 리소스(예: EC2, NLB, ECS)와 통신해야 하는 경우, VPC 링크를 사용하여 안전하게 연결할 수 있습니다.VPC 링크는 API Gateway와 VPC 내의 리소스 간의 프라이빗 통신을 가능하게 합니다. API Gateway가 VPC 링크를 사용하지 않고 설정되어 있다면, API Gateway는 AWS의 관리형 인프라에서 실행되며, 특정 VPC에 속하지 않습니다.API Gateway와 VPC 리소스 간의 연결이 필요하지 않은 경우에는 VPC 링크를 사용하지 않아도 됩니다. VPC 링크 생성하기먼저, VPC 링크를 생성합니다. API Gateway 콘솔에 로그인: AWS 관리 콘솔에서 API Gateway로 이동합니다. VPC 링크 생성: 왼쪽 메뉴에서 VPC Links를 선택한 후 Create 버튼을 클릭합니다. VPC 링크 이름을 지정하고, 연결할 네트워크 로드 밸런서(NLB)를 선택합니다. 필요한 경우, 태그를 추가합니다. Create 버튼을 눌러 VPC 링크를 생성합니다. API Gateway에 VPC 링크 설정하기이제, VPC 링크를 API Gateway의 특정 API 엔드포인트에 설정합니다. API 선택: API Gateway 콘솔에서 VPC 링크를 설정할 API를 선택합니다. 리소스 및 메서드 선택: 설정할 리소스 및 메서드를 선택합니다. 예를 들어, /plugin/test 리소스의 GET 메서드를 선택합니다. 통합 유형 선택: Integration Request 섹션으로 이동합니다. **통합 유형(Integration type)**에서 VPC Link를 선택합니다. VPC 링크 및 로드 밸런서 선택: 이전 단계에서 생성한 VPC 링크를 선택합니다. Endpoint URL 필드에 연결할 VPC 리소스의 URL을 입력합니다. 이 URL은 네트워크 로드 밸런서(NLB)의 DNS 이름과 해당 포트를 포함해야 합니다. 예시: http://:/ 저장 및 배포: 설정을 저장하고 API를 배포합니다. 참고https://inpa.tistory.com/entry/AWS-%F0%9F%93%9A-API-Gateway-%EA%B0%9C%EB%85%90-%EA%B8%B0%EB%B3%B8-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%A0%95%EB%A6%AC#rest_api","link":"/2024/08/05/API-Gateway-EC2/"},{"title":"API Gateway, Lambda로 배포하기","text":"ML 모델을 서비스로 배포하려면 어떻게 하는게 좋을지 고민하던 중, AWS에 Amazon SageMaker를 이용하여 배포하기로 결정했다. Developer Guide의 Get Started를 읽으며 따라해보았다. ML inference를 서비스로 만드려면?AWS Docs를 따라 진행해보겠다. 가상환경 생성12345678% python3 -m venv aws% python3 --versionPython 3.12.3% source ./aws/bin/activate또는$ python3 -m venv venv37$ . venv37/bin/activate 관련 라이브러리 설치1234% python3 -m pip install chalice% chalice new-project gemma-inference-server# 이후 app.py 수정 directory로 들어가보면 아래와같이 여러 파일들이 생성되어있다. 1234567891011% ls -latotal 24drwxr-xr-x 9 leehamin staff 288 Jun 15 16:55 .drwxr-xr-x 63 leehamin staff 2016 Jun 15 16:50 ..drwxr-xr-x 3 leehamin staff 96 Jun 15 16:50 .chalice-rw-r--r-- 1 leehamin staff 37 Jun 15 16:50 .gitignoredrwxr-xr-x 9 leehamin staff 288 Jun 15 17:01 .ideadrwxr-xr-x 6 leehamin staff 192 Jun 15 16:51 .venvdrwxr-xr-x 3 leehamin staff 96 Jun 15 16:55 __pycache__-rw-r--r-- 1 leehamin staff 1427 Jun 15 16:52 app.py-rw-r--r-- 1 leehamin staff 31 Jun 15 16:54 requirements.txt 지금은 .chalice 디렉터리를 무시해도 된다. 우리가 집중할 두 가지 주요 파일은 app.py와 요구사항.txt입니다. Chalice 로컬 환경 테스트123% chalice local# localhost로 접근 시 기본 샘플 동작하는 것 확인 AWS CLI 설정configure sso123456789101112aws configure sso # SSO session name: (임의로 입력해도 되는 것으로 추정)# SSO start URL: (Access Keys에 있는 값 복사)# SSO Region: (Access Keys에 있는 값 복사)# SSO registration scopes: (기본 값이 있는 것 같아 Enter 누르고 넘김) # 이후 code를 입력하는 링크가 주어지는데, AWS 콘솔에 로그인 한 환경에서 해당 링크에 접속하여 해당 code 입력To use this profile, specify the profile name using --profile, as shown:aws s3 ls --profile [profile명] Access key 입력1234567# Access keys에 있는 값 복사하여 입력 (어느정도 시간이 지나면 키나 토큰값이 달라지는 것 같으므로 주의)export AWS_ACCESS_KEY_ID=&quot;&quot;export AWS_SECRET_ACCESS_KEY=&quot;&quot;export AWS_SESSION_TOKEN=&quot;&quot;# 가이드엔 없는데, deploy시 에러 발생하여 추가로 실행export AWS_DEFAULT_REGION=&quot;ap-northeast-2&quot; 배포1chalice deploy","link":"/2024/06/13/API-Gateway-Lambda/"},{"title":"AWS EC2에 remote yum repository 설정하기","text":"CentOS 리눅스 시스템에서 사용되는 YUM 리포지토리 설정 파일의 예 리포지토리 설정 파일 생성먼저, 제공된 설정을 리포지토리 설정 파일에 저장해야 합니다. 이 파일은 일반적으로 /etc/yum.repos.d/ 디렉토리에 위치합니다. 예를 들어, 파일 이름을 nexus.repo로 할 수 있습니다. 1sudo vi /etc/yum.repos.d/nexus.repo 1234567[nexusrepo]name=Nexus Repositorybaseurl=https://{id}:{pw}{remote-repo-url}/repository/yum-dspace-group/7/x86_64enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-amazon-linux-2priority=1 YUM 캐시 클리어YUM 캐시를 클리어하여 최신 리포지토리 정보를 불러옵니다. 1sudo yum clean all 리포지토리 리스트 확인리포지토리가 제대로 추가되었는지 확인합니다. 1sudo yum repolist enabled 이 명령은 활성화된 모든 리포지토리의 리스트를 보여줍니다. nexusrepo가 리스트에 나타나야 합니다. 패키지 검색 테스트nexusrepo 리포지토리에서 특정 패키지를 검색해보아 테스트 할 수 있습니다. 1sudo yum --disablerepo=&quot;*&quot; --enablerepo=&quot;nexusrepo&quot; list available 패키지 설치 테스트리포지토리에서 패키지를 설치하여 테스트할 수 있습니다. 예를 들어 example-package를 설치하려면 다음 명령을 사용합니다. 1sudo yum --disablerepo=&quot;*&quot; --enablerepo=&quot;nexusrepo&quot; install example-package 이렇게 함으로써, 리포지토리 설정이 올바르게 작동하는지 확인할 수 있습니다. 패키지 이름은 해당 리포지토리에서 제공하는 실제 패키지 이름으로 교체해야 합니다.","link":"/2024/06/25/AWS-Change-Yum-Repo/"},{"title":"API Gateway에서 EC2의 application 호출","text":"회사에서 보안문제로, 외부망을 호출할 수 없게하여 EC2 에 다른 포트에 서빙중이었던 서비스에 접근하지 못한다고 proxy 역할을 할 다른 서버를 구축하고자 했다. 그래서 API-Gateway 로 EC2 를 직접 호출하게 했다. API Gateway를 REST API 유형으로 API 엔드포인트유형을 private으로 생성하기대상 그룹 생성 EC2에 서비스를 배포해놓았기 때문에, 대상 유형은 인스턴스로 선택 대상 그룹 이름과 프로토콜, VPC 등을 선택한다. 대상은 일단 EC2 앱들을 선택한다. 대상 그룹은 아래와 같이 하는것으로 변경했다둘 모두 같은 1번 ec2이며80포트에는 fastapi-server로 /plugin/test 요청 보내면 success를 돌려주는 api가8002포트에는 /chat?input=hi 이런식으로 보내면 llm 모델 응답을 보내주는 api가 올라가 있다 Network 로드 밸런서 생성 EC2에 가서, 로드밸런서 생성을 선택 기본구성을 해준다. 네트워크 매핑을 해준다, EC2가 있는 VPC를 선택해준다. -&gt; 주의사항 : 위 캡쳐처럼 app subnet으로 하는것이 아니라 elb를 선택해줘야 한다. 리스너 및 라우팅 설정을 해준다. 대상 그룹으로 연결해준다. VPC 링크 생성 API Gateway 대시보드로 가서 VPC 링크를 생성한다. API를 생성한다VPC 링크 연동 방식으로 API를 생성한다. -&gt; 주의사항 : 여기서 URL은 NLB의 DNS를 넣어줘야 한다. 주의사항 - NLB의 보안 그룹NLB의 보안그룹 편집에서, 보안설정 관련하여 PrivateLink 트래픽에 인바운드 규칙 허용을 선택하면 안된다. 메서드 생성 시 vpcLinkId와 vpcNLB 스테이지 변수 활용하기 배포 전 테스트 배포 후 테스트스테이지에 배포 후 url을 이용하여 외부 접근 테스트를 진행한다 chat api이번엔 EC2 8002번 포트에 /chat?input=hi 이런식으로 보내면 llm 응답을 돌려주는 api를 호출하도록 API Gateway에 새로운 메서드를 추가해보겠다. 앞의 다른 메서드와 유사하게 만들어준다. /chat?input=hi 이런식으로 쿼리 문자열 파라미터가 필요하므로 추가해준다 대상 그룹 등록 여부에 따른 호출 결과현재 8002포트의 /chat api는 에러가 있는데 대상그룹 등록여부에 따라 호출 결과가 다르다 대상 등록 시 대상 등록 취소 시 외부에서 호출1234567891011% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=baseball'{&quot;query&quot;:&quot;baseball&quot;,&quot;answer&quot;:&quot;\\&quot; baseball은 19세기 미국에서 발전한 야구 게임으로, 현재까지도 많은 사람들에게 사랑받고 있습니다. 그것은 미국에서 가장 인기 있는 스포츠 중 하나이며, 많은 선수들이 활동하고 있습니다.\\&quot;&quot;,&quot;ner&quot;:null}% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=football'{&quot;query&quot;:&quot;football&quot;,&quot;answer&quot;:&quot;검색 결과가 없습니다.&quot;,&quot;ner&quot;:null} % curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=LA' {&quot;query&quot;:&quot;LA&quot;,&quot;answer&quot;:&quot;LA는 5G 서비스 이용약관 계약의 성립에 대한 고객의 제출 서류입니다.&quot;,&quot;ner&quot;:null}% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=summary' {&quot;query&quot;:&quot;summary&quot;,&quot;answer&quot;:&quot;통화 기록을 요약한 결과를 제공합니다.\\n\\n summary = \\&quot;주말 점심 약속 조정\\&quot;\\n summary_detail = \\&quot;\\n - 오랜만에 연락하여 근황을 나눔\\n - 주말 점심 약속 제안 및 시간 조정\\n - 강남역 스타벅스에서 만나기로 장소 확정\\&quot;\\n \\&quot;event\\&quot;={\\n \\&quot;날짜\\&quot;: \\&quot;2024년 06월 01일\\&quot;,\\n \\&quot;시간\\&quot;: \\&quot;오후 1시\\&quot;,\\n \\&quot;장소\\&quot;: \\&quot;강남역 스타벅스\\&quot;,\\n \\&quot;대상\\&quot;: \\&quot;영수\\&quot;\\n }&quot;,&quot;ner&quot;:null} ReferencesAmzzon API Gateway 기반 VPC Link 활용 방법AWS API Gateway 에서 EC2 에 서빙중인 application HTTP 로 호출하기EC2 Instance Connect Endpoint를 통해 private EC2 접속하기","link":"/2024/08/11/AWS-Call-Application-hosted-on-EC2-from-AWS-API-Gateway/"},{"title":"파인튜닝한 bert 모델 서빙서버 EC2에 배포하기","text":"FastApi 프레임워크를 사용하여 웹 애플리케이션을 구축해보겠다.주요 기능으로는 텍스트 처리 및 AI 모델을 활용한 다양한 응답을 제공하는 API 엔드포인트 정의이다. FastAPI 코드1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import osfrom fastapi import FastAPIfrom fastapi.responses import RedirectResponsefrom langserve import add_routesfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipelinefrom transformers import AutoModel, AutoTokenizer, pipelinefrom pydantic import BaseModelfrom fastapi import FastAPI, HTTPExceptionapp = FastAPI()# 현재 파일의 위치를 기준으로 상대 경로 설정current_dir = os.path.dirname(os.path.abspath('./model'))model_path = os.path.join(current_dir, 'model')# 모델과 토크나이저 로드model = AutoModel.from_pretrained(model_path)tokenizer = AutoTokenizer.from_pretrained(model_path)pipe = pipeline(&quot;text2text-generation&quot;, model=model, tokenizer=tokenizer)huggingface_pipeline = HuggingFacePipeline(pipeline=pipe)# FastAPI 앱에 라우트 추가add_routes( app, huggingface_pipeline, path=&quot;/model&quot;)@app.get(&quot;/&quot;)async def redirect_root_to_docs(): return RedirectResponse(&quot;/docs&quot;)class TestRequest(BaseModel): text: str@app.post(&quot;/test&quot;)async def test_endpoint(request: TestRequest): try: inputs = tokenizer(request.text, return_tensors=&quot;pt&quot;)['input_ids'] output = model(inputs) logits = output[&quot;logits&quot;] logits_2 = output[&quot;logits2&quot;] intent = output[&quot;intent&quot;] ner = output[&quot;ner&quot;] predictions = torch.argmax( torch.FloatTensor(torch.softmax(logits, dim=1).tolist()), dim=1, ) predictions_ner = logits_2.argmax(-1) test_predict = predictions_ner[0][1:len(predictions_ner)-2] # 출력 예제 return { &quot;input&quot;: request.text, &quot;intent&quot;: label2intent(predictions.tolist()), &quot;tokens&quot;: tokenizer.tokenize(request.text), &quot;slot_labels&quot;: label2slot(test_predict), &quot;intent_raw&quot;: intent.tolist(), &quot;ner_raw&quot;: ner.tolist() } except Exception as e: raise HTTPException(status_code=500, detail=str(e))if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=5005) Dockerfile 코드1234567891011121314151617181920212223FROM python:3.11-slimRUN pip install poetry==1.6.1RUN poetry config virtualenvs.create falseWORKDIR /codeCOPY ./pyproject.toml ./README.md ./poetry.lock* ./COPY ./package[s] ./packagesRUN poetry install --no-interaction --no-ansi --no-rootCOPY ./app ./appCOPY ./model ./modelRUN poetry install --no-interaction --no-ansiEXPOSE 5004CMD exec uvicorn app.server:app --host 0.0.0.0 --port 5004 로컬에서 테스트해보기123456789 % curl --location 'http://127.0.0.1:5004/predict' \\--header 'Content-Type: application/json' \\--data '{ &quot;input&quot;: &quot;This is a test sentence.&quot;}'{&quot;input&quot;:&quot;This is a test sentence.&quot;,&quot;output&quot;:[[-1.1625256538391113,-0.7818309664726257,-1.2470957040786743,-0.07029110193252563,-1.252798318862915,1.1913777589797974,0.5518047213554382,...,-0.08304034173488617,0.22596704959869385]]} ECR에 도커이미지 push123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 도커 이미지 푸시 1234# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latestdocker push &lt;your-dockerhub-username&gt;/my-project:latest EC2에서 이미지 pull 하기1234567$ sudo yum install aws-cli -y$ aws configure$ export AWS_ACCESS_KEY_ID=[access_key_id]$ export AWS_SECRET_ACCESS_KEY=[aws_secret_access_key]$ export AWS_SESSION_TOKEN=[aws_session_token]$ aws ecr get-login-password --region [region] | docker login --username AWS --password-stdin [id].dkr.ecr.[region].amazonaws.com$ docker pull [id].dkr.ecr.[region].amazonaws.com/[project_name]:tag EC2에서 배포하기123456% docker run --name [app name] -p 5004:5004 [id].dkr.ecr.[region].amazonaws.com/[app name]:[tag]INFO: Started server process [1]INFO: Waiting for application startup.INFO: Application startup complete.INFO: Uvicorn running on http://0.0.0.0:5004 (Press CTRL+C to quit)INFO: 10.71.176.76:31790 - &quot;POST /predict HTTP/1.1&quot; 200 OK 테스트123$ curl --location 'http://[ec2_ip].compute.amazonaws.com:5004/predict' --header 'Content-Type: application/json' --data '{&quot;input&quot;:&quot;This is a test sentence.&quot;}'{&quot;input&quot;:&quot;This is a test sentence.&quot;,&quot;output&quot;:[[-1.1625256538391113,-0.7818318009376526,...,-0.08304007351398468,0.22596575319766998]]} API Gateway 연동하기API Gateway에서 메서드를 만들고 HTTP Request Header를 넣어준다 배포전에 테스트를 해준다 테스트 결과는 아래와 같이 나온다. 배포한 API 로컬 PC에서 호출해보기1234567% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/predict' \\--header 'accept: application/json' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;거실 조명을 좀 더 아늑한 느낌으로&quot;}'{&quot;input&quot;:&quot;거실 조명을 좀 더 아늑한 느낌으로&quot;,&quot;intent&quot;:&quot;조명따뜻하게설정&quot;,&quot;ner&quot;:[&quot;home&quot;,&quot;device&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;],&quot;raw_intent&quot;:&quot;조명따뜻하게설정&quot;,&quot;test_predict&quot;:[3,4,0,0,0,0,0],&quot;raw_ner&quot;:[&quot;home&quot;,&quot;device&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;],&quot;tokenized_text&quot;:[&quot;거실&quot;,&quot;조명&quot;,&quot;##을&quot;,&quot;좀&quot;,&quot;더&quot;,&quot;아늑한&quot;,&quot;느낌으로&quot;]","link":"/2024/08/12/AWS-Deploy-Fine-Tuned-BERT-Model-Serving-Server-on-EC2/"},{"title":"API Gateway Pattern에는 API Gateway가 없다.","text":"Microservices Architecture에 관해 이야기하면 왠지 API Gateway를 꼭 사용해야 할 것 같은 느낌이 든다. 과연 그럴까?왜 함부로 API Gateway를 사용하면 안 되는지, 그래도 사용해야 한다면 언제 사용할지에 대해 다뤄보려고 한다. 우아한형제들의 서버개발그룹장분이 잘 설명해주셔서, 두고두고 보려고 기록해두는 글이다. 출처 : https://www.youtube.com/watch?v=P2nM0_YptOA 다룰 내용 MSA에서 필수적인 API Gateway Pattern이 API Gateway Framework와 무관한 이유 API Gateway Pattern 이란? MSA의 API 애플리케이션의 역할 구분 하나씩 알아보는 API Gateway Framework을 사용하면 안되는 이유들 그래도 API Gateway Framework를 사용해도 되는 경우들 API Gateway Pattern 이란?API Gateway Pattern은 어디 나오나? 크리스 리처드슨은 본인이 생각한 MSA의 여러가지 패턴을 마이크로 서비스 패턴이라는 책으로 정리하였다.그 외에도 마이크로소프트 홈페이지에서도 API Gateway 패턴을 다루고 있다. API 게이트의 패턴은 여러 마이크로 서비스로 나뉜 것을 하나로 묶어 클라이언트가 호출하게 해주는 것으로 MSA에서는 필수적인 요소이다. 그렇다면 API 게이트웨이 패턴의 정의를 그림 한장으로 나타내면 무엇일까? 모바일 앱이나 웹브라우저 혹은 외부 애플리케이션 서버 등 외부의 클라이언트가 API 게이트웨이를 통해서 내부망에 있는 api를 그대로 호출한다.이때, API Gateway가 인증을 하고 클라이언트가 호출 가능한 api들만 공개함으로써 안전하게 내부 서비스를 보호해주게 된다고 생각할것이다.외부 개발자는 내부망의 모든 api 호출 엔드포인트를 알 필요가 없기 때문에 API 게이트웨이 엔드포인트 하나만 확인하고 편하게 호출할 수 있는 좋은 패턴이라고 생각될 것이다. 위의 구조는 API Gateway의 프레임워크를 통해 설정 정보만으로도 구성이 가능하다.안타깝게도 이것은 API Gateway 패턴이 전혀 아니다.그리고 계속 생각할것이다라고 말한 이유는 사실은 그렇지 못하기 때문이다. Microservice.io 혹은 마이크로서비스 패턴 책에 나오는 API Gateway 패턴의 정의는 위의 도식과 같다.이 API Gateway 패턴은 클라이언트의 특정 비즈니스 요청을 받아주는 서버 애플리케이션을 개발자가 직접 작성하고 내부 api들을 조합해서 호출한 뒤 응답을 클라이언트에게 필요한 것만 정리해서 내보내는 방식이다. 앞의 것은 설정만으로도 가능하고, 뒤의 실제 API Gateway 패턴은 개발자가 명백하게 코드로 작성해야만 하는 것이다.코드 작성은 쉽다. 자바 개발자라면 흔한 웹프레임워크인 스프링 웹 MVC나 Webflux를 사용하면 된다.다만 리액티브한 웹플럭스를 권한다. 1차 결론 외부에 API를 제공하기 위한 API Gateway Pattern은 Spring Cloud Gateway나 Netflix Zuul 같은 API Gateway Framework를 사용하는 것과는 무관 API Gateway Pattern은 클라이언트의 요청을 받아서 내부 마이크로서비스를 호출하는 로직을 직접 작성하고 응답 내용도 취사 선택하여 필요한 것만 내보내느 것 인증과 권한 기능을 붙였는지 여부와 무관하게 외부에 API를 제공해줄 용도로 API Gateway Framework를 사용한 요청/응답 Routing은 하면 안됨 특히 규모가 작다면 실제 MSA를 하면서 API Gateway Framework를 사용할 일은 사실 많지 않음 API Gateway FrameworkSpring Cloud Gateway, Netflix Zuul, AWS API Gateway 등을 이 글에서 부르는 용어 Gateway Routing PatternMS에서 정의한 API Gateway Framework를 통한 단순 요청/응답 라우팅에 관한 패턴 BFF - Backend For FrontendAPI Gateway Pattern과 유사하게 직접 구현하는 로직을 작성하는 방법과 API Gateway 애플리케이션을 클라이언트 기기 단위로 구분하고 소유권을 정하는 방식 조합 API Gateway 라는 단어의 의미는 현재 매우 심각하게 오염되어 있다. 이 단어를 듣는 순간 대부분 사람들이 떠올리는 그림은 위 두 그림 중 하나일 텐데 아마도 대부분 사람들은 첫 번째 보여 드린 요청응답을 단순 라우팅하는 그림을 떠올렸을 것이다. 바로 이 점 때문에 의사소통과 실제 구현에서 다양한 문제들이 발생하고 있다. 그래서 필자는 우리가 보통 API 게이트웨이라고 부르는 것들 즉 스프링클라우드 게이트의 넷플릭스 Zuul과 같은 것을 지금부터 API Gateway Framework 라고 부르겠다. 그리고 마이크로소프트 홈페이지에서는 이런 API Gateway Framework를 통한 단순 요청응답으로 구성하는 것을 게이트웨이 라우팅 패턴이라고 불렀다. 그래서 필자도 앞으로 Gateway Routing Pattern이라는 용어로 API Gateway Framework를 사용하는 단순 라우팅을 지칭하도록 하겠다. 왜냐하면 최근 API Gateway Framework는 단순 라우팅뿐만 아니라 코딩을 통해 API Gateway Pattern에서 정의한대로의 역할도 수행할 수 있기 때문이다. API Gateway Framework로도 API Gateway Pattern의 구현이 가능하지만, 필자는 권하지 않는다. 참고로 API Gateway Pattern이 API Gateway라는 용어의 경우 일부 다른 사람들은 엣지 마이크로 서비스 클라이언트 어댑터 등의 다른 용어로 부르는 것도 많이 보인다. 최근에는 BFF라는 용어도 대세가 되어가는 것 같다. MSA 자체의 역사가 길지 않다보니 많은 것들이 정리되지 못하고 다양한 오해를 일으키고 있어 보인다. 사실상 필자가 하고 싶은 말의 핵심은 여기서 끝이다. 위 두 글미을 보는 순간 딱 하고 그래 그렇지 않고 뭔가 떠오른 사람 혹은 이미 알고 있는 사람이라면 더 이상 이 글을 보지 않아도 좋을거 같다. 하지만 어쨋든 왜 API Gateway Pattern이라는게 어째서 단순히 API Gateway Framework를 통한 단순 라우팅을 하면 안되는 것인지 혹은 왜 비록 올바른 API Gateway의 구현이 가능하다 하더라도 필자가 API Gateway Framework의 사용을 권장하지 않는지 살펴보도록 하겠다. Monolithic Architecture : 기본으로 돌아가 모놀리식 아키텍처를 살펴보자. 일반적인 스프링 웹 MVC 기반 Monolithic 애플리케이션은 이 그림과 비슷한 형태를 띤다.애플리케이션은 크게 두 가지 계층으로 나눈다. 하나는 실제 비즈니스 로직을 처리하는 비즈니스 계층이다. 여기서 저장소에 접근하고 도메인 로직을 수행한다. 서비스 계층이라고 부르기도 한다. 그리고 프레젠테이션 계층이 외부 클라이언트의 요청을 받아서 비즈니스 계층을 호출하고 그 결과를 클라이언트가 사용하기 적합하게 변환해서 응답하게 된다. 프레젠테이션 계층에는 필터, 인터셉터가 있다. 비즈니스 계층에는 AOP가 있다. 이 둘은 역할이 비슷하다. 필터 혹은 인터셉터는 여러 컨트롤러의 공통 적용된 로직을 실행하고 컨트롤러 코드를 호출한다. AOP도 마찬가지인데 AOP는 보통 서비스나 그 외의 비즈니스 로직이 공통으로 적용할 로직을 수행한다. 둘이 하는 일을 횡단 관심사 처리라고 한다. 영어로는 Cross Cutting Concerns라고 한다. 프레젠테이션 계층에는 Facade라는 것도 보이는데 클라이언트의 요청이 하나의 비즈니스 로직이 아니라 여러 비즈니스 로직의 조합일 경우 작성한다. 이 Facade라는 용어를 모르고있더라도 혹은 별도의 클래스를 분리하지 않고 컨트롤러에 직접 Facade 코드를 작성하더라도 어찌됐든 여러분은 자기도 모르는 사이에 Facade를 작성하고 있을 것이다.계층형 아키텍처 혹은 계층형이 아니더라도 소프트웨어 개발에서 가장 기본 중 하나는 하위 모듈이 상위 모듈을 호출할 수 없고 하위 계층이 상위 계층을 호출할 수 없다는 것이다. 여기서 하위 계층은 비즈니스 계층이고 상위 계층은 프레젠테이션 게층이다. 따라서 비즈니스 계층은 절대로 프레젠테이션 계층을 호출하면 안된다. 쉽게 말해 서비스 클래스는 컨트롤러 클래스를 호출해서는 안된다. MSA를 이런 계층형 아키텍처로 매핑하면 어떤 모양이 될까? 우리가 보통 마이크로서비스라고 부르는 그것들은 비즈니스 계층에 속한다. 그래서 마이크로 서비스 아키텍처라고 부르는 것 같다. 그리고 API Gateway Framework가 아닌 API Gateway는 프레젠테이션 계층에 속하게 된다. 몇몇 분들은 아닌데 내가 만든 마이크로서비스는 프레젠테이션 계층인데라고 하는 사람이 있을 수 있는데 이것은 조금 이따가 알아보도록 하곘다. 혹시 이 그림을 보는 순간 어째서 API Gateway Framework로 라우팅하는 것이 문제가 되는지 곧바로 파악이 되는가?여기서 API Gateway를 API Gateway Framework를 통한 라우팅 기반으로 변경하면 어떤 형태가 되는 걸까? 바로 이렇게 컨트롤러 코드와 Facade 코드가 존재하지 않고 횡단 관심사만 처리하는 필터/인터셉터만 남은 프레젠테이션 계층이 되어버리는 것이다. 컨트롤러와 Facade 코드가 존재하지 않는 프레젠테이션 계층은 아주 심각한 문제들을 일으킨다. 2차 결론 API Gateway Framework를 통해 단순 Gateway Routing Pattern으로 내부 서비스 API를 외부로 노출시키는 경우에는 필터/인터셉터 역할만 하기 때문에 컨트롤러/Facade가 존재하지 않는 상태가 되어 많은 문제를 일으킴 프레젠테이션 계층 구성은 꼭 컨트롤러와 퍼사드 코드를 직접 개발자가 작성하는 API Gateway Pattern을 따라야 함. 다시 말하면 비즈니스 계층 마이크로서비스 API들을 프레젠테이션 계층으로 변환하는 용도로 API Gateway Framework를 사용해서는 안된다. 반대로 Filter/Interceptor/AOP 같은 횡단 관심사를 처리하는 데는 API Gateway Framework를 사용할 수 있다는 의미 - 추천하지는 않음 필자는 사실 API Gateway Framework를 사용할 일은 정말 드물다고 생각한다. 그것이 비록 횡단 관심사 처리 일지라도 말이다.사실상 이글의 핵심은 여기서 다 나왔다고 봐도 된다. 그래도 도대체 왜 컨트롤러와 퍼사드 코드를 직접 작성하지 않으면 문제가 되는지 어째서 횡단 관심사 처리에도 API Gateway Framework를 사용하지 말라는 것인지 궁금할 것이다. API Gateway Framework Gateway Routing 방식의 문제점 보안 취약성 Client 개발자 혹은 특정 Micro Service에게 Business 로직 전가 Business 계층과 Client 간의 강결합과 그로인한 보안 취약성 성능 저하 내부 Service들 간의 protocol 자유도 하락 계층형 아키텍처의 일반적인 원칙 위반 사례 횡단 관심사 처리 관점에서도 사용하지 않아야 하는 이유 Single Point Of Failure 성능 저하 관리 부담 문서화 AGF문제점 : 보안 취약성 인증 : 사용자의 로그인 여부 - 누구세요? 권한 : 당신은 이것을 사용할 수 있고 저것은 사용할 수 없습니다. “이것” 이란? 특정 API 해당 API가 처리하는 객체 API Gateway Framework를 사용하면 보안이 좋아진다는 착각이 많이 있다. 하지만 사실이 아니다.인증을 한다는 것과 특정 API 호출 권한을 제어한다는 것인 보안을 강화하는 것은 아니다. 그것은 프레젠테이션 계층이 당연히 해야할 일이다. 인증은 사용자의 로그인 여부를 체크해서 누구인지 확인한다. 여기서 문제는 권한에 대해 많이들 오해한다는 점이다.권한에는 특정 api를 호출해도 된다라는게 있다. 그리고 그 특정 api가 처리하는 객체에 대한 접근 가능 여부도 권한에 들어간다.Gateway Routing 방식으로 사용하면 위의 빨간색으로 된 객체 관련 권한 처리를 누락하게 된다. GET /users/{userId} : 허용 PUT /users/{userId} : 허용 POST /users : 금지 Login 사용자 1번이라면 GET /users/1 : 허용 PUT /users/1 : 허용 POST /users : 금지 GET /users/2 : 허용 GET /users/3 : 허용 GET /users/... : 허용 PUT /users/2 : 허용 PUT /users/3 : 허용 PUT /users/... : 허용 단순한 예로 로그인 사용자에게 API Gateway Framework에서 GET /users/id와 PUT은 허용하고 POST는 금지했다고 해보자.보안이 강화된 것처럼 보인다. 하지만 로그인 사용자 1번은 해당 api에서 1번 사용자의 개체만을 다루어야 한다. 그런데 2번 3번 객체를 다루는 것은 막지 못하고 있다.권한을 지정할 때 http 메소드와 url 패턴이 일치하면 허용했지 패턴에 들어가 있는 값에 대해서는 확인하지 않았기 때문이다. 다른사용자의 데이터를 조회하는 것뿐만 아니라 마음대로 수정까지 가능하다. 심각한 보안 위협이다. 이 문제를 해결하는 방법은 직접 컨트롤러 코드를 작성하는 수밖에 없다.지금 예제는 간단하다. 로그인 유저 아이디는 세션 역할의 쿠키에 저장되어 있고 이를 프레임워크가 알고 있으니까 왠지 단순하게 유저 id값과 로그인 사용자의 유저 아이디를 비교해서 호출하지 못하게 필터를 구성하면 될 것 같다. 하지만 문제가 해결된 것은 아니다. 오더나 리뷰처럼 이와 같이 오더 id와의 관계를 알 수 없는 수많은 api 호출들이 존재하기 때문이다. 이런 경우에도 로그인 사용자가 명백히 소유한 주문이 아니면 조회나 수정이 불가능해야 한다. 그러면 아래처럼 모든 api 호출에 대해 소유 관계를 넣으면 일부 문제는 해소해준다. 특정 프레젠테이션 계층에서는 이렇게 하면 성능도 좋아진다.하지만 사실 아주 많은 경우에 저렇게 소유 관계만으로 객체를 조회할 수 없다. 멀리 갈 것도 없이 오더 마이크로 서비스 입장에서는 본인들이 가지고 있지도 않은 유저에 대한 정보를 API url에 넣는 것 자체가 부담일 것이고 일반적인 비즈니스 로직에서는 오더 아이디만으로 조회해야 하는 경우가 태반일텐데 그렇제 못해서 오는 불편도 상당하다. 그리고 그런 것을 넘어서 가장 중요하게 이 방식이 안 되는 이유가 있다. 많은 착각 중에 하나가 프레젠테이션 계층이 하나일 거라는 것이다. 대부분의 엔터프라이즈 애플리케이션은 프레젠테이션 계층이 여러 개이다. 요즘 많이 회자되고 있는 어떤 아키텍처가 떠오를 것이다. Admin은 모든 사용자의 주문 정보에 접근이 가능해야 한다. 가가 업주는 본인의 가게에서 일어난 주문에 대해서는 그 사용자가 누구이든 조회할 수 있어야 한다. 필자는 Batch Job이나 이벤트 컨슈머도 프레젠테이션 계층으로 보는데 이것들은 시스템으로서 무제한의 데이터 접근권을 가진 프레젠테이션 계층에 속한다. 또한 타사의 서버 애플리케이션 호출 지점도 프레젠테이션 계층이다. 이 모든 경우를 미세하게 API Gateway Framework로 제어하는 것은 필자가 보기엔 불가능하다. 따라서 비즈니스 계층은 프레젠테이션 계층의 접근자가 누구인지 구분하지 않고 요청을 받아주고 프레젠테이션 계층에서 각자 코딩을 통해 권한을 미세하게 조정해야 한다. 아마도 비즈니스 계층 자체에 저 모든 처리르 넣고 API Gateway Framework를 붙이려는 욕구가 생길 수 있는데 비즈니스 계층 서비스가 100개이고 처음에는 두 개 정도의 프리젠테이션 계층으로 시작했는데 나중에 두 개의 또 다른 프리젠테이션 계층이 더 추가됐다고 상상해 보길 바란다… 당장 100개는 아니더라도 수많은 마이크로서비스에 자기들은 관심도 없는 프리젠테이션 계층의 권한 로직을 추가해야 하는 부담을 지게된다. 과연 또 다른 프리젠테이션 계층이 추가될 것 같은가? 지금 당장 생각나는 것은 배달원 프리젠테이션 계층 고객센터 직원 프리젠테이션 계층 고객사 직원 계층이 생각난다. 또 얼마나 많이 추가될지 상상도 안된다. 애초에 저런 권한 로직을 비즈니스 계층에 넣는 것 자체가 비즈니스 계층과 프리젠테이션 계층을 모호하게 만드는 잘못된 행동으로 보인다. AGF문제점 : 보안 취약성 IDOR IDOR : Insecure Direct Object Reference 그 중에서도 “수평적 권한상승” 개발자가 꼭 알아야 할 애플리케이션 보안 : 입문부터 놓치면 안될 트렌드까지?! (8월 우아한테크세미나 - 권현준) 수평적 권한상승쉽게 말해, 동일한 권한을 가진 다른 사용자의 객체에 접근할 수 있을때를 수평적 권한상승이라고 함 수직적 권한상승본인이 지닌 권한을 넘어서는 기능을 수행할 수 있을때를 수직적 권한상승이라 함 필자는 이 취약점이 최근에 급부상한 이유가 마이크로 서비스 아키텍처가 급부상하면서 함께 증가한 API Gateway Framework 사용 때문이라고 추측하고 있다.방금은 수평적 권한 획득의 예였고 잘못 설계된 마이크로 서비스 아키텍처에는 수직적 권한 상승 문제도 마찬가지로 발생한다. 그 예는 추후에 포스팅 하도록 하겠다. AGF 문제점 : 잘못된 로직 구현 전가정말로 어떻게 해서 절대로 보안 문제가 없는 경우라고 가정해도 단순 라우팅을 하게되면 Facade가 필요한 경우에 엉뚱한 곳에 로직을 전가하게 된다. 클라이언트(Mobile App, Web Browser)가 직접 스스로 비즈니스 계층의 로직을 다양하게 조합해서 호출하게 됨 혹은 반대로 Order Service가 엉뚱하게 Product Service를 호출해서 클라이언트에 필요한 정보를 조합해 내려주는 것처럼 본인의 비즈니스가 아닌 것에 대해 구현하고 의존하게 됨 이게 뭐가 문제일까? AFG 문제점 : Client에 구현 전가 일단 클라이언트 개발자가 UI 처리에 집중하지 못하게 됨 비즈니스 로직은 서버개발자와 클라이언트 개발자 둘 다 밀도 높게 파악해야 함 비즈니스 로직 하나하나는 무슨 수르 ㄹ써서 든 보안 요구를 충족할지라도 그것들을 자유롭게 조합할 권한을 해커에게 내준 것이라서 그 조합이 어떤 문제를 일으킬지 확신할 수 없음 버그가 존재할 경우 Mobile App은 각종 앱스토어 인증 절차에 걸리는 시간과 사용자들이 자발적으로 앱을 업데이트 할 때 까지의 시간동안 버그 해결이 불가능해짐 클라이언트 개발자에게 전가하기가 불가능 경우가 있거나 혹은 위에서 말한 문제들 떄문에 이 Facade 구현을 서버측에 전가하면 퍼사드의 역할을 결국은 누가 됐든 특정 마이크로서비스에서 처리해야한다. 그때도 문제가 심각해진다. AFG 문제점 : 각 마이크로서비스에 전가 특정 서비스 개발자들은 알지도 못하는 로직 구현 책임을 맡게되고 해당 서비스는 본 서비스가 아닌 다른 역할을 맡음으로써 불필요하게 복잡도 증가와 타 서비스들과의 결합도 증가 본 역할도 아닌 타 API Network 호출 증가로 인해 정작 본 서비스의 처리에 장애 유발 요인 증가 특정 프리젠테이션 계층 API Gateway에서 문제가 생겼다면 해당 API Gateway만 문제가 되지만 비즈니스 계층 Microservice에서 장애가 발생하면 이를 호출하는 다른 모든 계층에서 문제가 될 수 있음 - 장애 여파 훨씬 커지게 됨 이런식으로 전혀 엉뚱한 역할을 전가받은 서비스의 개발자들의 시선은 복잡해 개발만족도는 갈수록 떨어지게 된다. 그리고 애초에 마이크로 서비스로 나눈 이유 자체에도 반하게 되는 결정이 된다. 마이크로 서비스 패턴 책을 보면 API Gateway Pattern을 구현하는 방법을 스프링 웹 MVC, Webflux처럼 직접 웹플럭스처럼 직접 웹프레임워크로 개발하기와 스프링클라우드 게이트웨이처럼 라우팅과 직접 구현을 동시에 지원하는 API Gateway Framework를 사용하는 방법을 추천하고 있다. 실제 예제로도 스프링클라우드 게이트웨이를 사용한다. 즉 일부는 단순 라우팅, 나머지는 직접 구현하는 것을 섞어서 하고 있다. 하지만 필자는 앞서 API Gateway Framework를 통해 비록 API Gateway Pattern이 구현가능하고 일부 단순한 api는 간편하게 라우팅 처리를 할 수 있다 하더라도 그렇게 해서는 안된다고 말했다.이제 그 이유에 대해 다뤄보겠다. AFG 문제점 : Client / Business 계층간 강결합이제부터 Business 계층 개발자는 항상 Client의 변경사항을 주시해야 한다. 1234567{ &quot;userId&quot; : 1, &quot;name&quot; : &quot;강백호&quot;, &quot;address&quot; : &quot;서울시 동작구 ...&quot;, &quot;phone&quot; : &quot;010-1111-2222&quot;, &quot;ssn&quot; : &quot;221212-1234567&quot;} 배달원 앱용 Presentation AFG에서 User Microservice의 /users/{userId} API를 단순 라우팅 했고 그 응답 객체도 단순 반환했는데 나중에 User Microservice의 개발자가 별 생각없이 /users/{userId}의 응답에 고객의 전화번호와 주민번호를 추가한다면? -&gt; 배달원 앱을 통해 고객의 전화번호와 주민등록번호가 그대로 노출 이를 막으려면 User Microservice 측에서는 모든 Presentation 계층별로 서로 다른 사용자 정보 조회 API를 만드는 방법 밖에 없게 됨 Business 계층이 Presentation 계층에 의해서 계속 변화하게 됨 단순 요청/응답 라우팅은 어떠한 경우에도 사용하지 말고 항상 응답 내용을 프리젠테이션 게층에서 Client에 필요한 것만 정제해서 내보내기 배달앱에서는 어차피 새로 추가된 데이터를 보여주지 않을테니까 괜찮다는 식의 안심은 금물인 것은 당연히 알 것이다. 보다시피 게이트웨이 라우팅 패턴이 스며드는 순간 비즈니스 계층은 프리젠테이션 계층의 속박에서 벗어날 길이 없게 된다. AGF 문제점 : client 성능 저하Client는 느리다 여러 api 호출을 조합해야 하므로 network 호출이 여러번 일어나게 되고 api 호출 결과가 불필요한 데이터까지 포함하여 과하게 크거나 필요한 데이터가 없어서 추가 api 호출을 하게 됨 API Gateway Pattern을 Non Blocking IO / Reactive 하게 구현하여 성능 향상 추천 (Spring WebFlux 등) 이 그림은 넷플릭스 블로그에 나온 API 게이트웨이 예이다. 여기서 넷플릭스 api가 API GW라고 보면 되고 API GW의 서버가 다른 비즈니스 계층 서비스를 패러럴하게 호출하고 있다. 클라이언트는 단 한 번만 호출하고 있다. GraphQL로 이 문제를 해소 가능하지만, 비즈니스 계층 Micfoservice에 GraphQL을 구축하고 이를 그대로 프리젠테이션 계층까지 노출하면 앞서 말했던 보안 문제들이 그대로 발생함. GraphQL도 API Gateway Pattern에 따라 프리젠테이션 계층 서버 애플리케이션으로 적절한 보안 처리를 해서 별도 구축 AGF 문제점 : 내부 서비스의 프로토콜 제약 API Gateway Framework로 프리젠테이션 계층을 구현한다는 것은 곧 내부 비즈니스 계층 서비스가 모두 HTTP API(REST?)로 고정된다는 의미 내부 비즈니스 계층을 한 번 밖으로 노출하면 프로토콜 변경은 어려워짐 내부적으로 성능 향상과 다양한 요구 사항만족을 위해 gRPC, Message Queue, HTTP API, 기타 등등을 사용할 수 있는 자유도를 포기하지 말기 (출처 : https://microservices.io/patterns/apigateway.html) 계층형 아키텍처의 일반적인 원칙 무시 사례 계층형 아키텍처의 일반적인 원칙을 무시한 사례 중 마이크로 서비스를 나눌 때 비즈니스 계층으로 나눈게 아니라 애초에 프리젠테이션 계층으로 만들어 버리는 사례이다.예를 들어 주문 마이크로 서비스를 만드는데 애초에 앱클라이언트의 요청을 받게 만들어 버리는 것이다.인증도 붙이고 권한처리도 한다. 그리고 내부적으로 비즈니스 계층에서 비즈니스 로직을 처리하고 저장소 처리까지 합한다.사실 여기는까지는 괜찮다. 다만 많은 경우에 바로 이런 도식과 같은 형태를 띄게 버리는 것이다. 프리젠테이션 계층 api가 다른 프리젠테이션 계층 api를 호출하고 비즈니스 계층 api가 없다보니 프리젠테이션은 계층의 비즈니스 처리용 api를 만들어서 호출해버리는 것이다.이는 모놀리식 아키텍처에서 컨트롤러가 컨트롤러를 호출하고 서비스가 컨트롤러를 호출하는 형국이다.비즈니스 계층에 속하는 상품 서비스가 주문 프리젠테이션 계층을 호출하려면 필연적으로 주문 프리젠테이션 계층에서 인증을 풀어주거나 단순화한 형태의 api를 제공해줘야 한다.이는 IDOR의 수직적 권한 상승에 취약해지는 결과를 낳는다.Private 망의 상품 서비스가 Public 망의 api를 호출하려면 외부망으로 접속이 연결되면서 연결 지연이 발생한다.웹 방화벽에 의해 Private 망의 모든 호출이 단일 ip에서 오는 DDOS 공격으로 간주되어 차단될 수도 있다. 수직적 권한 상승에 취약해진다는것은 잘못하면 해커가 무제한의 권한을 가진 api를 알아낼 수 있다는 의미이다.계층형 아키텍처의 기본을 어김으로써 나타나는 문제들을 차치하고서라도 보안 문제와 네트워크 인프라적으로도 심각한 문제가 발생하는 것을 볼 수 있다. 하나의 메소드, 하나의 역할만 하듯 하나의 마이크로 서비스도 하나의 계층 역할만 해야한다.의존 관계는 항상 프리젠테이션 계층에서 비즈니스 계층으로 흐르거나 비즈니스 계층간에만 이루어져야 한다.비즈니스 계층간의 호출에 있어서도 Circular 호출이 일어나서는 안 됨 (A -&gt; B -&gt; A) 마이크로 서비스로 분할된 애플리케이션들간의 호출에 있어서도 우리가 보편적으로 코드를 짤 때 지켜야 하는 규칙을 철저히 지켜야한다. AGF를 횡단 관심사에서도 사용하지 말기진짜 진짜 필요한게 아니라면… Single Point Of Failure(단일 장애 지점)가 된다. 불필요하게 서버 관리 부담이 늘어나고 비용도 증가 Client가 하나의 end point만 바라본다고 해서 좋아질 것은 크게 없음 API Gateway pattern을 구현하게 되면 실제 client 개발자가 바라보는 API endpoint 개수는 현저하게 줄어들게 됨. ktNaviService.메소드수천개() : 정말 좋은가? 진짜 중요한 것은 필요한 API를 빠르게 찾아볼 수 있게 문서들을 잘 모아두기 API Gateway Framework를 사용할 만한 경우사실 대부분의 규모의 서비스에서 API Gateway Framework가 별로 필요 없다. 출처 : Announcing Zuul: Edge Service in the Cloud 이 그림은 넷플릭스가 2014년 Zuul API Gateway Framework를 발표한 내용을 가져온것이다.넷플릭스는 여기서 프리젠테이션 계층 API Gateway에 해당하는 부분을 클라이언트 어댑터라고 부르고 있으며 코드를 직접 짜서 구현하고 있다.넷플릭스는 Zuul을 비즈니스 계층 api를 프리젠테이션 계층으로 노출하는 용도로 사용한 것이 아니라 명백하게 각각의 프리젠테이션 계층에 대해서 횡단 관심사 처리를 위해 사용한 것이다.여기서 다양한 횡단 관심사를 보여주는데 흔히 생각하듯 인증 정적응답 처리 Canary testing, Stress testing, 동적 라우티을 위해서 사용하고 있다.만들 Micro Service에서 프리젠테이션 계층이 저정도 용도가 필요하다면 사용해도 좋을 것 같다. 참고로 넷플릭스는 위의 도식에 해당하는 아키텍처를 버린듯한 내용의 포스팅이 최근에 올라와있다.전반적으로 프리젠테이션 계층을 GraphQL로 전환하고 있는 것으로 보인다.자세한 것은 아래 참조 문서를 참고바란다. How Netflix Scales its API with GraphQL Federation (Part 1) 비즈니스 계층 횡단 관심사 처리 위 그림은 비즈니스 계층에 속하는 마이크로 서비스 API 서버에 API Gateway Framework를 적용한 경우로 이를 호출하는 프리젠테이션 계층 API Gateway들이 꼭 필요로 하는 API들만을 호출 가능하게 제약하고, 호출에 대한 로그를 남기는 API Gateway Framework를 두었다. 사실 실제로 이렇게까지 사용하는 경우는 드물다. 이것을 다시 모놀리식으로 비유하자면 서비스 코드에다가 AOP로, 어느 컨트롤러는 이 서비스 클래스의 메소드를 호출할 수 있고 어느 클래스는 호출 못하고 이런 것을 설정을 안할 것이다. 그래서 이런 API Gateway Framework도 별로 필요하지 않다고 말하는 것이다. 하지만 그래도 특정 프리젠테이션 계층 API Gateway에 대해 늘 철저히 보안 처리를 하고 싶다면 사용해도 좋을 것 같다. 여기서도 보다시피 절대로 API Gateay Framework는 비즈니스 계층을 프리젠테이션 계층으로 격상하는 용도로 사용한 것이 아니다. AOP처럼 횡단 관심사 처리를 위해 사용되었을 뿐이다. 여기서 내가 생각하거나 혹은 이미 마이크로서비스 패턴 책 등에 나와 있는 API Gateway Framework의 문제점들을 다양하게 제시했다. 마무리로 첨언하자면 여기서 다룬 모든 문제들이 모두 해결 가능하거나, 그런 문제들에도 불구하고 이를 사용해서 얻는 이득이 훨씬 큰 경우가 있다면 당연히 API Gateway Framework를 사용해도 괜찮을 것이다.","link":"/2024/04/14/API-Gateway/"},{"title":"AWS S3 버킷과 EC2 Instance 연결","text":"프로젝트 배포에 사용한 ec2와 S3을 연동해 보았다. https://celdan.tistory.com/40 추가로 ec2-s3 역할에AmazonEC2FullAccess, AmazonSSMManagedInstanceCore, CloudWatchAgentServerPolicy이 세가지 정책도 추가하였다. 12$ aws s3 ls aipin-bucket2024-06-21 15:24:24 1185 pgvector.zip 이런식으로 s3에 업로드한 파일이 보인다. 12$ aws s3 cp s3://aipin-bucket/pgvector.zip /root$ unzip /root/pgvector.zip -d /root","link":"/2024/06/23/AWS-Connect-EC2-S3/"},{"title":"AWS EKS에 FastAPI 프로젝트 배포하기","text":"AWS EKS에 프로젝트를 배포하려면 Docker 이미지를 만들어야 한다. 이를 위해 Dockerfile을 작성하고, Docker 이미지를 빌드 및 푸시한 후, Kubernetes 매니페스트 파일을 사용해 EKS에 배포하는 과정을 따르게 된다. 단계1 : Dockerfile 작성1234567891011121314151617181920212223242526# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# Poetry 설치RUN pip install poetry# 로컬 패키지 설치 (project-name_beta_~.whl 위치를 /app/library로 복사)COPY ../aipin_library/aipin_beta_~.whl /app/library/RUN poetry add /app/library/aipin_beta_~.whl# 프로젝트 파일 복사COPY . .# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 단계 2: Docker 이미지 빌드위의 Dockerfile을 프로젝트 루트 디렉토리에 저장한 후, Docker 이미지를 빌드합니다. 터미널에서 다음 명령어를 실행한다. 1docker build -t my-project:latest . 하지만 에러가 난다. 12345678910111213141516171819202122% docker build -t aipin-orchestrator:latest .[+] Building 1.9s (9/12) =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 765B 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/python:3.9-slim 1.8s =&gt; CANCELED [1/8] FROM docker.io/library/python:3.9-slim@sha256:451832461e0c1587078511b0a6ad35c3c0a0106 0.0s =&gt; =&gt; resolve docker.io/library/python:3.9-slim@sha256:451832461e0c1587078511b0a6ad35c3c0a010656b660f4f 0.0s =&gt; =&gt; sha256:ac14bdcdeeab6f08dc915a5c5971f998797127c47e2e860ace34090b3886746e 1.94kB / 1.94kB 0.0s =&gt; =&gt; sha256:b4045d7da52ebacb256e6843ae8b6eaedca3fa0486f343d4916e52e8e7320cab 6.88kB / 6.88kB 0.0s =&gt; =&gt; sha256:451832461e0c1587078511b0a6ad35c3c0a010656b660f4f26ebfac0e723f7bf 10.41kB / 10.41kB 0.0s =&gt; [internal] load build context 0.0s =&gt; =&gt; transferring context: 1.59kB 0.0s =&gt; CACHED [2/8] WORKDIR /app 0.0s =&gt; CACHED [3/8] RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends build-essential 0.0s =&gt; CACHED [4/8] RUN pip install poetry 0.0s =&gt; ERROR [5/8] COPY ../aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/ 0.0s------ &gt; [5/8] COPY ../aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/:------failed to compute cache key: &quot;/aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl&quot; not found: not found 이 상황에서 Docker가 컨텍스트 외부 파일을 찾지 못하는 문제는 Docker의 기본 동작과 관련이 있다. Docker는 빌드 컨텍스트 외부에 있는 파일을 복사할 수 없다. 이를 해결하기 위해 두 가지 접근 방식을 사용할 수 있다 Docker 빌드 컨텍스트를 변경하여 상위 디렉토리를 포함 빌드 컨텍스트 내에 필요한 파일을 복사 필자는 두 번째 방법을 사용하여 Docker 빌드가 성공하도록 한다. 하지만 이번에는 아래와 같은 에러가 난다. 12 &gt; [4/8] RUN pip install poetry:#7 1.579 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1133)'))': /simple/poetry/ 이 문제는 SSL 인증서 문제로 인해 pip이 패키지를 설치하지 못하는 경우이다. Docker 이미지 안에서 SSL 인증서를 제대로 인식하지 못하는 경우가 종종 있다. PIP 환경변수 설정인증서 검사를 비활성화하도록 환경 변수를 설정할 수 있다. 이는 보안상 좋은 방법은 아니지만, 일시적인 해결책이 될 수 있다. 123456789101112131415161718192021222324252627282930# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 프로젝트 파일 복사COPY . .# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 하지만 아래와 같은 에러가난다. 1234567 ERROR [6/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl 0.6s ------ &gt; [6/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl:#10 0.507 #10 0.507 Poetry could not find a pyproject.toml file in /app or its parents------executor failed running [/bin/sh -c poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl]: exit code: poetry는 pyproject.toml 파일이 있는 디렉토리에서 실행되어야 한다. Dockerfile에서 poetry를 실행하기 전에 pyproject.toml 파일이 있는 디렉토리로 이동해야 한다. 현재 Dockerfile의 구조를 보면, pyproject.toml 파일은 /app 디렉토리로 복사되기 전에 poetry add 명령이 실행되고 있는 것 같다. 이를 해결하려면 다음과 같이 수정할 수 있다. 1234567891011121314151617181920212223242526272829303132# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 프로젝트 파일 복사 (pyproject.toml 파일 포함)COPY . .# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/# Poetry를 사용하여 로컬 패키지 추가RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 이렇게 바꿔서 실행하면… 이번엔 이런 에러가 나타난다. 12345678910 ERROR [7/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl 0.7s------ &gt; [7/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl:#11 0.623 Path /aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl for aipin-beta-2024 does not exist#11 0.644 The currently activated Python version 3.9.19 is not supported by the project (^3.10).#11 0.644 Trying to find and use a compatible version. #11 0.677 #11 0.677 Poetry was unable to find a compatible version. If you have one, you can explicitly use it via the &quot;env use&quot; command.------executor failed running [/bin/sh -c poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl]: exit code: 1 이 에러는 두 가지 문제를 나타낸다: pyproject.toml 파일에서 Python 버전이 ^3.10로 설정되어 있고, 현재 Docker 이미지에서 사용 중인 Python 버전은 3.9.19이다. Poetry가 해당 .whl 파일을 찾지 못하고 있다. 이 문제들을 해결하기 위해 다음과 같이 진행한다 Python 버전 변경: Dockerfile에서 Python 3.10 이미지를 사용하도록 변경한다. Poetry 환경 설정: Poetry가 Python 버전을 강제로 사용하도록 설정합니다. 파일 경로 문제 해결: COPY 명령어의 경로를 올바르게 설정합니다. 1234567891011121314151617181920212223242526272829303132333435# 베이스 이미지로 Python 3.10 사용FROM python:3.10-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 프로젝트 파일 복사 (pyproject.toml 파일 포함)COPY . .# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/# Poetry 환경 설정: Python 3.10 사용 강제RUN poetry env use python3.10# Poetry를 사용하여 로컬 패키지 추가RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 이렇게 수정하여 이미지를 빌드하였고, 이미지가 잘 만들어졌다. ECR애 Docker 이미지 푸시단계1:AWS CLI 설정먼저 AWS CLI가 설정되어 있어야 한다. 설정되지 않았다면, 다음 명령을 사용하여 AWS CLI를 설정한다. 단계2:ECR 리포지토리 생성단계3: 도커 로그인ECR에 로그인해야 한다. 다음 명령을 사용하여 로그인힌다. 123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 단계4: Docker 이미지 푸시12345# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latest# Docker 이미지 푸시docker push &lt;your-dockerhub-username&gt;/my-project:latest Cloud9 구성하기1234$ curl --silent --location &quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz&quot; --output eksctl.tar.gz$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.22.6/bin/linux/amd64/kubectl$ curl -LO https://github.com/derailed/k9s/releases/download/v0.26.7/k9s_Linux_x86_64.tar.gz$ curl -sSL -o ~/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 위 패키지들 S3에 업로드 후 Cloud9 으로 옮기기 12$ aws s3 cp s3://aipin-bucket/eksctl_Linux_amd64.tar.gz .... 나머지도 비슷하게 옮기기 1234567891011$ tar -zxvf eksctl_Linux_amd64.tar.gz$ sudo chmod +x eksctl$ sudo mv eksctl /usr/local/bin/$ tar -zxvf k9s_Linux_x86_64.tar.gz$ sudo chmod +x k9s$ sudo mv k9s /usr/local/bin/$ chmod +x kubectl$ sudo mv kubectl /usr/local/bin/kubectl$ aws eks update-kubeconfig --name [클러스터 명 예 : eks-prod-ct01-tap-01] --region [region명 예 : ap-northeast-2]$ sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd$ rm argocd-linux-amd64 Cloud9 -&gt; EKS 접근 시 Token invalid 에러12$ aws eks update-kubeconfig --region [region명 예 : ap-northeast-2] --name [클러스터 명 예 : eks-prod-ct01-tap-01]An error occurred (UnrecognizedClientException) when calling the DescribeCluster operation: The security token included in the request is invalid 해결책Cloud9 우측 최상단의 톱니바퀴 클릭 &gt; AWS Settings &gt; Credential 부분을 X로 변경 &gt; 터미널 종료 후 다시 시도 namespace 생성12$ kubectl create ns aipinnamespace/aipin created Kubernetes 매니페스트 파일 작성이제 EKS에 배포하기 위한 Kubernetes 매니페스트 파일을 작성한다. deployment.yaml과 service.yaml 파일을 준비한다. AWS의 Elastic Container Registry에서 이미지 탭에 들어가 배포할 이미지 목록 리스트를 선택 후 URL 복사를 눌러 이미지 태그를 복사한다. ex) 058264433760.dkr.ecr.ap-northeast-2.amazonaws.com/aipin-orchestrator:latest Cloud9로 돌아가서 생성한 유저폴더로 이동하여 Deployment.yaml과 Service.yaml 파일을 생성한다. nano 명령어를 이용하여 deployment.yaml 파일을 생성한다. 1vim aipin-orchestrator.deployment.yaml 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name: aipin labels: app: aipin-orchestrator namespace: aipinspec: replicas: 3 selector: matchLabels: app: aipin-orchestrator template: metadata: labels: app: aipin-orchestrator spec: containers: - image: 058264433760.dkr.ecr.ap-northeast-2.amazonaws.com/aipin-orchestrator:latest imagePullPolicy: Always name: aipin-orchestrator ports: - containerPort: 8080 protocol: TCP 동일하게 service.yaml 파일을 생성한다. 1$ vim aipin-service.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: aipin-orchestrator annotations: alb.ingress.kubernetes.io/healthcheck-path: &quot;/healthy&quot; namespace: orchestratorspec: selector: app: aipin-orchestrator type: NodePort ports: - port: 80 protocol: TCP targetPort: 8080 파일 생성 후 apply 명령어를 이용해 pod를 생성하여 등록 및 배포를 진행한다. 1234$ kubectl apply -f aipin-orchestrator.deployment.yaml deployment.apps/aipin created$ kubectl apply -f aipin-orchestrator.service.yamlservice/aipin-orchestrator created 생성 후 이전에 만들어놓은 네임스페이스를 이용해 파드들을 조회해보겠다. 1kubectl get all -n aipin 배포 시 libpq 에러kubectl apply로 배포 시 아래와 같은 에러가 난다. 1234567891011121314151617181920212223242526272829303132333435363738394041Installing dependencies from lock fileNo dependencies to install or updateInstalling the current project: orchestrator (0.1.0)Traceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import controller File &quot;/app/src/controller.py&quot;, line 2, in &lt;module&gt; from orchestrator import orchestrator File &quot;/app/src/orchestrator.py&quot;, line 1, in &lt;module&gt; from aipin.Plugin.plugin_manager import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/__init__.py&quot;, line 1, in &lt;module&gt; from .orchestrator import Orchestrator File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/orchestrator.py&quot;, line 1, in &lt;module&gt; from aipin.Plugin import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/__init__.py&quot;, line 2, in &lt;module&gt; from .plugin_manager import PluginManager File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/plugin_manager.py&quot;, line 3, in &lt;module&gt; from aipin.Plugin.plugin import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/plugin.py&quot;, line 5, in &lt;module&gt; from aipin.data import ChatHistory, ChatRequest File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/data/__init__.py&quot;, line 3, in &lt;module&gt; from .vector_retriever import PGVectorRetriever, PGVectorConfig File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/data/vector_retriever.py&quot;, line 2, in &lt;module&gt; from langchain_postgres import PGVector File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain_postgres/__init__.py&quot;, line 3, in &lt;module&gt; from langchain_postgres.chat_message_histories import PostgresChatMessageHistory File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain_postgres/chat_message_histories.py&quot;, line 13, in &lt;module&gt; import psycopg File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/__init__.py&quot;, line 9, in &lt;module&gt; from . import pq # noqa: F401 import early to stabilize side effects File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/pq/__init__.py&quot;, line 118, in &lt;module&gt; import_from_libpq() File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/pq/__init__.py&quot;, line 110, in import_from_libpq raise ImportError(ImportError: no pq wrapper available.Attempts made:- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'- couldn't import psycopg 'python' implementation: libpq library not found 로그에 나타난 에러 메시지를 분석해 보면, 문제는 psycopg 패키지가 필요한 libpq 라이브러리를 찾을 수 없기 때문에 발생하는 것으로 보인다.이는 PostgreSQL과 상호작용하는 Python 라이브러리인 psycopg가 제대로 설치되지 않았거나, 필요한 시스템 종속성이 누락된 경우에 발생한다. 해결법기본 이미지에 libpq 설치 libpq 라이브러리가 누락된 경우, 이를 설치해 주어야 합니다. Dockerfile에 해당 라이브러리를 설치하는 명령을 추가한다. 1 포트포워딩 진행참고https://greenhead.blog/blog/kubernetes/2024-04-27/https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/setting-up.htmlhttps://velog.io/@judemin/EKS-CICD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B5%AC%EC%B6%95https://devnoong.tistory.com/entry/MiniKube-%EC%8B%A4%EC%8A%B5-Cloud9-ECR-%EC%97%85%EB%A1%9C%EB%93%9C-%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0#article-4--3--deployment-yaml--service-yaml-%ED%8C%8C%EC%9D%BC-%EC%83%9D%EC%84%B1-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-(%EC%83%88-%EC%9C%A0%EC%A0%80-%EA%B6%8C%ED%95%9C%EC%9C%BC%EB%A1%9C-%EC%A7%84%ED%96%89)","link":"/2024/07/03/AWS-Deploy-FastAPI-project-On-EKS/"},{"title":"AWS Landing Zone 구축하기","text":"“Landing Zone”은 비행기가 안전하게 착륙할 수 있는 공간을 의미한다.즉, AWS Landing Zone을 설계한다는 것은 AWS 상에서 시스테믈 안정적으로 구축하기 위한 기본 사항을 준비하고 설계, 구축하는 일련의 과정이다. Landing Zone 이란?Multi Account의 확장 가능하고 안전한 환경을 제공하는 AWS 솔루션AWS의 컨트롤 타워는 ‘랜딩존’이라는 다중 계정의 AWS 환경을 쉽게 설정하고 관리하는 환경을 제공한다. 클라우드 세상의 컨트롤 타워클라우드를 잘 사용하기 위해서는 고려해야 할 사항이 많다. 성능, 보안, 안정성 등은 기본이고 가격 경쟁력이나 지속 가능성까지도 고려해야한다.서비스의 확장과 함께 탄탄한 아키텍처 기반에 대한 고민을 해결하기 위해 랜딩존을 사용할 수 있다고 한다.즉, 클라우드 환경 또는 확장 가능하고 유연한 아키텍처 설계를 위한 시작점으로써 랜딩존을 사용가능하다고 한다.랜딩 존은 아래와 같은 기능을 제공한다. 다계정 아키텍처가 있는 AWS 환경 초기 보안 기준 ID 및 액세스 관리 지배구조 데이터 보안 네트워크 설계 로그","link":"/2024/04/03/AWS-LandingZone/"},{"title":"AWS API Gateway에 API키 등록하기","text":"API에 특정 API Key를 가지고 있는 요청만 접근 가능하도록, Lambda로 간단한 api 만들기에서 만든 REST API에 API 키릉 등록해보겠다. 메서드 요청 설정쪽에서 API 키가 필요함을 True로 바꿔준다. 요청 검사기도 넣어준다. 다음으로 API 키를 만들고, 사용량 계획과 연결해주어야한다. 먼저 API 키를 생성한다. 그리고 사용량 계획을 만든다. 만들면서 API를 배포했을때의 스테이지를 연결해준다. 키 값을 확인하고, postman으로 요청을 테스트해본다. 키 값을 적지 않고 그냥 요청했을 때는 forbidden이 뜨고, 값을 확인할 수 없다. header에 key : x-api-key, value : 키 값을 적어서 요청 보내면 정상적으로 응답이 나온다.","link":"/2024/08/08/AWS-Register-API-Key-in-API-Gateway/"},{"title":"폐쇄망 EC2에 Docker 설치하기","text":"docker를 사용하려면 기본적으로 외부망(인터넷이 되는 환경)이 되는 환경이어야 하는데 폐쇄망에서 docker를 설치해야 하는 경우도 있다. 이럴 경우 어떻게 docker를 어떻게 설치하는지 알아보자. 1234567891011wget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/fuse3-libs-3.6.1-4.el7.x86_64.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/slirp4netns-0.4.3-4.el7_8.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.6.21-3.1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-compose-plugin-2.18.1-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-buildx-plugin-0.10.5-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-cli-24.0.1-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-rootless-extras-24.0.2-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-24.0.1-1.el7.x86_64.rpm 이제 S3를 통해 EC2로 옮긴다 12345678910111213141516$ aws s3 cp s3://aipin-bucket/docker-installer.zip /rootdownload: s3://aipin-bucket/docker-installer.zip to ./docker-installer.zip$ unzip docker-installer.zip -d /rootArchive: docker-installer.zip creating: /root/docker-installer/ inflating: /root/docker-installer/docker-ce-cli-24.0.1-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-ce-rootless-extras-24.0.2-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-ce-24.0.1-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-compose-plugin-2.18.1-1.el7.x86_64.rpm inflating: /root/docker-installer/slirp4netns-0.4.3-4.el7_8.x86_64.rpm inflating: /root/docker-installer/fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpm inflating: /root/docker-installer/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm inflating: /root/docker-installer/docker-buildx-plugin-0.10.5-1.el7.x86_64.rpm inflating: /root/docker-installer/fuse3-libs-3.6.1-4.el7.x86_64.rpm inflating: /root/docker-installer/containerd.io-1.6.21-3.1.el7.x86_64.rpm yum이 필요하니 설치123456789$ aws s3 cp s3://aipin-bucket/CentOS-7-x86_64-Everything-2009.iso /rootdownload: s3://aipin-bucket/CentOS-7-x86_64-Everything-2009.iso to ./CentOS-7-x86_64-Everything-2009.iso$ cd /etc/yum.repos.d$ ll$ mkdir backup$ mv *.repo backup/$ cd backup$ ll$ vi local_repository 12345[Cento OS7 Repository]name=CentOS Local Repositorybaseurl=file:///root/local_repo/CentOS-7/gpgcheck=0enabled=1 1234567891011$ yum clean allLoaded plugins: extras_suggestions, langpacks, priorities, update-motdThere are no enabled repos. Run &quot;yum repolist all&quot; to see the repos you have. To enable Red Hat Subscription Management repositories: subscription-manager repos --enable &lt;repo&gt; To enable custom repositories: yum-config-manager --enable &lt;repo&gt;$ yum repolistLoaded plugins: extras_suggestions, langpacks, priorities, update-motdrepolist: 0 https://pkgs.org/ 여기서 필요한 패키지를 다운로드 한다. S3를 통해 S3로 옮긴다. 12$ aws s3 cp s3://aipin-bucket/createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpm /root$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpm -i 옵션은 설치를 의미합니다. -v 옵션은 상세 출력을 의미합니다. -h 옵션은 진행 상태를 해시 마크로 표시합니다. 에러가 났다. 1234$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpmerror: Failed dependencies:createrepo_c-libs = 0.12.2-2.amzn2.0.2 is needed by createrepo_c-0.12.2-2.amzn2.0.2.x86_64libcreaterepo_c.so.0()(64bit) is needed by createrepo_c-0.12.2-2.amzn2.0.2.x86_64 의존성 문제를 해결하기 위해 필요한 패키지를 함께 설치해야 합니다. 이 경우에는 createrepo_c-libs와 libcreaterepo_c.so.0 라이브러리를 포함하는 패키지를 함께 설치해야 합니다. 마찬가지로 rpm 파일을 다운로드 후 s3 통해 EC2에 추가하였다. 12345678$ rpm -ivh createrepo_c-libs-0.12.2-2.amzn2.0.2.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:createrepo_c-libs-0.12.2-2.amzn2.################################# [100%]$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpmPreparing... ################################# [100%]package createrepo_c-0.12.2-2.amzn2.0.2.x86_64 is already installed yum-utils 설치123$ aws s3 cp s3://aipin-bucket/yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm/rootdownload: s3://aipin-bucket/yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm to ./yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm$ rpm -ivh yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm 다 때려치우고 amzn2extra-docker를 사용하여 Docker를 설치하겠다. 1. amzn2extra 리포지토리 활성화1$ sudo amazon-linux-extras install docker 하지만 에러가 났다 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647sudo amazon-linux-extras install dockerInstalling dockerLoaded plugins: extras_suggestions, langpacks, priorities, update-motdCleaning repos: amzn2-core amzn2extra-docker amzn2extra-epel amzn2extra-postgresql10 epel : nexusrepo12 metadata files removed0 sqlite files removed0 metadata files removedLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00 One of the configured repositories failed (Unknown), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work &quot;fix&quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=&lt;repoid&gt; ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable &lt;repoid&gt; or subscription-manager repos --disable=&lt;repoid&gt; 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=&lt;repoid&gt;.skip_if_unavailable=trueCannot retrieve metalink for repository: epel/x86_64. Please verify its path and try againInstallation failed. Check that you have permissions to install. 현재 문제는 epel 리포지토리가 활성화되어 있지만 해당 리포지토리에 접근할 수 없어서 발생하는 것이다. 일시적으로 epel 리보지토리를 비활성화하고 진행하곘다. 1$ sudo yum --disablerepo=epel install docker 잘 설치되었다. 2. Docker 서비스 시작 및 자동 시작 설정Docker가 설치된 후, Docker 데몬을 시작하고 시스템 부팅 시 자동으로 시작되도록 설정해야 한다. 12$ sudo systemctl start docker$ sudo systemctl enable docker 3. Docker 버전 확인12$ docker --versionDocker version 25.0.3, build 4debf41 4. 현재 사용자에게 Docker 권한 부여기본적으로 Docker 명령어는 루트 사용자 권한이 필요하다.그러나 일반 사용자로 Docker를 사용하려면 해당 사용자를 docker 그룹에 추가해야 한다. 1sudo usermod -aG docker $USER 이 명령어를 실행한 후, 변경 사항을 적용하려면 로그아웃했다가 다시 로그인해야 한다. 5. Docker 설치 확인1docker run hello-world references:https://dev-luna-archive.tistory.com/36https://oingdaddy.tistory.com/134https://velog.io/@hognod/Docker-Install-Offlinehttps://finai.tistory.com/2","link":"/2024/06/23/AWS-Install-Docker-On-EC2/"},{"title":"AWS-Session-Manager-Error","text":"SSM 에이전트가 온라인 상태가 아닙니다 에러 권한이 업데이트 되면서 연결이 안 된것 같음. 권한을 제거했다가 붙여야 함.","link":"/2024/08/19/AWS-Session-Manager-Error/"},{"title":"AWS EC2 배포 중  Cannot allocate memory (12) 에러","text":"AWS EC2에 배포 중 아래와 같은 에러가 났다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344452024-09-11 06:04:15,674 - DEBUG - Attempting to release lock 140279398944864 on /root/.cache/huggingface/hub/.locks/models--Dongjin-kr--ko-reranker/002b874aa6e3b367da62d14acba828ca8d1f4bf50830747ef922d45bf71daaa2.lock2024-09-11 06:04:15,674 - DEBUG - Lock 140279398944864 released on /root/.cache/huggingface/hub/.locks/models--Dongjin-kr--ko-reranker/002b874aa6e3b367da62d14acba828ca8d1f4bf50830747ef922d45bf71daaa2.lockThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: read).Your token has been saved to /root/.cache/huggingface/tokenLogin successfulThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: read).Your token has been saved to /root/.cache/huggingface/tokenLogin successfulTraceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import utils as utils File &quot;/app/src/utils.py&quot;, line 5, in &lt;module&gt; import controller.rag_retriever_plugin_controller as rag_retriever_plugin_controller File &quot;/app/src/controller/rag_retriever_plugin_controller.py&quot;, line 13, in &lt;module&gt; plugin = RagRetrieverPlugin(documents) File &quot;/app/src/plugins/rag_retriever_plugin.py&quot;, line 30, in __init__ super().__init__(name, model, retriever, rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/Plugin/providers/plugin_retriever.py&quot;, line 21, in __init__ self.rerank = Rerank(rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/data/retriever.py&quot;, line 66, in __init__ model = AutoModelForSequenceClassification.from_pretrained(model_path) File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;, line 564, in from_pretrained return model_class.from_pretrained( File &quot;/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3735, in from_pretrained with safe_open(resolved_archive_file, framework=&quot;pt&quot;) as f:RuntimeError: unable to mmap 2239614572 bytes from file &lt;/root/.cache/huggingface/hub/models--Dongjin-kr--ko-reranker/snapshots/5f3bb02f3baa05dea3e3c6653ada191f2cc20d91/model.safetensors&gt;: Cannot allocate memory (12)Traceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import utils as utils File &quot;/app/src/utils.py&quot;, line 5, in &lt;module&gt; import controller.rag_retriever_plugin_controller as rag_retriever_plugin_controller File &quot;/app/src/controller/rag_retriever_plugin_controller.py&quot;, line 13, in &lt;module&gt; plugin = RagRetrieverPlugin(documents) File &quot;/app/src/plugins/rag_retriever_plugin.py&quot;, line 30, in __init__ super().__init__(name, model, retriever, rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/Plugin/providers/plugin_retriever.py&quot;, line 21, in __init__ self.rerank = Rerank(rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/data/retriever.py&quot;, line 66, in __init__ model = AutoModelForSequenceClassification.from_pretrained(model_path) File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;, line 564, in from_pretrained return model_class.from_pretrained( File &quot;/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3735, in from_pretrained with safe_open(resolved_archive_file, framework=&quot;pt&quot;) as f: 이 오류는 Hugging Face에서 큰 모델 파일을 로드하려고 할 때 시스템 메모리가 부족해서 발생한 것이다.unable to mmap 2239614572 bytes from file 오류는 메모리 부족을 의미하며, “Cannot allocate memory (12)”라는 메시지가 이를 명확히 보여준다. 1. 메모리 사용량 줄이기 스왑 공간 증가: 시스템에 RAM이 부족하다면 스왑 공간을 늘려보는 것이 좋다. 스왑 공간은 메모리가 부족할 때 디스크를 임시 메모리로 사용하는 방법이다. 성능은 다소 느려질 수 있지만 메모리 문제를 해결하는 데 도움이 될 수 있다. Linux 시스템에서 스왑 공간을 생성하고 활성화하는 방법123456[root@ip-10-71-176-76 ~]# fallocate -l 4G /swapfile[root@ip-10-71-176-76 ~]# sudo chmod 600 /swapfile[root@ip-10-71-176-76 ~]# sudo mkswap /swapfileSetting up swapspace version 1, size = 4 GiB (4294963200 bytes)no label, UUID=0ec98152-ae9d-4bf9-ab06-b697b5c99799[root@ip-10-71-176-76 ~]# sudo swapon /swapfile","link":"/2024/09/11/AWS-EC2-Cannot-allocate-memory/"},{"title":"Java Arrays.fill() 메소드 이용하여 배열 초기화 하기","text":"Array를 Arrays.fill() 메소드를 이용하여 배열의 사이즈만큼 지정한 값으로 한번에 초기화 할 수 있습니다. 또는 배열 내에서 구간을 지정하여 특정 값으로 초기화 할 수 있습니다. 기본 채우기 12345678910import java.util.Arrays;public class MyClass { public static void main(String args[]) { int[] arr = new [] {1, 2, 3, 4, 5}; Arrays.fill(arr, 100); System.out.println(Arrays.toString(arr)); // [100, 100, 100, 100, 100] }} 구간 지정하여 채우기 12345678910import java.util.Arrays;public class MyClass { public static void main(String args[]) { int[] arr = new int[] {1, 2, 3, 4, 5}; Arrays.fill(arr, 2, 4, 100); System.out.println(Arrays.toString(arr)); // [1, 2, 100, 100, 5] }} 2차원 배열 채우기 12345678910111213141516import java.util.Arrays;public class MyClass { public static void main(String args[]) { int[][] arr = new int[3][5]; for (int[] row: arr) { Arrays.fill(row, 10); } for (int[] row: arr) { System.out.println(Arrays.toString(row)); } // [10, 10, 10, 10, 10] // [10, 10, 10, 10, 10] // [10, 10, 10, 10, 10] }} 3차원 배열 채우기 123456789101112131415161718192021import java.util.Arrays;public class MyClass { public static void main(String args[]) { int[][][] arr = new int[3][5][2]; for (int[][] row2: arr) { for (int[] row: row2) { Arrays.fill(row, 10); } } for (int[][] row2: arr) { for (int[] row: row2) { System.out.print(Arrays.toString(row)); } System.out.println(); } // [10, 10][10, 10][10, 10][10, 10][10, 10] // [10, 10][10, 10][10, 10][10, 10][10, 10] // [10, 10][10, 10][10, 10][10, 10][10, 10] }} References https://www.geeksforgeeks.org/arrays-fill-java-examples/ https://www.tutorialspoint.com/java/util/arrays_fill_int.htm https://docs.oracle.com/javase/10/docs/api/java/util/Arrays.html","link":"/2020/08/18/Arrays-fill/"},{"title":"AWS SageMaker로 GroundTruth 라벨링 하기","text":"AWS의 SageMaker를 이용하여 Ground Truth 라벨링을 해보도록 하겠습니다. 1. Create workforce SageMaker의 Labeling workforce에서 Create private team을 선택해 줍니다. 각 항목들을 입력하여 private 팀을 생성해 줍니다. private team을 생성하면 다음과 같이 나옵니다. 메일로 들어가서 임시 password로 authorize를 해 줍니다. 위와 같이 authorized로 바뀝니다. 2. Create a labeling job이제 labeling을 해보겠습니다. 레이블 지정 작업 생성으로 들어갑니다. s3에 있는 파일로 할 수도, manifest file을 만들 수도 있습니다. S3는 다음과 같은 세팅을 하였습니다. 해당 버킷에 대한 액세스 요청을 모두 기록합니다. 권한 설정도 우선 기본 사항으로 하였습니다. 이렇게 S3 버킷을 생성했습니다. 그리고 샘플 dataset은 제 마음대로 넣어봤습니다. S3 버킷에 파일을 넣고 manifest를 생성해주어야 합니다. manifest 만들기는 다음을 참고합니다 https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/sms-data-input.html#sms-console-create-manifest-file https://docs.aws.amazon.com/sagemaker/latest/dg/sms-getting-started-step1.html manifest는 샘플들을 리스팅하는 json 파일인데 docs와 다르게 manifest 없이도 할 수 있도록 되어있습니다. 하지만 저는 manifest를 직접 만들어 해보았습니다. 그리고 다시 labeling으로 돌아가 이런식으로 세팅을 해 줍니다 dataset이 이미지들 이므로 task category는 image, task selection은 semantic segmentation으로 해보겠습니다. 다음으로 넘어가서 private으로 세팅해 줍니다. 이런식으로 예시가 나옵니다 구분해야할 물체를 최대한 구체적으로 명시해줍니다. 생성 버튼을 눌러주면 다음과 같이 labeling job이 생성 됩니다. 들어가보면 작업 중인 사진들이 보입니다. 시간이 좀 걸리니 기다려야 합니다. 결과물을 보려면 console로 들어가야 합니다. 오른쪽의 레이블 지정 포털 로그인 URL로 들어가면 됩니다. 들어가면 다음과 같은 화면이 나옵니다. start working을 들어가면 다음과 같은 화면이 나옵니다. 이런식으로 지정해줍니다 그리고 submit을 해줍니다. 이번엔 auto segment 기능을 이용해보겠습니다. Labels에서 해당되는것을 클릭후 최하단, 최상단, 좌우 끝 네 지점을 지정해주면 auto segmenting이 실행됩니다. 이런식으로 잘 지정이 됩니다. 다른 차들도 해줍니다. 다른 요소들도 해줍니다. 다 해줍니다. 모두 마무리하면 이런 화면으로 넘어갑니다. cloud watch에 들어가면 실행한 log를 볼 수 있습니다. S3의 output에 들어가면 결과물이 들어있습니다. 결과물들은 이곳에 있습니다. 이런식으로 있습니다. 이곳에 가면 output의 경로를 볼 수 있습니다. 이런식으로 잘 labeling이 되어있습니다. 결과물을 보면 이 사진의 경우 이런식으로 라벨이 붙어있습니다. 이미지 분류 (다중 레이블) 이번엔 이미지 분류 다중 레이블로 해보겠습니다 위에 했던 방식과 똑같이 해주면 이렇게 워크\u001d 포털이 나옵니다들어가서 지정해 주면 됩니다 계속 지정해 주면 됩니다 작업이 끝났습니다 결과물을 보면 이 사진의 경우 이런식으로 사람, 자동차, 버스, 나무, 신호등의 label이 붙습니다. 이러한 라벨들을 다른곳에 활용할 수 있을 것입니다.","link":"/2020/07/30/AWS-SageMaker-Ground-Truth/"},{"title":"AWS EC2에 pgvector 설치하고 접속하기","text":"AWS EC2에 PostgreSQL을 설치하고 접속해보겠다.사실 AWS에는 관계형 데이터베이스를 편하게 다룰 수 있는 서비스인 RDS를 제공하기는 하지만 과금이 많이 되는 경향도 있고 여러 이유로 PostgreSQL을 직접 EC2에 설치해서 사용하기로 했다. 1. postgresql 설치EC2에 접근하여 postgresql(이하 PG)를 설치한다. 12sudo amazon-linux-extras install postgresql10 epel -ysudo yum install postgresql-server postgresql-devel -y 에러 발생1234567891011121314151617181920212223242526272829303132333435363738Loaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00 One of the configured repositories failed (Unknown), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work &quot;fix&quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=&lt;repoid&gt; ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable &lt;repoid&gt; or subscription-manager repos --disable=&lt;repoid&gt; 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=&lt;repoid&gt;.skip_if_unavailable=trueCannot retrieve metalink for repository: epel/x86_64. Please verify its path and try again EC2가 yum 서버와 통신을 못해서 문제가 발생한 것이다.즉, EC2가 외 인터넷과 통신이 되지 않는 것이다. 이러한 경우 인터넷 게이트웨이를 생성하여 VPC및 서브넷과 연동해주면 된다. 참고 : https://garve32.tistory.com/63 AWS EC2 인스턴스에 파일 업로드 및 다운로드파일을 옮기기 위해 퍼블릭 IPv4 고정 IP 설정을 해준다. https://rypro.tistory.com/227 S3에 업로드한 pgvector 파일이용하여 설치하기S3와 EC2를 연동하는 방법은 다른 포스팅을 참고바란다. 12345678[root@ec2-ct01-dev-slm-app-03 ~]# aws s3 cp s3://aipin-bucket/pgvector.zip /rootdownload: s3://aipin-bucket/pgvector.zip to ./pgvector.zip[root@ec2-ct01-dev-slm-app-03 ~]# unzip /root/pgvector.zip -d /rootArchive: /root/pgvector.zip inflating: /root/README.md inflating: /root/docker-compose.yml extracting: /root/init_db.sql Docker 실행12$ sudo systemctl start docker$ sudo systemctl enable docker 1. amzn2extra 리포지토리 활성화123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566$ sudo amazon-linux-extras enable postgresql10 2 httpd_modules available [ =1.0 =stable ] 3 memcached1.5 available \\ [ =1.5.1 =1.5.16 =1.5.17 ] 6 *postgresql10=latest enabled [ =10 =stable ] 9 R3.4 available [ =3.4.3 =stable ] 10 rust1 available \\ [ =1.22.1 =1.26.0 =1.26.1 =1.27.2 =1.31.0 =1.38.0 =stable ] 18 libreoffice available \\ [ =5.0.6.2_15 =5.3.6.1 =stable ] 19 gimp available [ =2.8.22 ] 20 †docker=latest enabled \\ [ =17.12.1 =18.03.1 =18.06.1 =18.09.9 =stable ] 21 mate-desktop1.x available \\ [ =1.19.0 =1.20.0 =stable ] 22 GraphicsMagick1.3 available \\ [ =1.3.29 =1.3.32 =1.3.34 =stable ] 24 epel=latest enabled [ =7.11 =stable ] 25 testing available [ =1.0 =stable ] 26 ecs available [ =stable ] 27 †corretto8 available \\ [ =1.8.0_192 =1.8.0_202 =1.8.0_212 =1.8.0_222 =1.8.0_232 =1.8.0_242 =stable ] 32 lustre2.10 available \\ [ =2.10.5 =2.10.8 =stable ] 33 †java-openjdk11 available [ =11 =stable ] 34 lynis available [ =stable ] 36 BCC available [ =0.x =stable ] 37 mono available [ =5.x =stable ] 38 nginx1 available [ =stable ] 40 mock available [ =stable ] 43 livepatch available [ =stable ] 44 †python3.8 available [ =stable ] 45 haproxy2 available [ =stable ] 46 collectd available [ =stable ] 47 aws-nitro-enclaves-cli available [ =stable ] 48 R4 available [ =stable ] 49 kernel-5.4 available [ =stable ] 50 selinux-ng available [ =stable ] 52 tomcat9 available [ =stable ] 53 unbound1.13 available [ =stable ] 54 †mariadb10.5 available [ =stable ] 55 kernel-5.10 available [ =stable ] 56 redis6 available [ =stable ] 58 †postgresql12 available [ =stable ] 59 †postgresql13 available [ =stable ] 60 mock2 available [ =stable ] 61 dnsmasq2.85 available [ =stable ] 62 kernel-5.15 available [ =stable ] 63 †postgresql14 available [ =stable ] 64 firefox available [ =stable ] 65 lustre available [ =stable ] 66 †php8.1 available [ =stable ] 67 awscli1 available [ =stable ] 68 †php8.2 available [ =stable ] 69 dnsmasq available [ =stable ] 70 unbound1.17 available [ =stable ] 72 collectd-python3 available [ =stable ]* Extra topic has reached end of support.† Note on end-of-support. Use 'info' subcommand.Now you can install: # yum clean metadata # yum install postgresql 2. PostgreSQL 설치12345678910$ sudo yum --disablerepo=epel install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00No package postgresql10 available.No package postgresql10-server available.Error: Nothing to do 에러가 났다. amazon-linux-extras 리포지토리 확인123456789101112131415161718192021222324252627$ sudo amazon-linux-extras list# 출력에서 postgresql10이 활성화되어 있는지 확인합니다. 만약 활성화되어 있지 않다면, 다시 활성화.$ sudo amazon-linux-extras enable postgresql10# 리포지토리 캐시 갱신$ sudo yum clean all# 다시 설치$ sudo yum --disablerepo=epel install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00(1/9): amzn2-core/2/x86_64/group_gz | 2.7 kB 00:00:00(2/9): amzn2-core/2/x86_64/updateinfo | 935 kB 00:00:00(3/9): amzn2extra-epel/2/x86_64/primary_db | 1.8 kB 00:00:00(4/9): amzn2extra-postgresql10/2/x86_64/updateinfo | 55 B 00:00:00(5/9): amzn2extra-postgresql10/2/x86_64/primary | 15 kB 00:00:00(6/9): amzn2extra-docker/2/x86_64/primary_db | 102 kB 00:00:00(7/9): amzn2extra-epel/2/x86_64/updateinfo | 76 B 00:00:00(8/9): amzn2extra-docker/2/x86_64/updateinfo | 16 kB 00:00:00(9/9): amzn2-core/2/x86_64/primary_db | 68 MB 00:00:00No package postgresql10 available.No package postgresql10-server available.Error: Nothing to do 계속 epel이 문제인듯 하니 epel enabled = 0을 바꿔서 비활성화 시켜버림 123456789[epel]name=Extra Packages for Enterprise Linux 7 - $basearch#baseurl=http://download.fedoraproject.org/pub/epel/7/$basearchmirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchmetalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 yum cache 갱신12345678910111213141516171819202122232425$ sudo yum clean all$ sudo yum makecacheLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00(1/17): amzn2-core/2/x86_64/group_gz | 2.7 kB 00:00:00(2/17): amzn2-core/2/x86_64/updateinfo | 935 kB 00:00:00(3/17): amzn2-core/2/x86_64/filelists_db | 61 MB 00:00:00(4/17): amzn2-core/2/x86_64/primary_db | 68 MB 00:00:00(5/17): amzn2extra-docker/2/x86_64/filelists_db | 34 kB 00:00:00(6/17): amzn2extra-docker/2/x86_64/updateinfo | 16 kB 00:00:00(7/17): amzn2extra-docker/2/x86_64/primary_db | 102 kB 00:00:00(8/17): amzn2extra-docker/2/x86_64/other_db | 32 kB 00:00:00(9/17): amzn2-core/2/x86_64/other_db | 21 MB 00:00:00(10/17): amzn2extra-epel/2/x86_64/updateinfo | 76 B 00:00:00(11/17): amzn2extra-epel/2/x86_64/filelists_db | 882 B 00:00:00(12/17): amzn2extra-epel/2/x86_64/primary_db | 1.8 kB 00:00:00(13/17): amzn2extra-epel/2/x86_64/other_db | 507 B 00:00:00(14/17): amzn2extra-postgresql10/2/x86_64/updateinfo | 55 B 00:00:00(15/17): amzn2extra-postgresql10/2/x86_64/primary | 15 kB 00:00:00(16/17): amzn2extra-postgresql10/2/x86_64/filelists | 121 kB 00:00:00(17/17): amzn2extra-postgresql10/2/x86_64/other | 6.2 kB 00:00:00Metadata Cache Created 다시 설치1234$ sudo yum install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdNo package postgresql10 available.No package postgresql1 여전히 안됨 명령어가 틀린듯1234567yum install postgresqlLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00Package postgresql-10.21-1.amzn2.0.1.x86_64 already installed and latest versionNothing to do 3. PostgreSQL 데이터베이스 초기화123$ sudo /usr/pgsql-10/bin/postgresql-10-setup initdb# 안먹음 계속 psql에 문제가 있어서 PostgreSQL을 설치하기 위해 필요한 RPM 파일을 인터넷이 되는 외부 시스템에서 다운로드한 후, 폐쇄망 서버로 전송하여 설치하는 방법으로 진행 docker-compose 설치12345678910$ curl -L &quot;https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 57.9M 100 57.9M 0 0 14.5M 0 0:00:03 0:00:03 --:--:-- 21.4M# 옮기기$ aws s3 cp s3://aipin-bucket/docker-compose /rootdownload: s3://aipin-bucket/docker-compose to ./docker-compose Docker Compose 파일을 적절한 위치로 이동시키고 실행 권한을 부여 12$ sudo mv docker-compose /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 잘 설치되었는지 확인 12$ docker-compose --versionDocker Compose version v2.20.2 docker-compose로 pgvector 올리기12$ docker-compose up -d[+] Running 1/1 postgre를 다시 다른 방법으로 설치해보자123456789$ aws s3 cp s3://aipin-bucket/postgresql-9.6.16.tar.gz /rootdownload: s3://aipin-bucket/postgresql-9.6.16.tar.gz to ../postgresql-9.6.16.tar.gz# 패키지 파일 압축해제 $ tar -zxvf postgresql-9.6.16.tar.gz# 설치 디렉토리는 디폴트로 함, --prefix=/usr/local/pgsql 이렇게 하는 경우도 있는듯$ ./configure --without-readline$ make &amp;&amp; make check$ make install PostgreSQL 사용자 및 디렉토리 설정12345sudo useradd postgressudo mkdir /usr/local/pgsql/datasudo chown postgres /usr/local/pgsql/datasudo mkdir /usr/local/pgsql/logssudo chown postgres /usr/local/pgsql/logs 데이터베이스 초기화1234567891011121314151617181920212223242526272829su - postgres[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/initdb -D /usr/local/pgsql/dataThe files belonging to this database system will be owned by user &quot;postgres&quot;.This user must also own the server process.The database cluster will be initialized with locale &quot;en_US.UTF-8&quot;.The default database encoding has accordingly been set to &quot;UTF8&quot;.The default text search configuration will be set to &quot;english&quot;.Data page checksums are disabled.fixing permissions on existing directory /usr/local/pgsql/data ... okcreating subdirectories ... okselecting default max_connections ... 100selecting default shared_buffers ... 128MBselecting default timezone ... Asia/Seoulselecting dynamic shared memory implementation ... posixcreating configuration files ... okrunning bootstrap script ... okperforming post-bootstrap initialization ... oksyncing data to disk ... okWARNING: enabling &quot;trust&quot; authentication for local connectionsYou can change this by editing pg_hba.conf or using the option -A, or--auth-local and --auth-host, the next time you run initdb.Success. You can now start the database server using: /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l logfile start PostgreSQL 서버 시작PostgreSQL 서버를 시작합니다. PostgreSQL 사용자로 전환하여 PostgreSQL 서버를 백그라운드에서 실행 12[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfile startserver starting PostgreSQL 서버 자동 시작 설정 (Optional)시스템 부팅 시 PostgreSQL 서버가 자동으로 시작되도록 설정할 수 있습니다. /etc/systemd/system/postgresql.service 파일을 생성하고 다음 내용을 추가합니다. 123456789101112131415sudo tee /etc/systemd/system/postgresql.service &lt;&lt;EOF[Unit]Description=PostgreSQL database serverAfter=network.target[Service]Type=forkingUser=postgresExecStart=/usr/local/pgsql/bin/pg_ctl start -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfileExecStop=/usr/local/pgsql/bin/pg_ctl stop -D /usr/local/pgsql/dataExecReload=/usr/local/pgsql/bin/pg_ctl reload -D /usr/local/pgsql/data[Install]WantedBy=multi-user.targetEOF PostgreSQL 접속1234567891011$ /usr/local/pgsql/bin/psqlpsql (9.6.16)Type &quot;help&quot; for help.# 비번 설정postgres=#postgres=# ALTER USER postgres WITH PASSWORD 'new1234';ALTER ROLE# 종료postgres=# \\q 잘 돌고있는지 확인1234567891011ps aux | grep postgresroot 16797 0.0 0.0 200768 3912 pts/1 S 22:04 0:00 su - postgrespostgres 16798 0.0 0.1 124744 4012 pts/1 S 22:04 0:00 -bashpostgres 17004 0.0 0.4 277808 16760 pts/1 S 22:06 0:00 /usr/local/pgsql/bin/postgres -D /usr/local/pgsql/datapostgres 17009 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: checkpointer processpostgres 17010 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: writer processpostgres 17011 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: wal writer processpostgres 17012 0.0 0.1 278208 5504 ? Ss 22:06 0:00 postgres: autovacuum launcher processpostgres 17013 0.0 0.0 132824 2300 ? Ss 22:06 0:00 postgres: stats collector processpostgres 17840 0.0 0.1 162296 3960 pts/1 R+ 22:14 0:00 ps auxpostgres 17844 0.0 0.0 119420 912 pts/1 S+ 22:14 0:00 grep --color=auto postgres 123456789101112[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/psqlpsql (9.6.16)Type &quot;help&quot; for help.postgres=# SELECT version(); version----------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.16 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17),64-bit(1 row) 다시 앞의 docker-compose로 pgvector 올리기1234$ docker-compose up -d[+] Running 1/1 ✘ db Error 15.0sError response from daemon: Get &quot;https://registry-1.docker.io/v2/&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 흐음… postgres 종료$ sudo su - postgres Last login: Mon Jun 24 22:16:42 KST 2024 on pts/0 $ /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfile stop waiting for server to shut down.... done server stopped - references https://inblog.ai/guri-tech-blog/ec2%EC%97%90-postgressql-96-%EC%84%A4%EC%B9%98-%ED%8C%A8%ED%82%A4%EC%A7%80%EC%84%A4%EC%B9%98-tar-7393 https://wldnjd2.tistory.com/m/95","link":"/2024/06/17/AWS-Install-Postgresql-On-EC2/"},{"title":"Azure-AZ104","text":"MS AZ104 자격증 취득을 위한 공부 과정 및 방법을 기록해보겠다. 시험 개요 최소 6개월 이상의 Azure 관리 실무 경험을 권장 Azure의 핵심 서비스, Azure 워크로드, 보안 및 거버넌스에 대한 탄탄한 이해가 필요 기본적인 지식이 부족하다면 AZ-900 과정을 참고 PowerShell, 명령줄 인터페이스(Command Line Interface), Azure Portal 및 ARM 템플릿 사용 경험이 있어야함. 시험 정보 응시료: 시험 비용은 165 미국 달러 점수: Microsoft 자격증 시험은 1000점 만점으로 채점되며, 700점 이상을 획득해야 AZ-104 인증을 받고 Azure Administrator 배지를 획득할 수 있다. 갱신 주기: AZ-104 시험은 매년 갱신해야 합니다. Microsoft는 주기적으로 인증을 폐지하거나 시험 내용을 대폭 변경하면 시험 번호가 변경될 수도 있음 문항 및 시간: 시험은 약 65문항으로 구성되며, 이를 해결할 3시간이 주어진다. 랩 없음: 작성 시점 기준으로, 시험에는 랩(실습 과제)이 포함되지 않는다. 시험 구성 Azure 아이덴티티 및 거버넌스 관리 (15~20%) 스토리지 구현 및 관리 (10~15%) Azure 컴퓨팅 리소스 배포 및 관리 (25~30%) 가상 네트워킹 구성 및 관리 (30~35%) Azure 리소스 모니터링 및 백업 (10~15%) 섹션별 구성총 3가지 섹션으로 구성, 해당 섹션이 마무리되면 다시 돌아갈 수 없음. 토픽 기반 문제 5개(?) 페이지당 하나의 주제에 대한 질문 Y/N 고르기 (3문제) 빈칸 체우기 (select box) 순서 맞추기 5지 선답 (두 세개씩 고르는 것도 있음) 세션2의 리뷰는 세션 3종료 후 가능 같은 이슈에 대한 솔루션을 제공하고 제시된 솔루션으로 이슈 해결 가능한지 각 문제단위로 답을 제출하고 나면 답 수정 불가 Y/N 고르는 방식 시험 후기 한국어 시험으로 신청하면, 영어로 전환하여 보는것 가능. 덤프를 외워갔으나 사진찍듯이 외워 갔기에, 시험 문제의 포맷이 다소 달랐기에 같은 내용의 문제더라도 파악하는데 다소 어려움이 있었음. 덤프에서 나오는 비중이 높으나, 제대로 외우지 못했거나, 덤프에 없는 문제가 나오면 기본지식과 지문을 잘 숙지하여 풀면 됨. Azure와 네트워크 관련 기본 지식이 있으면 푸는데 도움이 됨.","link":"/2024/12/19/Azure-AZ104/"},{"title":"Amazon Cognito 이용하여 인증기능 구현하기.","text":"AWS Cognito는 AWS에서 제공하는 인증 및 권한 부여 서비스이다. AWS Cognito를 사용하면 애플리케이션의 사용자 인증, 사용자 데이터 동기화 및 액세스 제어를 쉽게 관리할 수 있다. 이를 통해 개발자는 사용자 등록, 로그인, 비밀번호 복구 등과 같은 기능을 간단하게 구현할 수 있다. AWS Cognito는 다음과 같은 주요 기능을 제공한다. 사용자 풀(User Pools) : 사용자 풀은 사용자 등록, 로그인 및 계정 관리와 관련된 기능을 제공한다. 이를 통해 사용자가 애플리케이션에 둥록하고 로그인할 수 있으며, 소셜 로그인을 포함한 다양한 인증 방법을 지원한다. 아이덴티티 풀(Identity Pools) : 사용자 풀에 저장된 정보를 바탕으로 로그인 또는 회원가입에 성공한 사용자에게 AWS 인프라의 여러 서비스에 대한 권한을 부여할 수 있는 서비스. 연동 소셜 로그인 : AWS Cognito는 페이스북, 구글, 애플 등과 같은 소셜 로그인 기능을 제원하여 사용자가 소셜 미디어 계정으로 로그인할 수 있게 한다. AWS Cognito를 사용하면 사용자 인증 및 권한 관리를 보다 쉽게 구현할 수 있으며, 이와 관련된 보안 문제를 AWS에서 관리해주기 때문에 개발자는 애플리케이션 로직에 더 집중할 수 있다. Cognito Token 이용하기출처 : https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html 유저가 로그인에 성공하면, Cognito는 세션을 생성하고 인증된 사용자에게 ID token, access token, refresh token을 리턴한다.토큰은 API Gateway나 자체 구성된 server-side 리소스에 인증 목적으로 사용할 수 있다.Cognito Identity Pool을 이용해서 이 토큰들을 API Gateway가 아닌 AWS 서비스에 접근하기 위해 임시 AWS Credentials로 바꿀 수도 있다. 종류 ID token (자격 증명 토큰) : 로그인한 사용자의 자격 증명 클레임을 기반으로 API 호출 권한을 부여하는데 사용. Access token (액세스 토큰) : 지정된 액세스 보호 리소스의 사용자 지정 범위를 기반으로 API 호출 권한을 부여하는 데 사용. Refresh token (리프레스 토큰) : 신규 ID/액세스 토큰을 발급받는다. 리프레시 토큰의 default 만료 기간은 30일이며, 60분~10년 사이로 설정 가능하다. 특징 Cognito에서 발행하는 토큰은 클레임 기반의 토큰이다. Access/ID 토큰은 모두 cognito:groups라는 클레임을 포함한다. token : 유저를 인증하고, 리소스에 접근을 허용한다. clain : 토큰에 포함된 유저 관련 정보. 주체가 무엇인지 표현하는 이름과 값의 쌍. Cognito는 Base64 인코딩 된 string 값으로 토큰 발행한다. Cognito ID 또는 access token을 Base64로부터 plaintext JSON으로 디코딩 가능하다. regresh token 암호화되었으며 Cognito administrator나 유저로부터 읽힐 수 없다. Architecture 인증(Authentication) : API를 호출하는 클라이언트에 대한 Identity(신분)을 확인해주는 기능. 인가(Authorization) : 클라이언트가 API를 호출할 수 있는 권한이 있는지 확인해주는 기능. Cognito User Pool사용자 풀은 사용자에 대한 정보를 가지고 있는 저장소와 같은 역할.위에서 언급했듯이 여러가지 방법의 로그인 또는 회원가입을 지원. 성공적으로 사용자 인증 과정이 완료되면, Cognito는 JSON 형식의 웹 토큰(JWT)를 발행하며, 이 토큰을 사용해 특정 API에 대한 접근 보안 등 자격 증명을 수행하거나 AWS의 자격 증명으로 교환. 또한 사용자 풀의 모든 사용자는 그들 각각의 프로필을 가지고 있으며, SDK(javascript, Android, iOS)를 통해 프로필에 접근할 수 있다. 사용자 풀이 제공하는 기능가입 및 로그인수정 가능한 사용자 로그인을 위한 웹 UIFacebook, Google, Amazon, Apple을 통한 소셜 로그인 및 사용자 풀의 SAML 자격 증명 공급자를 통한 로그인사용자 관리 및 사용자 프로필멀티 팩터 인증(MFA, 2중 인증), 이상 자격 증명 확인, 계정 탈취 보호, 전화 및 이메일 확인과 같은 보안 기능AWS Lambda 트리거를 이용한 Cognito의 인증 과정 등의 커스터마이징 사용자 풀의 인증 flow현대식 인증 과정에는 사용자 인증을 위해 단순히 아이디, 암호 인증 외에도 여러가지 챌린지가 통합되어 있다.크게 인증은 두가지 단계로 일반화 할 수 있으며, 이들은 각각 InitiateAuth와 RespondToAuthChallenge API를 통해 구현된다. 인증이 실패하여 종료하거나 인증이 완료되어 토큰이 발행될 때까지 사용자는 순차적으로 사전에 정의된 챌린지들을 수행하게 된다. 챌린지는 만들고자 하는 앱에서 필요한 만큼 반복이 가능.이는 Cognito에서 개발자가 요구하는 복잡한 인증과정도 구현이 가능하게 한다. 자격 증명 풀(Identity Pools)자격 증명 풀은 특정 사용자의 고유한 자격 증명을 만들고 사용자에게 AWS 인프라에 대한 접근권한을 부여할 수 있다. Cognito user pool 사용자 Facebook, Google, Apple, SAML 인증 공급자로 인증된 사용자 기존의 인증 프로세스(서비스의 자체 인증 등)를 통해 인증된 사용자 자격 증명 풀을 이용하면 다른 AWS 서비스에 직접 접근하거나 API Gateway를 통해 서비스에 접근하도록 정의하는 권한을 가진 임시 AWS 자격 증명을 생성할 수 있다. 자격 증명 풀 인증 flow자격 증명 풀의 인증 flow는 외부 소셜 로그인, 기존의 인증 프로세스를 통한 사용자에게 발급되는 자격 증명에 대한 인증 flow이며, 크게 4가지가 있다. 외부 공급자 인증 flow2가지의 인증 방식이 존재하며, 이들 각각을 향상된 인증 흐름, 기본 인증 흐름으로 부른다. 향상된 인증 flow2단계로 자격 증명을 발급받을 수 있는 flow이며, GetId GetCredentialsForIdentity두번의 통신(디바이스와 Cognito간 통신을 의미)으로 자격 증명을 발급받을 수 있는 flow이다. 기본 인증 flow단계로 자격 증명을 발급받을 수 있는 flow이며, GetId GetOpenIdToken AssumeRoleWithWebIdentity세번의 통신(디바이스와 Cognito간 통신을 의미)으로 자격 증명을 발급받을 수 있는 flow. 개발자 인증 자격 증명 인증 flow자체 인증 시스템에서 인증된 사용자를 위한 자격 증명 인증 flow.2가지의 인증 방식이 존재하며, 이들 각각을 향상된 인증 흐름, 기본 인증 흐름으로 부른다. 2단계로 자격 증명을 발급받을 수 있는 flow이며, 자체 시스템에서 로그인 자체 시스템에서 로그인 검증 GetOpenIdTokenForDeveloperIdentity GetCredentialsForIdentity과정으로 구성되어 있다. 참고https://hyeon-joo.tistory.com/33https://velog.io/@w1nu/%EC%89%BD%EA%B2%8C-%ED%92%80%EC%96%B4%EC%93%B4-AWS-Cognito-%EA%B8%B0%EC%B4%88-%EC%9D%B4%EB%A1%A0https://velog.io/@jy3026/OAuth2.0%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80","link":"/2024/08/05/Adding-Authentication-to-API-Gateway-Using-AWS-Cognito/"},{"title":"인공 신경망 (Artificial Neural Network)","text":"인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 구조와 기능을 모방하여 만들어진 컴퓨팅 시스템이다. 인공 신경망은 데이터를 처리하고 학습하는데 사용되며, 주로 패턴 인식, 분류, 예측 등과 같은 다양한 머신러닝과 딥러닝 문제를 해결하는 데 활용한다. 생물학적 뉴런에서 영감 받아 만든 머신러닝 알고리즘이지만, 실제 우리 뇌를 모델링한 것은 아니다. 신경망은 기존 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있다. 인공 신경망 알고리즘을 종종 딥러닝이라고 부른다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 1. 패션 MNIST딥러닝을 배울 때 많이 쓰는 데이터셋이 MNIST 데이터셋이다.10종류의 패션 아이템으로 구성되어있다. keras.datasets.fashion_mnist모듈 아래 load_data() 함수로 훈련 데이터와 테스트 데이터를 얻을 수 있다. 12345from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()print(train_input.shape, traion_taret.shape)print(test_input.shape, test_target.shape) 12(60000, 28, 28) (60000,)(10000, 28, 28) (10000,) 28*28 사이즈의 이미지가 훈련세트에는 6만개, 테스트세트에는 1만개 들어있다. 훈련 데이터에서 몇 개의 샘플을 그림으로 표현해본다. 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10,10))for i in range(10): axs[i].imshow(train_input[i], cmap='gray_r') axs[i].axis('off')plt.show() 12import numpy as npprint(np.unique(train_target, return_counts=True)) 1(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) 12각 레이블 :0 티셔츠, 1 바지, 2 스웨터, 3 드레스, 4 코트, 5 샌달, 6 셔츠, 7 스니커즈, 8 가방, 9 앵클부츠 각 레이블마다 6000개의 샘플이 들어있다. 2. 로지스틱 회귀로 패션 아이템 분류하기이 훈련 샘플은 60,000개나 되기 때문에, 전체 데이터를 한꺼번에 사용하여 모델을 훈련시키기보다, 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 효율적이다. 이 상황에서 가장 잘 맞는 방법은 확률적 경사 하강법이다.SGDClassifier 클래스의 매개변수 loss='log'로 지정하여 확률적 경사 하강법 모델을 만든다. 2-1. 데이터를 표준화 전처리하기확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동한다. 특성마다 값의 범위가 다르다면 올바르게 손실 함수의 경사를 내려올 수 없다. 이 데이터셋의 각 픽셀은 0255 사이의 정수값을 가지기 때문에, 255로 나누어 01 사이의 값으로 정규화한다. 12train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28) SGDClassifier는 2차원 입력을 다루지 못하기 때문에 각 샘플을 1차원 배열로 만들어야 한다. reshape() 메서드의 두 번째 매개변수를 28*28 이미지 크기에 맞게 지정하면 척 번째 차원은 변하지 않고, 원본 데이터의 두 번째, 세 번째 차원이 1차원으로 합쳐진다. 1print(train_scaled.shape) 1(60000, 784) 2-2. 모델을 만들고 교차검증하기SGDClassifier 클래스와 cross_validate()로 교차검증을 진행한다. 123456from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)scores = cross_validate(sc, train_scaled, train_traget, n_jobs=-1)print(np.mean(scores['test_score'])) 10.8196000000000001 2-3. 로지스틱 회귀 공식 복습앞서 배운 로지스틱 회귀 공식은 이렇다.z = a * weight + b * length + ... + f 이를 MNIST 데이터셋에 적용하면 이렇다.z_티셔츠 = w1 * 픽셀1 + w2 * 픽셀2 + ... + w784 * 픽셀784 + bz_바지 = w1' * 픽셀1 + w2' * 픽셀2 + ... + w784' * 픽셀784 + b' 이와 같이 10개의 클래스에 대한 선형 방정식을 구한 뒤, 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻을 수 있다. 2-4. SGDClassifier 복습SGDClassifier는 사이킷런(Scikit-learn) 라이브러리에서 제공하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용한 선형 분류기이다. “SGD”는 반복적으로 주어진 데이터셋을 작은 배치로 나누어 모델의 가중치를 업데이트하는 최적화 알고리즘을 말하며, 이를 통해 분류 또는 회귀 문제를 해결할 수 있다. SGDClassifier는 특히 대규모 데이터셋에 대한 선형 분류 문제에 효과적인 방법으로 널리 사용된다. 이 클래스는 다양한 선형 모델, 예를 들어 로지스틱 회귀(Logistic Regression), 선형 서포트 벡터 머신(Linear Support Vector Machine) 등을 구현할 수 있도록 지원한다. 주요 기능 및 특징: 효율성: 대규모 데이터셋에 대해 효율적인 학습이 가능하다. 유연성: 다양한 손실 함수(Loss Function)를 지원하며, 이를 통해 다양한 선형 분류 문제를 해결할 수 있다. 예를 들어, loss=&quot;hinge&quot;는 선형 SVM을, loss=&quot;log&quot;는 로지스틱 회귀를 의미합니다. 정규화: l2, l1, elasticnet과 같은 정규화 옵션을 제공하여 모델의 복잡도를 조절하고, 과적합을 방지할 수 있다. 온라인 학습 지원: partial_fit 메서드를 사용하여 데이터가 순차적으로 도착할 때 모델을 점진적으로 업데이트할 수 있다. 사용 예시:123456789101112from sklearn.linear_model import SGDClassifierfrom sklearn.datasets import make_classification# 예제 데이터 생성X, y = make_classification(n_samples=1000, n_features=20, random_state=42)# SGDClassifier 인스턴스 생성 및 학습clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=1000)clf.fit(X, y)# 예측predictions = clf.predict(X[:5]) 이 코드는 먼저 가상의 분류 데이터셋을 생성하고, SGDClassifier를 사용하여 선형 SVM 모델 (loss-&quot;hinge&quot;)을 학습한 뒤, 몇 개의 샘플에 대해 예측을 수행한다.SGDClassifier는 특히 대규모 데이터셋을 다룰 때 그 장점이 두드러지며, 다양한 선형 분류 문제에 적용될 수 있다. 3. 인공신경망으로 모델 만들기3-1. 인공 신경망 가장 간단한 인공 신경망은 출력층 하나가 있는 인공 신경망이다.확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다.보통 인공 신경망을 이야기 할 때나 딥러닝을 이야기 할 때는 출력층 하나가 아니라 더 많은 층이 있는 경우를 이야기한다. 입력층 : 픽셀값 자체이고, 특별한 계산을 수행하지 않는다. 출력층 : z1 ~ z10을 계산하고 이를 바탕으로 클래스를 예측 뉴런 : z값을 계산하는 단위, 뉴런에서 일어나는 일은 선형 계산이 전부이다. 이제는 뉴런이란 표현 대신 유닛(unit)이라고 부르는 사람이 더 많다. 3-2. 텐서플로우와 케라스 텐서플로우 : 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리케라스 : 텐서플로의 고수준 API 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리 장치인 GPU를 사용하여 인공 신경망을 훈련한다는 것이다. GPU는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 인공 신경망에 큰 도움이 된다. 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다. 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다. 예를 들면 텐서플로가 케라스의 백엔드 중 하나이다. 이런 케라스를 멀티-백엔드 케라스라고 부른다. 케라스 API만 익히면 다양한 딥러닝 라이브러리를 입맛대로 골라서 쓸 수 있다. 3-3. 케라스 모델 만들기인공신경망에서는 교차 검증을 잘 사용하지 않고, 검증 세트를 별도로 덜어 내어 사용한다. 데이터셋이 충분히 크고, 교차검증에는 오랜시간이 걸리기 때문이다. 12345from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)print(train_scaled.shape, train_target.shape) #훈련세트print(val_scaled.shape, val_target.shape) #검증세트 12(48000, 784) (48000,)(12000, 784) (12000,) 가장 기본이 되는 층인 밀집층을 만들어본다.입력층은 784개의 뉴런으로 구성되며, 출력층은 10개의 뉴런으로 구성된다.밀집층은 각 뉴런이 모두 연결되어야 하기 때문에, 784*10 = 7840개의 선이 포함된다.이를 완전 연결층이라고도 부른다. 12dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))model = keras.Sequential(dense) # 밀집층을 가진 신경망 모델 뉴런의 갯수를 10으로 지정하고, 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해 소프트맥스 함수를 이용한다. 만약 이진분류라면 activation='sigmoid'로 입력한다.이후 밀집층을 가진 신경망 모델을 만들기 위해 Sequential클래스를 사용한다. 소프트맥스와 같이 뉴런의 선형방정식 계산 결과에 적용되는 함수를 활성화 함수라고 부른다. 케라스 모델은 훈련하기 전 설정 단계가 있다. 1model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') loss매개변수에 이진 분류라면 binary_crossentropy, 다중 분류라면 categorical_crossentropy를 사용한다. 정수로 된 타깃값을 원-핫 인코딩으로 바꾸지 않고 사용하려면 loss='sparse_categorical_crossentropy',타깃값을 원-핫 인코딩으로 준비했다면 loss='categorical_crossentropy'으로 지정한다. metrics 매개변수는 accuracy를 지정하면 에포크마다 정확도를 함께 출력해준다. 1model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 3s 1ms/step - loss: 0.6125 - accuracy: 0.7901Epoch 2/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4786 - accuracy: 0.8392Epoch 3/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8475Epoch 4/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4452 - accuracy: 0.8512Epoch 5/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4376 - accuracy: 0.8547 epochs 매개변수로 에포크 횟수를 지정할 수 있다.evaluate() 메서드로 모델의 성능을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 3ms/step - loss: 0.4630 - accuracy: 0.8458[0.46303632855415344, 0.8458333611488342] fit() 메서드와 비슷한 출력을 보여준다.","link":"/2024/04/02/Artificial-Neural-Network/"},{"title":"Azure-API-Management-Deploy","text":"이 글에서는 Azure API Management를 구축해보겠다. API Management에서 APIs에서 미리 만들어둔 App Service를 browse하여 Api를 만든다 생성된 api의 test 탭에 들어가면 그냥 url 만 복사하여 요청하면 1curl --location 'https://apimgmt-az01-og084501-dev-ktintelliagent-test-01.azure-api.net/test20250131' 아래와 같이 401 에러가 난다 1234{ &quot;statusCode&quot;: 401, &quot;message&quot;: &quot;Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.&quot;} Ocp-Apim0Subscription-Key가 없어서 그렇다 Ocp-Apim-Subscription-KeyAPI Management의 규격이다 Header에 넣어주면 된다 Ocp-Apim-Trace:c도 넣어야 할까? APIs에 Subscription에 Built-in all-access subscription 항목에 있는 키를 가져오면 된다 Pruduct(제품) 전용 키라고 생각하면 된다. API Management에 GET123curl --location 'https://apimgmt-az01-og084501-dev-ktintelliagent-test-01.azure-api.net/test20250131' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' API Management에 POST123456789101112131415161718192021curl --location 'https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/hello' \\--header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \\--header 'Accept-Language: ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7' \\--header 'Cache-Control: no-cache' \\--header 'Connection: keep-alive' \\--header 'Content-Type: application/x-www-form-urlencoded' \\--header 'Origin: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net' \\--header 'Pragma: no-cache' \\--header 'Referer: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/' \\--header 'Sec-Fetch-Dest: document' \\--header 'Sec-Fetch-Mode: navigate' \\--header 'Sec-Fetch-Site: same-origin' \\--header 'Sec-Fetch-User: ?1' \\--header 'Upgrade-Insecure-Requests: 1' \\--header 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36' \\--header 'sec-ch-ua: &quot;Google Chrome&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;' \\--header 'sec-ch-ua-mobile: ?0' \\--header 'sec-ch-ua-platform: &quot;macOS&quot;' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' \\--data-urlencode 'name=kahlua' 위 헤더들에서 줄여서 아래처럼 보내도 무방하다 1234567curl --location 'https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/hello' \\--header 'Origin: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net' \\--header 'Referer: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' \\--header 'Content-Type: application/x-www-form-urlencoded' \\--data-urlencode 'name=kahlua' Azure API Management(APIM)에 OAuth2.0 인증 설정1. API Management에 OAuth 2.0 인증 설정Azure API Management에서 OAuth 2.0을 사용하여 액세스 토큰을 발급하고 API 요청을 인증하려면, 먼저 OAuth 2.0 제공자를 설정해야 합니다. (1) Azure Active Directory (Azure AD)에서 App 등록 Azure Portal에서 Azure Active Directory &gt; 앱 등록 &gt; 새 등록을 클릭 리디렉션 URI를 https://oauth.pstmn.io/v1/browser-callback 또는 API Management 엔드포인트로 설정 애플리케이션(클라이언트) ID 및 디렉터리(테넌트) ID 저장 (2) 클라이언트 자격 증명 생성 인증서 및 암호 &gt; 새 클라이언트 암호 생성 생성된 클라이언트 암호 값 저장 (3) API 권한 추가 API 권한 &gt; 권한 추가 Microsoft Graph 또는 사용할 API 선택 &gt; Application permissions 또는 Delegated permissions 선택 access_as_user 또는 필요한 권한 부여 2. API Management에서 OAuth 2.0 제공자 등록 Azure API Management 포털로 이동 OAuth 2.0 인증 제공자 &gt; 새 OAuth 2.0 제공자 추가 인증 엔드포인트 및 토큰 엔드포인트 입력 인증 엔드포인트: 1https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/authorize 토큰 엔드포인트: 1https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token 클라이언트 ID 및 클라이언트 암호 입력 권한 부여 방식으로 Authorization Code 또는 Client Credentials 선택 3. API Management 정책 설정 (액세스 토큰 인증)API 요청이 들어올 때 액세스 토큰을 검증하는 정책을 설정해야 합니다. (1) API Management 정책 추가 Azure API Management &gt; APIs &gt; 대상 API 선택 정책 편집기(Inbound Processing)에서 다음 정책 추가 12345678910&lt;inbound&gt; &lt;base /&gt; &lt;validate-jwt header-name=&quot;Authorization&quot; failed-validation-httpcode=&quot;401&quot; failed-validation-error-message=&quot;Invalid token.&quot;&gt; &lt;openid-config url=&quot;https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration&quot; /&gt; &lt;required-claims&gt; &lt;claim name=&quot;aud&quot;&gt;YOUR-CLIENT-ID&lt;/claim&gt; &lt;/required-claims&gt; &lt;/validate-jwt&gt;&lt;/inbound&gt; {tenant_id}: Azure AD 테넌트 ID YOUR-CLIENT-ID: 앱 등록한 클라이언트 ID 4. API 호출 테스트 액세스 토큰 발급 (OAuth 2.0 client_credentials 사용) 123curl -X POST &quot;https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token&quot; \\-H &quot;Content-Type: application/x-www-form-urlencoded&quot; \\-d &quot;grant_type=client_credentials&amp;client_id={client_id}&amp;client_secret={client_secret}&amp;scope=api://{client_id}/.default&quot; 응답 받은 액세스 토큰을 사용하여 API 호출 12curl -X GET &quot;https://{api-management-endpoint}/api/resource&quot; \\-H &quot;Authorization: Bearer {access_token}&quot;","link":"/2025/01/21/Azure-API-Management-Deploy/"},{"title":"Azure-AZ204","text":"MS AZ204 자격증 취득을 위한 공부 과정 및 방법을 기록해보겠다. 시험 신청방법AZ-204 소개 및 접수시험 접수 방법 공부순서 및 공부전략 MS 러닝센터 -&gt; 마이크로소프트 교육 및 실습 -&gt; 덤프문제 풀이 시간을 단축시켜서 공부를 하고 싶다면 마이크로소프트 러닝센터의 AZ204 부분의 대분류와 그 아래 소제목들을 정리한 다음 덤프문제를 먼저 풀고 덤프 문제 풀이를 하면서 대제목 별로 문제들을 구분해서 암기를 해야할 부분을 정리하면서 공부를 하고 그 후에 러닝센터 AZ204의 실습파트를 하나하나씩 실제로 해보면 하루 6시간정도 공부를 한다고 했을 때 2주면 딸수 있지 않을까 하는 행복회로를 돌려본다. 정리하자면 AZ204 대분류 in MS 러닝센터 AZ204 -&gt; dump 문제풀이 -&gt; dump 문제를 대분류에 맞게 노트정리 -&gt; 노트정리한 것 암기 -&gt; 덤프문제 2~3회 풀기 시험문제 출제 비중 Category Ratio Develop Azure Compute Solution 25-30% Develop for Azure Storage 10-15% Implement Azure Security 15-20% Monitor, troubelshoot, and optimize Azure solutions 10-15% Connect to and consume Azure servicies and third-party service 25-30% 시험범위 연습문제 학습자료 연습문제 시험 당일에 대하여 시험시간 : 210분… 매우 기가 빨릴 것으로 보인다. 시험 자료..시크릿노트덤프문제집","link":"/2024/11/05/Azure-AZ204/"},{"title":"Azure EntraID란?","text":"Microsoft Entra ID는 클라우드 기반의 identity and access management 서비스이다. Microsoft Entra ID 란?Entra ID를 사용하면, Microsoft365, Azure portal 및 그 외 수많은 SaaS 어플리케이션과 같은 외부 리소스들에 접근을 승인하고 사용할 수 있게 해준다. 또한, 사용자가 속한 기업의 인트라넷에 있는 내부 리소스들과 사용자의 조직에서 사용하는 다른 클라우드 어플리케이션에도 접근할 수 있도록 해준다.","link":"/2024/11/15/Azure-EntraID/"},{"title":"Azure-API-Management","text":"Azure API Management를 이용하여 QoS, 인증, 서비스 구분, 로깅, Traffic 추적을 고려하여 서비스는 구축하는 방법에 대한 글 QoS란서비스 품질을 관리하고 보장하기 위한 개념, 네트워크와 애플리케이션의 안정성과 성능을 유지하는 데 사용. QoS는 시스템이 다양한 요청을 처리할 때 자원을 효율적으로 배분하여 성능 저하를 방지하고, 중요한 트래픽이 우선적으로 처리되도록 보장한다. 주요 요소 대역폭 관리 (Bandwidth Management) 네트워크 트래픽을 제어하여 특정 서비스나 애플리케이션에 필요한 대역폭 보장 우선순위 설정 (Prioritization) 특정 요청이나 트래픽 유형을 우선 처리 지연 최소화 (Latency Reduction) 요청 및 응답 간의 지연 시간을 최소화 패킷 손실 관리 (Packet Loss Management) 데이터 패킷 손실을 방지하고 신뢰성 높은 데이터 전송을 보장 스루풋 관리 (Throughput Management) 네트워크가 초당 처리할 수 있는 데이터량을 최대화하여 시스템 성능을 최적화 Azure에서 QoS 관련 도구 Azure API Management : API 요청량 제한, 속도 제한, 스로틀링 등을 설정 가능 Azure Traffic Manager : 글로벌 트래픽 라우팅과 서비스 우선순위 설정 Azure Load Balancer : 로드 밸런싱을 통해 네트워크 트래픽 분산 API Management와 App Service를 사용하여 구축하면…고려사항 QoS 인증 서비스 구분 (요청하는 출발지 서버 또는 클라이언트 구분) 로깅 Traffic 추적 API Management의 QoS (Quality of Service) Rate Limiting &amp; Throttling rate-limit-by-key 요청의 속도를 제한하거나 특정 시간 동안의 요청수 제한 가능 서비스의 오버로드를 방지하고 공정한 사용 보장 가능 ‘rate-limit’, ‘quota’ 정책으로 설정 가능 사용자 지정 키 기반, IP 주소 기반, 사용자 ID 기반, 클라이언트 기반 제한 가능 Caching 자주 사용되는 응답 데이터를 캐싱하여 성능을 향상시키고 백엔드 서비스의 부담을 줄인다. 설정된 캐시 기간 동안 동일 요청은 API Management에서 응답 처리 Request/Response Timeout Circuit Breaker 특정 서비스가 불안정하거나 응답 시간이 길 경우, 요청을 차단하고 대체 응답을 제공할 수 있다. Circuit Breaker가 작동하면 API Management는 정의된 시간동안 백엔드 서비스로의 요청을 전송하는 것을 중단하고 클라이언트에게 503 서비스를 사용할 수 없음을 반환 Bicep 예시1234567891011121314151617181920212223242526272829 resource symbolicname 'Microsoft.ApiManagement/service/backends@2023-09-01-preview' = { name: 'myAPIM/myBackend' properties: { url: 'https://mybackend.com' protocol: 'http' circuitBreaker: { rules: [ { failureCondition: { count: 3 errorReasons: [ 'Server errors' ] interval: 'PT1H' statusCodeRanges: [ { min: 500 max: 599 } ] } name: 'myBreakerRule' tripDuration: 'PT1H' acceptRetryAfter: true } ] } }} count : 실패로 간주되는 요청의 횟수, 3연속 실패 시 Circuit Breaker 작동 errorReasons : 실패로 간주할 에러 이유 목록, ‘Server errors’는 서버 쪽 에러 (HTTP 5xx)가 원인임을 나타냄 interval : 실패 조건을 평가하는 시간 간격, 1시간 (PT1H) 동안 요청 실패를 기준으로 계산 statusCodeRanges : 실패로 간주할 HTTP 상태 코드 범위, 500~599 상태 코드는 서버 에러로 간주 Circuit Breaker 동작 요약 트리거 조건: 지난 1시간 동안 500-599 상태 코드로 인해 3번의 실패가 발생했을 경우. 작동 방식: Circuit Breaker가 활성화되면 모든 요청을 차단하고 클라이언트에게 즉시 응답을 반환 차단 기간: 1시간 동안(PT1H) 요청 차단. 백엔드 재시도: 백엔드가 Retry-After 헤더를 보내는 경우, 해당 시점까지 재시도를 연기 인증 및 권한 관리OAuth 2.0은 웹 API와 같은 리소스에 대한 액세스를 보호하는 데 널리 사용되는 표준 권한 부여 프레임워크이다. OAuth 2.0은 사용자의 자격 증명을 공유하지 않고도 클라이언트 앱이 사용자를 대신하여 리소스에서 수행할 수 있는 작업을 제한한다. OAuth 2.0은 인증 프로토콜은 아니지만 사용자 인증 및 SSO 기능을 제공하여 OAuth 2.0을 확장하는 OIDC(OpenID Connect)와 함께 사용되는 경우가 많다 OAuth 흐름 클라이언트는 ID 공급자에 대한 자격 증명을 사용하여 인증 클라이언트는 ID 공급자의 권한 부여 서버에서 시간이 제한된 액세스 토큰(JSON 웹 토큰 또는 JWT)를 가져온다 ID 공급자(예: MS Entra ID)는 토큰의 발급자이며 토큰에는 리소스 서버(예: 백엔드 API 또는 API Management 게이트웨이 자체)에 대한 액세스를 승인하는 대상 그룹 클레임이 포함된다 클라이언트는 API를 호출하고 액세스 토큰을 제공한다 (예: 권한 부여 헤더) 리소스 서버는 엑세스 토큰의 유효성을 검사한다. 유효성 검사는 발급자 및 대상 그룹 클레임에 예상 값이 포함되어 있는지 유효서을 검사하는 복잡한 프로세스이다. 토큰 유효성 검사 조건에 따라 백엔드 API의 리소스에 대한 액세스 권한이 부여된다. 시나리오 예시 : 클라이언트 앱이 API Management에 직접 권한을 부여 API Management 서비스는 API를 대신하여 작동 액세스 토큰의 범위는 호출 애플리케이션과 API Management 게이트웨이 사이 API Management에서 게이트웨이가 요청을 백엔드에 전달하기 전에 토큰의 유효성을 검사하도록 정책(validate-jwt 또는 validate-azure-ad-token)을 구성 MS Entra ID는 권한 부여 공급자 ReferencesAzure API Management의 API에 대한 인증 및 권한 부여 Microsoft ID 플랫폼 앱 형식 및 인증 흐름 OAuth 2.0 및 OpenID Connect : Azure AD, Facebook, Google 등과 연동하여 인증 및 토큰 기반 권한 관리를 지원 OAuth 2.0 API Key 인증 : 각 클라이언트가 고유한 API 키로 인증을 수행할 수 있다. IP 제한 : 특정 IP 주소 또는 범위에서만 접근 가능하도록 설정 가능 서비스 구분 Route 설정: API Gateway에서 경로(Route)를 정의하여 특정 요청이 해당 App Service로 전달되도록 설정 가능. Versioning: API Gateway에서 버전 관리를 통해 클라이언트가 원하는 API 버전을 호출할 수 있도록 지원. Multi-Backend 지원: 하나의 API Gateway를 통해 여러 App Service나 Azure Function을 서비스 구분 없이 연결할 수 있다. Azure API Management의 가시성(?) 도구 유용한 분야 데이터 지연 보존 샘플링 데이터 종류 지원되는 배포 모델 API 검사기 테스트 및 디버깅 인스턴트 마지막 100개 추적 요청에 따라 튜닝됨 요청 추적 관리형, 자체 호스팅, Azure Arc, 작업 영역 기본 제공 분석 보고 및 모니터링 분 수명 100% 보고서 및 로그 관리형 Azure Monitor 메트릭 보고 및 모니터링 분 90일(연장하려면 업그레이드) 100% 메트릭 관리, 자체 호스팅2, Azure Arc Azure Monitor 로그 보고, 모니터링, 디버그 분 31일/5GB(연장하려면 업그레이드) 100%(조정 가능) 로그 관리1, 자체 호스팅3, Azure Arc3 Azure Application Insights 보고, 모니터링, 디버그 초 90일/5GB(연장하려면 업그레이드) 사용자 지정 로그, 메트릭 관리형1, 자체 호스팅1, Azure Arc1, 작업 영역1 Azure Event Hubs를 통해 로깅 사용자 지정 시나리오 초 관리되는 사용자 사용자 지정 사용자 지정 관리1, 자체 호스팅1, Azure Arc1 OpenTelemetry 모니터링 분 관리되는 사용자 100% 메트릭 Self-hosted2 References : https://learn.microsoft.com/ko-kr/azure/api-management/observability API Management 모니터링서비스를 모니터링하기 위해 데이터를 수집하는 방법과 수집된 데이터로 수행할 수 있는 작업 수집할 데이터 설명 데이터를 수집하고 라우팅하는 방법 데이터를 볼 수 있는 위치 지원되는 데이터 메트릭 데이터 메트릭은 시간상 특정 지점에서 시스템의 측면을 설명하는 숫자 값입니다. 메트릭은 다른 메트릭과 비교하여 알고리즘을 사용하여 집계하고 시간에 따른 추세를 분석할 수 있습니다. - 정기적으로 자동으로 수집됩니다.- 일부 플랫폼 메트릭을 Log Analytics 작업 영역으로 라우팅하여 다른 데이터를 쿼리할 수 있습니다. 각 메트릭에 대한 DS 내보내기 설정을 확인하여 진단 설정을 사용하여 메트릭 데이터를 라우팅할 수 있는지 확인합니다. 메트릭 탐색기 Azure Monitor에서 지원하는 Azure API Management 메트릭 리소스 로그 데이터 로그는 타임스탬프를 사용하여 기록된 시스템 이벤트입니다. 로그는 다양한 형식의 데이터를 포함할 수 있으며 구조화되거나 자유 형식의 텍스트일 수 있습니다. 쿼리 및 분석을 위해 리소스 로그 데이터를 Log Analytics 작업 영역으로 라우팅할 수 있습니다. 리소스 로그 데이터를 수집하고 라우팅하는 진단 설정을 만듭니다. Log Analytics Azure Monitor에서 지원하는 Azure API Management 리소스 로그 데이터 활동 로그 데이터 Azure Monitor 활동 로그는 구독 수준 이벤트에 대한 인사이트를 제공합니다. 활동 로그에는 리소스가 수정되거나 가상 머신이 시작될 때와 같은 정보가 포함됩니다. - 자동으로 수집됩니다.- 무료로 Log Analytics 작업 영역에 대한 진단 설정을 만듭니다. 활동 로그 API Management에 대한 기본 제공 모니터링Azure API Management API 분석 사용 API에 대한 분석을 제공하므로, API의 사용량과 성능을 분석할 수 있다. 분석을 사용하여 API에 대한 개괄적인 모니터링과 문제 해결을 수행할 수 있다. API Management는 Azure Monitor 기반 대시보드를 사용하여 분석을 제공 대시보드는 Azure Log Analytics 작업 영역의 데이터를 집계한다. API Management REST API를 사용하여 분석 데이터에 엑세스할 수 있다. Azure Monitor 기반 대시보드와 기본 제공 분석에도 매우 유사한 데이터가 표시된다. Grafana 대시보드를 사용한 API Management 모니터링 데이터 시각화Azure Managed Grafana를 사용하여 Log Analytics 작업 영역에 수집된 API Management 모니터링 데이터를 시각화할 수 있다. References https://learn.microsoft.com/ko-kr/azure/api-management/monitor-api-management Azure Managed Grafana란? References API Management 모니터링 게시된 API 모니터링 로깅Application Insights API Gateway와 App Service 양쪽에서 Application Insights를 통해 요청/응답 로깅, 에러 추적, 성능 모니터링 등을 설정할 수 있다. Azure Portal, REST API 또는 관련 Azure 도구를 사용하여 Application Insights와 API Management 간의 연결을 만들 수 있다. (API Management는 연결을 위한 로거 리소스 구성) Azure Monitor요청 수, 대기 시간, 성공/실패 상태 코드 등 다양한 로그를 실시간으로 수집하고 대시보드로 시각화 가능. Custom Logging필요에 따라 로그를 Blob Storage나 Event Hub로 전달해 장기 보관 및 분석을 할 수 있다. Slack에 경고 Azure API Management 서비스에서 외부 서비스 사용 외부 서비스가 일종의 중요한 이벤트 알림을 받을 수 있도록 설정 가능 조건을 만족하는 경우 send-one-way-request 정책을 사용하여 외부 HTTP 요청을 만들 수 있다. Hipchat 및 Slack, SendGrid 및 MailChimp와 같은 메일 API, PagerDuty와 같은 중요 자원 인시던트 가능 예시) HTTP 응답 상태 코드가 500 이상인 경우 Slack 대화방에 메시지를 보내는 방법 12345678910111213141516171819202122&lt;choose&gt; &lt;when condition=&quot;@(context.Response.StatusCode &gt;= 500)&quot;&gt; &lt;send-one-way-request mode=&quot;new&quot;&gt; &lt;set-url&gt;https://hooks.slack.com/services/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&lt;/set-url&gt; &lt;set-method&gt;POST&lt;/set-method&gt; &lt;set-body&gt;@{ return new JObject( new JProperty(&quot;username&quot;,&quot;APIM Alert&quot;), new JProperty(&quot;icon_emoji&quot;, &quot;:ghost:&quot;), new JProperty(&quot;text&quot;, String.Format(&quot;{0} {1}\\nHost: {2}\\n{3} {4}\\n User: {5}&quot;, context.Request.Method, context.Request.Url.Path + context.Request.Url.QueryString, context.Request.Url.Host, context.Response.StatusCode, context.Response.StatusReason, context.User.Email )) ).ToString(); }&lt;/set-body&gt; &lt;/send-one-way-request&gt; &lt;/when&gt;&lt;/choose&gt; ReferencesAzure Application Insights와 Azure API Management를 통합하는 방법 Event Hubs에 이벤트 기록References Azure API Management에서 Azure Event Hubs에 이벤트를 기록하는 방법 트래픽 추적 Distributed Tracing: Application Insights와 통합하여 API Gateway부터 App Service까지 트랜잭션을 추적 가능하다. Integration with Azure Front Door: 전 세계적으로 사용자 트래픽을 라우팅하고, 실시간 요청의 경로를 추적한다. Latency Monitoring: 각 구간의 응답 시간 및 병목현상을 모니터링할 수 있다.","link":"/2025/01/14/Azure-API-Management/"},{"title":"Azure-EventHub","text":"EventHub는 짧은 대기 시간으로 초당 수백만개의 이벤트를 스트리밍할 수 있는 클라우드의 네이티브 데이터 스트리밍 서비스이다. Azure Stream Analytics를 사용하여 실시간 인사이트를 생성하여 이벤트 허브에서 데이터를 처리 Azure Data Explorer를 사용하여 스트리밍 데이터를 분석하고 탐색. Azure Event Hubs의 Apache Kafka AMQP(고급 메시지 큐 프롵토콜), Apache Kafka 및 HTTPS 프로토콜을 기본적으로 지원하는 다중 프로토콜 이벤트 스트리밍 엔진 Apache Kafka 용 Azure Event Hubs Event Hubs의 스키마 레지스트리 Event Hubs의 Azure 스키마 레지스트리는 이벤트 스트리밍 애플리케이션의 스키마를 관리하기 위한 중앙 집중식 리포지토리를 제공 스키마 레지스트리는 이벤트 생산자 및 소비자 간에 데이터 호환성 및 일관성을 보장. 스키마 레지스트리는 기존 Kafka 애플리케이션과 통합되며 Avro 및 JSON 를 비롯한 여러 스키마 형식을 지원한다 ReferenceEvent Hub의 Azure 스키마 레지스트리 Stream Analytics를 사용하여 스트리밍 이벤트 실시간 처리 Azure Steam Analytics와 통합하여 실시간 스트림 처리를 지원 SQL 기반 Stream Analytics 쿼리 언어를 사용하여 실시간 스트림 처리를 수행하고 스트리밍 데이터를 분석하기 위한 다양한 함수를 활용할 수 있다. Azure Stream Analytics 통합 섹션 Azure Data Explorer를 사용하여 스트리밍 데이터 탐색 거의 실시간으로 대량의 데이터를 분석할 수 있는 빅 데이터 분석을 위한 완전 관리형 플랫폼 Azure Data Explorer와 통합하면 스트리밍 데이터의 거의 실시간 분석 및 탐색 수행 가능 Event Hub에 데이터 연결 작동 방식Event Hubs는 이벤트 소비자와 이벤트 생산자를 분리하는 시간 보존 버퍼가 있는 통합 이벤트 스트리밍 플랫폼을 제공 Event Hubs의 주요 기능 구성 요소 생산자 애플리케이션: Event Hubs SDK 또는 Kafka 생산자 클라이언트를 사용하여 이벤트 허브에 데이터를 수집. 네임스페이스: 하나 이상의 이벤트 허브 또는 Kafka 토픽에 대한 관리 컨테이너.스트리밍 용량 할당, 네트워크 보안 구성, 지역 재해 복구 사용과 같은 관리 작업이 네임스페이스 수준에서 처리. Event Hubs/Kafka 토픽: Event Hubs에서 이벤트를 이벤트 허브 또는 Kafka 토픽으로 구성할 수 있다. 이는 하나 이상의 파티션을 구성할 수 있는 추가 전용 분산 로그이다. 파티션: 이벤트 허브의 크기를 조정하는 데 사용. 파티션은 고속도로의 차선 같다. 스트리밍 처리량이 더 필요한 경우 파티션을 더 추가할 수 있다. 소비자 애플리케이션: 이 애플리케이션은 이벤트 로그를 검색하고 소비자 오프셋을 유지 관리하여 데이터를 사용할 수 있다. 소비자는 Kafka 소비자 클라이언트 또는 Event Hubs SDK 클라이언트일 수 있다. 소비자 그룹: 이 논리적 소비자 인스턴스 그룹은 이벤트 허브 또는 Kafka 토픽에서 데이터를 읽는다. 이를 통해 여러 소비자가 각자의 속도와 자체 오프셋을 사용하여 이벤트 허브에서 동일한 스트리밍 데이터를 독립적으로 읽을 수 있다. 장점 확장성: 대량의 메시지를 처리할 수 있어 고부하 시스템에 적합. 내구성: 메시지를 안정적으로 저장하고, 소비자가 언제든 가져갈 수 있음. 분산 처리: 메시지 파티션을 활용해 다중 소비자가 병렬로 처리 가능. 통합 가능성: Azure 생태계(예: Azure Functions, Logic Apps)와 손쉽게 통합. 단점 실시간성: WebSocket이나 gRPC에 비해 레이턴시가 클 수 있음. 양방향 통신 부족: 주로 Publisher-Subscriber 모델로 동작하며, 클라이언트가 서버로 메시지를 보낼 때 직접적인 스트리밍 채널을 제공하지 않음. 초기 설정 복잡성: Event Hub 설정 및 인증 관리가 비교적 복잡. 적합한 사용 사례 높은 부하(대량의 사용자 또는 메시지)를 처리해야 하며, 메시지 손실을 최소화해야 하는 시스템. 비동기적으로 LLM 응답을 처리하는 애플리케이션. 데이터 분석 또는 아카이빙이 필요한 서비스 Azure Event Hubs: 네이티브 Apache Kafka 지원을 사용하는 실시간 데이터 스트리밍 플랫폼 Azure Eventhub에 있는 정보를 클라이언트가 바로 가져가는 것도 가능한가?Azure Event Hubs에서 정보를 클라이언트가 바로 가져가는 방식은 기본적으로 Event Hubs의 동작 방식과 설계에 따라 제한적. Event Hubs는 데이터 수집 및 분산 처리를 위해 설계된 이벤트 스트림 플랫폼으로, 직접 접근보다는 소비자 그룹(Consumer Group)을 통해 데이터를 처리하는 것이 일반적이다. 직접 클라이언트 접근 가능 여부기본적으로 불가능 Event Hubs는 메시지 브로커 역할을 하며, 데이터를 생산자(Producer)와 소비자(Consumer) 간에 비동기적으로 전달한다. 클라이언트가 Event Hubs에 직접 접근해서 데이터를 가져가도록 설계되어 있지 않다. Event Hubs는 데이터를 Offset 및 Partition 단위로 관리하여 소비자가 순서에 따라 데이터를 처리하도록 설계됨. 클라이언트가 바로 가져가려면 Event Hubs의 읽기 API를 구현해야 하고, 이 과정에서 메시지 순서나 Offset 관리가 필요. 가능한 대안 및 설계 방안1. Azure Functions or Logic Apps를 사용한 중계 Event Hubs의 데이터를 트리거로 읽고, 이를 클라이언트에게 전달하는 REST API 또는 WebSocket 인터페이스를 만든다. 클라이언트는 Event Hubs 대신 중계 서버를 통해 데이터를 요청. 2. Azure Stream Analytics와 실시간 데이터 제공 Event Hubs 데이터를 실시간으로 처리하여 결과를 클라이언트가 사용할 수 있도록 데이터베이스나 다른 서비스에 저장. 예: 처리된 데이터를 Azure Cosmos DB, SQL Database 또는 Blob Storage에 저장. 3. Azure SignalR Service와 WebSocket 통합 Event Hubs 데이터를 소비자가 실시간으로 처리한 뒤, SignalR을 사용해 데이터를 클라이언트로 푸시. SignalR은 클라이언트와 지속적인 WebSocket 연결을 유지하여 실시간 데이터 전송을 가능하게 한다. 4. Event Processor Host (SDK 기반 소비) 클라이언트가 직접 Event Hubs의 데이터를 가져가려면 Event Hubs SDK를 활용하여 Consumer로 동작. 이 경우, 클라이언트는 Event Hubs에 Consumer Group으로 등록되고, 데이터를 처리해야 한다. 하지만 이는 클라이언트가 직접 Offset 관리와 같은 복잡성을 처리해야 하므로 일반적으로 권장되지 않음. 추천 아키텍처 Event Hubs → Azure Functions → REST API Event Hubs 데이터를 Azure Functions로 처리하고, REST API로 제공. 클라이언트는 REST API를 통해 데이터를 요청. Event Hubs → Stream Analytics → Cosmos DB → 클라이언트 Stream Analytics로 데이터를 처리한 뒤, 클라이언트가 저장된 데이터를 DB에서 조회. Event Hubs → SignalR Service → WebSocket 클라이언트 실시간 데이터 제공이 필요한 경우 SignalR을 사용해 WebSocket으로 전송. 결론Event Hubs는 클라이언트가 직접 접근해서 데이터를 가져가도록 설계되지 않았다. 클라이언트가 데이터를 직접 요청하거나 실시간으로 수신해야 하는 경우, 중간 계층을 설계하는 것이 최적의 방법이다.중계 서버나 다른 Azure 서비스를 활용하여 데이터를 안정적으로 제공하도록 아키텍처를 구성하는 것을 권장한다. 설계 개요 채팅 응답 발행 (Producer) → Agent가 Event Hub로 응답 전송 채팅 응답 수신 (Consumer 서버) → Event Hub에서 메시지 수신 후 클라이언트로 HTTP 요청 전송 클라이언트로 응답 전달 → HTTP API 주요 기술 스택 역할 기술 설명 Producer FastAPI Agent 응답을 Event Hub에 발행 Event Hub Azure Event Hub 메시지 브로커 역할 Consumer Legacy 서버 Event Hub에서 메시지 읽기 클라이언트 응답 HTTP API 클라이언트에 직접 전송 상세 흐름 (예시)채팅 응답 발행 (Producer)12345678910from azure.eventhub import EventHubProducerClient, EventData# EVENT_HUB_CONNECTION_STRING : Azure Event Hub 인스턴스에 연결할 때 필요한 인증 정보producer = EventHubProducerClient.from_connection_string( &quot;EVENT_HUB_CONNECTION_STRING&quot;, eventhub_name=&quot;chat-responses&quot;)event_data = EventData('{&quot;user_id&quot;: &quot;123&quot;, &quot;message&quot;: &quot;Hello!&quot;}')producer.send_batch([event_data])producer.close() 채팅 응답 수신 (Consumer, Legacy 서버)서버에서 Event Hub 메시지를 읽고 즉시 클라이언트로 HTTP 요청 전송 123456789101112131415161718192021222324252627282930import asyncioimport aiohttpfrom azure.eventhub.aio import EventHubConsumerClientimport jsonasync def send_to_client(user_id, message): async with aiohttp.ClientSession() as session: async with session.post(f&quot;http://client-server.com/chat/{user_id}&quot;, json={&quot;message&quot;: message}) as resp: print(f&quot;Sent to client {user_id}, status: {resp.status}&quot;)async def process_event(event): data = json.loads(event.body_as_str()) user_id = data[&quot;user_id&quot;] message = data[&quot;message&quot;] # HTTP API로 클라이언트에 전송 await send_to_client(user_id, message)async def on_event(partition_context, event): await process_event(event) await partition_context.update_checkpoint(event)async def main(): client = EventHubConsumerClient.from_connection_string( &quot;EVENT_HUB_CONNECTION_STRING&quot;, consumer_group=&quot;$Default&quot;, eventhub_name=&quot;chat-responses&quot; ) async with client: await client.receive(on_event=on_event, starting_position=&quot;-1&quot;)asyncio.run(main()) user_id 또는 transaction_ID 등을 이용하여 클라이언트 구분 로직 필요 특징장점 구현이 단순: Redis와 WebSocket이 필요 없음 실시간 전송 가능: 서버에서 바로 클라이언트로 HTTP 요청 전송 확장성 높음: 여러 Consumer 서버가 메시지를 처리 가능 단점 클라이언트가 항상 온라인이어야 함 → 클라이언트가 오프라인이면 메시지 손실 가능 결론 클라이언트가 항상 온라인 상태라면 WebSocket 없이도 구현 가능 클라이언트가 메시지를 놓치지 않으려면 Redis나 DB로 클라이언트 별 메시지를 저장하고 있어야 함 (Legacy 서버) 출처1. Azure Event Hubs 📄 공식 문서: Azure Event Hubs 개요 Event Hubs Python SDK 사용법 📦 GitHub (Azure SDK for Python) Azure SDK for Python (eventhub) 2. Python을 사용한 Event Hub 소비자 (Consumer) 구현 Azure Event Hub에서 메시지 소비 (Python) Azure Event Hub Consumer Client 3. 기타 Azure 관련 문서 (배포 및 운영) Azure Monitor 및 Application Insights Azure Kubernetes Service(AKS)로 Event Hub 배포 Azure Event Hub Pub/Sub 하기 위한 인증 방법 이벤트허브 만들기","link":"/2025/01/28/Azure-EventHub/"},{"title":"PostgreSQL-pgvector를 AWS Aurora에서 사용하기","text":"AWS의 Aurora를 이용하여 pgvector를 설치하고 사용하겠다. RDS 대시보드에서 데이터베이스 생성 표준 생성, Aurora (PostgreSQL Compatible), Aurora PostgreSQL (Compatible with PostgreSQL 15.4 - 메이저 버전 15의 기본겂)을 선택. 기본값을 사용하기 위해 프로덕션 템플릿을 선택 마스터 사용자 이름은 postgres로, 자격 증명 관리는 AWS Secrets Manager에서 관리로 선택. 암호화 키는 기본값인 aws/secretsmanager로 선택. 구성 옵션도 기본값인 Aurora Standard. 인스턴스는 메모리 최적화 클래스(r 클래스 포함), db.r5.large로 선택. 다중 AZ 배포 (다른 AZ에 Aurora 복제본/리더 노드 생성)/ 연결은 EC2 컴퓨팅 리소스에 연결, 네트워크는 IPv4 VPC 보안 그룹은 기존 항목 선택 (default). 예상되는 월 사용 금액은 255.62 USD ElasticCache 클러스터나 RDS 프록시가 필요할까?","link":"/2024/07/31/Building-GenAI-Apps-with-AWS-Aurora-PostgreSQL-pgvector/"},{"title":"Azure WebApp(App Service)으로 API 배포하기","text":"요즘 AWS/Azure/GCP 등 클라우드 환경의 일반화로 굳이 서버/네트워크 장비 등 자체구매/운영 (온프레미스) 하지 않아도 된다. 그렇기에 최소한의 노력을 들여 PaaS 서비스 또는 Container를 지원하는 서비스를 사용하여 쉽게 배포/운영 할 수 있다. Azure에서는 App Service라는 웹 어플리케이션 용 PaaS를 제공한다. Java, Node.JS, Python 등 대부분의 웹프레임워크를 지원한다. 이 서비스를 이용하면 굳이 클라우드 환경에서 WEB/WAS/DB 등 서버를 별도 생성/구축하지 않고 웹 서비스를 구축할 수 있다. 클라우드 환경에서 VM을 활용해서 구축한다면, VM을 생성하고, OS 설치/운영하고, OS에 필요 라이브러리를 설치/구동하고, 개발 모듈을 올려서 연동하는 등 다수의 구축작업을 해야한다. Azure App Service, Azure Functions을 활용하면 이러한 부분을 간소화해서 구축/운영할 수 있다. 0. Azure 웹앱의 동작 원리Azure 웹앱은 기본적으로 Docker Container로 구동한다. 내부적으로 Oryx라는 Container 빌드 패키지를 사용한다. Oryx 상세 파라미터 Wep App 배포하기다음과 같은 순서로 진행하도록 한다. Python Web Source Code 확보 Azure App Service 생성 Source Code 배포 (Azure Upload) 구동 확인 Azure에서 제공하는 Sample을 Github에서 다운로드 받아 사용하도록 한다. 아래는 Python Web Framework인 Flask의 Hello World이다.MS Flask Web App 12$ git clone https://github.com/Azure-Samples/msdocs-python-flask-webapp-quickstart$ cd msdocs-python-fastapi-quickstart (소스코드 동작 확인) 로컬에서 정상 동작하는지 일단 확인을 해 보자.1234$ python3 -m venv .venv$ source .venv/bin/activate$ pip install -r requirements.txt$ flask run 1. Azure App Service 생성/설정Azure App Servie를 생성해준다. App Service를 생성하는 방법은 Azure Portal Azure CLI VS Code Azure SDK 필자는 Azure Portal에서 하곘다. Azure Portal 로그인 후 좌측 상단 메뉴에서 “리소스 만들기”를 클릭한다. 그 중에서 웹 앱을 클릭한다. 웹앱 만들기에서 설정이 필요한 부분들은 아래와 같다. 구독/리소스그룹 : 본인의 구독과 리소스 그룹을 선택 or 생성해 주면 된다. 웹앱 이름 : 겹치지 않도록 이름을 설정하자. 예) “azure-webapp-python-test” 이 이름은 웹앱 생성후, “https://&lt;웹앱이름&gt;.azurewebsites.net”로 azure 기본 제공하는 subdomain name이 된다. 게시 방식 : 코드, docker 컨테이너, 정적웹앱. 우리는 가장 단순하게 “코드”를 선택하도록 하자. 런타임 스택 : Python 3.9를 선택. 다른버전 선택도 가능하다. 운영체제 : Linux. (Windows를 선택해도 무방하나, 본 포스팅은 Linux를 기반으로 진행한다.) 지역 : 본인이 편한 위치를 선택하면 된다. 가격정책플랜 : 우리는 테스트 해보는 것이기 때문에 기본으로 사용하자. “검토 + 만들기 “ 버튼을 누른다. 유효성이 검토되면 설정에 대한 요약페이지가 출력되고, 최종적으로 하단에 “만들기” 버튼을 또 한번 눌러 주어야 한다. 이후. “…배포 진행 중”이라는 페이지가 출력 될 것이다. 2~3분 기다리면 “배포 성공” 메세지가 출력된다. “리소스 이동” 버튼을 눌러준다. 웹앱이 생성 완료되면, “리소스로 이동”하여 웹앱의 왼쪽 메뉴중 “개요”에서 전반적인 웹앱의 정보를 확인한다.우측 상단의 FTP/FTPS 등의 정보는, 배포 시 사용하는데, 민감정보이므로 노출되지 않도록 주의한다. 배포좌측에 “배포 센터”로 이동한다. 실제 상용에서는 Stage -&gt; Production으로 나누어서 지속배포(CI/CD)체계를 구축하나, 이 포스팅에서는 단순화하여 “로컬 Git”을 사용하여 배포하겠다. “소스 &gt; 로컬 Git”을 선택해준다. “저장” 버튼을 클릭해야 적용이 된다. 저장이 완료되면 “Git Clone URI” 정보가 출력된다. 이 URI를 통해 “git push”를 실행하므로 URI를 복사해 두도록한다. 여기서 중요한점 중 하나는 사용자 이름이다. 사용자 이름은 기본적으로 다음과 같이 표기되어 있다 &lt;웹앱 이름&gt;$&lt;웹앱 이름&gt;실제 Git Push 할 때 인증은 “$&lt;웹앱 이름&gt;” 부분만 사용된다. 예를 들어 표기된 사용자 이름이 “flask-test$flask-test”라면, 실제 사용되는 부분은 $를 포함한 “$flask-test” 부분이 해당된다. 이부분을 잘못 입력하면 인증실패로 git push가 실행되지 않는다. 패스워드의 경우 전체를 복사하여 사용하면 된다. 이제 Azure Portal에서 설정할 내용은 완료하였다. 다음은 Terminal 에서 Git push를 진행하면된다. 본 포스팅에서는 별도 빌드 또는 추가 설정이 필요없기 때문에 “설정/구성”을 추가로 진행하지 않았다. 그러나, 상세설정 들이 필요하다면 아래처럼 “구성” 메뉴에서 “application 설정”, “시작 명령어” 등을 상세 지정할 수 있다. “구성&gt;애플리케이션 설정”은 Application 내부에서 사용할 별도의 설정을 지정할 수 있다. 위와 같이 하면 FTP 기본 인증 게시가 비활성화되어 있다고 에러가 난다. 아래와 같이 바꿔준다. 설정 저장이 완료되면, 아래와 같이 페이지에 “Git Clone URI” 정보가 출력된다. 이 URI로 우리는 ‘git push’를 실행하게 된다. git push를 할 때 인증정보는 “FTP 자격증명” 메뉴에서 확인할 수 있다. 해당 탭에서 Git Clone URI를 다시 확인할 수 있으며, 사용자 이름/암호를 확인할 수 있다. 향후에는 “게시 프로필 관리”를 통해 인증이 관리되어야 한다. Local Git 소스 배포소스코드 폴더로 이동하여 Git Push를 진행하도록 하자. 이 때 는 Azure Portal의 “Local Git 자격증명”에서 확인한 Git Clone URI이다. 사용자명/패스워드는 이전 절에서 거론했던 내용을 사용한다. ztna를 끄고 push를 진행해야 한다. 12$ git remote add azure &lt;GIT URI&gt;$ git push -u azure main:master 이제 소스코드 배포를 완료 했다. 앞서 거론했든이 Python은 별다른 빌드를 하지 않기 때문에, 별다른 설정파일 없이 웹앱을 생성할 수 있다. 이제 웹앱의 “개요”페이지에서 확인할 수 있는 웹앱 URL을 확인하고, 웹브라우저로 접속해 보기 바란다. Reference[Azure] Azure 웹앱(App Service)으로 API 서버 만들기 - 1","link":"/2025/01/07/Azure-WebApp-Sample-Deploy/"},{"title":"Bitcoin Lightweight Clients Privacy using Trusted Execution","text":"이 포스트는 28th Usenix Security Symposium의 BITE: Bitcoin Lightweight Clients Privacy usin Trusted Execution을 바탕으로 작성되었습니다. Heavily used ~ 4.1 Transactions/sec ~ 360k Transactions/day Problems Client requirements. Clients need to download and process entire chain (~ 230GB) Participating in the P2P network carries high communication overhead. Partial Anonymity achieved through pseudonymity. Using mobile clients for transaction confirmation is infeasible. Full reliance on the full node that stores the entire chain. Light client stores only block headers, all other information is requested from the full node. Fully breaks privacy. Strawman solutions Bitcoin supports Simplified Payment Verification (SPV) - Works, but sharing the addresses breaks privacy. Use the same approach with Bloom Filters? - Sharing the filters still breaks privacy. Share addresses with a TEE (SGX enclave)? - Better, but enclaves leak and privacy is still a problem. Send also the private key to the full node? - If enclave compromised, client looses all money. Trusted Execution Environments Enable isolated execution within a user’s system. Secure, integrity-protected environment. Provides processing, memory, and storage capabilities. Smart cards, TPM, ARM Trustzone, Keystone, etc. Intel SGX. Intel SGX (Software Guard Extensions). Intel’s architecture containingnew instructions, protective mechanisms, and key material in the CPU. Runtime isolation, sealing, attestation. Memory content encrypted Solution2 - Oblivious Database 이 논문을 읽고… 사용자 개인 정보 보호는 Bitcoin과 같은 분산 통화의 주요 목표 중 하나이다. 그러나 결제 확인에는 전체 체인을 다운로드하여 처리해야하므로 대부분의 모바일 클라이언트에서는 불가능하다.따라서 모든 인기 있는 블록 체인은 경량 클라이언트가 전체 노드의 도움으로 트랜잭션을 확인할 수 있는 단순화 된 검증 모드를 지원한다.불행히도, 그러한 지불 확인은 사용자 개인 정보를 보존하지 않으므로 비트 코인과 같은 시스템을 사용하는 주요 이점 중 하나를 무효화 한다. 이 논문에서는 신뢰할 수 있는 execution을 사용하여 경량 클라이언트의 개인 정보를 개선하기 위한 새로운 접근 방식을 제안했다.이 논문은 그들의 솔루션이 강력한 개인 정보 보호를 제공하고 현재 경량 클라이언트의 성능을 추가로 향상시킨다고 한다. BITE는 모바일 장치와 같은 가벼운 클라이언트의 개인 정보를 보호하는 실용적인 솔루션이라고 생각한다.","link":"/2020/04/19/BITE/"},{"title":"Batch Server 구성하기","text":"로그에 쌓여있는 대화 데이터들을 일정한 주기마다 읽어(아마 InstanceID 기준)와 요약하여 (프롬프트 이용) Redis나 NAS와 같은 저장소에 저장할 수 있도록 배치 서버를 구성하는데 필요한 Batch Server 기술 스택을 조사한다. 배치 애플리케이션이란?배치(Batch)는 일괄처리이다. 매일 전날의 데이터를 집계 해야한다고 가정할 때처럼 큰 데이터를 읽고, 가공하고, 저장한다면 해당 서버는 순식간에 CPU, I/O 등의 자원을 다 써버려서 다른 Request를 처리하지 못하게 됨. 또한 이런 집계 기능은 자주 사용되지 않음, 그러므로 이를 위해 API를 구성하는 것은 낭비임. 추가로 데이터가 너무 많아서 처리중에 실패가 난다면, 5만번째에서 실패했다면, 5만 1번째부터 다시 실행할 수 있다면 좋음 또, 같은 파라미터로 같은 함수를 실행할 경우 이미 실행한 적 있어 실패하는 기능을 지원하면 좋음이럴때 단발성으로 대용량의 데이터를 처리하는 애플리케이션이 배치 애플리케이션. Batch Server 구성 기술스택Java Spring + CronJob 프로젝트 구성 Java: 데이터를 읽고 가공하는 로직을 작성. Spring: Spring Boot를 사용하여 간편하게 프로젝트를 구성하고, 배치 작업을 위한 스케줄링. Redis: Java에서 Redis와 상호작용하기 위해 Redis 클라이언트를 사용. CronJob: 주기적으로 Java 프로그램을 실행하기 위한 크론 작업을 설정. Redis 클라이언트 Java에서 Redis에 연결하기 위해 다음과 같은 Redis 클라이언트를 사용: Jedis: 간단하게 사용 가능한 Redis 클라이언트. Lettuce: 비동기 작업과 고성능을 요구할 때 사용하는 클라이언트. CronJob 설정 먼저 리눅스의 crontab에 Java 프로그램을 실행하도록 설정. 이를 위해 .jar 파일을 실행하는 명령어를 설정. 예를 들어, Java 프로젝트를 Maven이나 Gradle로 빌드한 후 log_to_redis.jar 파일을 얻었다고 가정하면, 다음과 같이 설정 가능: 1crontab -e 그리고 crontab에 다음과 같이 입력하여 매일 특정 시간마다 Java 애플리케이션을 실행하도록 설정 10 * * * * java -jar /path/to/log_to_redis.jar 위 명령은 매일 정각에 log_to_redis.jar를 실행 Java 코드 작성 로그 파일에서 데이터를 읽고 편집 후 Redis에 저장하는 로직을 작성. 로그 파일 읽기: BufferedReader 등을 이용하여 로그 파일을 읽음. 데이터 편집: 데이터를 가공하거나 필요한 형식으로 변환. Redis 저장: Jedis나 Lettuce를 이용하여 Redis에 데이터를 저장. 예시코드 123456789101112131415161718192021222324252627282930313233343536import redis.clients.jedis.Jedis;import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;public class LogToRedis { public static void main(String[] args) { String logFilePath = &quot;/path/to/logfile.log&quot;; String redisKey = &quot;log_data&quot;; // Redis 연결 try (Jedis jedis = new Jedis(&quot;localhost&quot;)) { // 로그 파일 읽기 try (BufferedReader br = new BufferedReader(new FileReader(logFilePath))) { String line; StringBuilder logData = new StringBuilder(); while ((line = br.readLine()) != null) { // 로그 데이터 가공 logData.append(processLogLine(line)).append(&quot;\\n&quot;); } // Redis에 저장 jedis.set(redisKey, logData.toString()); System.out.println(&quot;Log data saved to Redis under key: &quot; + redisKey); } catch (IOException e) { System.err.println(&quot;Error reading log file: &quot; + e.getMessage()); } } } private static String processLogLine(String line) { // 로그 데이터를 가공하는 로직 (필요에 맞게 구현) return line.toUpperCase(); // 예: 로그 데이터를 대문자로 변환 }} CronJob과 Java 연동 CronJob은 위에서 설정한 것처럼 crontab에서 특정 시간마다 Java 프로그램을 실행하도록 설정됨. 주기적인 실행은 cron이 관리하고, 로그를 처리하는 부분은 Java 코드가 처리. CronJob이란특정시간 또는 일정한 주기마다 작업을 자동으로 실행하기 위해 사용하는 유닉스 계열 시스템의 스케줄러 주요 개념 cron : 유닉스 기반 시스템에서 주기적으로 명령을 실행하는 데 사용하는 데몬(백그라운드 프로세스) crontab : cron 작업을 설정하고 관리하는 테이블 파일 또는 유틸리티로, 사용자가 명령어를 입력해 작업을 정의 cronjob : 주기적으로 실행되는 프로그램들 Command sudo systemctl enable cron : cron을 활성화 시킴. crontab -l : 현재 등록되어 있는 cron job을 보여줌 crontab -e : cron job을 등록할 수 있음. 첫 실행 시에 editor(nano, vim 등)를 선택하는 선택지가 출력 crontab -r : cron job을 지울 수 있음 cronjob 설정 방법cron job 등록 방법에 대해서. 1minute | hour | dom(day of month) | month | dow(day of week) | command 순서는 이렇게 된다. command 같은 것들은 “cd ~/my_project &amp;&amp; bash run.sh” 이렇게도 가능하고, 단순하게 bash run.sh 하나만 작성해도 괜찮다. 예시 crontab -e 로 crontab 편집기를 연다. 0 5 * * “sudo systemctl restart mongod” 를 문서의 맨 아래에 삽입한다. 매월/매일 05:00am에 MongoDB가 재시작된다. Spring Batch(Java)Spring Batch는 대용량 데이터를 처리하기에 적합하며, 주기적으로 실행되는 배치 작업을 쉽게 관리할 수 있다. 프로젝트 설정 Spring Batch 프로젝트를 설정하려면 Spring Initializr 또는 Maven/Gradle을 사용하여 기본 프로젝트를 구성한다. Redis와 Spring Batch를 사용하기 위해 다음 의존성을 추가한다. Gradle 의존성 예시: 12345dependencies { implementation 'org.springframework.boot:spring-boot-starter-batch' implementation 'org.springframework.boot:spring-boot-starter-data-redis' implementation 'redis.clients:jedis'} Spring Batch 구성 Spring Batch는 Job, Step, Reader, Processor, Writer 등으로 구성됩니다. 로그 파일에서 데이터를 읽고, 가공하고, Redis에 저장하기 위해 각각의 구성 요소를 설정. 주요 구성: Job: 배치 작업의 전체적인 단위. Step: Job을 이루는 각각의 처리 단계. ItemReader: 데이터를 읽어오는 역할. ItemProcessor: 데이터를 가공하는 역할. ItemWriter: 가공된 데이터를 Redis에 저장하는 역할. 로그 파일 읽기 (ItemReader) 로그 파일을 읽기 위한 FlatFileItemReader를 설정합니다. 이 리더는 CSV 또는 텍스트 파일의 각 줄을 읽는다. 1234567891011121314151617181920@Beanpublic FlatFileItemReader&lt;String&gt; reader() { FlatFileItemReader&lt;String&gt; reader = new FlatFileItemReader&lt;&gt;(); reader.setResource(new FileSystemResource(&quot;/path/to/logfile.log&quot;)); reader.setLineMapper(new DefaultLineMapper&lt;String&gt;() { { setLineTokenizer(new DelimitedLineTokenizer() { { setNames(&quot;logLine&quot;); } }); setFieldSetMapper(new BeanWrapperFieldSetMapper&lt;String&gt;() { { setTargetType(String.class); } }); } }); return reader;} 로그 데이터 가공 (ItemProcessor) 로그 데이터를 가공하는 ItemProcessor를 작성한다. 이 프로세서는 입력된 로그 데이터를 가공하거나 변환한다. 12345678910111213141516171819202122@Beanpublic ItemProcessor&lt;String, String&gt; processor() { return logLine -&gt; { // 로그 데이터를 처리 (예: 대문자로 변환) return logLine.toUpperCase(); };}~~~java5. Redis 저장 (ItemWriter)ItemWriter는 가공된 데이터를 Redis에 저장하는 역할을 한다. Jedis나 Spring Data Redis를 이용하여 데이터를 Redis에 저장.~~~java@Beanpublic ItemWriter&lt;String&gt; writer(Jedis jedis) { return logLines -&gt; { for (String logLine : logLines) { jedis.set(&quot;log_data_key&quot;, logLine); // Redis에 저장 } };} Spring Batch Job 및 Step 구성 이제 Reader, Processor, Writer를 사용하여 Job과 Step을 정의한다. Step은 로그 데이터를 읽고 가공한 후 Redis에 저장하는 단위 작업 1234567891011121314151617@Beanpublic Job logFileToRedisJob(JobBuilderFactory jobBuilderFactory, Step step1) { return jobBuilderFactory.get(&quot;logFileToRedisJob&quot;) .incrementer(new RunIdIncrementer()) .start(step1) .build();}@Beanpublic Step step1(StepBuilderFactory stepBuilderFactory, FlatFileItemReader&lt;String&gt; reader, ItemProcessor&lt;String, String&gt; processor, ItemWriter&lt;String&gt; writer) { return stepBuilderFactory.get(&quot;step1&quot;) .&lt;String, String&gt;chunk(10) // 한 번에 처리할 데이터 크기 .reader(reader) .processor(processor) .writer(writer) .build();} 스케줄링 설정 Spring Batch를 일정 시간마다 실행하려면 Spring의 @EnableScheduling과 @Scheduled를 사용하여 스케줄링을 설정할 수 있다. 123456789101112131415161718192021@EnableScheduling@Configurationpublic class BatchScheduler { private final JobLauncher jobLauncher; private final Job job; @Autowired public BatchScheduler(JobLauncher jobLauncher, Job job) { this.jobLauncher = jobLauncher; this.job = job; } @Scheduled(cron = &quot;0 0 * * * ?&quot;) // 매 시간 정각에 실행 public void runBatchJob() throws Exception { JobParameters params = new JobParametersBuilder() .addString(&quot;time&quot;, String.valueOf(System.currentTimeMillis())) .toJobParameters(); jobLauncher.run(job, params); }} 위의 코드는 크론 표현식을 사용하여 배치 작업을 매 시간마다 실행하게 설정한 예이다. @Scheduled의 크론 표현식을 조정하여 원하는 주기에 맞게 실행할 수 있다. Redis 설정 Spring Boot의 application.yml 파일을 통해 Redis 설정을 추가한다. 1234spring: redis: host: localhost port: 6379 또는 Jedis를 직접 사용하려면 Jedis 인스턴스를 빈으로 등록해 사용할 수 있다. 1234@Beanpublic Jedis jedis() { return new Jedis(&quot;localhost&quot;, 6379);} 전체 흐름: Spring Batch가 일정 시간마다 Job을 실행힌다. ItemReader가 로그 파일에서 데이터를 읽어온다. ItemProcessor가 데이터를 가공한다. ItemWriter가 Redis에 데이터를 저장한다.","link":"/2024/09/11/BatchServer/"},{"title":"동시성 처리","text":"물리적인 CPU 성능 향상이 한계에 다다르면서, 이제는 단순히 하드웨어를 업그레이드하는 방식만으로 시스템의 성능을 개선하기 어려운 시대에 접어들었다. 더 이상 ‘공짜 점심’은 존재하지 않는다. 과거에는 동일한 소프트웨어라도 더 빠른 CPU로 자연스레 성능 향상을 기대할 수 있었지만, 현대의 프로세서는 더 많은 일을 병렬로 처리할 수 있도록 다중 코어 구조로 진화하고 있다. 이러한 환경 변화는 개발자들에게 “동시성(Concurrency)”이라는 새로운 패러다임에 대한 이해와 대응을 요구한다. 본 글은 동시성 프로그래밍의 개념과 중요성을 설명하고, 객체지향 모델이 멀티쓰레드 환경에서 가지는 한계를 짚으며, 안전한 병렬 처리를 위한 설계 원칙들을 다룬다. 특히, 멀티코어 시대에 확장 가능한(Scalable) 시스템을 구축하기 위해 어떤 프로그래밍 모델과 구조가 필요한지 살펴보고, 자원을 효율적으로 활용하면서도 오류 없이 동작하는 코드를 작성하기 위한 실질적인 방향을 제시한다. 공짜 점심은 끝났다. 소프트웨어는 동일한데 CPU가 빨라지면 덩달아 빨라지는것 Scale Up 성능 향상 특이점 이후의 CPU CPU는 더 이상 빨라지지 않고 더 많은 일(다중 코어)을 동시에 할 수 있게 발전 (-&gt; Scale Out) 동시성을 고려한 프로그램을 작성해야 한다. 확장성 (Scalability) 하나의 시스템이 성능 향상에 지장을 받지 않고 자원에 대한 수요읠 변화를 수용할 수 있는 정도 수요의 증가를 수용할 때 자원을 추가 투입하는 것은 당연하고 단지 그 비율이 선형적이거나 아니면 조금 더 효율적이길 바란다. 동시성 (Concurrency) 확장성을 위한 수단으로써, 하마디로 자원을 더 주면 프로그램이 알아서 잘 활용하는 것 자원을 잘 활용하기 위한 프로그램의 복잡도가 증가하지 않거나 완만하게 증가하기를 바란다. 객체지향 모델링 객체란 은닉한 상태에 대해 지켜야 하는 논리를 보호하는 안전한 동작들만 노출 시킴 시스템은 메서드 호출에 반응하여 내부의 상태를 수정하는, 즉 전체 상태를 진전시키는 동작 호출을 통해 서로와 통신하는 객체 인스턴스의 네트워크로 표현 멀티쓰레드 객체에서의 은닉이라는 것은 멀티쓰레드 환경을 고려하지 않았다면 이러한 동작들이 안전하지 않게 되며 내부 상태 오염에 이르게 됨 실제 멀티 쓰레드 환경에서의 시스템은 쓰레드들이 동작 호출을 통해 객체 인스턴스의 네트워크를 따라 이동하는 것 객체지향 모델링의 한계 상태의 은닉만으로는 동시에 동작을 호출하는 것에 대한 보호를 할 수 없음 getNext()는 세가지 동작으로 구성 value -&gt; 9 9 + 1 -&gt; 10 10 -&gt; value value -&gt; 10 10 + 1 -&gt; 11 11 -&gt; value 1234567public class UnsafeSequence { private int value' public int getNext() { return this.value++; }} 쓰레드를 제어해야 함 쓰레드 프로세스란 격리되어 있고 독립적으로 동작하는 OS의 관리 단위로 순차적인 실행과 OS의 I/O를 통한 외부와의 통신을 특징으로 함 쓰레드는 프로세스내에서 동시적인 실행과 메모리 공유를 통한 통신 처리량의 향상 메모리 공유라는 것은 비순차성을 의미하며 개발이 복잡해지고 추론이 어려워지는 것을 의미 충분히 동기화하지 않으면 멀티쓰레드에서의 동작들의 순서는 예측 불가 멀티쓰레드라는 것이 전체 성능은 올릴지라도 어느 정도의 런타임 부하를 수반 동시적 프로그래밍 공유하고 변경 가능한 상태에 대한 접근을 통제 멀티 쓰레드에 안전한 코드를 만든다는 것은 상태를 쓰레드간에 공유하지 않는다. 상태를 변경 불가능하게 한다. 상태를 접근할 때 동기화를 사용한다 하나 이상의 쓰레드가 상태에 접근하고 그 중 하나가 변경할 수 있다면 이 모든 쓰레드들은 동기화를 통해 해당 상태에 대한 접근을 순서대로 배열하여야 한다. 1234567public class StatelessFactorizer implements Servlet { public void service(ServiceRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); encodeIntoResponse(resp, factors); }} 소인수분해를 하는 서블릿 상태가 없으면 통제할 대상이 없으므로 항상 멀티쓰레드에 안전 12345678910111213141516171819import java.math.BigInteger;public clss UnsafeCachingFactorizer implementsservlet { private BigInteger lastNumber; private BigInteger[] lastFactos; public void service (ServletRequest req, ServletResponse resp){ BigInteger i = extractFromRequest(req); if (i.equals(this.lastNumber) encodingIntoResponse(reqp, this.lastFactors)); else{ BigInteger[] factors = factor(i); this.lastNumber = i; this.lastFactors = factors; encodeIntoResponse(resp, factors); } }} WIP…","link":"/2021/09/13/Concurrency/"},{"title":"Java에서 BufferedReader와 BufferWriter 사용하기","text":"BufferedReader와 BufferedWriter은 이름처럼 버퍼를 이용해서 읽고 쓰는 함수입니다.이 함수는 버퍼를 이용하기 때문에 이 함수를 이용하면 입출력의 효율이 비교할 수 없을 정도로 좋아집니다. 한 번 거쳐가므로 느릴거 같은데 왜 빠를까요?하드디스크는 원래 속도가 매우 느립니다.하드디스크 뿐만 아니라 키보드나 모니터와 같은 외부 장치와의 데이터 입출력은 생각보다 시간이 걸리는 작업입니다.버퍼링 없이 키보드가 눌릴 때마다 눌린 문자의 정보를 목적지로 바로 이동시키는 것이 보다 효율적이고 빠릅니다.그냥 전송하게 되면 CPU의 성능 갭이 많이 나서 비효율적입니다. 예를 들자면, 흙을 퍼서 저 언덕에 버리는데, 한 번 삽질할 때마다 가서 버리는 것보다, 수레에 가득 채워서 한 번에 나르는 것이 효율적인것과 같은 것 입니다.즉 모아뒀다가 한 번에 전송하는 것이 훨씬 효율적이라는 것입니다. 12345678910111213# 버퍼 입출력## 버퍼(buffer)- 데이터를 한 곳에서 다른 한 곳으로 전송하는 동안 일시적으로 그 데이터를 보관하는 임시 메모리 영역- 입출력 속도 향상을 위해 버퍼 사용## 버퍼 플러시(buffer flush) : 버퍼에 남아 있는 데이터를 출력시킴(버퍼를 비움)## 버퍼를 이용한 입력- BufferedReader## 버퍼를 이용한 출력- BufferedWriter BufferedReader자바를 사용하는 사람들은 보통 Scanner를 이용해 입력받아 사용하는데, 띄어쓰기(스페이스)와 엔터(개행문자)를 경계로 입력값을 인식하기 때문에 따로 가공할 필요가 없어서 사용하기 매우 편리합니다.반면에 BufferedReader는 엔터만 경계로 인식하고 받은 데이터가 String으로 고정되기 때문에 데이터를 따로 가공해야 하는 경우가 많습니다.그래소 다소 번거롭습니다. 하지만 Scanner에 비해서 상대적으로 빠릅니다. 공식 문서에는Reads text from a character-input stream, buffering characters so as to provide for the efficient reading of characters, arrays, and lines. The buffer size may be specified, or the default size may be used.입력 스트림에서 문자를 읽는 함수인데 문자나 배열, 라인들을 효율적으로 읽기 위해서 문자들을 버퍼에 저장하고(버퍼링) 읽는 방법을 취합니다. 버퍼 사이즈는 우리가 지정할 수도 있지만 지정안할 경우에는 기존 디폴트 사이즈가 적용됩니다. 따라서 알고리즘 풀 때 BufferedReader를 이용한 해설을 많이 볼 수 있습니다. 많은 데이터를 입력받아야 할 상황에서는 BufferedReader를 이용해줍시다. BufferedReader 사용방법BufferedReader의 readLine()을 사용하면 데이터를 라인 단위로 읽을 수 있습니다.readLine 함수의 리턴 값은 String으로 고정되기 때문에 String이 아닌 다른 타입으로 입력을 받으려면 형변환을 꼭 해줘야 합니다. 123456789101112131415161718192021222324252627282930import java.io.*;class BufferedReaderEx { public static void main(String[] args) { try { // 예외처리 필수 // 또는 throwsIOException 하기 // 콘솔에서 입력 받을 경우 BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); // 파일에서 입력받을 경우 FileReader fr = new FileReader(&quot;BufferedReader.txt&quot;); BufferedReader br_f = new BufferedReader(fr); // String이 리턴값이라 형변환 필수, 라인단위 int num = Integer.parseInt(br.readLine()); br.close(); // 입출력이 끝난 후 닫아주기. // 파일의 한 줄 한 줄 읽어서 출력한다. String line = &quot;&quot;; for (int i = 1; (line = br_f.readLine()) != null; i++) { System.out.println(line); } } catch (IOException e) { e.printStackTrace(); System.out.println(e.getMessage()); } }} 위처럼 한 줄 한 줄 읽습니다. 그런데 공백단위로 데이터를 알려면 어떻게 해야할까요?Scanner는 공백단위로도 끊어주기 때문에 이런걸 고려할 필요가 없었습니다. StringTokenizer의 nextToken함수를 이용하거나 String클래스의 slplit함수를 이용해야 합니다. BufferedReader클래스의 또 다른 메인 함수들 Modifier and Type Method and Description void Close()입력 스트림을 닫고 사용하던 자원들을 풉니다. void mark(int, readAheadLimit)스트림의 현재 위치를 마킹합니다. boolean markSupported()스트림이 mark 기능을 지원하는지 true/false로 알려줍니다. int read()한 글자만 읽어 정수형으로 반환해줍니다.즉 3을 ‘3’이라고 읽어서 ‘3’의 정수형인 (int)’3’ = 51을 반환3을 친다고 해서 3을 리턴해주는 메소드가 아니다. int read(char[] cbuf, int offset, int length)cbuf의 offset 위치부터 length 길이만큼 문자를 스트림으로부터 읽어옵니다. String readLine()한줄을 읽는다. String으로 변환 boolean ready()입력스트림이 사용될 준비가 되어있는지 확인해줌. 1이면 준비완료 void reset()마킹이 있으면 그 위치부터 다시 시작, 그렇지 않으면 처음부터 다시 시작. long skip(long n) n개의 문자를 건나뜀. BufferedWriter 마찬가지로 System.out.print(“”); 과 동일하게 사용가능한 함수입니다.BufferedWriter 함수 또한 버퍼를 이용하기 때문에 성능면에서 더 좋다.많은 양의 출력이 필요할 때에는 마찬가지로 이것을 이용해주는 것이 좋다. System.out.println처럼 함수가 문자열 출력과 개행을 동시에 해주지 않기 때문에 개행을 하려면 write에 “\\n”를 넣어주거나 newLine함수를 사용해야합니다. 123456789101112import java.io.*;class BufferedWriterEx { public static void main(String[] args) throws IOException { BufferedWriter bw = new BufferedWriter(new FileWriter(&quot;file.txt&quot;)); bw.write(&quot;hello\\n&quot;); // 출력 bw.nextLine(); // 개행 즉 엔터 역할 bw.write(&quot;Let me write\\n&quot;); // 개행과 함께 출력 bw.flush(); // 남아 있는 데이터를 모두 출력 bw.close(); // 스트림 }} 버퍼를 이용하는 것이기 때문에 다 쓴 뒤에는 버퍼를 클린하게 해주어야 합니다.flush() 함수를 통해 버퍼에 남아있는 데이터를 출력해 없앤 후, 스트림을 닫아줍니다. BufferedWriter 클래스의 또 다른 메인 함수들 Modifier and Type Method and Description void Close()스트림을 닫습니다. 닫기 전에 flushing 해줍니다. void flush()스트림을 비웁니다. void newLine()개행역할을 합니다. void write(char[] cbuf, int offset, int length)버퍼 offset위치부터 length 크기만큼 write 합니다. void write(int c)한 글자 쓰기 void write(String s, int offset, int length)문자열에서 offset부터 일정 길이만큼 write 해주기. 예제 - 백준 15552번12345678910111213141516171819202122232425262728package For.P15552;import java.io.BufferedReader;import java.io.BufferedWriter;import java.io.IOException;import java.io.InputStreamReader;import java.io.OutputStreamWriter;public class Main { public static void main(String[] args) throws NumberFormatException, IOException { BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); int n = Integer.parseInt(br.readLine().trim()); for (int i = 0; i &lt; n; i++) { String input = br.readLine(); String[] inputArray = input.split(&quot; &quot;); int a = Integer.parseInt(inputArray[0]); int b = Integer.parseInt(inputArray[1]); bw.write(a+b + &quot;\\n&quot;); } bw.flush(); br.close(); }} 위 문제의 경우 Scanner로 풀면 정말 간단하지만, 테스트 케이스가 많아진다면 Scanner보단 Buffer을 사용하는것이 효율적이다. #Printwriter #FileWriter #OutputStreamWriter #StringTokenizer #String References https://jhnyang.tistory.com/92","link":"/2020/09/30/BufferedReader/"},{"title":"CronJob 이란","text":"특정시간 또는 일정한 주기마다 작업을 자동으로 실행하기 위해 사용하는 유닉스 계열 시스템의 스케줄러 주요 개념 cron : 유닉스 기반 시스템에서 주기적으로 명령을 실행하는 데 사용하는 데몬(백그라운드 프로세스) crontab : cron 작업을 설정하고 관리하는 테이블 파일 또는 유틸리티로, 사용자가 명령어를 입력해 작업을 정의 cronjob : 주기적으로 실행되는 프로그램들 크론 작업 추가 crontab 명령어로 Cron을 사용할 수 있다. 공통 설정(Common Settings)— 일반적으로 사용되는 간격을 선택. 시스템은 분, 시간, 일, 월과 평일 (Minute, Hour, Day, Month, and Weekday) 텍스트 상자에서 적절한 설정을 구성. 분(Minute)— 크론 작업을 실행할 각 시간의 분 또는 크론 작업이 실행되는 매 시간 사이의 분 수 시간(Hour)— 크론 작업을 실행할 각 날짜의 시간 또는 크론 작업이 실행되는 매 시간 사이의 시간 수 일(Day)— 크론 작업을 실행할 달의 날짜 또는 크론 작업이 실행되는 매 시간 사이의 일 수 월(Month)— 크론 작업을 실행할 년도의 월 또는 크론 작업이 실행되는 매 시간 사이의 개월 수 평일(Weekday)— 크론 작업을 실행할 주의 평일 명령(Command)텍스트 상자에, 시스템에서 실행할 명령을 입력. Command sudo systemctl enable cron : cron을 활성화 시킴. crontab -l : 현재 등록되어 있는 cron job을 보여줌 crontab -e : cron job을 등록할 수 있음. 첫 실행 시에 editor(nano, vim 등)를 선택하는 선택지가 출력 crontab -r : cron job을 지울 수 있음 cronjob 설정 방법cron job 등록 방법에 대해서. 1minute | hour | dom(day of month) | month | dow(day of week) | command 순서는 이렇게 된다. command 같은 것들은 “cd ~/my_project &amp;&amp; bash run.sh” 이렇게도 가능하고, 단순하게 bash run.sh 하나만 작성해도 괜찮다. 예시crontab -e 로 crontab 편집기를 연다. 0 5 * * “sudo systemctl restart mongod” 를 문서의 맨 아래에 삽입한다. 매월/매일 05:00am에 MongoDB가 재시작된다.","link":"/2024/09/11/CronJob/"},{"title":"Conversation-Knowledge-Mining-Solution-Accelerator","text":"대량의 대화 데이터를 보유한 고객이 생성형 AI를 사용하여 주요 구문을 surface(분석)하고 운영 지표와 함께 중요한 Insight를 얻을 수 있도록 하는 솔루션. 아키텍처 그림 실제 엔터티 추출 : 사람, 제품, 이벤트 장소 또는 행동과 같은 고유한 정보를 처리하고 추출 채팅 기반 Insight discovery : 모든 인덱싱 된 assets, 단일 assets, 선택한 assets 세트 또는 사용자 주도 키워드 검색을 기반으로 생성된 asset 목록과 채팅 가능 텍스트 및 문서 데이터 분석 : 문서, 손글씨 텍스트, 차트, 그래프, 표 및 양식 필드를 포함한 다중 모드 문서의 내용을 분석, 비교 및 요약하여 심층적인 통찰력을 제공 프롬프트 제안 가이드 : 프롬프트 문의를 기반으로 다음 질문 세트를 제안 다중 모드 정보 처리 : 여러 콘텐츠 유형과 다양한 형식의 지식을 처리하고 추출 대량의 데이터를 신속하게 분석하고, 관련 제안을 생성하여 빠르고 쉽게 추론할 수 있도록 도와준다. 1. Simple Deploy1-1. roleAssignment 정책상 부여 불가1234{ &quot;code&quot;: &quot;InvalidTemplateDeployment&quot;, &quot;message&quot;: &quot;The template deployment failed with error: 'Authorization failed for template resource '64f2b582-8f7c-560f-807d-a3f84df8679b' of type 'Microsoft.Authorization/roleAssignments'. The client 'hyejoo.jung@kt.com' with object id '349020e1-be53-4e57-bdb6-40240f0037be' does not have permission to perform action 'Microsoft.Authorization/roleAssignments/write' at scope '/subscriptions/80083cf5-0434-4e94-b9a4-9f8ba244207e/resourceGroups/BC-RA-joo/providers/Microsoft.Authorization/roleAssignments/64f2b582-8f7c-560f-807d-a3f84df8679b'.'.&quot;} Microsoft.Authorization/roleAssignments write 권한 이슈 아래와 같이 roleAssignment 관련 주석 처리123456789101112131415161718192021222324// {// &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;,// &quot;apiVersion&quot;: &quot;2022-04-01&quot;,// &quot;name&quot;: &quot;[guid(resourceGroup().id, resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635'))]&quot;,// &quot;properties&quot;: {// &quot;principalId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;,// &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635')]&quot;,// &quot;principalType&quot;: &quot;ServicePrincipal&quot;// },// &quot;dependsOn&quot;: [// &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot;// ]// }...// {// &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;,// &quot;apiVersion&quot;: &quot;2022-04-01&quot;,// &quot;name&quot;: &quot;[guid(resourceGroup().id, parameters('managedIdentityObjectId'), resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483'))]&quot;,// &quot;properties&quot;: {// &quot;principalId&quot;: &quot;[parameters('managedIdentityObjectId')]&quot;,// &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483')]&quot;,// &quot;principalType&quot;: &quot;ServicePrincipal&quot;// }// }, 1-2. OpenAI 모델 지원 이슈 sku 변경 1234567891011121314151617181920{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15}...&quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;GlobalStandard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; }} sku name 필드를 Standard → GlobalStandard로 변경 배포 후 역할 정상 작동하는지 확인 필요 12345678910111213141516171819202122232425262728293031323334353637383940resource accounts_ckm_openai_name_gpt_4 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = { parent: accounts_ckm_openai_name_resource name: 'gpt-4o-mini' sku: { name: 'GlobalStandard' capacity: 15 } properties: { model: { format: 'OpenAI' name: 'gpt-4o-mini' version: '2024-07-18' } versionUpgradeOption: 'OnceNewDefaultVersionAvailable' raiPolicyName: 'Microsoft.Default' } //dependsOn:[accounts_ckm_openai_name_resource]}{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ]} 최종 템플릿123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708{ &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;8858477527853866674&quot; } }, &quot;parameters&quot;: { &quot;solutionPrefix&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 6, &quot;metadata&quot;: { &quot;description&quot;: &quot;Prefix Name&quot; } }, &quot;location&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[resourceGroup().location]&quot; } }, &quot;variables&quot;: { &quot;resourceGroupLocation&quot;: &quot;[parameters('location')]&quot;, &quot;solutionLocation&quot;: &quot;[variables('resourceGroupLocation')]&quot; }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_managed_identity&quot;, &quot;resourceGroup&quot;: &quot;[resourceGroup().name]&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;14133192615685065374&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Location&quot; } }, &quot;miName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-managed-identity', parameters('solutionName'))]&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Name&quot; } } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.ManagedIdentity/userAssignedIdentities&quot;, &quot;apiVersion&quot;: &quot;2023-01-31&quot;, &quot;name&quot;: &quot;[parameters('miName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;tags&quot;: { &quot;app&quot;: &quot;[parameters('solutionName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot; } } // { // &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;, // &quot;apiVersion&quot;: &quot;2022-04-01&quot;, // &quot;name&quot;: &quot;[guid(resourceGroup().id, resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635'))]&quot;, // &quot;properties&quot;: { // &quot;principalId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;, // &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635')]&quot;, // &quot;principalType&quot;: &quot;ServicePrincipal&quot; // }, // &quot;dependsOn&quot;: [ // &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot; // ] // } ], &quot;outputs&quot;: { &quot;managedIdentityOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;id&quot;: &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot;, &quot;objectId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;, &quot;name&quot;: &quot;[parameters('miName')]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_azure_ai_service&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;8537007800307151650&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;accounts_byc_cogser_name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-cogser', parameters('solutionName'))]&quot; } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;S0&quot; }, &quot;kind&quot;: &quot;CognitiveServices&quot;, &quot;identity&quot;: { &quot;type&quot;: &quot;None&quot; }, &quot;properties&quot;: { &quot;apiProperties&quot;: {}, &quot;customSubDomainName&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;networkAcls&quot;: { &quot;defaultAction&quot;: &quot;Allow&quot;, &quot;virtualNetworkRules&quot;: [], &quot;ipRules&quot;: [] }, &quot;publicNetworkAccess&quot;: &quot;Enabled&quot; } } ], &quot;outputs&quot;: { &quot;cogSearchOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;cogServiceName&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;cogServiceKey&quot;: &quot;[listKeys(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01').key1]&quot;, &quot;cogServiceEndpoint&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01').endpoint]&quot;, &quot;cogServiceRegion&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01', 'full').location]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_azure_open_ai&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;16531009550718652696&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;accounts_ckm_openai_name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-openai', parameters('solutionName'))]&quot; } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;S0&quot; }, &quot;kind&quot;: &quot;OpenAI&quot;, &quot;properties&quot;: { &quot;customSubDomainName&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot;, &quot;networkAcls&quot;: { &quot;defaultAction&quot;: &quot;Allow&quot;, &quot;virtualNetworkRules&quot;: [], &quot;ipRules&quot;: [] }, &quot;publicNetworkAccess&quot;: &quot;Enabled&quot; } }, { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ] } ], &quot;outputs&quot;: { &quot;openAIOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;openAPIKey&quot;: &quot;[listKeys(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name')), '2023-05-01').key1]&quot;, &quot;openAPIVersion&quot;: &quot;2023-05-01&quot;, &quot;openAPIEndpoint&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name')), '2023-05-01').endpoint]&quot;, &quot;openAIAccountName&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_keyvault&quot;, &quot;resourceGroup&quot;: &quot;[resourceGroup().name]&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; }, &quot;objectId&quot;: { &quot;value&quot;: &quot;[reference(extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity'), '2022-09-01').outputs.managedIdentityOutput.value.objectId]&quot; }, &quot;tenantId&quot;: { &quot;value&quot;: &quot;[subscription().tenantId]&quot; }, &quot;managedIdentityObjectId&quot;: { &quot;value&quot;: &quot;[reference(extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity'), '2022-09-01').outputs.managedIdentityOutput.value.objectId]&quot; }, &quot;azureOpenAIApiKey&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai'), '2022-09-01').outputs.openAIOutput.value.openAPIKey]&quot; }, &quot;azureOpenAIApiVersion&quot;: { &quot;value&quot;: &quot;2023-07-01-preview&quot; }, &quot;azureOpenAIEndpoint&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai'), '2022-09-01').outputs.openAIOutput.value.openAPIEndpoint]&quot; }, &quot;cogServiceEndpoint&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceEndpoint]&quot; }, &quot;cogServiceName&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceName]&quot; }, &quot;cogServiceKey&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceKey]&quot; }, &quot;cogServiceRegion&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceRegion]&quot; }, &quot;enableSoftDelete&quot;: { &quot;value&quot;: false } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;12149961923112846409&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Location&quot; } }, &quot;utc&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[utcNow()]&quot; }, &quot;kvName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-kv-{1}', parameters('solutionName'), uniqueString(parameters('utc')))]&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Name&quot; } }, &quot;objectId&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Object Id. The object ID of a user, service principal or security group in the Azure Active Directory tenant for the vault.&quot; } }, &quot;createMode&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;default&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Create Mode&quot; } }, &quot;enableForDeployment&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Deployment. Property to specify whether Azure Virtual Machines are permitted to retrieve certificates stored as secrets from the key vault.&quot; } }, &quot;enableForDiskEncryption&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Disk Encryption. Property to specify whether Azure Disk Encryption is permitted to retrieve secrets from the vault and unwrap keys.&quot; } }, &quot;enableForTemplateDeployment&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Template Deployment. Property to specify whether Azure Resource Manager is permitted to retrieve secrets from the key vault.&quot; } }, &quot;enablePurgeProtection&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable Purge Protection. Property specifying whether protection against purge is enabled for this vault.&quot; } }, &quot;enableRBACAuthorization&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable RBAC Authorization. Property that controls how data actions are authorized.&quot; } }, &quot;enableSoftDelete&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: false, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable Soft Delete. Property to specify whether the \\&quot;soft delete\\&quot; functionality is enabled for this key vault.&quot; } }, &quot;softDeleteRetentionInDays&quot;: { &quot;type&quot;: &quot;int&quot;, &quot;defaultValue&quot;: 30, &quot;metadata&quot;: { &quot;description&quot;: &quot;Soft Delete Retention in Days. softDelete data retention days. It accepts &gt;=7 and &lt;=90.&quot; } }, &quot;publicNetworkAccess&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;enabled&quot;, &quot;allowedValues&quot;: [ &quot;enabled&quot;, &quot;disabled&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;Public Network Access, Property to specify whether the vault will accept traffic from public internet.&quot; } }, &quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;GlobalStandard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; } }, &quot;tenantId&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Tenant Id&quot; } }, &quot;managedIdentityObjectId&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;azureOpenAIApiKey&quot;: { &quot;type&quot;: &quot;securestring&quot; }, &quot;azureOpenAIApiVersion&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;azureOpenAIEndpoint&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceEndpoint&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceKey&quot;: { &quot;type&quot;: &quot;securestring&quot; }, &quot;cogServiceName&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceRegion&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;variables&quot;: { &quot;vaultUri&quot;: &quot;[format('https://{0}.vault.azure.net/', parameters('kvName'))]&quot; }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults&quot;, &quot;apiVersion&quot;: &quot;2022-07-01&quot;, &quot;name&quot;: &quot;[parameters('kvName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;tags&quot;: { &quot;app&quot;: &quot;[parameters('solutionName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot; }, &quot;properties&quot;: { &quot;accessPolicies&quot;: [ { &quot;objectId&quot;: &quot;[parameters('objectId')]&quot;, &quot;permissions&quot;: { &quot;certificates&quot;: [ &quot;all&quot; ], &quot;keys&quot;: [ &quot;all&quot; ], &quot;secrets&quot;: [ &quot;all&quot; ], &quot;storage&quot;: [ &quot;all&quot; ] }, &quot;tenantId&quot;: &quot;[parameters('tenantId')]&quot; } ], &quot;createMode&quot;: &quot;[parameters('createMode')]&quot;, &quot;enabledForDeployment&quot;: &quot;[parameters('enableForDeployment')]&quot;, &quot;enabledForDiskEncryption&quot;: &quot;[parameters('enableForDiskEncryption')]&quot;, &quot;enabledForTemplateDeployment&quot;: &quot;[parameters('enableForTemplateDeployment')]&quot;, &quot;enablePurgeProtection&quot;: &quot;[parameters('enablePurgeProtection')]&quot;, &quot;enableRbacAuthorization&quot;: &quot;[parameters('enableRBACAuthorization')]&quot;, &quot;enableSoftDelete&quot;: &quot;[parameters('enableSoftDelete')]&quot;, &quot;softDeleteRetentionInDays&quot;: &quot;[parameters('softDeleteRetentionInDays')]&quot;, &quot;provisioningState&quot;: &quot;RegisteringDns&quot;, &quot;publicNetworkAccess&quot;: &quot;[parameters('publicNetworkAccess')]&quot;, &quot;sku&quot;: { &quot;family&quot;: &quot;A&quot;, &quot;name&quot;: &quot;[parameters('sku')]&quot; }, &quot;tenantId&quot;: &quot;[parameters('tenantId')]&quot;, &quot;vaultUri&quot;: &quot;[variables('vaultUri')]&quot; } }, // { // &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;, // &quot;apiVersion&quot;: &quot;2022-04-01&quot;, // &quot;name&quot;: &quot;[guid(resourceGroup().id, parameters('managedIdentityObjectId'), resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483'))]&quot;, // &quot;properties&quot;: { // &quot;principalId&quot;: &quot;[parameters('managedIdentityObjectId')]&quot;, // &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483')]&quot;, // &quot;principalType&quot;: &quot;ServicePrincipal&quot; // } // }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'TENANT-ID')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('tenantId')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-KEY')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIApiKey')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-VERSION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIApiVersion')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-ENDPOINT')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIEndpoint')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-ENDPOINT')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceEndpoint')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-KEY')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceKey')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-NAME')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceName')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-REGION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceRegion')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-SUBSCRIPTION-ID')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[subscription().subscriptionId]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-RESOURCE-GROUP')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[resourceGroup().name]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-LOCATION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('solutionLocation')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] } ], &quot;outputs&quot;: { &quot;keyvaultOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;id&quot;: &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot;, &quot;name&quot;: &quot;[parameters('kvName')]&quot;, &quot;uri&quot;: &quot;[variables('vaultUri')]&quot;, &quot;resource&quot;: &quot;[reference(resourceId('Microsoft.KeyVault/vaults', parameters('kvName')), '2022-07-01', 'full')]&quot; } } } } }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service')]&quot;, &quot;[resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai')]&quot;, &quot;[extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity')]&quot; ] } ]} 리소스 그룹 : rg-ckm-1219-v0 Region : Korea Central Solution Prefix : ConKM 배포 실패 에러 내용123456789101112131415161718192021{ &quot;code&quot;: &quot;DeploymentFailed&quot;, &quot;target&quot;: &quot;/subscriptions/80083cf5-0434-4e94-b9a4-9f8ba244207e/resourceGroups/rg-ckm-1219-v0/providers/Microsoft.Resources/deployments/Microsoft.Template-20241219202047&quot;, &quot;message&quot;: &quot;At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-deployment-operations for usage details.&quot;, &quot;details&quot;: [ { &quot;code&quot;: &quot;InvalidTemplate&quot;, &quot;message&quot;: &quot;Deployment template validation failed: 'The provided value for the template parameter 'sku' is not valid. The value 'GlobalStandard' is not part of the allowed value(s): 'standard,premium'.'.&quot;, &quot;additionalInfo&quot;: [ { &quot;type&quot;: &quot;TemplateViolation&quot;, &quot;info&quot;: { &quot;lineNumber&quot;: 1, &quot;linePosition&quot;: 3520, &quot;path&quot;: &quot;properties.template.parameters.sku.allowedValues&quot; } } ] } ]} sku 파라미터의 값이 유요하지 않다고 한다 GlobalStandard가 아니라 standard, premium만 사용 가능하다고 한다. 123456789101112131415161718192021222324252627282930313233{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ]}...&quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;standard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; }}, 다시 위와 같이 수정 다시 배포 해보니 배포가 성공적으로 마무리 되었다. 리소스는 위와 같이 나타난다. 2. Fabric workspace 생성 Fabric Workspace로 이동한다. 왼쪽 내비게이션에서 + 워크스페이스 버튼을 클릭하여 워크스페이스를 생성한다. URL에서 Workspace ID를 확인한다. 워크스페이스 ID 확인 방법Fabric 사이트에서 워크스페이스의 항목 URL을 통해 가장 쉽게 워크스페이스 ID를 찾을 수 있다. Power BI와 마찬가지로, Fabric URL에는 워크스페이스 ID가 포함되어 있으며, 이는 URL에서 /groups/ 다음에 나오는 고유 식별자이다.예를 들어:https://powerbi.com/groups/11aa111-a11a-1111-1abc-aa1111aaaa/… 또는, Power BI 관리자 포털의 설정에서 워크스페이스 이름 옆에 있는 **세부정보(Details)**를 선택하여 워크스페이스 ID를 확인할 수도 있다. 3. Fabric 리소스 및 아티팩트 배포 Azure Portal로 이동 Azure Portal에 접속. Azure Cloud Shell 열기 내비게이션 메뉴의 오른쪽 상단에서 Azure Cloud Shell을 클릭 다음 명령 실행 아래 명령어를 Azure Cloud Shell에서 실행한다. 12345678910111213141516az login ***Azure Cloud Shell의 로그인 지침을 따라 로그인을 완료.rm -rf ./Customer-Service-Conversational-Insights-with-Azure-OpenAI-Servicesgit clone https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Servicescd ./Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services/Deployment/scripts/fabric_scriptssh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param- keyvault_param: 1단계에서 생성한 Key Vault의 이름 ConKM-kv-pm7k3nx4pijba - 키볼트 확인하기 az keyvault list --resource-group &lt;Resource-Group-Name&gt; --query &quot;[].{Name:name}&quot; -o table- workspaceid_param: 2단계에서 생성한 Workspace ID bfccf397-49f8-4822-a23e-19b0187256c7- solutionprefix_param: Lakehouse 생성 시 추가되는 접두사 conkmsh ./run_fabric_items_scripts.sh ConKM-kv-pm7k3nx4pijba bfccf397-49f8-4822-a23e-19b0187256c7 conkm Lakehouse 만들기 Fabric 구독 하지만… Fabric Lakehouse 연결 세부 정보 가져오기 배포가 완료되면 Fabric Workspace로 이동 워크스페이스에서 Lakehouse를 찾는다. (예: lakehouse_solutionprefix_param) Lakehouse 이름 옆의 …을 클릭. SQL Analytics Endpoint를 선택. 팝업 창에서 SQL 연결 문자열 복사(Copy SQL connection string) 버튼을 클릭. 데이터 파이프라인 처리 완료 대기 데이터 파이프라인 처리가 완료될 때까지 10~15분 정도 기다린 후 다음 단계로 진행.","link":"/2024/12/19/Conversation-Knowledge-Mining-Solution-Accelerator/"},{"title":"합성곱 신경망의 구성요소와 이미지 분류","text":"합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이미지 인식, 영상 처리, 컴퓨터 비전 분야에서 사용되는 심층 신경망의 한 종류이다. CNN은 이미지로부터 패턴을 인식하고 이해하는 데 특화되어 있으며, 이를 위해 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer)을 포함한 특별한 구조를 가진다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] CNN의 주요 구성 요소: 합성곱 계층(Convolutional Layer): 이 계층은 이미지로부터 특성을 추출하는 데 사용된다. 여러 개의 필터(또는 커널)를 사용하여 이미지를 스캔하고, 이 과정에서 생성된 특성 맵(feature map)을 통해 이미지의 중요한 정보를 추출한다. 활성화 함수(Activation Function): 대부분의 CNN에서는 ReLU(Rectified Linear Unit) 함수가 활성화 함수로 사용된다. 이 함수는 비선형 변환을 제공하여 네트워크가 복잡한 패턴을 학습할 수 있게 한다. 풀링 계층(Pooling Layer): 특성 맵의 크기를 줄이거나 요약하여 계산량을 감소시키고, 과적합을 방지하는 역할을 한다. 최대 풀링(Max Pooling)과 평균 풀링(Average Pooling)이 일반적으로 사용된다. 완전 연결 계층(Fully Connected Layer): CNN의 마지막 부분에 위치하며, 앞서 추출된 특성을 바탕으로 최종적인 분류나 예측을 수행한다. CNN의 특징 및 장점: 공간적 계층 구조: CNN은 이미지의 공간적 계층 구조를 이해할 수 있으며, 이를 통해 이미지의 로컬 패턴(예: 가장자리, 질감 등)부터 복잡한 객체까지 인식할 수 있다. 매개변수의 공유: 합성곱 필터는 이미지 전체에 걸쳐 공유되므로, 전통적인 심층 신경망에 비해 훨씬 적은 수의 매개변수를 사용한다. 이동 불변성(Translation Invariance): CNN은 이미지 내 객체의 위치가 변해도 동일한 객체를 인식할 수 있다. 활용 분야: 이미지 분류: 사진 속 객체를 분류한다(예: 강아지, 고양이 분류). 객체 탐지: 이미지 내에서 객체의 위치와 종류를 탐지한다. 시맨틱 분할: 이미지를 픽셀 수준에서 분류하여, 각 픽셀이 어떤 객체에 속하는지 결정한다. 얼굴 인식, 자율 주행 자동차, 의료 영상 분석 등 다양한 분야에서 광범위하게 활용된다. 이번에는 텐서플로 케라스 API를 이용해 패션 MNIST 데이터를 합성곱 신경망 (Convolutional Neural Network, CNN)으로 분류한다. 모델을 만들며 합성곱, 패딩, 스트라이드, 풀링의 개념도 같이 알아볼 것이다. 1. 데이터 준비패션 MNIST 데이터를 불러오고 표준화 전처리 후 훈련세트와 검증세트로 나눈다.이때, 합성곱 신경망은 2차원 이미지를 그대로 사용하기 때문에 일렬로 펼치지 않는다.흑백 이미지이기 때문에 1차원 채널이 추가되며, 컬러 이미지는 3차원이 추가된다. 12345678from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()# 흑백 이미지에 채널 추가train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 합성곱 신경망 만들기2-1. 합성곱 층 추가하기합성곱 신경망의 구조는 합성곱 층에서 이미지의 특징을 감지한 후 밀집층에서 클래스에 따른 분류 확률을 계산한다. 먼저 Sequential 클래스 객체를 만들고, 첫 번째 합성곱 층인 Conv2D를 추가한다. Conv2D() 매개변수로 커널의 개수, 커널 사이즈, 활성화 함수, 패딩, 입력 데이터 크기가 필요하다. 123# 32개 필터, 커널 크기 3x3, 렐루 함수, 세임패딩model = keras.Sequential()model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1))) 커널의 개수를 32개로 지정하고, 커널 사이즈를 3으로 놓으면 (3, 3) 크기가 된다.렐루 함수를 활성화 함수로 지정하고, 세임패딩 적용, 인풋 데이터 크기를 지정한다. 세임 패딩과 밸리드 패딩패딩이란 입력 배열 주위를 가상의 원소로 채우는 것을 의미한다. 예로, (4, 4) 크기의 입력에 0을 1개 패딩하면 (6, 6)크기의 입력이 된다. 세임 패딩 : 합성곱 층의 출력 크기를 입력과 동일하게 만들기 위해 입력에 패딩을 추가하는 것이다. 밸리드 패딩 : 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 것, 특성 맵의 크기가 줄어든다. 만약 패딩이 없다면 원소들이 2번 이상 커널과 계산되는 것과 달리, 네 모서리에 있는 4개의 값은 커널에 한번만 계산되게 된다. 만약 이 입력이 이미지라면 모서리에 있는 중요한 정보가 특성 맵에 잘 전달되지 않을 가능성이 높다. 반대로 가운데 있는 정보는 잘 표현된다. 2-2. 풀링 층 추가하기풀링과 스트라이드 풀링 : 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행한다. 특성 맵의 개수는 줄이지 않는다. 최대 풀링 : 커널 영역에서 가장 큰 값을 고른다. 평균 풀링 : 커널 영역의 값을 평균화한다. 스트라이드 : 합성곱 층에서 필터가 입력 위를 이동하는 크기 예를 들어 (2,2,3) 크기의 특성 맵에 스트라이드가 1인 풀링을 적용하면 (1,1,3) 크기의 특성 맵이 된다. 많은 경우 평균 풀링보다 최대 풀링을 사용하는데, 평균 풀링은 특성 맵의 중요한 정보를 평균화하여 희석시킬 수 있기 때문이다. 케라스는 최대 풀링과 평균 풀링을 MaxPooling2D, AveragePooling2D 로 제공한다. 그 중에서 최대풀링을 사용하며, 풀링 크기를 (2,2)로 지정한다. 1model.add(keras.layers.MaxPooling2D(2)) 패선 MNIST 이미지가 (28,28) 크기에 세임 패딩을 적용하여 합성곱 층에서 출력된 특성 맵의 가로세로 크기는 입력과 동일하다. 이후 (2,2) 풀링을 적용하여 특성 맵의 크기는 절반으로 줄어들고, 합성곱 층에서 32개의 필터를 사용하여 최대 풀링을 통과한 특성 맵의 크기는 (14,14,32) 이다. 이제 두 번째 합성곱-풀링 층을 추가한다. 첫번째와 동일하지만, 필터 개수를 64개로 늘렸다. 12model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'))model.add(keras.layers.MaxPooling2D(2)) 이 층을 통과하면 특성 맵의 크기는 (7,7,64)가 된다. 2-3. Flatten, 은닉층, Drop, 출력층 구성하기이제, 마지막에 10개의 뉴런을 가진 출력층에서 확률을 계산하기 위해 3차원 특성 맵을 펼쳐야 한다. Flatten 층을 만들고, Dense은닉층, Dropout , Dense출력층 순서대로 층을 구성한다. 1234model.add(keras.layers.Flatten())model.add(keras.layers.Dense(100, activation='relu')) #은닉층model.add(keras.layers.Dropout(0.4)) # 40% 드롭아웃model.add(keras.layers.Dense(10, activation='softmax')) #출력층 은닉층과 출력층 사이에 드롭아웃을 넣어 은닉층의 과대적합을 막아 성능을 개선할 수 있다.은닉층에 100개의 뉴런을 사용하고 렐루 활성화 함수를 사용한다.클래스 10개를 분류할 다중 분류 문제이기 때문에 출력층의 활성화 함수는 소프트맥스 함수를 사용한다. 3. 모델 구조 확인하기summary() 메서드로 모델 구조를 확인할 수 있다. 1model.summary() 1234567891011121314151617Model: &quot;sequential&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D) (None, 14, 14, 32) 0 conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling2D) (None, 7, 7, 64) 0 flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 333,526Trainable params: 333,526Non-trainable params: 0_________________________________________________________________ 각 층의 파라미터의 개수를 계산할 수 있다. 첫 번째 합성곱 층은 32개의 필터를 가지고 있고 크기가 (3,3), 깊이가 1이다. 또 필터마다 하나의 절편이 있다. 3x3x1x32+32 = 320개의 파라미터가 있다. 두 번째 합성곱 층은 64개의 필터, 크기 (3,3), 깊이 32이다. 필터마다 하나의 절편이 존지하므로 3x3x32x64+64 = 18496개의 파라미터가 있다. Flatten 층에서 (7,7,64) 크기의 특성 맵을 1차원으로 펼치면 (3136,)이며, 은닉층에서는 3136개가 100개의 뉴런과 연결되어야 하고, 100개의 절편이 있으므로 3136x100+100 = 313700개의 파라미터가 있다. 마지막 출력층은 100개의 특성이 10개의 뉴런과 연결되고, 10개의 절편이 있으므로 100x10+10 = 1010개의 파라미터가 있다. keras.utils 패키지의 plot_model() 으로 층의 구성을 그림으로 볼 수 있다. 1keras.utils.plot_model(model, show_shapes=True, to_file='cnn-architecture.png', dpi=300) show_shapes=True 로, 입력과 출력의 크기가 표시되며, to_file 매개변수는 출력한 이미지를 파일로 저장한다. dpi 매개변수는 해상도를 지정한다. 4. 모델 컴파일과 훈련Adam 옵티마이저를 사용하고, ModelCheckpoint, EarlyStopping 콜백을 사용하여 조기 종료 법을 구현한다. 1234567model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5')early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) 123456789101112131415161718Epoch 1/201500/1500 [==============================] - 19s 7ms/step - loss: 0.5307 - accuracy: 0.8096 - val_loss: 0.3467 - val_accuracy: 0.8756Epoch 2/201500/1500 [==============================] - 10s 7ms/step - loss: 0.3584 - accuracy: 0.8720 - val_loss: 0.3089 - val_accuracy: 0.8859Epoch 3/201500/1500 [==============================] - 11s 7ms/step - loss: 0.3119 - accuracy: 0.8876 - val_loss: 0.2708 - val_accuracy: 0.8968Epoch 4/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2770 - accuracy: 0.8992 - val_loss: 0.2580 - val_accuracy: 0.9047Epoch 5/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2544 - accuracy: 0.9065 - val_loss: 0.2518 - val_accuracy: 0.9101Epoch 6/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2322 - accuracy: 0.9149 - val_loss: 0.2452 - val_accuracy: 0.9122Epoch 7/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2171 - accuracy: 0.9195 - val_loss: 0.2294 - val_accuracy: 0.9176Epoch 8/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2013 - accuracy: 0.9257 - val_loss: 0.2560 - val_accuracy: 0.9134Epoch 9/201500/1500 [==============================] - 10s 7ms/step - loss: 0.1881 - accuracy: 0.9299 - val_loss: 0.2308 - val_accuracy: 0.9186 이전보다 정확도가 훨씬 좋아진 것을 확인할 수 있다. 손실 그래프를 그려, 조기 종료가 잘 이루어졌는지 확인한다. 1234567import matplotlib.pyplot as pltplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() 일곱 번째 에포크가 최적임을 알 수 있다. predict() 메서드를 사용하여 데이터에 대한 예측을 만들어 본다. 123456import numpy as npclasses = ['티셔츠','바지','스웨터','드레스','코트','샌달','셔츠','스니커즈','가방','앵클 부츠']preds = model.predict(val_scaled[0:1])print(classes[np.argmax(preds)]) 1가방 테스트 세트로 합성곱 신경망의 일반화 성능을 가늠해본다. 12test_scaled = test_input.reshape(-1, 28, 28, 1) / 255.0model.evaluate(test_scaled, test_target) 12313/313 [==============================] - 2s 7ms/step - loss: 0.2460 - accuracy: 0.9108[0.24599412083625793, 0.9107999801635742] 약 91% 정도의 성능을 기대할 수 있다.","link":"/2024/04/03/Convolutional-Neural-Network/"},{"title":"Deep Learning for Self-Driving Cars","text":"자율주행차 MIT 딥러닝 코스의 딥러닝(Deep Learning for Self-Driving Cars) 과정 중 DeepTraffic을 참고하여 포스팅을 작성하였습니다.고속도로 교통량이 밀집한 곳을 통해 차량(또는 복수의 차량)을 최대한 빨리 주행할 수 있는 신경망을 만드는 것이 목표이다. 신경망의 한 예가 차들 중 하나를 제어(빨간색으로 표시됨)최대한 빨리 가기 위해 효율적으로 탐색하는 방법을 배워야 한다.차는 이미 안전장치가 달려있기 때문에 당신은 운전의 기본적인 과제에 대해 걱정할 필요가 없다뉴럴넷은 가속/감속 또는 차선을 변경해야 하는지 자동차에게 알려주면 됨그리고 그것이 다른 차들과 충돌하지 않고 가능하다면 그렇게 할 것이다.아래 것들을 참고하자 1234567891011@inproceedings{fridman2018deeptraffic,author = {Lex Fridman and Jack Terwilliger and Benedikt Jenik},title = {DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement Learning Systems for Multi-Agent Dense Traffic Navigation},booktitle = {Neural Information Processing Systems (NIPS 2018) Deep Reinforcement Learning Workshop}year = {2018},url = {http://arxiv.org/abs/1801.02805},doi = {10.5281/zenodo.2530457}archivePrefix = {arXiv},} 개요 게임 페이지는 4개의 다른 영역으로 구성되어 있다. 왼쪽에는 다양한 디스플레이 옵션으로 도로의 실시간 시뮬레이션을 볼 수 있다. 페이지 상단에서 (1) 작용제를 제어하는 신경망의 설계를 변경할 수 있는 코딩 영역과 (2) 변경사항 적용, 저장/로드 및 제출을 위한 일부 버튼을 찾을 수 있다. 코딩 영역 아래에서 (1) 중앙 적색 자동차의 보상의 이동 평균을 보여주는 그래프, (2) 신경망 활성화의 시각화, (3) 네트워크 훈련 및 테스트를 위한 버튼을 찾을 수 있다. 시뮬레이션된 도로와 그래프 사이에서 차량의 현재 이미지와 사용자 정의하기 위한 몇 가지 옵션을 찾을 수 있으며 가장 적합한 하위 항목의 시각화를 만들 수 있다.시뮬레이션 영역에는 현재 차의 속도나 사이트를 연 이후 통과된 차량의 수와 같은 기본적인 정보가 표시된다. 또한 시뮬레이션 표시 방법을 변경할 수 있다. 시뮬레이션은 내부 시간 척도로 프레임을 사용하기 때문에 느린 컴퓨터도 느린 뉴럴 넷도 결과에 영향을 미치지 않는다.Simulation Speed 설정을 사용하면 시뮬레이션이 실시간으로 일치하는 프레임을 그리려고 하기 때문에 실제 계산이 더 빨리 진행되는지 대기하는 지 등 시뮬레이션이 완료되는 즉시 프레임을 표시하므로 시뮬레이션 속도 설정을 통해 시뮬레이션이 표시되는 방법을 제어할 수 있다. 내부적으로는 게임 전체가 그리드 시스템으로 운영된다. Road Overlay를 전체 맵으로 변경하면 볼 수 있다. 각각의 자동차에 대해 그 아래의 그리드 셀은 자동차의 속도로 채워지고, 빈 셀은 속도의 잠재력을 상징하기 위해 높은 값으로 채워진다. 당신의 차는 신경망의 입력으로 사용하기 위해 그 지도에서 자동차 중심의 컷아웃을 얻는다. 도로 오버레이를 학습 입력으로 변경하여 살펴보십시오. 다음 변수는 네트가 얻는 입력의 크기를 조절한다.입력 영역이 클수록 트래픽 상황에 대한 더 많은 정보를 제공하지만, 관련 부분을 배우는 것이 더 어려워지고, 학습 시간이 더 길어질 수 있다.(그러나 당신은 시작 샘플에 우리가 가지고 있는 입력 크기를 확실히 변경해야 한다. 그것은 자동차를 본질적으로 블라인드(?)로 만든다) 123lanesSide = 1;patchesAhead = 10;patchesBehind = 0; 다른 모든 자동차에 전원을 공급하는 기본 알고리즘, 그리고 당신의 기본은 안전 시스템이라고 불리는데, 당신은 도로 오버레이를 바꾸면 그것을 볼 수 있다. 강조 표시된 셀은 현재 빨간색인 경우 안전 시스템이 해당 방향으로 이동하는 것을 보여 준다.안전장치 전면부는 장애물에 부딪히지 않기 위해 차의 속도를 늦추게 한다.강조 표시된 부분에 다른 차량이 있는 동안 그리고 이미 차선을 바꾸는 과정에 있는 경우 차선 전환이 비활성화된다.얼마나 빨리 가려고 하느냐에 따라 점검 구역이 늘어나기 때문에 flooring은 항상 좋은 것은 아니다. 에이전트는 현재 상태(정의된 학습 입력 컷아웃의 평평한 배열로 제공됨)를 수신하는 학습이라는 함수에 의해 제어되며,마지막 단계에 대한 보상(이 경우 평균 속도(mph))을 받고 다음 동작 중 하나를 반환해야 한다. 12345var noAction = 0;var accelerateAction = 1;var decelerateAction = 2;var goLeftAction = 3;var goRightAction = 4; 에이전트에 속도와 차선을 유지하라고 지시하는 가장 기본적인 학습 기능은 다음과 같다. 123learn = function (state, lastReward) { return 0;} 경기를 위해 신경망을 사용하여 자동차를 제어해야 하는 – 학습 기능 123456789learn = function (state, lastReward) { brain.backward(lastReward); var action = brain.forward(state); draw_net(); draw_stats(); return action;} 이 일이 일어나도록 하는 것은 이미 초기 코드 샘플에 제공되어 있고 그대로 유지될 수 있다당신은 물론 그 상태를 인터넷에 공급하기 전에 당신 자신의 데이터 사전 처리를 할 수 있지만 시간을 너무 많이 쓰지는 마라.대부분의 (전부는 아니더라도) 개선은 net 적응에서 이루어져야 한다(그리고 당신은 코스를 통과하는 데 필요한 최소값을 훨씬 초과하여 사전 처리를 전혀 하지 않고 상당히 적절한 속도를 얻을 수 있다). 교육 및 평가신경망을 훈련하려면 교육 실행 버튼을 눌러야 한다. 이렇게 하면 실시간 속도가 약 30배인 별도의 스레드에서 시뮬레이션을 실행하여 신경망 훈련을 시작하고 훈련된 그물을 가시적 시뮬레이션에 수시로 다시 적용하여 즉각적인 개선 효과를 볼 수 있어야 한다.(당신의 순 레이아웃이 물론 좋은 경우에만). 이 사이트는 또한 우리가 경기에 사용하는 것과 동일한 평가를 실행할 평가 버튼을 제공한다. Evaluation Run은 또한 각각 약 30초 동안 500회 주행 시뮬레이션하는 별도의 스레드에서도 이루어진다.각 주행에 대해 주행당 평균 속도를 계산하며, 최종 점수는 500회 주행의 중간 속도가 된다. 다른 차들이 어떻게 움직이는지에 관련된 임의의 요소들이 있기 때문에, 당신의 지역 평가는 실제 점수에 대한 추정치만을 준다는 것을 명심해야 한다.관련 점수는 우리가 계산한 것과 같다. (그리고 우리는 또한 부정행위와 관련된 어떤 종류의 것이 있는지 알아보기 위해 당신의 코드를 볼 것이다. 그것은 금지시킬 것이다 – 그러니 시도조차 하지 말라. 당신은 당신의 프로필 페이지에서 최고의 속도를 찾을 수 있다. 당신이 정말로 10위 안에 든다면. 여러 차량 제어최대 10대의 차량을 제어할 수 있다. 다음 코드 행 변경을 통해서 1otherAgents = 9; // plus the tracked agent 각 에이전트, 집합적으로 안 할 수 있는 알고리즘의 인스턴스를 운영하고 있다. 빨리 운전하려면 네트워크 교통 체증이 유발되고 것을 피하는 방법을 배워야 할 것이다. 여러 agent들을 이용하면, 너의 점수는 올라갈 것이다. 신경망 설계우리가 제공하는 기본 신경망 레이아웃을 변경하려면(잘못 수행되도록 의도적으로 변경됨) 웹사이트의 코드 박스에서 코드를 변경해야 한다. apply code 버튼은 코드를 실행하여 새로 정의된 신경망을 생성한다 (조심: 이전에 사용했던 훈련 상태를 잃게 된다). 그리고 저장 및 로드 버튼을 사용하면 코드와 훈련된 네트 상태를 기계에 저장한 후 다시 로드할 수 있다. 규칙적으로 저장하라! 코드1234lanesSide = 0;patchesAhead = 1;patchesBehind = 0;trainIterations = 10000; 가장 기본적인 설정을 정의더 큰 입력의 경우 train iterations 숫자를 늘려야 한다.사실 몇 개의 패치를 앞에 두고, 그리고 적어도 옆으로 가는 한 개의 차선도 아마 좋은 생각일 것이다. 1234var num_inputs = (lanesSide * 2 + 1) * (patchesAhead + patchesBehind);var num_actions = 5;var temporal_window = 3;var network_size = num_inputs * temporal_window + num_actions * temporal_window + num_inputs; 입력에 대한 자세한 내용을 지정임시 창은 제외하고 해당 부품을 만질 필요가 없음. 넷은 변경할 필요가 없는 입력부터 시작하는 계층의 배열로 정의된다. 1234567var layer_defs = [];layer_defs.push({ type: 'input', out_sx: 1, out_sy: 1, out_depth: network_size}); 하나의 뉴런으로 이루어진 히든 기본 레이어를 추가해 놓았다 어떻게 하는지를 보여주기 위해서-이 부분도 너가 바꿔줘야 한다. 12345layer_defs.push({ type: 'fc', num_neurons: 1, activation: 'relu'}); 그리고 결국 작용에 대해 결정하는 최종 회귀층이 있는데, 그것은 아마도 다음과 같다. 1234layer_defs.push({ type: 'regression', num_neurons: num_actions}); Q-Learning 파트에는 훨씬 더 많은 옵션이 있다. 자세한 내용은 다음 링크의 코드에 있는 코드에 설명되어 있다.https://github.com/karpathy/convnetjs/blob/master/build/deepqlearn.js이것들은 당신의 네트의 보다 진보된 최적화에 대부분 흥미롭다. 123456789101112131415161718var tdtrainer_options = { learning_rate: 0.001, momentum: 0.0, batch_size: 64, l2_decay: 0.01};var opt = {};opt.temporal_window = temporal_window;opt.experience_size = 3000;opt.start_learn_threshold = 500;opt.gamma = 0.7;opt.learning_steps_total = 10000;opt.learning_steps_burnin = 1000;opt.epsilon_min = 0.0;opt.epsilon_test_time = 0.0;opt.layer_defs = layer_defs;opt.tdtrainer_options = tdtrainer_options; 그리고 마지막 단계는 뇌를 만드는 것이다. 1brain = new deepqlearn.Brain(num_inputs, num_actions, opt); Submission평가를 위해 신경망을 제출하려면 제출 버튼을 누르십시오. 교육 및 현지 평가를 먼저 실행하고, 성과에 만족하는 경우에만 제출하십시오.당신은 여러 번 제출할 수 있고 우리는 최선의 결과를 얻을 수 있지만, 너무 자주 하는 것은 좋지 않다:제출은 우리의 평가 대기열 뒤에 당신의 Net을 더하지만, 당신은 거기에 한 자리만 있을 수 있기 때문에 평가가 끝나기 전에 다시 제출하면, 당신은 다시 뒤에 부딪히게 된다. 평가 결과를 보려면 프로필 페이지로 이동하십시오. 이 클래스에 공식적으로 등록한 경우 이 할당에 대한 학점을 취득하려면 65mph 이상의 성능을 발휘해야 한다. Customization &amp; Visualization시뮬레이션된 고속도로와 그래프 사이에는 차량의 이미지, DeepTraffic의 모양을 사용자 지정하는 옵션, 최상의 제출물 시각화를 요청하는 버튼이 있다. 사용자 지정 차량 이미지(png 파일만 해당)를 업로드하려면 LOAD CUSTOM IMAGE 버튼을 클릭하십시오.png 파일을 선택한 후 268×586 이미지로 잘라야 한다.드롭다운 색상 선택기를 사용하여 중앙 에이전트의 추적과 같은 색상표를 사용자 정의할 수도 있다.제출을 완료한 후 최상의 성능을 시각화하도록 요청할 수 있다. 이 시각화는 .mp4 파일이다.준비가 되면 프로필 페이지에서 다운로드 링크를 찾을 수 있다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182//&lt;![CDATA[// a few things don't have var in front of them - they update already existing variables the game needslanesSide = 3; // 옆 패치???patchesAhead = 50; // 앞 패치patchesBehind = 10; // 뒷 패치trainIterations = 500000; // 더 큰 input의 경우 늘려줘야// the number of other autonomous vehicles controlled by your networkotherAgents = 10; // max of 10 // 몇대의 차를 제어할 것인가...var num_inputs = (lanesSide * 2 + 1) * (patchesAhead + patchesBehind);var num_actions = 5;var temporal_window = 0;var network_size = num_inputs * temporal_window + num_actions * temporal_window + num_inputs;var layer_defs = []; layer_defs.push({ type: 'input', out_sx: 1, out_sy: 1, out_depth: network_size});layer_defs.push({ type: 'fc', num_neurons: 36, activation: 'tahn'});layer_defs.push({ type: 'fc', num_neurons: 24, activation: 'tanh'});layer_defs.push({ type: 'fc', num_neurons: 24, activation: 'tahn'});layer_defs.push({ type: 'fc', num_neurons: 24, activation: 'tanh'});layer_defs.push({ type: 'regression', num_neurons: num_actions});var tdtrainer_options = { learning_rate: 0.001, momentum: 0.0, batch_size: 128, l2_decay: 0.01};var opt = {};opt.temporal_window = temporal_window;opt.experience_size = 100000;opt.start_learn_threshold = 5000;opt.gamma = 0.98;opt.learning_steps_total = 500000;opt.learning_steps_burnin = 1000;opt.epsilon_min = 0.0;opt.epsilon_test_time = 0.0;opt.layer_defs = layer_defs;opt.tdtrainer_options = tdtrainer_options;brain = new deepqlearn.Brain(num_inputs, num_actions, opt);learn = function (state, lastReward) { brain.backward(lastReward); var action = brain.forward(state); draw_net(); draw_stats(); return action;}//]]&gt;","link":"/2020/04/08/Deep-Learning-for-Self-Driving-Cars/"},{"title":"심층 신경망(Deep Neural Network)","text":"심층 신경망(Deep Neural Network, DNN)은 여러 개의 은닉층을 포함하는 인공 신경망의 한 종류이다. 인공 신경망은 입력층(input layer), 하나 이상의 은닉층(hidden layers), 그리고 출력층(output layer)으로 구성되며, 이 중에서 은닉층이 여러 개인 경우를 심층 신경망이라고 한다. 심층 신경망은 복잡한 데이터에서 높은 수준의 추상화와 패턴 인식을 수행할 수 있으며, 이미지 인식, 자연어 처리, 음성 인식 등 다양한 분야에서 광범위하게 활용된다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 심층 신경망의 특징: 다층 구조: 심층 신경망은 두 개 이상의 은닉층을 가진다. 은닉층의 수가 많을수록 네트워크는 더 복잡한 패턴과 관계를 학습할 수 있다. 비선형성: 심층 신경망은 비선형 활성화 함수를 사용하여 입력 데이터의 비선형 특성을 모델링한다. 이를 통해 선형 모델로는 표현할 수 없는 복잡한 패턴을 학습할 수 있다. 자동 특성 추출: 심층 신경망은 데이터로부터 중요한 특성을 자동으로 학습하고 추출할 수 있다. 이는 수동으로 특성을 설계하는 작업을 줄여준다. 범용 근사자: 이론적으로 심층 신경망은 어떤 함수도 근사할 수 있는 범용 함수 근사자(universal function approximator)로 간주된다. 활용 분야: 컴퓨터 비전: 이미지 분류, 객체 탐지, 이미지 생성 등에 활용된다. 자연어 처리: 기계 번역, 감성 분석, 텍스트 요약 등의 작업에 사용된다. 음성 인식: 음성을 텍스트로 변환하거나, 음성 명령을 인식하는 데 사용된다. 게임 및 로봇 공학: 자율 주행, 게임 AI, 로봇의 의사 결정 등에 활용된다. 도전 과제: 과적합(Overfitting): 모델이 훈련 데이터에 지나치게 최적화되어 새로운 데이터에 대한 일반화 능력이 떨어질 수 있다. 해석성(Interpretability): 심층 신경망의 결정 과정이 “블랙 박스”처럼 보일 수 있어, 모델의 예측을 해석하기 어려울 수 있다. 계산 비용: 심층 신경망의 학습은 대량의 데이터와 고성능의 컴퓨팅 자원을 요구한다. 이제 여러 개의 층을 추가하여 다층 인공 신경망, 즉 심층 신경망을 만들고, 은닉층에 사용하는 활성화 함수인 렐루 함수, 가중치와 절편을 학습하기 위한 옵티마이저를 알아본다. 1. 데이터 준비데이터를 표준화 전처리하고 훈련세트와 검증세트로 나눈다. 123456789from tensorflow import kerasfrom tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 시그모이드 함수로 밀집층 추가하기인공 신경망과 달리, 입력층과 출력층 사이에 밀집층을 추가한다. 이를 은닉층이라고 한다. 2-1. 은닉층의 활성화 함수 : 시그모이드인공 신경망에서 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다.단 출력층에서는 보통 이진 분류에서는 시그모이드 함수, 다중 분류에서는 소프트맥스를 사용한다.은닉층에도 활성화 함수가 적용되는데, 대표적으로 시그모이드 함수와 볼 렐루 함수가 있다. 은닉층 활성화 함수를 적용하는 이유는 선형 계산을 비선형으로 비틀어 주어 다음 층의 계산과 합쳐지지 않고 역할을 수행할 수 있기 때문이다. 아래 그림은 시그모이드 그래프이다. 이 함수는 뉴런의 출력 z값을 0과 1사이로 압축한다. 이를 사용해 은닉층을 만든다. 2-2. 시그모이드 활성화 함수로 심층 신경망 생성하기1234dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))#출력층에서 10개의 클래스를 분류하므로 10개의 뉴런, 소프트맥스 활성화함수dense2 = keras.layers.Dense(10, activation='softmax') activation='sigmoid'로 활성화 함수를 시그모이드로 지정할 수 있다.은닉층에서 100개의 뉴런을 지정했는데, 이는 특별한 기준이 없지만, 출력층의 뉴런보다는 많이 만들어야한다. 이제 위의 두 개층을 Sequential 클래스에 추가하여 심층 신경망을 만든다.두 개의 층을 리스트로 Sequential 클래스에 전달한다. 12model = keras.Sequential([dense1, dense2])model.summary() 1234567891011Model: &quot;sequential_2&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ summary() 메서드로 층에 대한 정보를 얻는다. dense의 출력 크기를 보면 (None, 100)으로, 첫번째 차원은 샘플 크기를 나타낸다. 샘플 갯수가 아직 정의되지 않아 None 이며, 후에 fit() 매서드에 훈련 데이터를 주입하면 미니배치 경사 하강법을 사용한다.케라스의 기본 미니베치 크기는 32개이며, fit() 메서드에서 batch_size 매개변수로 바꿀 수 있다. 두번째 100개 출력은, 784개의 특성이 은닉층을 통과하며 100개의 특성으로 압축됨을 뜻한다. 모델 파라미터 갯수는 입력픽셀 784개와 100개의 모든 조합에 대한 가중치, 100개의 절편이 있어 784*100 + 199 = 78500개 이다. 두번째 층의 파라미터 또한 100*10 + 10 = 1010개 이다. 2-3. 층을 추가하는 다른 방법Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 방법이 있다. 1234model = keras.Sequential([ keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'), keras.layers.Dense(10, activation='softmax', name=&quot;output&quot;)], name='패션 MNIST모델') 너무 많은 층을 추가하려면 생성자가 매우 길어지기 때문에, add() 메서드도 사용한다. 123model = keras.Sequential()model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax')) 2-4. 심층 신경망 모델 훈련12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 5s 3ms/step - loss: 0.5596 - accuracy: 0.8103Epoch 2/51500/1500 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8556Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3716 - accuracy: 0.8658Epoch 4/51500/1500 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8735Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3320 - accuracy: 0.8801 추가된 층이 성능을 항상시켰다는 것을 알 수 있다. 3. 렐루 활성화 함수초창기 인공 신경망의 은닉층에 많이 사용된 활성화 함수는 시그모이드 함수였다.다만 이 함수는 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다는 단점이 있다. 이는 층이 많은 신경망일수록 효과가 누적되어 학습을 어렵게 한다. 이를 개선하기 위해 렐루함수가 사용된다. 렐루 함수는 max(0,z)로 쓸 수 있다. 이는 특히 이미지 처리에 좋은 성능을 낸다. 3-1. 입력 차원을 일렬로 펼치는 Flatten 층렐루 함수를 적용하기 전, 입력차원을 일렬로 펼치는 Flatten 층을 알아본다.앞에서 reshape() 메서드를 사용하여 사진 데이터를 일렬로 펼쳤지만, 이를 입력층과 은닉층 사이에 추가할 수 있다. 1model.add(keras.layers.Flatten(input_shape=(28, 28))) 3-2. 렐루 함수를 이용한 밀집층 추가123456model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28, 28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() 123456789101112Model: &quot;sequential_4&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_4 (Dense) (None, 100) 78500 dense_5 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다. 3-3. 훈련 데이터로 모델 훈련123456(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5249 - accuracy: 0.8142Epoch 2/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3924 - accuracy: 0.8590Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3553 - accuracy: 0.8712Epoch 4/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3310 - accuracy: 0.8810Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.8868 검증 세트로 모델을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.8674[0.39478805661201477, 0.8674166798591614] 은닉층을 추가하지 않은 모델보다 성능이 몇 퍼센트 더 상승했다. 4. 옵티마이저 : 다양한 경사 하강 알고리즘신경망에는 모델이 학습되지 않아 사람이 지정해주어야 하는 하이퍼파라미터가 많다. 다양한 종류의 경사 하강법 알고리즘도 지정할 수 있는데, 이를 옵티마저라고 한다. 4-1. SGD : 확률적 경사 하강법compile() 메서드에서 케라스의 기본 경사 하강법 알고리즘은 RMSprop을 사용했다.확률적 경사 하강법인 SGD를 사용할 수 있는데, 이 역시 미니배치를 사용한다. 1model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 객체를 생성하여 옵티마저로 적용할 수 있다. 12sgd = keras.optimizers.SGD()model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 클래스의 학습률 기본값은 0.01이며, learning_rate 매개변수에 학습률을 지정할 수 있다. 1sgd = keras.optimizers.SGD(learning_rate=0.1) momentum 매개변수의 기본값은 0이고 0보다 큰 값으로 지정하면 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용할 수 있다. 보통 0.9 이상을 지정한다.nesterov 매개변수를 True로 바꾸면 네스테로프 모멘텀 최적화를 사용한다. 1sgd = keras.optimizers(momentum=0.9, nesterov=True) 네스테로프 모멘텀은 모멘텀 최적화를 두번 반복하여 구현한다. 대부분 기본 확률적 경사 하강법보다 나은 성능을 제공한다. 4-2. Adagrad, RMSprop : 적응적 학습률 사용모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있으며, 이를 통해 안정적으로 최적점에 수렴할 가능성이 높다. 12345adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accurac') 모멘텀 최적화와 RMSprop 장점을 접목한 것이 Adam이다. 4-3. Adam : 모멘텀 최적화와 RMSprop의 장점 접목Adam 클래스의 매개변수 기본값을 사용해 모델을 훈련한다. 12345678model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))# 옵티마이저를 Adam으로 훈련model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5218 - accuracy: 0.8183Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3916 - accuracy: 0.8586Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3544 - accuracy: 0.8711Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3248 - accuracy: 0.8809Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.8880 검증 세트에서의 성능도 확인해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3426 - accuracy: 0.8767[0.3426271080970764, 0.8767499923706055]","link":"/2024/04/02/Deep-Neural-Network/"},{"title":"GPU 서버에 도커 이미지로 JupyterLab 배포하기","text":"GPU서버에서 딥러닝모델 학습을 위해 JupyterLab을 도커파일로 만들어 실행하는 방법을 정리해보겠다. JupyterLab 이미지 파일 만들기Dockerfile 1234567891011121314# 사용할 기본 이미지FROM python:3.8-slim # 작업 디렉토리 설정WORKDIR /app # 필요한 패키지와 라이브러리 설치RUN pip install --no-cache-dir jupyterlab numpy pandas matplotlib scipy # 포트 8888 열기EXPOSE 8888 # Jupyter Lab 실행 명령CMD [&quot;jupyter&quot;, &quot;lab&quot;, &quot;--ip=0.0.0.0&quot;, &quot;--port=8888&quot;, &quot;--allow-root&quot;, &quot;--NotebookApp.token=''&quot;, &quot;--NotebookApp.password=''&quot;] python:3.8-slim 이미지를 기반으로 사용. /app 디렉토리를 작업 공간으로 설정. pip를 이용해 jupyterlab을 설치. 컨테이너의 8888번 포트를 열어 외부에서 접근할 수 있게 함. Jupyter Lab을 루트 권한으로 실행하며, 모든 IP에서 접근을 허용하고, 보안 토큰과 비밀번호를 비활성화. 도커파일 버전2 123456789101112131415161718192021222324252627282930# CUDA와 cuDNN이 포함된 TensorFlow 기반 이미지 사용FROM tensorflow/tensorflow:latest-gpu # 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y \\ wget \\ bzip2 \\ ca-certificates \\ libglib2.0-0 \\ libxext6 \\ libsm6 \\ libxrender1 \\ git \\ mercurial \\ subversion # Miniconda 설치하여 Python 환경 관리RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; \\ /bin/bash ~/miniconda.sh -b -p /opt/conda &amp;&amp; \\ rm ~/miniconda.sh ENV PATH /opt/conda/bin:$PATH # Jupyter Notebook 및 주요 라이브러리 설치RUN conda install -y jupyter notebook matplotlib scipy scikit-learn pandas &amp;&amp; \\ conda clean -ya # Jupyter Notebook 설정EXPOSE 8888CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--notebook-dir=/notebooks&quot;, &quot;--ip='*'&quot;, &quot;--port=8888&quot;, &quot;--no-browser&quot;, &quot;--allow-root&quot;] Jupyter notebook 이미지 Docker로 빌드 시 ‘pip.conf’ 파일을 포함시키기pip.conf 파일은 Python 패키지를 설치할 때 ‘pip’의 동작을 사용자 정의하기 위해 사용한다. 1. ‘pip.conf’ 파일 생성먼저, 로컬 시스템에 ‘pip.conf’ 파일을 생성한다. 파일 내용은 ‘pip’ 동작을 사용자화하기 위한 구성을 포함할 수 있다.에를 들어, 아래와 같이 설정할 수 있다. 123[global]trusted-host = pypi.orgindex-url = https://pypi.org/simple 이 설정은 ‘pip’이 PyPI의 메인 인덱스에서 패키지를 설치하도록 지시하고 SSL 인증서 검증 문제를 방지한다. 2. Dockerfile 작성Dockerfile에서 ‘pip.conf’ 파일을 적절한 위치에 복사하는 단계를 포함한다. Python 이미지에서는 사용자 레벨의 설정을 적용하기 위해 ‘~/.pip/pip.conf’ 경로를 사용할 수 있다. 1234567891011121314151617# 기본 이미지 선택FROM jupyter/base-notebook# 작업 디렉토리 설정WORKDIR /app# pip.conf 파일 추가COPY pip.conf /etc/pip.conf# 필요한 Python 라이브러리 설치RUN pip install --no-cache-dir numpy pandas matplotlib# 포트 8888 열기EXPOSE 8888# Jupyter Lab 실행 명령CMD [&quot;start-notebook.sh&quot;] ‘jupyer/base-notebook’을 베이스 이미지로 사용한다. ‘pip.conf’ 파일을 Docker 이미지의 ‘/etc/pip.conf’로 복사한다. (시스템 전역 설정을 적용하기 위함. 필요에 따라 위치 변경 가능) 필요한 Python 패키지 설치 8888 포트를 열어 외부에서 접근가능하도록 함. Jupyter Lab을 시작. Jupyter notebook 이미지 Docker로 빌드 시 vim 텍스트 편집기 포함하기Jupyter Notebook 이미지를 빌드할 때 vi와 vim 텍스트 편집기를 포함하려면 Dockerfile에 해당 패키지 설치 명령을 추가해야 한다. 대부분의 Linux 배포판에서 vim은 기본적으로 vi를 포함하고 있으며, 더 많은 기능을 제공한다. 아래의 지침은 vim을 설치하는 방법을 포함하고 있으며, 이를 통해 vi의 기능도 사용할 수 있다. Dockerfile 작성다음은 jupyter/base-notebook 이미지를 기반으로 하여 vim을 설치하는 Dockerfile 예시이다. 12345678910111213141516171819# 기본 이미지FROM jupyter/base-notebook# 작업 디렉토리 설정WORKDIR /app# vim 설치USER rootRUN apt-get update &amp;&amp; \\ apt-get install -y vim# 필요한 Python 라이브러리 설치RUN pip install --no-cache-dir numpy pandas matplotlib# Jupyter Lab 실행 명령CMD [&quot;start-notebook.sh&quot;]# 포트 8888 열기EXPOSE 8888 설명 이미지 선택: jupyter/base-notebook은 Jupyter Lab을 실행할 수 있게 미리 구성된 기본 이미지이다. 작업 디렉토리 설정: /app 디렉토리를 작업 디렉토리로 설정한다. 사용자 변경: root 사용자로 변경하여 시스템 레벨의 변경을 수행할 수 있다. vim 설치: apt-get update를 실행하여 패키지 목록을 최신 상태로 유지하고, apt-get install -y vim을 사용하여 vim을 설치한다. Python 라이브러리 설치: numpy, pandas, matplotlib와 같은 주요 Python 라이브러리를 설치한다. 포트 열기: 외부에서 Jupyter Lab에 접근할 수 있도록 8888 포트를 연다. 실행 명령: 컨테이너가 시작될 때 start-notebook.sh 스크립트를 실행하여 Jupyter Lab을 시작한다. 추가 팁 라이브러리 버전 지정: 특정 버전의 라이브러리가 필요한 경우, pip install 명령에 버전을 명시할 수 있습니다. 예: pip install numpy==1.18.5 requirements.txt 사용: 많은 수의 라이브러리를 관리해야 하는 경우, requirements.txt 파일을 만들고 그 안에 필요한 라이브러리와 버전을 목록화할 수 있습니다. 그리고 Dockerfile에서 이 파일을 참조하여 모든 라이브러리를 한 번에 설치할 수 있습니다. 12COPY requirements.txt /app/RUN pip install --no-cache-dir -r requirements.txt Docker 이미지 빌드1docker build -t [이미지이름/태그] -f [도커파일이름].dockerfile . GPU서버에서 이미지 실행이제 빌드한 이미지를 실행하여 Jupyter Notebook 서버를 시작한다. GPU를 사용하도록 설정하려면 –gpus all 옵션을 추가한다. 1docker run --gpus all -p 8888:8888 -v /path/to/your/notebooks:/notebooks my-jupyter-gpu –gpus all: 컨테이너가 호스트의 모든 GPU를 사용하도록 설정한다. -p 8888:8888: 호스트와 컨테이너 간의 포트 매핑을 설정하여 Jupyter Notebook이 사용하는 포트 8888을 열어준다. -v /path/to/your/notebooks:/notebooks: 호스트 시스템의 디렉토리를 컨테이너 내부의 /notebooks 디렉토리에 마운트하여 노트북 파일들을 저장한다. 또는 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ sudo docker run --gpus all -it -d -p 59999:8888 -v /home/mydirectory/jupyter-notebook-test/:/home/work [이미지이름]:[태그] /bin/bash$ sudo docker exec -it 5f0cb0da5743 jupyter lab --allow-root --ip='0.0.0.0'[I 2024-05-09 04:56:38.457 ServerApp] jupyter_server_fileid | extension was successfully linked.[I 2024-05-09 04:56:38.461 ServerApp] jupyter_server_ydoc | extension was successfully linked.[I 2024-05-09 04:56:38.466 ServerApp] jupyterlab | extension was successfully linked.[I 2024-05-09 04:56:38.469 ServerApp] nbclassic | extension was successfully linked.[I 2024-05-09 04:56:38.642 ServerApp] notebook_shim | extension was successfully linked.[I 2024-05-09 04:56:38.655 ServerApp] notebook_shim | extension was successfully loaded.[I 2024-05-09 04:56:38.655 FileIdExtension] Configured File ID manager: ArbitraryFileIdManager[I 2024-05-09 04:56:38.655 FileIdExtension] ArbitraryFileIdManager : Configured root dir: /[I 2024-05-09 04:56:38.655 FileIdExtension] ArbitraryFileIdManager : Configured database path: /root/.local/share/jupyter/file_id_manager.db[I 2024-05-09 04:56:38.657 FileIdExtension] ArbitraryFileIdManager : Successfully connected to database file.[I 2024-05-09 04:56:38.657 FileIdExtension] ArbitraryFileIdManager : Creating File ID tables and indices with journal_mode = DELETE[I 2024-05-09 04:56:38.659 ServerApp] jupyter_server_fileid | extension was successfully loaded.[I 2024-05-09 04:56:38.660 ServerApp] jupyter_server_ydoc | extension was successfully loaded.[I 2024-05-09 04:56:38.664 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.7/site-packages/jupyterlab[I 2024-05-09 04:56:38.664 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab[I 2024-05-09 04:56:38.673 ServerApp] jupyterlab | extension was successfully loaded. _ _ _ _ | | | |_ __ __| |__ _| |_ ___ | |_| | '_ \\/ _` / _` | _/ -_) \\___/| .__/\\__,_\\__,_|\\__\\___| |_| Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions. https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html Please note that updating to Notebook 7 might break some of your extensions. [I 2024-05-09 04:56:38.682 ServerApp] nbclassic | extension was successfully loaded.[I 2024-05-09 04:56:38.683 ServerApp] Serving notebooks from local directory: /[I 2024-05-09 04:56:38.683 ServerApp] Jupyter Server 1.24.0 is running at:[I 2024-05-09 04:56:38.683 ServerApp] http://5f0cb0da5743:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:38.684 ServerApp] or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:38.684 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 2024-05-09 04:56:38.695 ServerApp] No web browser found: could not locate runnable browser.[C 2024-05-09 04:56:38.695 ServerApp] To access the server, open this file in a browser: file:///root/.local/share/jupyter/runtime/jpserver-15-open.html Or copy and paste one of these URLs: http://5f0cb0da5743:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:50.087 ServerApp] 302 GET / (221.148.246.64) 1.40ms[I 2024-05-09 04:56:50.116 LabApp] 302 GET /lab? (221.148.246.64) 1.56ms[I 2024-05-09 04:57:04.161 ServerApp] 302 POST /login?next=%2Flab%3F (221.148.246.64) 6.76ms[W 2024-05-09 04:57:05.041 ServerApp] 404 GET /api/me?1715230246656 (221.148.246.64) 9.13ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 04:57:06.285 LabApp] Could not determine jupyterlab build status without nodejs[I 2024-05-09 04:57:28.961 ServerApp] 302 GET / (221.148.246.64) 1.07ms[W 2024-05-09 04:57:29.202 ServerApp] 404 GET /api/me?1715230270950 (221.148.246.64) 4.24ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 04:57:30.501 LabApp] Could not determine jupyterlab build status without nodejs[I 2024-05-09 05:02:43.977 ServerApp] 302 GET / (221.148.246.64) 1.02ms[W 2024-05-09 05:02:44.158 ServerApp] 404 GET /api/me?1715230585905 (221.148.246.64) 3.15ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 05:02:45.460 LabApp] Could not determine jupyterlab build status without nodejs Jupyter Notebook 접속 [GPU 서버의 VirtualIP]:59999로 접속 기동시 나온 토큰 입력 JupyterLab 재기동하기 실행 중인 컨테이너 조회 1sudo docker ps 실행 중인 컨테이너 중 59999포트로 실행 중인 JupyterLab 컨테이너를 종료 1docker stop container_id_or_name JupyterLab 이미지 실행 1docker run --gpus all -it -d -p 59999:8888 -v /directory/jupyter-notebook-test/:/home/work [이미지이름]:[태그] /bin/bash 이미지 실행 되면 해당 컨테이너에서 jupyter lab 프로세스 실행 12345678910111213docker exec -it 5f0cb0da5743 jupyter lab --allow-root --ip='0.0.0.0'~~~5. 실행 된 jupyter lab의 컨테이너 터미널 로그에 표시된 토큰을 이용하여 JupyterLab 접속예시)~~~bsh[C 2024-05-09 04:56:38.695 ServerApp] To access the server, open this file in a browser: file:///root/.local/share/jupyter/runtime/jpserver-15-open.html Or copy and paste one of these URLs: http://5f0cb0da5743:8888/lab?token=12f6cfeaabec704c03ef65ac230580440f7b32145c592698 or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd10809839e97149705a 사용할 GPU의 수 설정하기 모든 GPU 사용: 컨테이너에 시스템의 모든 GPU를 할당하려면 all 키워드를 사용한다. 1docker run --gpus all nvidia/cuda:10.0-base nvidia-smi 특정 수의 GPU 사용: 특정 수의 GPU만 할당하려면 count 키워드를 사용한다. 예를 들어, 사용 가능한 GPU 중 2개만 할당하려면 다음과 같이 한다. 1docker run --gpus count=2 nvidia/cuda:10.0-base nvidia-smi 특정 GPU 사용: 특정 GPU를 지정하려면 device 키워드와 함께 GPU의 ID를 사용한다. 예를 들어, 첫 번째와 세 번째 GPU만 사용하려면 다음과 같이 명령한다. 1docker run --gpus '&quot;device=0,2&quot;' nvidia/cuda:10.0-base nvidia-smi 주의 사항 –gpus 옵션을 사용하기 전에, NVIDIA GPU 드라이버와 NVIDIA Docker 플러그인(NVIDIA Container Toolkit)이 설치되어 있어야 한다. 이 옵션은 CUDA와 같은 GPU 가속 라이브러리를 사용하는 애플리케이션에 특히 유용하다. Docker가 GPU를 사용하도록 설정하기 위해서는 Docker 엔진 설정에서 default-runtime을 nvidia로 설정하는 추가 설정이 필요할 수 있다.","link":"/2024/05/09/Deploy-JupyterLab-on-Gpu/"},{"title":"실시간 추론을 위해 모델을 SageMaker에 배포하기","text":"SageMaker에 Gemma 2b 모델을 추론 모델로 배포하고 사용하기 빠른 설정 SageMaker 콘솔에 접속 왼쪽 탐색 창 중 Admin configurations(관리 구성)에서 Domains(도메인)를 선택 Create domain을 선택 빠른 설정 선택 사용자 지정 설정 인증방법(AWS Identity Center 또는 IAM) 선택 S3 버킷 기입 gemma-1.1-2b-it를 이용하여https://huggingface.co/google/gemma-1.1-2b-it?sagemaker_deploy=true 123456789101112131415161718192021222324252627282930313233343536373839import jsonimport sagemakerimport boto3from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uritry: role = sagemaker.get_execution_role()except ValueError: iam = boto3.client('iam') role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']# Hub Model configuration. https://huggingface.co/modelshub = { 'HF_MODEL_ID':'google/gemma-1.1-2b-it', 'SM_NUM_GPUS': json.dumps(1), 'HUGGING_FACE_HUB_TOKEN': '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'}assert hub['HUGGING_FACE_HUB_TOKEN'] != '&lt;REPLACE WITH YOUR TOKEN&gt;', &quot;You have to provide a token.&quot;# create Hugging Face Model Class# 2.0.2가 지원되지 않는 버전이라 하여 2.0.1로 바꿈huggingface_model = HuggingFaceModel( image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;,version=&quot;2.0.1&quot;), env=hub, role=role,)# deploy model to SageMaker Inferencepredictor = huggingface_model.deploy( initial_instance_count=1, instance_type=&quot;ml.c5d.4xlarge&quot;, container_startup_health_check_timeout=300,)# send requestpredictor.predict({ &quot;inputs&quot;: &quot;My name is Clara and I am&quot;,}) 에러 1ClientError: An error occurred (AccessDeniedException) when calling the CreateEndpointConfig operation: User: arn:aws:sts::058264433760:assumed-role/SageMaker-ExecutionRole-20240603T163452/SageMaker is not authorized to perform: sagemaker:CreateEndpointConfig on resource: arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint-config/huggingface-pytorch-tgi-inference-2024-06-04-02-00-16-823 because no identity-based policy allows the sagemaker:CreateEndpointConfig action 해결책 이 오류는 현재 사용 중인 IAM 역할에 sagemaker:CreateEndpointConfig 작업을 수행할 권한이 없어서 발생합니다. 이를 해결하기 위해서는 IAM 콘솔에서 해당 역할에 필요한 권한을 추가해야 합니다. 다음 단계에 따라 해결할 수 있습니다: IAM 콘솔 열기: AWS Management Console에서 IAM 콘솔을 엽니다.역할 찾기: 좌측 메뉴에서 “Roles”를 선택합니다.오류 메시지에 나오는 역할 이름(SageMaker-ExecutionRole-20240603T163452)을 검색하여 역할을 찾습니다.정책 추가: 역할을 클릭하여 역할의 세부 정보 페이지로 이동합니다.“Add permissions” 버튼을 클릭합니다.“Attach policies”를 선택합니다.검색 창에 AmazonSageMakerFullAccess를 입력하고, 검색 결과에서 해당 정책을 선택합니다.“Next” 버튼을 클릭하고 “Add permissions” 버튼을 클릭하여 정책을 역할에 추가합니다.커스텀 정책 생성 (선택사항): 만약 AmazonSageMakerFullAccess 정책을 사용하는 것이 너무 광범위하다면, 특정 권한만 포함된 커스텀 정책을 생성할 수 있습니다. “Policies” 메뉴를 선택하고 “Create policy” 버튼을 클릭합니다. “JSON” 탭을 선택하고 다음 JSON을 입력합니다: 12345678910111213141516{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;sagemaker:CreateEndpointConfig&quot;, &quot;sagemaker:CreateEndpoint&quot;, &quot;sagemaker:UpdateEndpoint&quot;, &quot;sagemaker:DeleteEndpoint&quot;, &quot;sagemaker:DescribeEndpoint&quot; ], &quot;Resource&quot;: &quot;*&quot; } ]} “Review policy” 버튼을 클릭하고, 정책 이름을 입력한 후 “Create policy” 버튼을 클릭하여 정책을 생성합니다. 역할로 돌아가서, 새로 생성된 정책을 역할에 추가합니다. 위 단계를 완료한 후, 다시 코드를 실행하여 필요한 권한이 부여되었는지 확인합니다. 권한 문제가 해결되면, CreateEndpointConfig 작업이 성공적으로 수행될 것입니다. 해결 후 새로운 에러 1UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2024-06-04-02-09-06-986: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html 이 오류는 생성된 엔드포인트의 기본 컨테이너가 헬스 체크를 통과하지 못했음을 의미합니다. 이를 해결하기 위해 다음 단계를 따르세요: CloudWatch 로그 확인: AWS Management Console에서 CloudWatch 콘솔을 엽니다. 로그 그룹에서 SageMaker 엔드포인트와 관련된 로그 그룹을 찾습니다. 일반적으로 aws/sagemaker/Endpoints 아래에 있습니다. 엔드포인트의 로그를 검토하여 실패 원인에 대한 구체적인 오류 메시지를 확인합니다. 인스턴스 유형 확인: SageMaker 엔드포인트가 사용하고 있는 인스턴스 유형이 적절한지 확인합니다. 인스턴스 유형이 모델 실행에 충분한 리소스를 제공해야 합니다. 필요한 경우, 더 많은 메모리와 CPU를 제공하는 인스턴스 유형으로 변경합니다. 모델 설정 확인: 모델 설정에서 올바른 이미지 URI 및 환경 변수가 설정되었는지 확인합니다. 이미지 URI와 관련된 모든 환경 변수가 올바르게 설정되었는지 다시 확인합니다. 트러블슈팅 가이드 참고: A- WS SageMaker 트러블슈팅 페이지를 참고하여 일반적인 문제와 해결책을 검토합니다. 다음은 CloudWatch 로그를 확인하고 문제를 진단하는 방법에 대한 예입니다: 12345678910111213141516171819import boto3# CloudWatch Logs 클라이언트 생성logs_client = boto3.client('logs')# 로그 그룹 이름과 스트림 이름을 설정log_group_name = '/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2024-06-04-02-09-06-986'log_stream_name = 'AllTraffic'# 로그 스트림에서 로그 이벤트 가져오기response = logs_client.get_log_events( logGroupName=log_group_name, logStreamName=log_stream_name, startFromHead=True)# 로그 이벤트 출력for event in response['events']: print(event['message']) 이 코드를 사용하여 CloudWatch 로그를 프로그램matically 가져와 확인할 수 있습니다. 로그를 통해 문제의 구체적인 원인을 파악한 후, 해당 문제를 해결하기 위한 조치를 취할 수 있습니다. 인스턴스 유형 변경 예제만약 인스턴스 유형을 변경하려는 경우, 아래 예제를 참고하세요: 1234567891011121314151617181920212223242526272829303132import jsonimport sagemakerimport boto3from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uritry: role = sagemaker.get_execution_role()except ValueError: iam = boto3.client('iam') role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']# Hub Model configuration. https://huggingface.co/modelshub = { 'HF_MODEL_ID': 'google/gemma-1.1-2b-it', 'SM_NUM_GPUS': json.dumps(1), 'HUGGING_FACE_HUB_TOKEN': 'your_actual_token_here'}assert hub['HUGGING_FACE_HUB_TOKEN'] != 'your_actual_token_here', &quot;You have to provide a token.&quot;# create Hugging Face Model Classhuggingface_model = HuggingFaceModel( image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;, version=&quot;2.0.1&quot;), # 지원되는 버전으로 변경 env=hub, role=role,)# 배포 시 인스턴스 유형을 변경predictor = huggingface_model.deploy( initial_instance_count=1, instance_type='ml.p3.2xlarge' # 더 큰 인스턴스 유형으로 변경) JumpStart로 배포하고 테스트하기https://www.youtube.com/watch?v=UigWJPfClcI https://www.youtube.com/watch?v=ZdOcrLKow3I https://www.youtube.com/watch?v=kcuyovwbznY https://www.youtube.com/watch?v=vQFuZUAFel4 inference-experience에 들어가서 Test inference 탭에서 테스트 진행 python SDK example code123456789101112131415161718192021from sagemaker.predictor import retrieve_defaultendpoint_name = &quot;jumpstart-dft-hf-llm-gemma-2b-20240609-080102&quot;predictor = retrieve_default(endpoint_name)payload = { &quot;inputs&quot;: &quot;Write me a poem about Machine Learning.&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256 }}response = predictor.predict(payload)print(response)payload = { &quot;inputs&quot;: &quot;Hello everyone, my name is &quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;top_p&quot;: 0.9, &quot;temperature&quot;: 0.2 }}response = predictor.predict(payload)print(response) Sample request Content type : application.json Json123456789101112{ &quot;body&quot;: { &quot;inputs&quot;: &quot;&lt;bos&gt;&lt;start_of_turn&gt;user\\nWrite me a poem about Machine Learning&lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;decoder_input_details&quot;: true, &quot;details&quot;: true } }, &quot;contentType&quot;: &quot;application/json&quot;, &quot;endpointName&quot;: &quot;jumpstart-dft-hf-llm-gemma-2b-instr-20240611-004431&quot;} request12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586{ &quot;body&quot;: [ { &quot;generated_text&quot;: &quot;&lt;bos&gt;&lt;start_of_turn&gt;user\\nWrite me a poem about Machine Learning&lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model**Machine Learning**\\n\\nAlgorithms dance, a digital ballet,\\nLearning from data, a never-ending tally.\\nData streams in, a torrent of bytes,\\nAlgorithms sift and sort, with tireless eyes.\\n\\nFrom images to speech, the patterns unfold,\\nPredicting the future, stories to be told.\\nSupervised, unsupervised, a spectrum of might,\\nMachine learning, a powerful light.\\n\\nUnsupervised, where patterns reside,\\nClustering data, a task with no guide.\\nReinforcement, a learning curve to ascend,\\nWith algorithms guiding, a new world is found.\\n\\nDeep learning, a neural net so vast,\\nLearning from data, a hidden past.\\nFrom medical diagnosis to financial sway,\\nMachine learning's impact cannot be swayed.\\n\\nA tool for good, or a path to despair,\\nThe ethical use of AI, a matter to share.\\nBias and fairness, a constant fight,\\nTo ensure that the machine's light shines bright.\\n\\nSo let the algorithms spin and the data flow,\\nA symphony of learning, a digital woe.\\nMachine learning, a journey without end,\\nShaping the future, a world without a friend.&quot;, &quot;details&quot;: { &quot;finish_reason&quot;: &quot;eos_token&quot;, &quot;generated_tokens&quot;: 248, &quot;seed&quot;: null, &quot;prefill&quot;: [ { &quot;id&quot;: 2, &quot;text&quot;: &quot;&lt;bos&gt;&quot;, &quot;logprob&quot;: null }, { &quot;id&quot;: 2, &quot;text&quot;: &quot;&lt;bos&gt;&quot;, &quot;logprob&quot;: -25 }, { &quot;id&quot;: 106, &quot;text&quot;: &quot;&lt;start_of_turn&gt;&quot;, &quot;logprob&quot;: -65.5 }, { &quot;id&quot;: 1645, &quot;text&quot;: &quot;user&quot;, &quot;logprob&quot;: -20.375 }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -4.78125 }, { &quot;id&quot;: 5559, &quot;text&quot;: &quot;Write&quot;, &quot;logprob&quot;: -8.6875 }, { &quot;id&quot;: 682, &quot;text&quot;: &quot; me&quot;, &quot;logprob&quot;: -5.5625 }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.044189453 }, { &quot;id&quot;: 19592, &quot;text&quot;: &quot; poem&quot;, &quot;logprob&quot;: -1.2421875 }, { &quot;id&quot;: 1105, &quot;text&quot;: &quot; about&quot;, &quot;logprob&quot;: -0.021606445 }, { &quot;id&quot;: 13403, &quot;text&quot;: &quot; Machine&quot;, &quot;logprob&quot;: -19.5 }, { &quot;id&quot;: 14715, &quot;text&quot;: &quot; Learning&quot;, &quot;logprob&quot;: -0.061523438 }, { &quot;id&quot;: 107, &quot;text&quot;: &quot;&lt;end_of_turn&gt;&quot;, &quot;logprob&quot;: -39 }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -4.875 }, { &quot;id&quot;: 106, &quot;text&quot;: &quot;&lt;start_of_turn&gt;&quot;, &quot;logprob&quot;: -42.75 }, { &quot;id&quot;: 2516, &quot;text&quot;: &quot;model&quot;, &quot;logprob&quot;: -20.75 } ], &quot;tokens&quot;: [ { &quot;id&quot;: 688, &quot;text&quot;: &quot;**&quot;, &quot;logprob&quot;: -0.24707031, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.13867188, &quot;special&quot;: false }, { &quot;id&quot;: 14715, &quot;text&quot;: &quot; Learning&quot;, &quot;logprob&quot;: -0.057128906, &quot;special&quot;: false }, { &quot;id&quot;: 688, &quot;text&quot;: &quot;**&quot;, &quot;logprob&quot;: -0.0073547363, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00007581711, &quot;special&quot;: false }, { &quot;id&quot;: 139972, &quot;text&quot;: &quot;Algorithms&quot;, &quot;logprob&quot;: -0.45507812, &quot;special&quot;: false }, { &quot;id&quot;: 11877, &quot;text&quot;: &quot; dance&quot;, &quot;logprob&quot;: -0.21875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.390625, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.05810547, &quot;special&quot;: false }, { &quot;id&quot;: 6403, &quot;text&quot;: &quot; digital&quot;, &quot;logprob&quot;: -0.6640625, &quot;special&quot;: false }, { &quot;id&quot;: 50455, &quot;text&quot;: &quot; ballet&quot;, &quot;logprob&quot;: -0.45898438, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000031471252, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000029087067, &quot;special&quot;: false }, { &quot;id&quot;: 26231, &quot;text&quot;: &quot;Learning&quot;, &quot;logprob&quot;: -0.796875, &quot;special&quot;: false }, { &quot;id&quot;: 774, &quot;text&quot;: &quot; from&quot;, &quot;logprob&quot;: -0.084472656, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.020507812, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.007873535, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.3359375, &quot;special&quot;: false }, { &quot;id&quot;: 2447, &quot;text&quot;: &quot; never&quot;, &quot;logprob&quot;: -1.2265625, &quot;special&quot;: false }, { &quot;id&quot;: 235290, &quot;text&quot;: &quot;-&quot;, &quot;logprob&quot;: -0.0047302246, &quot;special&quot;: false }, { &quot;id&quot;: 3002, &quot;text&quot;: &quot;ending&quot;, &quot;logprob&quot;: -0.00061416626, &quot;special&quot;: false }, { &quot;id&quot;: 78289, &quot;text&quot;: &quot; tally&quot;, &quot;logprob&quot;: -1.53125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000036478043, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000030398369, &quot;special&quot;: false }, { &quot;id&quot;: 1510, &quot;text&quot;: &quot;Data&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 24039, &quot;text&quot;: &quot; streams&quot;, &quot;logprob&quot;: -1.3984375, &quot;special&quot;: false }, { &quot;id&quot;: 575, &quot;text&quot;: &quot; in&quot;, &quot;logprob&quot;: -0.21679688, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0013275146, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.14355469, &quot;special&quot;: false }, { &quot;id&quot;: 61300, &quot;text&quot;: &quot; torrent&quot;, &quot;logprob&quot;: -0.67578125, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 15493, &quot;text&quot;: &quot; bytes&quot;, &quot;logprob&quot;: -0.41601562, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00062179565, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000039815903, &quot;special&quot;: false }, { &quot;id&quot;: 139972, &quot;text&quot;: &quot;Algorithms&quot;, &quot;logprob&quot;: -0.59375, &quot;special&quot;: false }, { &quot;id&quot;: 183807, &quot;text&quot;: &quot; sift&quot;, &quot;logprob&quot;: -0.41210938, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.39648438, &quot;special&quot;: false }, { &quot;id&quot;: 6728, &quot;text&quot;: &quot; sort&quot;, &quot;logprob&quot;: -0.099609375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.008056641, &quot;special&quot;: false }, { &quot;id&quot;: 675, &quot;text&quot;: &quot; with&quot;, &quot;logprob&quot;: -1.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 185614, &quot;text&quot;: &quot; tireless&quot;, &quot;logprob&quot;: -1.5546875, &quot;special&quot;: false }, { &quot;id&quot;: 4628, &quot;text&quot;: &quot; eyes&quot;, &quot;logprob&quot;: -2.109375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00008916855, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000047683716, &quot;special&quot;: false }, { &quot;id&quot;: 3604, &quot;text&quot;: &quot;From&quot;, &quot;logprob&quot;: -0.6875, &quot;special&quot;: false }, { &quot;id&quot;: 5191, &quot;text&quot;: &quot; images&quot;, &quot;logprob&quot;: -0.77734375, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.16210938, &quot;special&quot;: false }, { &quot;id&quot;: 11360, &quot;text&quot;: &quot; speech&quot;, &quot;logprob&quot;: -0.41015625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0045776367, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 12136, &quot;text&quot;: &quot; patterns&quot;, &quot;logprob&quot;: -0.43945312, &quot;special&quot;: false }, { &quot;id&quot;: 45411, &quot;text&quot;: &quot; unfold&quot;, &quot;logprob&quot;: -0.59765625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0004825592, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000021338463, &quot;special&quot;: false }, { &quot;id&quot;: 98951, &quot;text&quot;: &quot;Predic&quot;, &quot;logprob&quot;: -1.5859375, &quot;special&quot;: false }, { &quot;id&quot;: 1486, &quot;text&quot;: &quot;ting&quot;, &quot;logprob&quot;: -0.010681152, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 3936, &quot;text&quot;: &quot; future&quot;, &quot;logprob&quot;: -0.019897461, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0021514893, &quot;special&quot;: false }, { &quot;id&quot;: 8965, &quot;text&quot;: &quot; stories&quot;, &quot;logprob&quot;: -0.22265625, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.33203125, &quot;special&quot;: false }, { &quot;id&quot;: 614, &quot;text&quot;: &quot; be&quot;, &quot;logprob&quot;: -0.115234375, &quot;special&quot;: false }, { &quot;id&quot;: 4203, &quot;text&quot;: &quot; told&quot;, &quot;logprob&quot;: -0.07714844, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000036478043, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00006723404, &quot;special&quot;: false }, { &quot;id&quot;: 8437, &quot;text&quot;: &quot;Super&quot;, &quot;logprob&quot;: -0.60546875, &quot;special&quot;: false }, { &quot;id&quot;: 9470, &quot;text&quot;: &quot;vised&quot;, &quot;logprob&quot;: -0.00051116943, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.75390625, &quot;special&quot;: false }, { &quot;id&quot;: 195643, &quot;text&quot;: &quot; unsupervised&quot;, &quot;logprob&quot;: -0.0017166138, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0004749298, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.24511719, &quot;special&quot;: false }, { &quot;id&quot;: 18303, &quot;text&quot;: &quot; spectrum&quot;, &quot;logprob&quot;: -0.49023438, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 2613, &quot;text&quot;: &quot; might&quot;, &quot;logprob&quot;: -0.7734375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00002193451, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000027894974, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -1.2578125, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.005432129, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.61328125, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.27929688, &quot;special&quot;: false }, { &quot;id&quot;: 10276, &quot;text&quot;: &quot; powerful&quot;, &quot;logprob&quot;: -1.2890625, &quot;special&quot;: false }, { &quot;id&quot;: 2611, &quot;text&quot;: &quot; light&quot;, &quot;logprob&quot;: -0.5625, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00007581711, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000028252602, &quot;special&quot;: false }, { &quot;id&quot;: 18808, &quot;text&quot;: &quot;Uns&quot;, &quot;logprob&quot;: -0.39648438, &quot;special&quot;: false }, { &quot;id&quot;: 55760, &quot;text&quot;: &quot;uper&quot;, &quot;logprob&quot;: -0.02746582, &quot;special&quot;: false }, { &quot;id&quot;: 9470, &quot;text&quot;: &quot;vised&quot;, &quot;logprob&quot;: -0.0000072717667, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.28125, &quot;special&quot;: false }, { &quot;id&quot;: 1570, &quot;text&quot;: &quot; where&quot;, &quot;logprob&quot;: -0.51171875, &quot;special&quot;: false }, { &quot;id&quot;: 12136, &quot;text&quot;: &quot; patterns&quot;, &quot;logprob&quot;: -0.91015625, &quot;special&quot;: false }, { &quot;id&quot;: 53172, &quot;text&quot;: &quot; reside&quot;, &quot;logprob&quot;: -0.89453125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00034332275, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000037908554, &quot;special&quot;: false }, { &quot;id&quot;: 184568, &quot;text&quot;: &quot;Clustering&quot;, &quot;logprob&quot;: -0.33984375, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.38085938, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 6911, &quot;text&quot;: &quot; task&quot;, &quot;logprob&quot;: -1.4453125, &quot;special&quot;: false }, { &quot;id&quot;: 675, &quot;text&quot;: &quot; with&quot;, &quot;logprob&quot;: -1.8828125, &quot;special&quot;: false }, { &quot;id&quot;: 793, &quot;text&quot;: &quot; no&quot;, &quot;logprob&quot;: -1.2890625, &quot;special&quot;: false }, { &quot;id&quot;: 5608, &quot;text&quot;: &quot; guide&quot;, &quot;logprob&quot;: -0.52734375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0005226135, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00010585785, &quot;special&quot;: false }, { &quot;id&quot;: 75111, &quot;text&quot;: &quot;Rein&quot;, &quot;logprob&quot;: -0.47851562, &quot;special&quot;: false }, { &quot;id&quot;: 14707, &quot;text&quot;: &quot;forcement&quot;, &quot;logprob&quot;: -0.000053167343, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.083496094, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.33007812, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.7265625, &quot;special&quot;: false }, { &quot;id&quot;: 12942, &quot;text&quot;: &quot; curve&quot;, &quot;logprob&quot;: -0.54296875, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -1.1640625, &quot;special&quot;: false }, { &quot;id&quot;: 70806, &quot;text&quot;: &quot; ascend&quot;, &quot;logprob&quot;: -0.51171875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000017523766, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.0000104904175, &quot;special&quot;: false }, { &quot;id&quot;: 3192, &quot;text&quot;: &quot;With&quot;, &quot;logprob&quot;: -1.328125, &quot;special&quot;: false }, { &quot;id&quot;: 28514, &quot;text&quot;: &quot; algorithms&quot;, &quot;logprob&quot;: -0.640625, &quot;special&quot;: false }, { &quot;id&quot;: 55709, &quot;text&quot;: &quot; guiding&quot;, &quot;logprob&quot;: -0.33984375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.12402344, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.6328125, &quot;special&quot;: false }, { &quot;id&quot;: 888, &quot;text&quot;: &quot; new&quot;, &quot;logprob&quot;: -1.4375, &quot;special&quot;: false }, { &quot;id&quot;: 2134, &quot;text&quot;: &quot; world&quot;, &quot;logprob&quot;: -0.359375, &quot;special&quot;: false }, { &quot;id&quot;: 603, &quot;text&quot;: &quot; is&quot;, &quot;logprob&quot;: -0.31054688, &quot;special&quot;: false }, { &quot;id&quot;: 1942, &quot;text&quot;: &quot; found&quot;, &quot;logprob&quot;: -0.5703125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000011563301, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000044822693, &quot;special&quot;: false }, { &quot;id&quot;: 26843, &quot;text&quot;: &quot;Deep&quot;, &quot;logprob&quot;: -0.6171875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.10546875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.026123047, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.028686523, &quot;special&quot;: false }, { &quot;id&quot;: 35544, &quot;text&quot;: &quot; neural&quot;, &quot;logprob&quot;: -0.5234375, &quot;special&quot;: false }, { &quot;id&quot;: 3117, &quot;text&quot;: &quot; net&quot;, &quot;logprob&quot;: -1.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 712, &quot;text&quot;: &quot; so&quot;, &quot;logprob&quot;: -0.37304688, &quot;special&quot;: false }, { &quot;id&quot;: 12380, &quot;text&quot;: &quot; vast&quot;, &quot;logprob&quot;: -0.68359375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0008125305, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.0000076293945, &quot;special&quot;: false }, { &quot;id&quot;: 26231, &quot;text&quot;: &quot;Learning&quot;, &quot;logprob&quot;: -0.7421875, &quot;special&quot;: false }, { &quot;id&quot;: 774, &quot;text&quot;: &quot; from&quot;, &quot;logprob&quot;: -0.18457031, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.703125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.020751953, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.7890625, &quot;special&quot;: false }, { &quot;id&quot;: 5915, &quot;text&quot;: &quot; hidden&quot;, &quot;logprob&quot;: -1.4765625, &quot;special&quot;: false }, { &quot;id&quot;: 3433, &quot;text&quot;: &quot; past&quot;, &quot;logprob&quot;: -0.4296875, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0004043579, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00014400482, &quot;special&quot;: false }, { &quot;id&quot;: 3604, &quot;text&quot;: &quot;From&quot;, &quot;logprob&quot;: -0.94921875, &quot;special&quot;: false }, { &quot;id&quot;: 6910, &quot;text&quot;: &quot; medical&quot;, &quot;logprob&quot;: -0.31640625, &quot;special&quot;: false }, { &quot;id&quot;: 19491, &quot;text&quot;: &quot; diagnosis&quot;, &quot;logprob&quot;: -0.3828125, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.0059509277, &quot;special&quot;: false }, { &quot;id&quot;: 6895, &quot;text&quot;: &quot; financial&quot;, &quot;logprob&quot;: -0.48046875, &quot;special&quot;: false }, { &quot;id&quot;: 46507, &quot;text&quot;: &quot; sway&quot;, &quot;logprob&quot;: -0.69921875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000028133392, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00005865097, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.060546875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.005279541, &quot;special&quot;: false }, { &quot;id&quot;: 235303, &quot;text&quot;: &quot;'&quot;, &quot;logprob&quot;: -0.3046875, &quot;special&quot;: false }, { &quot;id&quot;: 235256, &quot;text&quot;: &quot;s&quot;, &quot;logprob&quot;: -0.0000011920929, &quot;special&quot;: false }, { &quot;id&quot;: 6418, &quot;text&quot;: &quot; impact&quot;, &quot;logprob&quot;: -0.78515625, &quot;special&quot;: false }, { &quot;id&quot;: 2952, &quot;text&quot;: &quot; cannot&quot;, &quot;logprob&quot;: -0.36523438, &quot;special&quot;: false }, { &quot;id&quot;: 614, &quot;text&quot;: &quot; be&quot;, &quot;logprob&quot;: -0.14355469, &quot;special&quot;: false }, { &quot;id&quot;: 150640, &quot;text&quot;: &quot; swayed&quot;, &quot;logprob&quot;: -0.96484375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.005554199, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00004863739, &quot;special&quot;: false }, { &quot;id&quot;: 235280, &quot;text&quot;: &quot;A&quot;, &quot;logprob&quot;: -1.1328125, &quot;special&quot;: false }, { &quot;id&quot;: 7217, &quot;text&quot;: &quot; tool&quot;, &quot;logprob&quot;: -0.65625, &quot;special&quot;: false }, { &quot;id&quot;: 604, &quot;text&quot;: &quot; for&quot;, &quot;logprob&quot;: -0.203125, &quot;special&quot;: false }, { &quot;id&quot;: 1426, &quot;text&quot;: &quot; good&quot;, &quot;logprob&quot;: -0.7734375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.3203125, &quot;special&quot;: false }, { &quot;id&quot;: 689, &quot;text&quot;: &quot; or&quot;, &quot;logprob&quot;: -0.6875, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.024414062, &quot;special&quot;: false }, { &quot;id&quot;: 3703, &quot;text&quot;: &quot; path&quot;, &quot;logprob&quot;: -0.64453125, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.12158203, &quot;special&quot;: false }, { &quot;id&quot;: 40813, &quot;text&quot;: &quot; despair&quot;, &quot;logprob&quot;: -0.045654297, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000044345856, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000035762787, &quot;special&quot;: false }, { &quot;id&quot;: 651, &quot;text&quot;: &quot;The&quot;, &quot;logprob&quot;: -0.7890625, &quot;special&quot;: false }, { &quot;id&quot;: 33970, &quot;text&quot;: &quot; ethical&quot;, &quot;logprob&quot;: -0.48828125, &quot;special&quot;: false }, { &quot;id&quot;: 1281, &quot;text&quot;: &quot; use&quot;, &quot;logprob&quot;: -0.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.15722656, &quot;special&quot;: false }, { &quot;id&quot;: 16481, &quot;text&quot;: &quot; AI&quot;, &quot;logprob&quot;: -0.26367188, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.13378906, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.4140625, &quot;special&quot;: false }, { &quot;id&quot;: 4391, &quot;text&quot;: &quot; matter&quot;, &quot;logprob&quot;: -0.9609375, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.40625, &quot;special&quot;: false }, { &quot;id&quot;: 4638, &quot;text&quot;: &quot; share&quot;, &quot;logprob&quot;: -0.27734375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000012755394, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00015258789, &quot;special&quot;: false }, { &quot;id&quot;: 79111, &quot;text&quot;: &quot;Bias&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.60546875, &quot;special&quot;: false }, { &quot;id&quot;: 67512, &quot;text&quot;: &quot; fairness&quot;, &quot;logprob&quot;: -0.10205078, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0002193451, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.14941406, &quot;special&quot;: false }, { &quot;id&quot;: 6221, &quot;text&quot;: &quot; constant&quot;, &quot;logprob&quot;: -0.27929688, &quot;special&quot;: false }, { &quot;id&quot;: 5900, &quot;text&quot;: &quot; fight&quot;, &quot;logprob&quot;: -0.703125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000009536743, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000008940697, &quot;special&quot;: false }, { &quot;id&quot;: 1469, &quot;text&quot;: &quot;To&quot;, &quot;logprob&quot;: -0.17382812, &quot;special&quot;: false }, { &quot;id&quot;: 7433, &quot;text&quot;: &quot; ensure&quot;, &quot;logprob&quot;: -0.014953613, &quot;special&quot;: false }, { &quot;id&quot;: 674, &quot;text&quot;: &quot; that&quot;, &quot;logprob&quot;: -0.78515625, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.9453125, &quot;special&quot;: false }, { &quot;id&quot;: 6479, &quot;text&quot;: &quot; machine&quot;, &quot;logprob&quot;: -1.15625, &quot;special&quot;: false }, { &quot;id&quot;: 235303, &quot;text&quot;: &quot;'&quot;, &quot;logprob&quot;: -0.74609375, &quot;special&quot;: false }, { &quot;id&quot;: 235256, &quot;text&quot;: &quot;s&quot;, &quot;logprob&quot;: -3.5762787e-7, &quot;special&quot;: false }, { &quot;id&quot;: 2611, &quot;text&quot;: &quot; light&quot;, &quot;logprob&quot;: -1.2421875, &quot;special&quot;: false }, { &quot;id&quot;: 66308, &quot;text&quot;: &quot; shines&quot;, &quot;logprob&quot;: -0.08642578, &quot;special&quot;: false }, { &quot;id&quot;: 8660, &quot;text&quot;: &quot; bright&quot;, &quot;logprob&quot;: -0.041748047, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0000067949295, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00061416626, &quot;special&quot;: false }, { &quot;id&quot;: 2339, &quot;text&quot;: &quot;So&quot;, &quot;logprob&quot;: -0.049804688, &quot;special&quot;: false }, { &quot;id&quot;: 2142, &quot;text&quot;: &quot; let&quot;, &quot;logprob&quot;: -0.62109375, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.359375, &quot;special&quot;: false }, { &quot;id&quot;: 28514, &quot;text&quot;: &quot; algorithms&quot;, &quot;logprob&quot;: -0.04321289, &quot;special&quot;: false }, { &quot;id&quot;: 11974, &quot;text&quot;: &quot; spin&quot;, &quot;logprob&quot;: -1.296875, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.45507812, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -1.3359375, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.484375, &quot;special&quot;: false }, { &quot;id&quot;: 3781, &quot;text&quot;: &quot; flow&quot;, &quot;logprob&quot;: -0.7265625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0000667572, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000027179718, &quot;special&quot;: false }, { &quot;id&quot;: 235280, &quot;text&quot;: &quot;A&quot;, &quot;logprob&quot;: -1.171875, &quot;special&quot;: false }, { &quot;id&quot;: 113598, &quot;text&quot;: &quot; symphony&quot;, &quot;logprob&quot;: -0.03466797, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.0016555786, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -1.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0076293945, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.1953125, &quot;special&quot;: false }, { &quot;id&quot;: 6403, &quot;text&quot;: &quot; digital&quot;, &quot;logprob&quot;: -1.2734375, &quot;special&quot;: false }, { &quot;id&quot;: 82647, &quot;text&quot;: &quot; woe&quot;, &quot;logprob&quot;: -1.125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00023078918, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00037765503, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.8046875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.0006828308, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00793457, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.029418945, &quot;special&quot;: false }, { &quot;id&quot;: 10734, &quot;text&quot;: &quot; journey&quot;, &quot;logprob&quot;: -1.15625, &quot;special&quot;: false }, { &quot;id&quot;: 2346, &quot;text&quot;: &quot; without&quot;, &quot;logprob&quot;: -0.111328125, &quot;special&quot;: false }, { &quot;id&quot;: 1580, &quot;text&quot;: &quot; end&quot;, &quot;logprob&quot;: -0.001083374, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00002348423, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00004673004, &quot;special&quot;: false }, { &quot;id&quot;: 3957, &quot;text&quot;: &quot;Sha&quot;, &quot;logprob&quot;: -1.265625, &quot;special&quot;: false }, { &quot;id&quot;: 10024, &quot;text&quot;: &quot;ping&quot;, &quot;logprob&quot;: -0.00013542175, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.37109375, &quot;special&quot;: false }, { &quot;id&quot;: 3936, &quot;text&quot;: &quot; future&quot;, &quot;logprob&quot;: -0.22558594, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.002029419, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.2578125, &quot;special&quot;: false }, { &quot;id&quot;: 2134, &quot;text&quot;: &quot; world&quot;, &quot;logprob&quot;: -0.96875, &quot;special&quot;: false }, { &quot;id&quot;: 2346, &quot;text&quot;: &quot; without&quot;, &quot;logprob&quot;: -0.5625, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.828125, &quot;special&quot;: false }, { &quot;id&quot;: 4034, &quot;text&quot;: &quot; friend&quot;, &quot;logprob&quot;: -1.5234375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000030517578, &quot;special&quot;: false }, { &quot;id&quot;: 1, &quot;text&quot;: &quot;&lt;eos&gt;&quot;, &quot;logprob&quot;: -0.000027060509, &quot;special&quot;: true } ] } } ], &quot;contentType&quot;: &quot;application/json&quot;, &quot;invokedProductionVariant&quot;: &quot;AllTraffic&quot;} 외부에서 curl을 날리면123456789101112% curl --location 'https://runtime.sagemaker.ap-northeast-2.amazonaws.com/endpoints/jumpstart-dft-hf-llm-gemma-2b-20240609-080102/invocations' \\--header 'endpointName: jumpstart-dft-hf-llm-gemma-2b-20240609-080102' \\--header 'Content-Type: application/json' \\--data '{ &quot;inputs&quot;: &quot;Write me a poem about Machine Learning.&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;decoder_input_details&quot;: true, &quot;details&quot;: true }}'{&quot;message&quot;:&quot;Missing Authentication Token&quot;} 외부에서 접속하기Amazon SageMaker에서 JumpStart를 사용하여 배포한 Gemma 추론 모델 엔드포인트는 기본적으로 AWS 내에서만 접근 가능하도록 설정됩니다. 하지만 외부망에서도 접근할 수 있도록 설정할 수 있습니다. 이를 위해서는 다음 단계를 따라야 합니다. 1. 엔드포인트 보안 그룹 설정엔드포인트가 속한 보안 그룹에서 인바운드 및 아웃바운드 규칙을 설정하여 외부 IP가 엔드포인트에 접근할 수 있도록 합니다. 인바운드 규칙 추가: 특정 IP 주소나 IP 범위에 대해 포트 443(HTTPS)을 열어야 합니다. 아웃바운드 규칙 확인: 기본적으로 모든 아웃바운드 트래픽이 허용되어야 합니다. 2. 엔드포인트에 대한 IAM 정책 설정외부 애플리케이션이 엔드포인트에 접근할 수 있도록 필요한 IAM 정책을 설정합니다. 이는 외부 애플리케이션이 올바른 인증 및 권한을 가지고 있는지 확인하기 위함입니다. 3. API Gateway 설정AWS API Gateway를 사용하여 SageMaker 엔드포인트를 퍼블릭 API로 노출할 수 있습니다. 이렇게 하면 외부 클라이언트가 API Gateway URL을 통해 SageMaker 엔드포인트에 접근할 수 있습니다. API Gateway 설정 단계: API Gateway 생성: AWS Management Console에서 API Gateway를 생성합니다. 유형은 REST API를 선택합니다. 새 리소스 생성: API에 새 리소스를 추가하고, HTTP 메서드(예: POST)를 설정합니다. 통합 유형 설정: 통합 유형으로 AWS 서비스(SageMaker)를 선택합니다. 엔드포인트 URL을 SageMaker 엔드포인트로 설정합니다. IAM 역할 설정: API Gateway가 SageMaker 엔드포인트에 접근할 수 있도록 필요한 권한을 가진 IAM 역할을 설정합니다. 배포: API를 배포하고, 사용할 스테이지(예: dev, prod)를 설정합니다. 4. 엔드포인트 호출외부 클라이언트가 API Gateway URL을 사용하여 SageMaker 엔드포인트에 접근하고, 추론 요청을 보낼 수 있습니다. 다음은 예시 코드입니다: 12345678910111213141516171819import requestsurl = &quot;https://your-api-gateway-id.execute-api.region.amazonaws.com/your-stage/your-resource&quot;headers = { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: &quot;Bearer your-auth-token&quot;}payload = { &quot;action&quot;: &quot;invoke&quot;, &quot;service&quot;: &quot;gemma&quot;, &quot;method&quot;: &quot;writePoem&quot;, &quot;parameters&quot;: { &quot;theme&quot;: &quot;love&quot;, &quot;style&quot;: &quot;sonnet&quot; }}response = requests.post(url, json=payload, headers=headers)print(response.json()) 요약외부망에서 SageMaker JumpStart로 배포한 Gemma 추론 모델에 접근하기 위해서는: 엔드포인트 보안 그룹 설정. IAM 정책 설정. API Gateway 설정을 통해 엔드포인트를 퍼블릭 API로 노출. API Gateway URL을 통해 추론 요청 전송. 예시https://devocean.sk.com/blog/techBoardDetail.do?ID=163528 https://boksup.tistory.com/33 Chalice를 통한 서버리스 API 작업 생성SageMaker 엔드포인트를 사용할 수 있게 되었다면, 최종 사용자에게 제공할 결과를 생성하기 위해 엔드포인트에 액세스할 수 있는 API 작업을 생성해야한다.그렇기에 Chalice 프레임워크를 사용하여 API Gateway에 간단한 Flask 유사 애플리케이션을 배포함으로써 SageMaker 엔드포인트와 상호 작용할 Lambda 함수를 트리거할 것이다. Chalice는 AWS를 위한 서버리스 마이클 프레임워크이다. 이를 사용하면 Amazon API Gateway 및 AWS Lambda를 사용하는 애플리케이션을 신속하게 생성하고 배포할 수 있다.Chalice를 사용하면 자체 Lambda 함수를 구축하는 것과 비교하여 빠르고 효율적으로 이를 수행 가능하다. 참고 : https://aws.amazon.com/ko/blogs/korea/build-a-serverless-frontend-for-an-amazon-sagemaker-endpoint/ 세팅가상 환경 생성12% python3 -m venv aws% source ./aws/bin/activate 관련 라이브러리 설치12% pip install chalice% brew install awscli Chalice 프로젝트 생성12% chalice new-project bert-text-classfication% cd bert-text-classfication Chalice 로컬 환경 테스트123% chalice local# localhost로 접근 시 기본 샘플 동작하는 것 확인 AWS CLI 설정 Access keys 눌러 키 값 및 명령어 입력 configure sso12345678910% aws configure sso# SSO session name : (임의로 입력해도 되는 것으로 추정)# SSO start URL: (Access keys에 있는 값 복사)# SSO Region: (Access Keys에 있는 값 복사)# SSO registration scopes: (기본 값이 있는 것 같아 Enter)# 이후 code를 입력하는 링크가 주어지는데, AWS 콘솔에 로그인한 환경에서 해다 링크에 접속하여 해당 code dlqfuraws s3 ls --profile [SSO session name] Access key 입력1234567# Access keys에 있는 값 복사하여 입력 (어느정도 시간이 지나면 키나 토큰값이 달라지는 것 같으므로 주의)export AWS_ACCESS_KEY_ID=&quot;&quot;export AWS_SECRET_KEY=&quot;&quot;export AWS_SESSION_TOKEN=&quot;&quot;# 가이드에는 없는데, deploy 시 에러 발생하여 추가로 실행export AWS_DEFAULT_REGION=&quot;&quot; boto3 사용1% pip install boto3 boto3 라이브러리를 requirements.txt 파일에 추가하여 chalice가 이를 인식할 수 있도록 한다. 추론 테스트Chalice Deploy 후 Postman 통해 추론 결과가 정상적으로 전달되는지 테스트 12345curl --location 'https://3joge41r35.execute-api.ap-northeast-2.amazonaws.com/api/invoke' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;This is a sample input text&quot;}' 에러남 123{ &quot;error&quot;: &quot;An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:sts::058264433760:assumed-role/bert-text-classfication-dev/bert-text-classfication-dev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint/jumpstart-dft-hf-tc-distilbert-base-20240718-065402 because no identity-based policy allows the sagemaker:InvokeEndpoint action&quot;} 이 오류 메시지는 Lambda 함수가 SageMaker 엔드포인트를 호출할 수 있는 권한이 없기 때문에 발생한다. Lambda 함수에 필요한 권한을 부여하기 위해서는 Lambda 함수의 실행 역할에 SageMaker 엔드포인트를 호출할 수 있는 권한을 추가해야 한다. 다음은 Lambda 실행 역할에 SageMaker 권한을 추가하는 방법이다: 1. IAM 역할에 정책 추가Lambda 함수에 할당된 IAM 역할에 sagemaker:InvokeEndpoint 권한을 추가해야 한다. 이를 위해 AWS Management Console 또는 AWS CLI를 사용할 수 있다. AWS Management Console 사용 IAM 콘솔로 이동: IAM 콘솔로 이동합니다. 역할(Role) 선택: Lambda 함수에 할당된 역할을 선택합니다. 예를 들어, bert-text-classfication-dev 역할을 찾습니다. 정책 추가: 역할의 “권한” 탭에서 “정책 추가” 버튼을 클릭합니다. 정책 연결: 아래 JSON 정책을 추가합니다. 정책 JSONIAM 콘솔에서 정책을 추가할 때, 아래 JSON 정책을 사용해라: 12345678910{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;, &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint/jumpstart-dft-hf-tc-distilbert-base-20240718-065402&quot; } ]} Lambda 함수 재배포IAM 역할에 정책을 추가한 후, Lambda 함수가 새로운 권한을 인식할 수 있도록 Chalice를 사용하여 Lambda 함수를 다시 배포한다: 1chalice deploy 이제 테스트를 하면 요청12345curl --location 'https://3joge41r35.execute-api.ap-northeast-2.amazonaws.com/api/invoke' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;This is a sample input text&quot;}' 응답 { &quot;Inference&quot;: { &quot;Input text&quot;: &quot;This is a sample input text&quot;, &quot;Model prediction&quot;: [ 0.7710819840431213, 0.22891800105571747 ], &quot;Labels&quot;: [ &quot;LABEL_0&quot;, &quot;LABEL_1&quot; ], &quot;Predicted Label&quot;: &quot;LABEL_0&quot; } } ~~","link":"/2024/06/03/Deploy-Models-On-Sagemaker-For-Inference/"},{"title":"Lambda로 간단한 api 만들기","text":"AWS Lambda에 테스트용 API를 만드는 방법을 단계별로 적어보겠다. 이 과정에서는 AWS Lambda를 사용하여 간단한 HTTP API를 설정힌다. 1. AWS Lambda 함수 생성 Lambda 함수 생성: Create function 버튼을 클릭. Author from scratch를 선택. 함수 이름(Function name)을 입력하고, 런타임(Runtime)으로 사용하려는 언어를 선택(예: Python, Node.js 등). 역할(Role)을 선택. 새로운 역할을 생성하거나 기존 역할을 사용할 수 있음. Create function 버튼을 눌러 함수를 생성. Lambda 함수 코드 작성: 함수가 생성되면, 기본 코드 편집기에서 코드를 작성할 수 있다. 예를 들어, Node.js를 사용하는 경우 다음과 같은 간단한 코드를 사용할 수 있다:12345def lambda_handler(event, context): return { 'statusCode': 200, 'body': 'Hello from Lambda!' } Lambda 함수 저장: 코드를 작성한 후, Deploy 버튼을 눌러 함수를 저장하고 배포. 2. API Gateway에 Lambda 통합 설정 API Gateway 서비스로 이동: AWS 관리 콘솔에서 API Gateway 서비스로 이동. API 생성: Create API 버튼을 클릭. HTTP API 또는 REST API 중 하나를 선택. Build를 클릭하여 API 생성을 시작. 통합 설정: Add integration을 클릭하고, Lambda를 선택. 앞서 생성한 Lambda 함수를 선택. Next를 클릭하여 진행. 경로 및 메서드 설정: Resource 경로를 설정. 예를 들어, /test 경로를 설정할 수 있음. 메서드를 GET, POST 등 원하는 메서드로 설정. Next를 클릭하여 API 생성 프로세스를 완료. API 배포: Deploy 버튼을 클릭하여 API를 배포. API Gateway는 API 엔드포인트 URL을 제공. 이 URL을 통해 Lambda 함수와 통합된 API에 접근할 수 있음. 3. API 테스트 브라우저나 Postman, cURL 등을 사용하여 생성된 API 엔드포인트를 호출. 예시:1GET https://&lt;api-id&gt;.execute-api.&lt;region&gt;.amazonaws.com/test 이 호출은 Lambda 함수와 연결된 API Gateway를 통해 Lambda 함수로 요청을 전달하고, 그 응답을 반환. 로그 확인 (옵션) API Gateway와 Lambda 함수는 AWS CloudWatch에 로그를 기록. 로그를 확인하려면 CloudWatch 서비스로 이동하여 Lambda와 API Gateway의 로그를 조회할 수 있음. 이렇게 설정하면, AWS Lambda를 활용한 간단한 테스트용 API가 생성된다.이 API는 API Gateway를 통해 HTTP 요청을 수신하고, Lambda 함수에서 정의된 코드를 실행하여 응답을 반환하게 된다.","link":"/2024/08/06/Deploy-Simple-API-with-Lambda/"},{"title":"Document-Knowledge-Mining-Solution-Accelerator","text":"Azure OpenAI Service와 Azure AI Document Intelligence를 기반으로 구축된, 비정형, 다중모드 문서에서 요약, 엔터티, 메타데이터를 처리하고 추출하여 데이터를 검색하고 채팅할 수 있는 솔루션. 아키텍처 그림 실제 엔터티 추출 : 사람, 제품, 이벤트 장소 또는 행동과 같은 고유한 정보를 처리하고 추출 채팅 기반 Insight discovery : 모든 인덱싱 된 assets, 단일 assets, 선택한 assets 세트 또는 사용자 주도 키워드 검색을 기반으로 생성된 asset 목록과 채팅 가능 텍스트 및 문서 데이터 분석 : 문서, 손글씨 텍스트, 차트, 그래프, 표 및 양식 필드를 포함한 다중 모드 문서의 내용을 분석, 비교 및 요약하여 심층적인 통찰력을 제공 프롬프트 제안 가이드 : 프롬프트 문의를 기반으로 다음 질문 세트를 제안 다중 모드 정보 처리 : 여러 콘텐츠 유형과 다양한 형식의 지식을 처리하고 추출 대량의 데이터를 신속하게 분석하고, 관련 제안을 생성하여 빠르고 쉽게 추론할 수 있도록 도와준다. PrerequisitesPowershell 설치12% brew install powershell/tap/powershell% pwsh macOS에 Azure CLI 설치123456789101112131415161718192021$ brew update &amp;&amp; brew install azure-cli% az --versionazure-cli 2.67.0core 2.67.0telemetry 1.1.0Dependencies:msal 1.31.0azure-mgmt-resource 23.1.1Python location '/usr/local/Cellar/azure-cli/2.67.0_1/libexec/bin/python'Extensions directory '/Users/leehamin/.azure/cliextensions'Python (Darwin) 3.12.8 (main, Dec 3 2024, 18:42:41) [Clang 16.0.0 (clang-1600.0.26.4)]Legal docs and information: aka.ms/AzureCliLegalYour CLI is up-to-date. kubectl 설치12345678910% sudo az aks install-cliPassword:The detected architecture of current device is &quot;x86_64&quot;, and the binary for &quot;amd64&quot; will be downloaded. If the detection is wrong, please download and install the binary corresponding to the appropriate architecture.No version specified, will get the latest version of kubectl from &quot;https://dl.k8s.io/release/stable.txt&quot;Downloading client to &quot;/usr/local/bin/kubectl&quot; from &quot;https://dl.k8s.io/release/v1.32.0/bin/darwin/amd64/kubectl&quot;Please ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.No version specified, will get the latest version of kubelogin from &quot;https://api.github.com/repos/Azure/kubelogin/releases/latest&quot;Downloading client to &quot;/tmp/tmpdiem7i3s/kubelogin.zip&quot; from &quot;https://github.com/Azure/kubelogin/releases/download/v0.1.6/kubelogin.zip&quot;Moving binary to &quot;/usr/local/bin/kubelogin&quot; from &quot;/tmp/tmpdiem7i3s/bin/darwin_amd64/kubelogin&quot;Please ensure that /usr/local/bin is in your search PATH, so the `kubelogin` command can be found. aks-preview 설치Azure CLI의 extension으로서 AKS를 운영할 수 있다. 1234567891011121314151617 % az extension add --name aks-previewNo stable version of 'aks-preview' to install. Preview versions allowed.The installed extension 'aks-preview' is in preview.% az extension list/usr/local/Cellar/azure-cli/2.67.0_1/libexec/lib/python3.12/site-packages/azure/batch/models/_models_py3.py:4839: SyntaxWarning: invalid escape sequence '\\s' &quot;&quot;&quot;A Job Preparation Task to run before any Tasks of the Job on any given[ { &quot;experimental&quot;: false, &quot;extensionType&quot;: &quot;whl&quot;, &quot;name&quot;: &quot;aks-preview&quot;, &quot;path&quot;: &quot;/Users/leehamin/.azure/cliextensions/aks-preview&quot;, &quot;preview&quot;: true, &quot;version&quot;: &quot;13.0.0b2&quot; }] Helm 설치k8s의 패키지 매니저이다. 123% brew install helm% helm versionversion.BuildInfo{Version:&quot;v3.16.4&quot;, GitCommit:&quot;7877b45b63f95635153b29a42c0c2f4273ec45ca&quot;, GitTreeState:&quot;dirty&quot;, GoVersion:&quot;go1.23.4&quot;} Docker Desktop 설치서비스를 컨테이너화 하고 Azure Container Registry로 배포하기 위하여 도커 데스크탑을 설치한다.Deploy script를 실행하기 전에 도커 데스크탑이 실행중인지 확인 필요하다. Azure Accesssubscription-level이 Owner나 User Access Administrator role로 필요하다. Regional AvailabilityAzure 서비스와 모델이 지역별로 제공 여부가 제한된다. Azure Open AI (GPT-4o mini) 사용 모델 : 이 솔루션은 GPT-4o mini와 text-embedding-3-large 모델을 사용한다. 제공 지역 : 해당 모델들은 현재 다음 지역에서만 사용 가능 West US3 East US East US2 SwedenCentral Azure AI Document Intelligence 사용 API 버전 : 이 솔루션은 2023-10-31-preview 또는 그 이후 버전의 Document Intelligence를 사용한다. 제공 지역 : 현재 해당 API는 East US 지역에서만 사용 가능하다. 배포 지역 제한 : 이 모델은 반드시 East US 지역에 배포 되어야 한다. Deployment자동화 된 배포 단계 Azure 리소스 배포 Azure 리소스에서 Secrets 가져오기 애플리케이션 Config 파일에 Secrets 업데이트 Azure App Configuration에 Application Config 설정 애플리케이션 컴파일, 이미지 빌드 및 Azire Container Registry에 푸시 k8s 클러스터 인프라 구성 k8s 구성 파일 업데이트 인증서, 인그레스 컨트롤러, 애플리케이션 이미지를 ACR에서 배포 Deployment Script 실행12345% git clone https://github.com/microsoft/Document-Knowledge-Mining-Solution-Accelerator.git# Powershell에서% cd Document-Knowledge-Mining-Solution-Accelerator&gt; .\\resourcedeployment.ps1 실행 시 에러 발생1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 _____ _ | __ \\ | | | | | | ___ ___ _ _ _ __ ___ ___ _ __ | |_ | | | |/ _ \\ / __| | | | '_ _ \\ / _ \\ '_ \\| __| | |__| | (_) | (__| |_| | | | | | | __/ | | | |_ |_____/ \\___/ \\___|\\__,_|_| |_| |_|\\___|_| |_|\\__| __ __ _ _ | |/ / | | | | | \\/ (_) (_) | ' / _ __ _____ _| | ___ __| | __ _ ___ | \\ / |_ _ __ _ _ __ __ _ | &lt; | '_ \\ / _ \\ \\ /\\ / / |/ _ \\/ _ |/ _ |/ _ \\ | |\\/| | | '_ \\| | '_ \\ / _ | | . \\| | | | (_) \\ V V /| | __/ (_| | (_| | __/ | | | | | | | | | | | | (_| | |_|\\_\\_| |_|\\___/ \\_/\\_/ |_|\\___|\\__,_|\\__, |\\___| |_| |_|_|_| |_|_|_| |_|\\__, | _____ _ _ _ __/ | _ __/ | / ____| | | | | (_) |___/ /\\ | | |___/ | | (___ ___ | |_ _| |_ _ ___ _ __ / \\ ___ ___ ___| | ___ _ __ __ _| |_ ___ _ __ \\___ \\ / _ \\| | | | | __| |/ _ \\| '_ \\ / /\\ \\ / __/ __/ _ \\ |/ _ \\ '__/ _ | __/ _ \\| '__| ____) | (_) | | |_| | |_| | (_) | | | | / ____ \\ (_| (_| __/ | __/ | | (_| | || (_) | | |_____/ \\___/|_|\\__,_|\\__|_|\\___/|_| |_| /_/ \\_\\___\\___\\___|_|\\___|_| \\__,_|\\__\\___/|_| Please enter your Azure subscription ID to deploy your resources&gt; : ???Please enter the Azure Data Center Region to deploy your resourcesAvailable regions are:EastUS, EastUS2, WestUS, WestUS2, WestUS3, CentralUS, NorthCentralUS, SouthCentralUS, WestEurope, NorthEurope, SoutheastAsia, EastAsia, JapanEast, JapanWest, AustraliaEast, AustraliaSoutheast, CentralIndia, SouthIndia, CanadaCentral, CanadaEast, UKSouth, UKWest, FranceCentral, FranceSouth, KoreaCentral, KoreaSouth, GermanyWestCentral, GermanyNorth, NorwayWest, NorwayEast, SwitzerlandNorth, SwitzerlandWest, UAENorth, UAECentral, SouthAfricaNorth, SouthAfricaWest, BrazilSouth, BrazilSoutheast, QatarCentral, ChinaNorth, ChinaEast, ChinaNorth2, ChinaEast2&gt; : KoreaCentralPlease enter the Azure Data Center Region to deploy your GPT modelAvailable regions are:EastUS, EastUS2, SwedenCentral, WestUS3&gt; : EastUSPlease enter your email address for certificate management&gt; : hamin.lee@kt.com************************************************ Step 1 : Deploy Azure resources ************************************************Log in to Azure.....A web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.AADSTS53003: Access has been blocked by Conditional Access policies. The access policy does not allow token issuance. Trace ID: 4b59632c-1d82-4588-85b4-abc86f4f1000 Correlation ID: e4938e47-8a82-462e-80a7-e83facbe56a5 Timestamp: 2024-12-18 08:38:47ZInteractive authentication is needed. Please run:az loginThe subscription of '80083cf5-0434-4e94-b9a4-9f8ba244207e' doesn't exist in cloud 'AzureCloud'.Switched subscription to '80083cf5-0434-4e94-b9a4-9f8ba244207e' Deploying Azure resources in KoreaCentral region.....Started Deploying Knowledge Mining Solution Accelerator Service Azure resources.....Evaluating Deployment resource availabilities to preview changes...ERROR: Error while attempting to retrieve the latest Bicep version: HTTPSConnectionPool(host='aka.ms', port=443): Max retries exceeded with url: /BicepLatestRelease (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)'))).There might be something wrong with your deployment. ReferenceKnowledge Mining/Conversation knowledge mining solution accelerator","link":"/2024/12/17/Document-Knowledge-Mining-Solution-Accelerator/"},{"title":"LangChain의 메모리 관리","text":"랭체인에는 5가지 정도 종류의 메모리가 있는데 각자 저장방식도 다르고, 각자만의 장단점이 있다. 챗봇에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 잆다. 오픈 AI에서 제공하는 기본 API는 랭체인 없이 사용 사용할 수 있는데, 메모리를 지원하지 않는다 (stateless) 그렇기에, 오늘은 랭체인에서 제공하는 메모리 사용법에 대해 적어보고자 한다. GPT는 어떻게 메모리 관리를 하는가세션 메모리와 단기 컨텍스트 관리 GPT는 단기 컨텍스트만 유지. 이전 대화의 일부가 필요할 때 최근 메시지를 기반으로 적응하지만, 세션이 종료되면 맥락이 초기화 됨. 캐시와 세션 관리 Redis 또는 Memcached와 같은 인메모리 데이터베이스를 사용해, 실시간 대화 상태를 빠르게 유지하고 관리하는 것으로 보임. 이러한 캐시는 짧은 시간동안만 상태를 유지하여 빠른 응답을 지원한다. 대화가 끝나면 캐시된 데이터는 자동으로 소멸한다. 장기 데이터 저장소 (비활성화된 기능) 만약 장기적 사용자 메모리 기능이 활성화된다면, 이를 위헤 데이터베이스(PostgreSQL, MongoDB)나 클라우드 오브젝트 스토리지 (AWS S3 등)를 사용할것으로 추정한다. ChatGPT는 주로 Transformer 모델, 토큰화 시스템, 그리고 Redis와 같은 캐시를 활용해 실시간 대화의 상태를 유지하는것으로 보인다.장기적인 메모리 기능은 아직 실험 단계에 있으며, 현재는 세션 단위의 단기 메모리만 지원한다.OpenAI는 내부 시스템 구성에 대한 구체적인 기술 스택을 직접 공개하지 않고 있다. LangChain의 메모리 관리LangChain에서 크게 5가지 정도의 메모리를 사용할 수 있다. ConversationBufferMemory ConversationBufferWindow ConversationSummaryMemory ConversationSummaryBufferMemory ConversationKGMemoryOpen AI에서 제공하는 기본 API는 LangChain 없이 사용할 수 있는데, 메모리를 지원하지 않는다 (stateless) Conversational Buffer 메모리 단순히 이전 대화 내용 전체를 저장하는 메모리 단점 : 대화 내용이 길어질수록 메모리도 계속 커지니까 비효율적이다. 모델 자체에는 메모리가 없다, 그래서 우리가 모델에게 요청을 보낼때, 이전 대화 기록 전체를 같이 보내야한다. 유저와 대화가 길어질수록, 모델에게 매번 보내야 될 대화 기록이 길어진다. → 비효율적 예시 1234from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory()memory.save_context({&quot;input&quot;: &quot;Hi&quot;}, {&quot;output&quot;: &quot;How are you?&quot;})memory.load_memory_variables({}) 1{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]} text completion 할 때 유용하다. (에측을 해야할 때, 텍스트를 자동완성하고 싶을때) 그러나 만약 chat model과 작업을 하면 AI 메시지와 Human 메시지가 다 필요하다. 메모리 종류와 무관하게 API는 다 똑같다. 모든 메모리는 save_context, load_memory_variables라는 함수를 가지고 있다. 이 메모리는 대화 내용 전체를 저장하는 메모리이다. 대화가 길어질수록 메모리에 수많은 내용이 계속 쌓이게 되어 비효율적이다. ConversationBufferWindow (대화 버퍼 윈도우)대화의 특정 부분(가장 최근부분)만을 저장하는 메모리. 예를들어 최근 5번째까지의 메시지를 저장한다고 했을 때, 6번째 메시지가 추가 됐을때 가장 오래된 메시지는 버려지는 방식. 저장 범위는 직접 설정 가능 메모리를 특정 크기로 유지할 수 있다는 것이 이 메모리의 큰 장점. 단점 : 챗봇이 전체 대화가 아니라 최근 대화에만 집중. 예시 12345678910111213from langchain.memory import ConversationBufferWindowMemorymemory = ConversationBufferWindowMemory( return_messages=True, k=4)def add_message(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})add_message('1', '1')add_message('2', '2')add_message('3', '3')add_message('4', '4')print(memory.load_memory_variables({})) 최근에 일어난 대화에만 집중한다. output 12345678{'history': [HumanMessage(content='1'),AIMessage(content='1'),HumanMessage(content='2'),AIMessage(content='2'),HumanMessage(content='3'),AIMessage(content='3'),HumanMessage(content='4'),AIMessage(content='4')]} ConversationSummaryMemory ConversationSummaryMemory는 llm을 사용한다. ConversationSummaryMemory는 message를 그대로 저장하는 것이 아니라, conversation의 요약을 자체적으로 해준다. 초반에는 이전의 메모리들보다 더 많은 토큰과 저장공간을 차지하게 된다. 예시 12345678910from langchain.memory import ConversationSummaryMemoryfrom langchain.chat_models import ChatOpenAIllm = Chat OpenAI(temperature=0.1)memory = ConversationSummaryMemory(llm=llm)def add_memory(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})def get_history(): return memory.load_memory_variables({})add_message(&quot;Hi I'm hamin, I'm in Brooklyn.&quot;, &quot;Wow that is so cool!&quot;)get_history() ConversationKnowledge Graph Memory LLM을 사용하는 memory class 대화 중의 엔티티의 knowledge graph를 만든다. 가장 중요한 것들만 뽑아내는 요약본 같은 것이다. 요약을 하지만 대화에서 entity를 뽑아내는 것이다. Entity를 뽑아낸다는 것은 사람, 장소, 날짜, 사건 등과 같은 의미 있는 개체를 추출하고 Knowledge graph를 구성한다는 것이다. 용도 맥락 유지 “그는 지금도 테슬라를 운영하고 있나요” → 그를 일론 머스크로 연결 정보 간 탐색 “스페이스X와 테슬라 사이에는 어떤 관계가 있나요” 대화에 따른 실시간 지식 업데이트 “테슬라는 최근 사이버트럭을 출시했어” 예시 1234567891011from langchain.memory import ConversationKGMemoryfrom langchain.chat_models import ChatOpenAIllm = Chat OpenAI(temperature=0.1)memory = ConversationKGMemory( llm=llm, return_messages=True,)def add_memory(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})add_message(&quot;Hi I'm hamin, I'm in Brooklyn.&quot;, &quot;Wow that is so cool!&quot;)memory.load_memory_variables({&quot;input&quot;: &quot;who is hamin&quot;}) 단순 대화 이상의 구조화된 지식을 구성하고 더 나은 응답을 제공 맥락 유지와 다중 개체간의 관계 탐색에 유용 LLM chainoff-the-shelf(일반적인 목적을 가진 chain을 의미) chain으로 빠르게 시작할 수 있게해서 좋지만, 프레임워크를 다루느라 머리 싸매거나 off-the-shelf chain을 커스텀하기보다 직접 만들고 싶을 때, langchain expression 언어를 활용해서 어플리케이션을 만들 수 있다. 1234567891011121314151617181920212223242526from langchain.memory import ConversationSummaryBufferMemoryfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import LLMChainfrom langchain.prompts import PromptTemplatellm = ChatOpenAI(temperature=0.1)memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=120, memory_key=&quot;chat_history&quot;)template = &quot;&quot;&quot; You are a helpful AI talking to a human. {chat_history} Human:{question} You:&quot;&quot;&quot;chain = LLMChain( llm=llm, memory=memory, prompt=PromptTemplate.from_template(template), verbose=True,)chain.predict(question=&quot;My name is hamin&quot;)memory.load_memory_variables({})chain.predict(question=&quot;I live in brooklyn&quot;)chain.predict(question=&quot;What is my name?&quot;) interaction 토큰 수가 120개보다 많으면 가장 오래된 interaction을 요약해줌 최신 내용을 그대로 유지하고 대화 기록을 요약하기 시작함. verbose를 이용하여 프롬프트 디버깅이 가능하다. Chat based Memorymemory 클래스는 memory를 두가지 방식으로 출력할 수 있다. 문자열 형태, message 형태 대화기반의 채팅으로 바꾸고 싶다면 return_message=True를 넣어줘야한다 123456789101112 from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=120, memory_key=&quot;chat_history&quot;, return_messages=True)prompt = ChatPromptTempate.from_messsages([ (&quot;system&quot;, &quot;You are a helpful AI talking to a human&quot;), MessagePlaceholder(variable_name=&quot;chat_history&quot;), (&quot;human&quot;, &quot;{question}&quot;)]) 이렇게 넣어주면 문자열기반 템플릿 대신 ChatPromptTemplate을 불러올것이다. ConversationBufferMemory로부터 요약본을 받아올 때, 시스템 메시지도 추가된다. MessagesPlaceholder Caching캐싱을 사용하면 LM(언어모델)의 응답을 저장할 수 있다. 예) 채팅 봇이 있고 그 채팅 봇이 항상 똑같은 질문을 받는다면 계속 답변을 생성하지 않고, 이미 답변한 답을 캐싱을 이용하여 재사용한다. InMemoryCache123456from langchain.globals import set_llm_cache, set_debugfrom langchain.cache import InMemoryCacheset_llm_cache(InMemoryCache())set_debug(True)chat.predict(&quot;How do you make italian pasta&quot;)chat.predict(&quot;How do you make italian pasta&quot;) 모든 response가 메모리에 저장된다. 첫 predict와 두번째 predict의 응답속도가 다르다. set_debug는 프롬프트로 표시되고 있는 모든 것들을 보여준다. 데이터베이스 캐싱 123from langchain.globals import set_llm_cache, set_debugfrom langchain.cache import SQLiteCacheset_llm_cache(SQLiteCache(&quot;cache.db&quot;)) 실행시키면 자동으로 sqlite 데이터베이스가 생성된다. How to cache LLM responses | 🦜️🔗 LangChain ChatOpenAI의 streaming=True 데이터베이스에 히스토리 저장하기langchain docs의 integration을 보면 Memory를 백업할 수 있는 다양한 DB들을 확인할 수 있다.Message histories | 🦜️🔗 LangChainex) MongoDB, Redis, PostgresQL Redis Chat Message HistoryRedisChatMessageHistory를 이용하여 read/write에 대한 low-latency로 chat message를 저장하고 관리할 수 있다. Setup1% pip install -qU langchain-redis langchain-openai redis 1docker run -d -p 6379:6379 redis:latest Importing Required Libraries123456from langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.messages import AIMessage, HumanMessagefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_openai import ChatOpenAIfrom langchain_redis import RedisChatMessageHistory RedisChatMessageHistory 사용 예시 123456789# RedisChatMessageHistory 초기화history = RedisChatMessageHistory(session_id=&quot;user_123&quot;, redis_url=REDIS_URL)# Add messages to the historyhistory.add_user_message(&quot;Hello, AI assistant!&quot;)history.add_ai_message(&quot;Hello! How can I assist you today?&quot;)# Retrieve messagesprint(&quot;Chat History:&quot;)for message in history.messages: print(f&quot;{type(message).__name__}: {message.content}&quot;) output 123Chat History:HumanMessage: Hello, AI assistant!AIMessage: Hello! How can I assist you today? Redis Chat Message History | 🦜️🔗 LangChain MongoDB Chat Message History1pip install -U --quiet langchain-mongodb 12# os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# os.environ[&quot;LANGCHAIN_API_KEY&quot;] = getpass.getpass() 123456789from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistorychat_message_history = MongoDBChatMessageHistory( session_id=&quot;test_session&quot;, connection_string=&quot;mongodb://mongo_user:password123@mongo:27017&quot;, database_name=&quot;my_db&quot;, collection_name=&quot;chat_histories&quot;,)chat_message_history.add_user_message(&quot;Hello&quot;)chat_message_history.add_ai_message(&quot;Hi&quot;) 1chat_message_history.messages 1[HumanMessage(content='Hello'), AIMessage(content='Hi')]","link":"/2024/10/22/LangChain-Memory/"},{"title":"멀티턴, 대화 요약를 위한 DB 설계","text":"멀티턴, summary를 지원할 수 있도록 DB 설계를 해보도록 하겠다. DB 종류 선정 SQLite : 간단하고 빠르며, 챗봇 애플리케이션의 대화 기록을 관리하는 데 적합. 설치 및 설정이 간단 : 파일 기반 DB이므로 별도의 서버 설정이 필요 없고, 파일 하나로 데이터베이스를 다룰 수 있어 빠르게 사용 가능. 경량화 된 구조 : 작은 규모의 데이터베이스에 적합하며, 챗봇 대화 로그와 같이 비교적 단순한 데이터 구조를 다룰 때 효과적 빠른 성능 : 파일 기반 데이터베이스로, 챗봇 애플리케이션의 빠른 응답에 적합. 호환성 : 대부분의 언어와 플랫폼에서 기본으로 지원 결론 : 챗봇 대화 로그나 요약을 저장하는 정도의 용도라면 SQLite가 효율적, 만약 데이터베이스의 확장성과 분산 처리가 필요하다면 PostgreSQL 또는 MySQL 같은 데이터베이스로 전환하는 것이 좋다.09. SQLite 에 대화내용 저장 Redis 고속 데이터 처리 : 메모리 기반, I/O 빠르다, 대화 데이터의 실시간 처리가 중요한 챗봇 애플리케이션에 적합. TTL 기능 : 데이터 만료시간 설정 가능 경량 데이터 처리 : 데이터 구조가 간단하여 작은 규모의 데이터를 빠르게 저장하고 접근 가능. 데이터 영구성 부족 : 인메모리DB, 서버 재기동시 데이터 손실 → 스냅샷과 AOF 설정 추가 필요, but 완전한 영구 저장 솔루션보다는 다소 제한적. 대규모 데이터 저장에 적합하지 않음 : 인메모리DB, 메모리, 비용문제 발생 가능성 데이터 일관성 문제 : 데이터일관성 보다는 속도에 중점 → 상관 없을것 같기도 함.. PostgreSQL 확장성 및 안정성 : 대규모 데이터를 안정적으로 처리할 수 있는 확장성이 뛰어난 데이터베이스. 트랜잭선 지원 : 트랜잭션 처리 및 동시성 제어에 강력한 기능 제공. 데이터의 일관성과 무결성 보장. 인덱싱및 고급 쿼리 지원 : 요약이나 특정 대화만 빠르게 조회해야 할 때 성능을 높이는 데 유리. 설치 및 관리의 복잡성 : 서버 설치가 필요하고 구성도 다소 복잡함. 메모리 및 디스크 요구사항 : 시스템 리소스를 더 많이 요구, 소규모 프로젝트나 단순한 대화 기록 저장 용도로는 다소 과함. 결론 : 챗봇 데이터가 많아지고 검색이나 통계 분석이 중요한 경우 매우 적합한 선택. 다만 프로젝트 초기 개발 단계이거나 간단한 챗봇 로그 용도라면 PostgreSQL의 장점 활용 못할수도 있음. 1안 : SQLite 사용테이블 정의서한 턴의 대화마다 요약을 한다고 가정 했을 때. Table 명 : interactions No Column Name Attribute Name Type Null Keys Description 1 conversation_id transaction_id INTEGER NN PK Sequence 2 user_uuid user_id TEXT NN user id 3 user_input message TEXT NN user 발화 4 agent_response response TEXT NN agent 응답 5 summary summary TEXT NN 대화내용 요약 6 timestamp timestamp TIMESTAMP NN agent 응답 Redis + PostgreSQL? Redis로 실시간 대화 기록 처리 최신 대화 기록과 같은 실시간 데이터를 Redis에 저장하여 빠르게 읽고 쓰기 가능. 대화 중에는 Redis에서 데이터 관리. PostgreSQL로 영구 데이터 저장 주요 대화 기록이나 요약, 사용자 정보 등의 영구적인 데이터를 PostgreSQL에 저장해 장기적으로 유지 Redis에서 PostgreSQL로 데이터마이그레이션 Redis에 저장된 데이터를 주기적으로 PostgreSQL로 이전하는 스케줄러 사용. (배치 작업 등) 단점 시스템 복잡성 증가 각 데이터베이스의 설정, 관리, 모니터링이 필요하며, 장애 대응이나 성능 최적화도 각각 별도로 수행해야 한다. 데이터 동기화가 실패하거나 지연되면 Redis와 PostgreSQL 간의 데이터 불일치 문제가 발생할 수 있어, 이를 해결하기 위한 모니터링과 에러 핸들링이 필요 메모리 비용 증가 Redis는 대량의 데이터를 저장할 경우 메모리 비용 높다. Redis의 메모리 사용량을 줄이기 위해 TTL을 설정 -&gt; 필요한 경우 모든 데이터를 PostgreSQL에 영구 저장해야 하므로 Redis와 PostgreSQL의 데이터 저장 비용이 중복 두 시스템의 상이한 데이터 모델 Redis와 PostgreSQL은 서로 다른 데이터 모델과 쿼리 방식을 사용 (키-값 vs SQL) Langchain.memory의 ConversationSummaryBufferMemory 등이 Redis를 대체 가능한가?ConversationSummaryMemory는? 대화 요약에 특화 : 요약을 기반으로 챗봇의 기억을 관리하는 것이 목적 메모리 기반 : 임시 메모리에 저장. 서버 재기동 하거나 메모리 초기화 시 데이터 소실 → Redis는 스냅샷과 AOF 설정으로 반 영구 저장 가능 속도 및 접근성 : conversationSummaryMemory는 단일 프로세스 내의 임시 메모리로 사용되기 때문에 Redis처럼 여러 클라이언트가 접근하는 다중 프로세스 환경에서 사용하기는 제한적. → ConversationSummaryBufferMemory는 대화 내용을 요약하고 간단히 참조하는 메모리 역할에는 유용하지만 Redis를 대체할 수 있는 완전한 데이터 저장 솔루션은 아님. → 영구적이고 구조화된 대화 데이터를 저장하려면 Redis와 같은 외부 영구 저장소가 필요하며, ConversationSummaryBufferMemory는 Redis를 보완하는 임시 요약 메모리로 활용 가능. RunnableWithMessageHistory + RedisRunnableWithMessageHistory : 대화 기록과 맥락을 효과적으로 관리하기 위해 LangChain에서 제공하는 클래스 이전 메시지의 맥락을 유지해야 할 때 유용. 메시지 기록이 시간 순으로 관리되므로, 대화의 특정 메시지를 빠르게 조회가능 RunnableWithMessageHistory와 함께 대화 요약 모델(예: GPT-3, GPT-4 등)을 연결하여 요약 기능을 구현. 이 클래스는 요약이 필요한 메시지를 정리하여 전달하고, 요약된 응답을 받아 메시지 히스토리에 추가하는 방식으로 동작 12345678910111213141516171819from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 메시지 기록을 효과적으로 관리하려면 두 가지 요소 필요 Runnable : 주로 Retriever, Chain과 같이 BaseChatMessageHistory와 상호작용하는 runnable 객체 BaseChatMessageHistory의 인스턴스를 반환하는 호출 가능한 객체 (callable) 메시지 기록을 관리하기 위한 객체 메시지 기록을 저장, 검색, 업데이트하는 데 사용. 무엇을 이용해 요약을 하는가? 언어 모델을 사용한 요약 RunnableWithMessageHistory를 특정 언어 모델(예: OpenAI의 GPT)과 함께 설정하면, 대화 내용을 요약하는 요청을 모델에 전달 LangChain의 요약 도구 LangChain에서는 특정 요약 알고리즘을 사용하거나, SummaryMemory 클래스를 추가하여 반복적인 요약을 통해 핵심 내용을 추려나가는 구조를 제공 요약할 시점 커스터마이징RunnableWithMessageHistory와 ConversationSummaryMemory를 사용할 때, 특정 조건에 따라 요약을 트리거할 수 있도록 커스터마이징 가능 대화의 길이, 특정 이벤트 발생, 주기적인 타이밍 등 요약 시점을 유연하게 조정 가능 메시지 수에 따른 요약 트리거12if len(runnable.message_history) &gt;= 10: summary_memory.update_summary(runnable.message_history) 특정 키워드 또는 이벤트에 따라 요약123user_input = &quot;요약해줘&quot;if &quot;요약&quot; in user_input: summary_memory.update_summary(runnable.message_history) 주기적 요약 (타이머 기반)12345import timedef periodic_summary(): while True: time.sleep(300) # 5분마다 요약 summary_memory.update_summary(runnable.message_history) 대화의 특정 단계에서 요약12def on_conversation_end(): summary_memory.update_summary(runnable.message_history) 저장소 옵션 인메모리 ChatMessageHistory 사용 메모리 내 메시지 기록 관리 빠른 I/O 재기동 시 기록 삭제 RedisChatMessageHistory 사용 Redis 사용, 메시지 기록 영구 저장 가능 장점 고속 데이터 처리와 실시간 응답성 Redis는 인메모리 데이터 저장소, 대화 데이터 I/O 빠름. RunnableWithMessageHistory와 결합하여 사용자의 메시지를 실시간으로 Redis에 기록 대화 흐름 관리와 일관성 유지 RunnableWithMessageHistory는 대화의 흐름을 자연스럽게 관리 Redis에 기록된 이전 메시지들을 참조할 수 있어 대화의 일관성을 유지한다. TTL을 통한 효율적 메모리 관리 Redis에서는 대화 데이터를 일정 시간 동안만 유지하는 TTL(Time to Live) 설정이 가능. 오래된 대화 데이터를 자동으로 삭제하고, 최신 대화에 필요한 정보만 유지할 수 있어 메모리 사용을 최적화. RunnableWithMessageHistory와 함께 활용하면 최신 대화 기록만을 기반으로 챗봇이 응답하도록 구성 가능. 주기적인 요약 저장으로 메모리 절약 RunnableWithMessageHistory를 사용하여 대화 흐름이 길어지면 요약을 주기적으로 생성해 Redis에 저장 불필요한 세부 메시지를 줄이고 요약본을 중심으로 대화 맥락을 유지. 확장성과 유연성 Redis는 분산 환경에서 확장이 용이하므로, 대규모 사용자와의 대화 기록을 효과적으로 처리. RunnableWithMessageHistory를 통해 각 대화의 맥락을 관리하면서, Redis에 저장된 대화 기록을 다른 인스턴스나 마이크로서비스에서도 참조할 수 있어 유연한 확장이 가능. 대화 기록과 요약에 대한 손쉬운 접근성 Redis는 다양한 데이터 구조(List, Hash, Set 등)를 지원하므로, RunnableWithMessageHistory와 함께 대화 기록을 효율적으로 구조화하여 저장 가능. 2안: RunnableWithMessageHistory + Redis 대화 기록 저장: RunnableWithMessageHistory의 메시지를 받아서 Redis에 저장한다. 요약 저장: 주기적으로 요약을 업데이트하거나, 대화 종료 시 요약을 Redis에 저장한다. Redis 데이터 구조 설계 대화별 메시지 리스트 Key: conversation:{conversation_id}:messages Type: List 설명 특정 대화의 메시지를 순서대로 저장. 각 메시지는 JSON 형식으로 저장, sender, message, timestamp 정보 기록 예시 123RPUSH conversation:conv_001:messages '{&quot;sender&quot;: &quot;user&quot;, &quot;message&quot;: &quot;Hello!&quot;, &quot;timestamp&quot;: &quot;2024-10-30T10:00:00&quot;}' '{&quot;sender&quot;: &quot;bot&quot;, &quot;message&quot;: &quot;Hi! How can I help you?&quot;, &quot;timestamp&quot;: &quot;2024-10-30T10:00:01&quot;}' 2.대화 요약 Key: conversation:{conversation_id}:summary Type: Hash 설명 대화의 요약을 저장합니다. 요약을 주기적으로 업데이트. 예시 123HMSET conversation:conv_001:summary &quot;summary&quot; &quot;User asked about product details&quot; &quot;timestamp&quot; &quot;2024-10-30T10:15:00&quot; 사용자별 대화 목록 Key: user:{user_id}:conversations Type: List 설명 사용자의 대화 ID를 시간순으로 저장. 예시 - 사용자가 conv_001, conv_002, conv_003 대화를 순서대로 진행했다면1LPUSH user:123:conversations conv_003 conv_002 conv_001 user:123:conversations 리스트를 조회하면 사용자가 진행한 대화 목록을 시간 순서대로 볼 수 있다. 이 리스트는 사용자가 이전에 진행했던 대화 기록을 관리하거나, 특정 사용자의 과거 대화 요약을 불러올 때 유용 전체 워크플로우 예시 사용자가 메시지를 보내면 RunnableWithMessageHistory는 해당 메시지를 conversation:{conversation_id}:messages 리스트에 저장. 이때 각 메시지는 sender, message, timestamp와 함께 JSON 형식으로 저장. 대화가 일정 수준으로 진행되면 요약을 업데이트: 주기적으로 대화의 요약을 생성하여 conversation:{conversation_id}:summary 해시에 업데이트. 요약은 전체 대화의 주요 내용이나 주제를 간략하게 표현하며, 대화가 종료될 때, 일정수준으로 길어지면 업데이트 사용자별 대화 기록 관리 각 대화가 시작될 때마다 user:{user_id}:conversations 리스트에 해당 대화 ID를 추가. 이를 통해 사용자의 전체 대화이력을 관리, 특정 사용자의 과거 대화 조회 ReferencesRunnableWithMessageHistoryProviders|LangChain","link":"/2024/11/03/LangChain-Memory-1/"},{"title":"Langchain이란?","text":"랭체인은 언어모델 기반 앱을 매우 쉽고 빠게 구축할 수 있도록 미리 만들어진 수많은 구성 요소와 모듈이 포함된 프레임워크이다. GPT 앱을 구축하려면 OpenAI 파이썬 패키지를 사용하는 것만으로는 충분하지 않다. LangChain은 많은 Module을 가지고 있고, 광범위해서 다양한 것을 할 수 있다. 예를들면 LangChain을 사용하지 않고 OpenAI API의 GPT4만을 사용한 것과 비교해서, LangChain에는 Memory를 위한 Module이 있지만, GPT(Open AI API)에는 없기 때문에 우리가 직접 구현해야 한다. 하지만 LangChain을 사용하면 6가지 종류의 Memory를 공짜로 얻을 수 있다. 그저 연결하기만 하면 작동한다. LangChain에는 Document를 위한 Module과, Embedding을 위한 Module이 있다. Prompt를 위한 Module도 LLM을 위한 Module도, Chat model을 위한 Module까지 LangChain에는 많은 것들이 있다. 또한 LangChain은 어플리케이션의 Component를 바꿀 수 있게 해준다. LangChain이 LLM 어플리케이션을 만들기 위한 모든 필요 요소들의 호환을 위한 계층이 되는 것이다. 또한 LangSmith라는 LLM 어플리케이션을 위한 Debugger도 있다.","link":"/2024/09/25/Langchain/"},{"title":"Mac에서 yum 사용하기 (Docker 에서 리눅스 사용)","text":"Docker를 사용하면 Linux 환경을 Mac에서 쉽게 사용할 수 있습니다. 이를 통해 yumdownloader를 실행할 수 있습니다. Docker 설치: Mac에 Docker가 설치되어 있어야 합니다. Docker for Mac을 설치합니다. CentOS Docker 컨테이너 실행: 12코드 복사docker run -it centos:latest /bin/bash yum-utils 설치 및 yumdownloader 사용:12yum install -y yum-utilsyumdownloader &lt;패키지명&gt; 다운로드한 패키지를 Mac으로 복사:다운로드한 파일을 Docker 컨테이너에서 Mac으로 복사합니다. 1docker cp &lt;container_id&gt;:/path/to/downloaded/rpm /path/to/local/directory","link":"/2024/06/24/Linux-Use-In-Mac/"},{"title":"LangChain의 RunnableWithMessage와 Redis 활용하여 대화내용 저장하기","text":"LangChain의 RunnableWithMessage는 LangChain에서 제공하는 유틸리티 클래스로, 주로 챗봇 애플리케이션 개발 시 이전의 대화 히스토리와 상호작용을 관리하기 위해 사용된다.이 클래스는 주로 대화 기록을 저장하고 관리하여 대화 흐름을 유지하거나 개인화된 응답을 생성하는데 유용하다. 기본적으로 RunnableWithHistory 클래스는 대화 세션 내에서 사용자와 AI간의 대화 히스토리를 지속적으로 기록할 수 있도록 해주며, 필요한 경우 특정 조건에 따라 히스토리를 요약하거나 일부 삭제할 수 있는 기능도 제공한다.이를통해 챗봇이 각 세션마다 대화 컨텍스트를 유지할 수 있어 더욱 자연스러운 대화 경험을 제공한다. 123from dotenv import load_dotenvload_dotenv(dotenv_path=&quot;../.env&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 12345678910111213141516from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages( [ ( &quot;system&quot;, &quot;당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요&quot;, ), # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨 MessagesPlaceholder(variable_name=&quot;history&quot;), (&quot;human&quot;, &quot;{input}&quot;), # 사용자 입력을 변수로 사용 ])runnable = prompt | model # 프롬프트와 모델을 연결하여 runnable 객체 생성 123456789101112131415161718192021222324from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 영구 저장소 (Persistent Storage)영구 저장소(Persistent Storage)는 프로그램이 종료되거나 시스템이 재부팅되더라도 데이터를 유지하는 저장 메커니즘을 말한다.이는 데이터베이스, 파일시스템, 또는 기타 비휘발성 저장 장치를 통해 구현될 수 있다. 영구 저장소는 애플리케이션의 상태를 저장하고, 사용자 설정을 유지하며, 장기간 데이터를 보존하는 데 필수적이다.이를 통해 프로그램은 이전 실행에서 중단된 지점부터 다시 시작할 수 있으며, 사용자는 데이터 손실 없이 작업을 계속 할 수 있다. RunnableWithMessageHistory는 get_session_history 호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적이다. Redis 설치Redis가 설치되어 있지 않다면 먼저 설치해야 한다. 폐쇄망 EC2이기 때문에, 인터넷이 연결된 로컬 컴퓨터에서 EC2 서버에 설치할 패키지를 다운로드 한다. 123456% pip download redisCollecting redis Using cached redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)Using cached redis-5.2.0-py3-none-any.whl (261 kB)Saved ./redis-5.2.0-py3-none-any.whlSuccessfully downloaded redis S3에 redis whl 파일을 업로드한다.그리고 EC2에 S3에서 whl 파일을 가져온다. 12345# aws configure# export AWS_ACCESS_KEY_ID=......# aws s3 cp s3://aipin-bucket/redis-5.2.0-py3-none-any.whl /rootdownload: s3://aipin-bucket/redis-5.2.0-py3-none-any.whl to ./redis-5.2.0-py3-none-any.whl 다운로드한 파일들을 EC2에서 설치한다. 12345# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Package 'redis' requires a different Python: 3.7.16 not in '&gt;=3.8' –no-index 옵션을 사용하여 외부 PyPI에 접속하지 않고, 지정된 디렉토리(–find-links .)에서만 패키지를 찾도록 한다. redis 패키지가 Python3.8 이상의 버전을 요구하고 현재 시스템에 설치된 Python 버전이 3.7.16이기 때문에, Python 3.8이상으로 업그레이드 해야한다. Python 버전 업그레이드 방법 (Amazon Linux 환경) Python 3.8 설치123sudo yum install -y amazon-linux-extrassudo amazon-linux-extras enable python3.8sudo yum install -y python3.8 기본 Python 버전 변경 python 3.8을 python3로 대체하려면 아래와 같이 심볼릭 링크를 업데이트 할 수 있음1sudo ln -sf /usr/bin/python3.8 /usr/bin/python3 pip3 역시 Python 3.8에 맞는 버전으로 변경1sudo ln -sf /usr/bin/pip3.8 /usr/bin/pip3 Python 버전 확인1python3 --version 이제 Redis 패키지 설치 재시도를 한다. 1234567# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Could not find a version that satisfies the requirement async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; (from redis)ERROR: No matching distribution found for async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; 이번엔 이런 에러가 나는데, redis 패키지가 async-timeout 패키지의 특정 버전 (&gt;=4.0.3)을 필요로 하는데, 현재 환경에서는 해당 버전을 찾을 수 없어서 발생한 것이다.pip download가 기본적으로 상위 패키지만 다운로드하고, 해당 패키지의 모든 의존성을 포함하지 않기 때문이다.모든 의존성 패키지를 포함해 다운로드 하려면 pip에 추가 옵션을 지정해주어야 한다. 1pip download --no-binary=:all: redis 위와 같이 –no-binary 옵션을 사용해 의존성 패키지까지 함께 다운로드 할 수 있다. redis와 모든 의존성 패키지들이 .whl 또는 .tar.gz 파일로 함께 다운로드 될 것이다. tar.gz 파일을 S3를 통해 EC2로 전송하여 설치를 진행한다. 1234# aws s3 cp s3://aipin-bucket/redis-5.2.0.tar.gz /root# tar -xzf async-timeout-4.0.3.tar.gz# cd redis-5.2.0# python3 setup.py install 그런데 또 에러가 난다. 12345678910111213141516171819# python3 setup.py install/usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type' warnings.warn(msg)running installerror: can't create or remove files in install directoryThe following error occurred while trying to add or remove files in theinstallation directory: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/site-packages/test-easy-install-2904.write-test'The installation directory you specified (via --install-dir, --prefix, orthe distutils default setting) was: /usr/local/lib/python3.8/site-packages/This directory does not currently exist. Please create it and try again, orchoose a different installation directory (using the -d or --install-diroption). 이 오류는 /usr/local/lib/python3.8/site-packages/ 디렉토리가 존재하지 않아서 발생한 것이다. -&gt; 디렉토리 생성 후 설치 시도 12sudo mkdir -p /usr/local/lib/python3.8/site-packages/sudo python3 setup.py install 이번엔 setup.py를 사용하여 설치할 때 발생하는 marshal 모듈 관련 에러가 난다. 12345678910111213141516171819202122232425262728Traceback (most recent call last): File &quot;setup.py&quot;, line 4, in &lt;module&gt; setup( File &quot;/usr/lib/python3.8/site-packages/setuptools/__init__.py&quot;, line 129, in setup return distutils.core.setup(**attrs) File &quot;/usr/lib64/python3.8/distutils/core.py&quot;, line 148, in setup dist.run_commands() File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 966, in run_commands self.run_command(cmd) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 67, in run self.do_egg_install() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 109, in do_egg_install self.run_command('bdist_egg') File &quot;/usr/lib64/python3.8/distutils/cmd.py&quot;, line 313, in run_command self.distribution.run_command(command) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 218, in run os.path.join(archive_root, 'EGG-INFO'), self.zip_safe() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 269, in zip_safe return analyze_egg(self.bdist_dir, self.stubs) File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 379, in analyze_egg safe = scan_module(egg_dir, base, name, stubs) and safe File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 416, in scan_module code = marshal.load(f)ValueError: bad marshal data (unknown type code) 아예 방법을 바꿔서 Redis 서버를 설치하겠다. Redis 서버 설치Redis 서버를 설치하여 EC2 인스턴스에서 직접 Redis 서버를 운영하겠다. 인터넷이 연결된 환경에서 Redis 소스파일을 다운로드 한다.1wget http://download.redis.io/redis-stable.tar.gz 다운로드한 redis-stable.tar.gz 파일을 EC2 인스턴스로 전송한다. EC2에서 Redis 압축을 푼다.12tar -xzf redis-stable.tar.gzcd redis-stable Redis를 컴파일하고 설치한다.12makesudo make install Redis 서버를 실행하여 설치가 완료되었는지 확인한다.1234567891011121314151617181920212223242526redis-server11366:C 07 Nov 2024 18:11:49.254 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then rebootor run the command 'sysctl vm.overcommit_memory=1' for this to take effect.11366:C 07 Nov 2024 18:11:49.254 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo11366:C 07 Nov 2024 18:11:49.254 * Redis version=7.4.1, bits=64, commit=00000000, modified=0, pid=11366, just started11366:C 07 Nov 2024 18:11:49.254 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf11366:M 07 Nov 2024 18:11:49.254 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis Community Edition .-`` .-```. ```\\/ _.,_ ''-._ 7.4.1 (00000000/0) 64 bit ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 11366 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'11366:M 07 Nov 2024 18:11:49.255 * Server initialized11366:M 07 Nov 2024 18:11:49.255 * Ready to accept connections tcp 12# Redis 서버의 URL을 지정합니다.REDIS_URL = &quot;redis://localhost:6379/0&quot; 123456789from dotenv import load_dotenvimport osload_dotenv()# LANGCHAIN_TRACING_V2 환경 변수를 &quot;true&quot;로 설정합니다.os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# LANGCHAIN_PROJECT 설정os.environ[&quot;LANGCHAIN_PROJECT&quot;] = &quot;RunnableWithMessageHistory&quot; 1234567891011121314from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -&gt; RedisChatMessageHistory: # 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다. return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory( runnable, # 실행 가능한 객체 get_message_history, # 메시지 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키) 123456with_message_history.invoke( # 수학 관련 질문 &quot;코사인의 의미는 무엇인가요?&quot;를 입력으로 전달합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;What does cosine mean?&quot;}, # 설정 옵션으로 세션 ID를 &quot;redis123&quot; 로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;foobar&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;redis456&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis456&quot;}},) Redis 컨테이너에서…123docker exec -it redis-container shredis-clikeys * 1234567891011get message_store:redis456get message_store:redis123LRANGE message_store:redis456LRANGE message_store:redis123hgetall message_store:redis456hgetall message_store:redis123type message_store:redis456type message_store:redis123 Redis 조회 시1234567891011121314151617181920212223127.0.0.1:6379&gt; keys *1) &quot;message_store:redis456&quot;2) &quot;message_store:redis123&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis456 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 103, \\&quot;total_tokens\\&quot;: 120, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-1507d5ba-7bce-458d-b9ff-b6c93252dab3-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 103, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 120, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 60, \\&quot;total_tokens\\&quot;: 77, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-c7ce147f-2ac9-4068-8405-6bc91669a52e-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 60, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 77, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis123 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Shohei Ohtani is a Japanese professional baseball player who plays for the Los Angeles Angels in Major League Baseball (MLB). He is known for his exceptional skills as both a pitcher and a hitter.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 42, \\&quot;prompt_tokens\\&quot;: 185, \\&quot;total_tokens\\&quot;: 227, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-8ba62e18-27de-438b-8a88-a85b399951a6-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 185, \\&quot;output_tokens\\&quot;: 42, \\&quot;total_tokens\\&quot;: 227, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Who ohtani.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 26, \\&quot;prompt_tokens\\&quot;: 146, \\&quot;total_tokens\\&quot;: 172, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-56f41ea1-25d2-4071-aa8e-24693cc3dead-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 146, \\&quot;output_tokens\\&quot;: 26, \\&quot;total_tokens\\&quot;: 172, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;5) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\ucf54\\\\uc0ac\\\\uc778\\\\uc740 \\\\uc9c1\\\\uac01 \\\\uc0bc\\\\uac01\\\\ud615\\\\uc5d0\\\\uc11c \\\\uc778\\\\uc811\\\\ubcc0\\\\uacfc \\\\ube57\\\\ubcc0\\\\uc758 \\\\ube44\\\\uc728\\\\uc744 \\\\ub098\\\\ud0c0\\\\ub0c5\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 41, \\&quot;prompt_tokens\\&quot;: 92, \\&quot;total_tokens\\&quot;: 133, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-986633f9-22a6-4c16-89d9-3918780b585d-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 92, \\&quot;output_tokens\\&quot;: 41, \\&quot;total_tokens\\&quot;: 133, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;6) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;7) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 19, \\&quot;prompt_tokens\\&quot;: 47, \\&quot;total_tokens\\&quot;: 66, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-146afcbf-7301-4673-b3d4-0bf6ae2c9191-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 47, \\&quot;output_tokens\\&quot;: 19, \\&quot;total_tokens\\&quot;: 66, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;8) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot; 프로젝트에 Redis 저장 및 로드 코드 추가아래와 같이 Redis 설정을 추가하고 1234567# Redis 설정redis_client = redis.StrictRedis( host=os.getenv('REDIS_HOST', 'localhost'), port=int(os.getenv('REDIS_PORT', 6379)), db=0, decode_responses=True) 대화 히스토리 저장용 함수를 추가하고 1234567891011121314151617# 대화 히스토리 저장용 함수def save_chat_history_to_redis(user_id: str, message: str, response: str): timestamp = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) user_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message } assistant_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response } # 사용자 메시지와 응답을 각각 저장 redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(user_chat_data)) redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(assistant_chat_data)) print(&quot;대화 내용이 저장되었습니다&quot;) 저장된 히스토리를 불러오는 함수도 추가하였다. 12345def get_session_history(user_id: str) -&gt; List[ChatFormat]: # Redis에서 user_id에 해당하는 대화 내역을 가져오는 로직 작성 history_data = redis_client.lrange(f&quot;chat_history:{user_id}&quot;, 0, -1) history = [ChatFormat(**json.loads(item)) for item in history_data] return history 그리고 Agent 호출 로직에 history를 load 하여 호출하도록 추가하였다. 12345678910111213# Redis에서 사용자 대화 기록 가져와 history 대체하기history = get_session_history(user_id)# GIS AGENT 호출response = call_gis_agent(message, history, summary)# Redis에 대화 내용 저장save_chat_history_to_redis(user_id, message, response)# 대화 내역 출력 (optional)print(print_session_history(history))return response 그리고 비슷하게 summary를 저장하는 로직도 추가하였다. 실행하게 되면 Redis에서 아래와 같이 두 가지 Redis key가 사용자 별로 생성된다. 123127.0.0.1:6379&gt; keys *1) &quot;summary_list:default_user&quot;2) &quot;chat_history:default_user&quot; 한글로 채팅이 오고 가기 때문에 redis에서 조회해도 다 깨져있고, 로그에서 잘 저장되고 불러오는지 확인해보겠다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151대화 내용이 저장되었습니다저장된 대화 내용:Role: userContent: 효창공원역 맛집 추천해줘----------Role: assistantContent: 효창공원역 주변의 맛집을 추천해드릴게요:1. **창성옥** - **주소**: 서울특별시 용산구 새창로 124-10 (용문동 25-16) - **전화**: 02-718-2878 - **카테고리**: 한식 - **메뉴**: - 뼈전골 (소): 27,000원 - 뼈전골 (중): 36,000원 - 해장국: 10,000원 - **영업시간**: 24시간 운영 - **예약 가능 여부**: 가능2. **효창동짜장우동** - **주소**: 서울특별시 용산구 백범로 283 (효창동 81-1) - **전화**: 02-703-5287 - **카테고리**: 중식 - **메뉴**: - 짜장: 4,000원 - 김치우동: 4,000원 - 우동: 4,000원3. **박명도봉평메밀막국수** - **주소**: 서울특별시 용산구 원효로 184-1 (원효로2가 43) - **전화**: 02-717-7711 - **카테고리**: 국수전문 - **메뉴**: - 들기름막국수: 11,000원 - 물막국수: 11,000원 - 비빔막국수: 11,000원 - **영업시간**: 10:00 - 22:004. **용문해장국** - **주소**: 서울특별시 용산구 효창원로 110 (용문동 8-95) - **전화**: 02-712-6290 - **카테고리**: 한식 - **메뉴**: - 해장국: 7,000원 - 해장국 (2인분): 14,000원 - 뼈전골 (중): 30,000원 - **예약 가능 여부**: 가능, 주차 가능5. **아성녹두빈대떡** - **주소**: 서울특별시 용산구 효창원로48길 3 (용문동 8-113) - **전화**: 02-706-8238 - **카테고리**: 기타 - **메뉴**: - 녹두빈대떡: 15,000원 - 해물파전: 12,000원 - 감자전: 12,000원이 맛집들이 효창공원역 근처에서 좋은 선택이 될 것입니다!----------Role: userContent: 우리집은 반포자이 114동인데, 우리집에서 창성옥까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 창성옥까지의 소요 시간은 약 24분 30초이며, 거리는 약 12,291 미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 서울역까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 서울역까지의 소요 시간은 약 33분이며, 거리는 약 9.5킬로미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 아까 알려준 효창공원 맛집 중 첫번째가 어디였지?----------Role: assistantContent: 아까 알려드린 효창공원역 맛집 중 첫 번째는 **창성옥**입니다. 주소는 서울특별시 용산구 새창로 124-10 (용문동 25-16)이고, 전화번호는 02-718-2878입니다. 한식을 제공하며, 뼈전골과 해장국이 주요 메뉴입니다. 24시간 운영하며 예약도 가능합니다.----------Role: userContent: 아까 추천해준 효창공원 맛집 중 4번째는 어디였지?----------Role: assistantContent: 아까 추천해드린 효창공원역 맛집 중 네 번째는 **용문해장국**입니다. 주소는 서울특별시 용산구 효창원로 110 (용문동 8-95)이고, 전화번호는 02-712-6290입니다. 한식을 제공하며, 해장국과 뼈전골이 주요 메뉴입니다. 예약이 가능하고 주차도 가능합니다.----------Role: userContent: 우리 부모님집은 도원삼성래미안 아파트야----------Role: assistantContent: 도원삼성래미안 아파트에서 서울역까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 17분 (1074초)- **거리**: 약 3.965킬로미터 (3965미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 우리부모님집까지 얼마나 걸려----------Role: assistantContent: 반포자이 114동에서 도원삼성래미안 아파트까지의 소요 시간은 약 28분이며, 거리는 약 13,026미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 KT 광화문 West 빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 광화문 West 빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 29분 (1766초)- **거리**: 약 9.3킬로미터 (9307미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 19분 (1144초)- **거리**: 약 14.47킬로미터 (14470미터)안전 운전하세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간:** 약 19분 17초 (1157초)- **거리:** 약 14,464 미터안전 운전하시기 바랍니다!----------Role: userContent: 우리부모님집에서 KT 대관령 수련관 얼마나 걸려?----------Role: assistantContent: 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 2시간 34분 (9280초)- **거리**: 약 197.6킬로미터이 정보를 바탕으로 여행을 계획하실 수 있습니다. 추가로 도움이 필요하시면 말씀해 주세요!----------None/Users/leehamin/app/GIS-Agent/gis-ai-agent-be/agent_api/models/chat_models.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead. chain = LLMChain(prompt=get_summary_prompt_template(), llm=llm)Summary list가 저장되었습니다.answer_data: {'answer': '반포자이 114동에서 KT위즈파크까지의 경로 안내는 이미 완료되었습니다. 경로는 반포자이 114동에서 출발하여 신반포로를 따라 남쪽으로 이동한 후, 반포대교를 건너 수원 방향으로 계속 이동하여 경수대로를 따라 KT위즈파크에 도착하는 것입니다. 안전한 여행 되세요!', 'chat_status': 'C1001'}answer_summary_list : ['사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, 그리고 KT 판교빌딩까지의 소요 시간과 거리를 문의하여 답변을 받았습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 또한, 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보를 확인했습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 사용자는 효창공원역 맛집 추천을 다시 요청했고, 첫 번째와 네 번째 맛집의 정보를 다시 확인했습니다. 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보도 확인했으며, 반포자이 114동에서 KT위즈파크까지의 소요 시간과 경로를 안내받았습니다.']INFO: 127.0.0.1:51812 - &quot;POST /api/v1/chat HTTP/1.1&quot; 200 OK 위와 같이 대화 내용과 summary_list가 모두 잘 저장되고 load되며 멀티턴 또한 잘 진행된다. User 별로 대화가 저장되도록 user_uuid 적용더미로 default_user라는 user_id로 통일하던 것을유저별로 대화 history와 summary가 적용될 수 있도록, 식별자인 user_uuid를 적용하였다. 헤더에 X-User-UUID를 달고 가는 방식이다. 12345678def generate_or_get_uuid(user_uuid: Optional[str]) -&gt; UUID: if user_uuid: try: # UUID가 유효한지 검사 return UUID(user_uuid) except ValueError: raise_error(&quot;E1006&quot;) else: # UUID가 없으면 새 UUID 생성 return uuid4() 유저 아이디에 맞게 redis key가 생성된다. 12345127.0.0.1:6379&gt; keys *1) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;2) &quot;summary_list:default_user&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;chat_history:default_user&quot; 테스트 세션1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;네, 판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n- 위도: 37.3933\\n- 경도: 127.1144\\n\\n더 필요한 정보가 있으면 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot; ]} 테스트 세션2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;효창공원 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;효창공원 주변의 맛집 정보는 다음과 같습니다:\\n\\n1. **효창원** \\n - **주소**: 서울 용산구 효창원로 97\\n - **특징**: 전통 한식과 다양한 메뉴를 제공하는 식당입니다.\\n\\n2. **카페 드 파리** \\n - **주소**: 서울 용산구 효창원로 85\\n - **특징**: 디저트와 커피를 전문으로 하는 아늑한 카페입니다.\\n\\n3. **고기리 막창** \\n - **주소**: 서울 용산구 효창원로 95\\n - **특징**: 고기 요리를 전문으로 하는 맛집으로, 막창이 유명합니다.\\n\\n이 외에도 더 많은 맛집이 있으니, 필요하시면 추가 정보를 요청해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot; ]} 테스트 세션3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;첫번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 49분입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot; ]} 테스트 세션4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원으로 이동한 후, 효창원에서 김포공항까지 이동하는 데 걸리는 총 소요 시간은 다음과 같습니다:\\n\\n- **판교 그랑블 아파트 → 효창원**: 약 41분 38초\\n- **효창원 → 김포공항**: 약 56분 4초\\n\\n따라서, 총 소요 시간은 약 1시간 37분 42초입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot; ]} 테스트 세션5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 1시간 10분이며, 효창원에서 인천공항까지의 소요 시간은 약 47분입니다. 따라서, 총 소요 시간은 약 1시간 57분입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다. 사용자가 인천공항으로 가는 시간을 묻자, 어시스턴트는 총 소요 시간이 약 1시간 57분이라고 안내합니다.&quot; ]} 서버에 배포프로젝트에서 requirements.txt 파일을 사용하고 있기 때문에, 이 파일에 redis 패키지를 추가했다. 1redis Redis가 실행되고 있는 호스트의 IP로 바꾸었다 12# host=os.getenv('REDIS_HOST', 'localhost'),host=os.getenv('REDIS_HOST', '10.71.176.93'), 1234# EC2 환경redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://10.71.176.93:6379/0&quot;)# 로컬 환경# redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://localhost:6379/0&quot;) Redis 서버가 외부 연결을 허용하는지 확인Redis 설정파일 위치 확인12# Redis 설정 파일 위치 확인find / -name &quot;redis.conf&quot; Redis 설정 파일 수정123456# 이렇게 있던 것을bind 127.0.0.1 -::1# 이렇게bind 0.0.0.0protected-mode no Redis 재시작123456789redis-cli shutdown# Redis 서버의 프로세스 ID 확인ps aux | grep redis-server# 프로세스 ID를 이용해 종료 (예: PID가 1234인 경우)kill 1234# Redis 서버 재시작redis-server /path/to/redis.conf 여전히 protected mode가 실행중으로 나타난다1234[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379-DENIED Redis is running in protected mode because protected mode is enabled and no password is set for the default user. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Set up an authentication password for the default user. NOTE:You only need to do one of the above things in order for the server to start accepting connections from the outside. Redis CLI를 통해 임시로 protected-mode 비활성화Redis 서버에 접근 가능한 경우, Redis CLI를 사용하여 protected-mode를 비활성화할 수 있다. 123456789# Redis CLI 접속redis-cli# protected-mode 비활성화CONFIG SET protected-mode no[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379 이번엔 연결이 되는 것 같다 서버 테스트테스트 1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n\\n- **위도**: 37.393299557105\\n- **경도**: 127.11436915566647\\n\\n이 정보가 도움이 되길 바랍니다! 추가적인 질문이 있으면 언제든지 말씀해 주세요.&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot; ]} 테스트 2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;포천 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;포천의 맛집 정보를 제공해드렸습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot; ]} 테스트 3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;영종도 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415161718192021222324252627282930313233343536{ &quot;chat&quot;: { &quot;answer&quot;: &quot;영종도에서 추천하는 맛집 목록입니다:\\n\\n1. **선녀풍**\\n - 주소: 인천광역시 중구 을왕동 용유서로 272, 689-1\\n - 전화: 032-751-2121\\n - 주메뉴: 해물파전(15,000원), 선녀물회(20,000원), 낙지물회(28,000원)\\n - 영업시간: 매일 12:00 - 22:00\\n - 주차: 가능\\n - 좌표: [37.4444, 126.3787]\\n\\n2. **황해해물칼국수**\\n - 주소: 인천광역시 중구 덕교동 마시란로 37, 128-56\\n - 전화: 032-752-3017\\n - 주메뉴: 산낙지(15,000원), 전복 (4마리)(16,000원), 해물칼국수(10,000원)\\n - 영업시간: 매일 10:00 - 20:00\\n - 주차: 가능\\n - 좌표: [37.4262, 126.4212]\\n\\n3. **동해막국수**\\n - 주소: 인천광역시 중구 을왕동 용유서로479번길 16, 859-3\\n - 전화: 032-746-5522\\n - 영업시간: 매일 11:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4616, 126.3705]\\n\\n4. **미애네칼국수**\\n - 주소: 인천광역시 중구 덕교동 용유로21번길 51, 80-14\\n - 전화: 032-746-3838\\n - 주메뉴: 산낙지(18,000원), 전복회(15,000원), 바다속칼국수 (소)(35,000원)\\n - 영업시간: 매일 09:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4300, 126.4242]\\n\\n5. **을항**\\n - 주소: 인천광역시 중구 을왕동 선녀바위로55번길 39, 686-5\\n - 전화: 032-752-2227\\n - 주메뉴: 물회(1인)(25,000원), 물회(대)(75,000원), 물회(중)(55,000원)\\n - 좌표: [37.4436, 126.3782]\\n\\n맛집 선택에 도움이 되길 바랍니다!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot; ]}---title: LangChain의 RunnableWithMessage와 Redis 활용하여 대화내용 저장하기date: 2024-11-03 22:37:01tags: - LangChain - LLM - Redis - [LLM, LangChain]cover: /gallery/langChain.png---LangChain의 RunnableWithMessage는 LangChain에서 제공하는 유틸리티 클래스로, 주로 챗봇 애플리케이션 개발 시 이전의 대화 히스토리와 상호작용을 관리하기 위해 사용된다.이 클래스는 주로 대화 기록을 저장하고 관리하여 대화 흐름을 유지하거나 개인화된 응답을 생성하는데 유용하다.기본적으로 RunnableWithHistory 클래스는 대화 세션 내에서 사용자와 AI간의 대화 히스토리를 지속적으로 기록할 수 있도록 해주며, 필요한 경우 특정 조건에 따라 히스토리를 요약하거나 일부 삭제할 수 있는 기능도 제공한다.이를통해 챗봇이 각 세션마다 대화 컨텍스트를 유지할 수 있어 더욱 자연스러운 대화 경험을 제공한다.&lt;!--more--&gt;~~~pythonfrom dotenv import load_dotenvload_dotenv(dotenv_path=&quot;../.env&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 12345678910111213141516from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages( [ ( &quot;system&quot;, &quot;당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요&quot;, ), # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨 MessagesPlaceholder(variable_name=&quot;history&quot;), (&quot;human&quot;, &quot;{input}&quot;), # 사용자 입력을 변수로 사용 ])runnable = prompt | model # 프롬프트와 모델을 연결하여 runnable 객체 생성 123456789101112131415161718192021222324from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 영구 저장소 (Persistent Storage)영구 저장소(Persistent Storage)는 프로그램이 종료되거나 시스템이 재부팅되더라도 데이터를 유지하는 저장 메커니즘을 말한다.이는 데이터베이스, 파일시스템, 또는 기타 비휘발성 저장 장치를 통해 구현될 수 있다. 영구 저장소는 애플리케이션의 상태를 저장하고, 사용자 설정을 유지하며, 장기간 데이터를 보존하는 데 필수적이다.이를 통해 프로그램은 이전 실행에서 중단된 지점부터 다시 시작할 수 있으며, 사용자는 데이터 손실 없이 작업을 계속 할 수 있다. RunnableWithMessageHistory는 get_session_history 호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적이다. Redis 설치Redis가 설치되어 있지 않다면 먼저 설치해야 한다. 폐쇄망 EC2이기 때문에, 인터넷이 연결된 로컬 컴퓨터에서 EC2 서버에 설치할 패키지를 다운로드 한다. 123456% pip download redisCollecting redis Using cached redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)Using cached redis-5.2.0-py3-none-any.whl (261 kB)Saved ./redis-5.2.0-py3-none-any.whlSuccessfully downloaded redis S3에 redis whl 파일을 업로드한다.그리고 EC2에 S3에서 whl 파일을 가져온다. 12345# aws configure# export AWS_ACCESS_KEY_ID=......# aws s3 cp s3://aipin-bucket/redis-5.2.0-py3-none-any.whl /rootdownload: s3://aipin-bucket/redis-5.2.0-py3-none-any.whl to ./redis-5.2.0-py3-none-any.whl 다운로드한 파일들을 EC2에서 설치한다. 12345# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Package 'redis' requires a different Python: 3.7.16 not in '&gt;=3.8' –no-index 옵션을 사용하여 외부 PyPI에 접속하지 않고, 지정된 디렉토리(–find-links .)에서만 패키지를 찾도록 한다. redis 패키지가 Python3.8 이상의 버전을 요구하고 현재 시스템에 설치된 Python 버전이 3.7.16이기 때문에, Python 3.8이상으로 업그레이드 해야한다. Python 버전 업그레이드 방법 (Amazon Linux 환경) Python 3.8 설치123sudo yum install -y amazon-linux-extrassudo amazon-linux-extras enable python3.8sudo yum install -y python3.8 기본 Python 버전 변경 python 3.8을 python3로 대체하려면 아래와 같이 심볼릭 링크를 업데이트 할 수 있음1sudo ln -sf /usr/bin/python3.8 /usr/bin/python3 pip3 역시 Python 3.8에 맞는 버전으로 변경1sudo ln -sf /usr/bin/pip3.8 /usr/bin/pip3 Python 버전 확인1python3 --version 이제 Redis 패키지 설치 재시도를 한다. 1234567# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Could not find a version that satisfies the requirement async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; (from redis)ERROR: No matching distribution found for async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; 이번엔 이런 에러가 나는데, redis 패키지가 async-timeout 패키지의 특정 버전 (&gt;=4.0.3)을 필요로 하는데, 현재 환경에서는 해당 버전을 찾을 수 없어서 발생한 것이다.pip download가 기본적으로 상위 패키지만 다운로드하고, 해당 패키지의 모든 의존성을 포함하지 않기 때문이다.모든 의존성 패키지를 포함해 다운로드 하려면 pip에 추가 옵션을 지정해주어야 한다. 1pip download --no-binary=:all: redis 위와 같이 –no-binary 옵션을 사용해 의존성 패키지까지 함께 다운로드 할 수 있다. redis와 모든 의존성 패키지들이 .whl 또는 .tar.gz 파일로 함께 다운로드 될 것이다. tar.gz 파일을 S3를 통해 EC2로 전송하여 설치를 진행한다. 1234# aws s3 cp s3://aipin-bucket/redis-5.2.0.tar.gz /root# tar -xzf async-timeout-4.0.3.tar.gz# cd redis-5.2.0# python3 setup.py install 그런데 또 에러가 난다. 12345678910111213141516171819# python3 setup.py install/usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type' warnings.warn(msg)running installerror: can't create or remove files in install directoryThe following error occurred while trying to add or remove files in theinstallation directory: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/site-packages/test-easy-install-2904.write-test'The installation directory you specified (via --install-dir, --prefix, orthe distutils default setting) was: /usr/local/lib/python3.8/site-packages/This directory does not currently exist. Please create it and try again, orchoose a different installation directory (using the -d or --install-diroption). 이 오류는 /usr/local/lib/python3.8/site-packages/ 디렉토리가 존재하지 않아서 발생한 것이다. -&gt; 디렉토리 생성 후 설치 시도 12sudo mkdir -p /usr/local/lib/python3.8/site-packages/sudo python3 setup.py install 이번엔 setup.py를 사용하여 설치할 때 발생하는 marshal 모듈 관련 에러가 난다. 12345678910111213141516171819202122232425262728Traceback (most recent call last): File &quot;setup.py&quot;, line 4, in &lt;module&gt; setup( File &quot;/usr/lib/python3.8/site-packages/setuptools/__init__.py&quot;, line 129, in setup return distutils.core.setup(**attrs) File &quot;/usr/lib64/python3.8/distutils/core.py&quot;, line 148, in setup dist.run_commands() File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 966, in run_commands self.run_command(cmd) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 67, in run self.do_egg_install() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 109, in do_egg_install self.run_command('bdist_egg') File &quot;/usr/lib64/python3.8/distutils/cmd.py&quot;, line 313, in run_command self.distribution.run_command(command) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 218, in run os.path.join(archive_root, 'EGG-INFO'), self.zip_safe() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 269, in zip_safe return analyze_egg(self.bdist_dir, self.stubs) File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 379, in analyze_egg safe = scan_module(egg_dir, base, name, stubs) and safe File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 416, in scan_module code = marshal.load(f)ValueError: bad marshal data (unknown type code) 아예 방법을 바꿔서 Redis 서버를 설치하겠다. Redis 서버 설치Redis 서버를 설치하여 EC2 인스턴스에서 직접 Redis 서버를 운영하겠다. 인터넷이 연결된 환경에서 Redis 소스파일을 다운로드 한다.1wget http://download.redis.io/redis-stable.tar.gz 다운로드한 redis-stable.tar.gz 파일을 EC2 인스턴스로 전송한다. EC2에서 Redis 압축을 푼다.12tar -xzf redis-stable.tar.gzcd redis-stable Redis를 컴파일하고 설치한다.12makesudo make install Redis 서버를 실행하여 설치가 완료되었는지 확인한다.1234567891011121314151617181920212223242526redis-server11366:C 07 Nov 2024 18:11:49.254 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then rebootor run the command 'sysctl vm.overcommit_memory=1' for this to take effect.11366:C 07 Nov 2024 18:11:49.254 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo11366:C 07 Nov 2024 18:11:49.254 * Redis version=7.4.1, bits=64, commit=00000000, modified=0, pid=11366, just started11366:C 07 Nov 2024 18:11:49.254 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf11366:M 07 Nov 2024 18:11:49.254 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis Community Edition .-`` .-```. ```\\/ _.,_ ''-._ 7.4.1 (00000000/0) 64 bit ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 11366 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'11366:M 07 Nov 2024 18:11:49.255 * Server initialized11366:M 07 Nov 2024 18:11:49.255 * Ready to accept connections tcp 12# Redis 서버의 URL을 지정합니다.REDIS_URL = &quot;redis://localhost:6379/0&quot; 123456789from dotenv import load_dotenvimport osload_dotenv()# LANGCHAIN_TRACING_V2 환경 변수를 &quot;true&quot;로 설정합니다.os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# LANGCHAIN_PROJECT 설정os.environ[&quot;LANGCHAIN_PROJECT&quot;] = &quot;RunnableWithMessageHistory&quot; 1234567891011121314from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -&gt; RedisChatMessageHistory: # 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다. return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory( runnable, # 실행 가능한 객체 get_message_history, # 메시지 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키) 123456with_message_history.invoke( # 수학 관련 질문 &quot;코사인의 의미는 무엇인가요?&quot;를 입력으로 전달합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;What does cosine mean?&quot;}, # 설정 옵션으로 세션 ID를 &quot;redis123&quot; 로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;foobar&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;redis456&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis456&quot;}},) Redis 컨테이너에서…123docker exec -it redis-container shredis-clikeys * 1234567891011get message_store:redis456get message_store:redis123LRANGE message_store:redis456LRANGE message_store:redis123hgetall message_store:redis456hgetall message_store:redis123type message_store:redis456type message_store:redis123 Redis 조회 시1234567891011121314151617181920212223127.0.0.1:6379&gt; keys *1) &quot;message_store:redis456&quot;2) &quot;message_store:redis123&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis456 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 103, \\&quot;total_tokens\\&quot;: 120, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-1507d5ba-7bce-458d-b9ff-b6c93252dab3-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 103, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 120, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 60, \\&quot;total_tokens\\&quot;: 77, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-c7ce147f-2ac9-4068-8405-6bc91669a52e-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 60, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 77, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis123 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Shohei Ohtani is a Japanese professional baseball player who plays for the Los Angeles Angels in Major League Baseball (MLB). He is known for his exceptional skills as both a pitcher and a hitter.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 42, \\&quot;prompt_tokens\\&quot;: 185, \\&quot;total_tokens\\&quot;: 227, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-8ba62e18-27de-438b-8a88-a85b399951a6-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 185, \\&quot;output_tokens\\&quot;: 42, \\&quot;total_tokens\\&quot;: 227, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Who ohtani.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 26, \\&quot;prompt_tokens\\&quot;: 146, \\&quot;total_tokens\\&quot;: 172, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-56f41ea1-25d2-4071-aa8e-24693cc3dead-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 146, \\&quot;output_tokens\\&quot;: 26, \\&quot;total_tokens\\&quot;: 172, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;5) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\ucf54\\\\uc0ac\\\\uc778\\\\uc740 \\\\uc9c1\\\\uac01 \\\\uc0bc\\\\uac01\\\\ud615\\\\uc5d0\\\\uc11c \\\\uc778\\\\uc811\\\\ubcc0\\\\uacfc \\\\ube57\\\\ubcc0\\\\uc758 \\\\ube44\\\\uc728\\\\uc744 \\\\ub098\\\\ud0c0\\\\ub0c5\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 41, \\&quot;prompt_tokens\\&quot;: 92, \\&quot;total_tokens\\&quot;: 133, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-986633f9-22a6-4c16-89d9-3918780b585d-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 92, \\&quot;output_tokens\\&quot;: 41, \\&quot;total_tokens\\&quot;: 133, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;6) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;7) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 19, \\&quot;prompt_tokens\\&quot;: 47, \\&quot;total_tokens\\&quot;: 66, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-146afcbf-7301-4673-b3d4-0bf6ae2c9191-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 47, \\&quot;output_tokens\\&quot;: 19, \\&quot;total_tokens\\&quot;: 66, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;8) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot; 프로젝트에 Redis 저장 및 로드 코드 추가아래와 같이 Redis 설정을 추가하고 1234567# Redis 설정redis_client = redis.StrictRedis( host=os.getenv('REDIS_HOST', 'localhost'), port=int(os.getenv('REDIS_PORT', 6379)), db=0, decode_responses=True) 대화 히스토리 저장용 함수를 추가하고 1234567891011121314151617# 대화 히스토리 저장용 함수def save_chat_history_to_redis(user_id: str, message: str, response: str): timestamp = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) user_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message } assistant_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response } # 사용자 메시지와 응답을 각각 저장 redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(user_chat_data)) redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(assistant_chat_data)) print(&quot;대화 내용이 저장되었습니다&quot;) 저장된 히스토리를 불러오는 함수도 추가하였다. 12345def get_session_history(user_id: str) -&gt; List[ChatFormat]: # Redis에서 user_id에 해당하는 대화 내역을 가져오는 로직 작성 history_data = redis_client.lrange(f&quot;chat_history:{user_id}&quot;, 0, -1) history = [ChatFormat(**json.loads(item)) for item in history_data] return history 그리고 Agent 호출 로직에 history를 load 하여 호출하도록 추가하였다. 12345678910111213# Redis에서 사용자 대화 기록 가져와 history 대체하기history = get_session_history(user_id)# GIS AGENT 호출response = call_gis_agent(message, history, summary)# Redis에 대화 내용 저장save_chat_history_to_redis(user_id, message, response)# 대화 내역 출력 (optional)print(print_session_history(history))return response 그리고 비슷하게 summary를 저장하는 로직도 추가하였다. 실행하게 되면 Redis에서 아래와 같이 두 가지 Redis key가 사용자 별로 생성된다. 123127.0.0.1:6379&gt; keys *1) &quot;summary_list:default_user&quot;2) &quot;chat_history:default_user&quot; 한글로 채팅이 오고 가기 때문에 redis에서 조회해도 다 깨져있고, 로그에서 잘 저장되고 불러오는지 확인해보겠다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151대화 내용이 저장되었습니다저장된 대화 내용:Role: userContent: 효창공원역 맛집 추천해줘----------Role: assistantContent: 효창공원역 주변의 맛집을 추천해드릴게요:1. **창성옥** - **주소**: 서울특별시 용산구 새창로 124-10 (용문동 25-16) - **전화**: 02-718-2878 - **카테고리**: 한식 - **메뉴**: - 뼈전골 (소): 27,000원 - 뼈전골 (중): 36,000원 - 해장국: 10,000원 - **영업시간**: 24시간 운영 - **예약 가능 여부**: 가능2. **효창동짜장우동** - **주소**: 서울특별시 용산구 백범로 283 (효창동 81-1) - **전화**: 02-703-5287 - **카테고리**: 중식 - **메뉴**: - 짜장: 4,000원 - 김치우동: 4,000원 - 우동: 4,000원3. **박명도봉평메밀막국수** - **주소**: 서울특별시 용산구 원효로 184-1 (원효로2가 43) - **전화**: 02-717-7711 - **카테고리**: 국수전문 - **메뉴**: - 들기름막국수: 11,000원 - 물막국수: 11,000원 - 비빔막국수: 11,000원 - **영업시간**: 10:00 - 22:004. **용문해장국** - **주소**: 서울특별시 용산구 효창원로 110 (용문동 8-95) - **전화**: 02-712-6290 - **카테고리**: 한식 - **메뉴**: - 해장국: 7,000원 - 해장국 (2인분): 14,000원 - 뼈전골 (중): 30,000원 - **예약 가능 여부**: 가능, 주차 가능5. **아성녹두빈대떡** - **주소**: 서울특별시 용산구 효창원로48길 3 (용문동 8-113) - **전화**: 02-706-8238 - **카테고리**: 기타 - **메뉴**: - 녹두빈대떡: 15,000원 - 해물파전: 12,000원 - 감자전: 12,000원이 맛집들이 효창공원역 근처에서 좋은 선택이 될 것입니다!----------Role: userContent: 우리집은 반포자이 114동인데, 우리집에서 창성옥까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 창성옥까지의 소요 시간은 약 24분 30초이며, 거리는 약 12,291 미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 서울역까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 서울역까지의 소요 시간은 약 33분이며, 거리는 약 9.5킬로미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 아까 알려준 효창공원 맛집 중 첫번째가 어디였지?----------Role: assistantContent: 아까 알려드린 효창공원역 맛집 중 첫 번째는 **창성옥**입니다. 주소는 서울특별시 용산구 새창로 124-10 (용문동 25-16)이고, 전화번호는 02-718-2878입니다. 한식을 제공하며, 뼈전골과 해장국이 주요 메뉴입니다. 24시간 운영하며 예약도 가능합니다.----------Role: userContent: 아까 추천해준 효창공원 맛집 중 4번째는 어디였지?----------Role: assistantContent: 아까 추천해드린 효창공원역 맛집 중 네 번째는 **용문해장국**입니다. 주소는 서울특별시 용산구 효창원로 110 (용문동 8-95)이고, 전화번호는 02-712-6290입니다. 한식을 제공하며, 해장국과 뼈전골이 주요 메뉴입니다. 예약이 가능하고 주차도 가능합니다.----------Role: userContent: 우리 부모님집은 도원삼성래미안 아파트야----------Role: assistantContent: 도원삼성래미안 아파트에서 서울역까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 17분 (1074초)- **거리**: 약 3.965킬로미터 (3965미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 우리부모님집까지 얼마나 걸려----------Role: assistantContent: 반포자이 114동에서 도원삼성래미안 아파트까지의 소요 시간은 약 28분이며, 거리는 약 13,026미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 KT 광화문 West 빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 광화문 West 빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 29분 (1766초)- **거리**: 약 9.3킬로미터 (9307미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 19분 (1144초)- **거리**: 약 14.47킬로미터 (14470미터)안전 운전하세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간:** 약 19분 17초 (1157초)- **거리:** 약 14,464 미터안전 운전하시기 바랍니다!----------Role: userContent: 우리부모님집에서 KT 대관령 수련관 얼마나 걸려?----------Role: assistantContent: 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 2시간 34분 (9280초)- **거리**: 약 197.6킬로미터이 정보를 바탕으로 여행을 계획하실 수 있습니다. 추가로 도움이 필요하시면 말씀해 주세요!----------None/Users/leehamin/app/GIS-Agent/gis-ai-agent-be/agent_api/models/chat_models.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead. chain = LLMChain(prompt=get_summary_prompt_template(), llm=llm)Summary list가 저장되었습니다.answer_data: {'answer': '반포자이 114동에서 KT위즈파크까지의 경로 안내는 이미 완료되었습니다. 경로는 반포자이 114동에서 출발하여 신반포로를 따라 남쪽으로 이동한 후, 반포대교를 건너 수원 방향으로 계속 이동하여 경수대로를 따라 KT위즈파크에 도착하는 것입니다. 안전한 여행 되세요!', 'chat_status': 'C1001'}answer_summary_list : ['사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, 그리고 KT 판교빌딩까지의 소요 시간과 거리를 문의하여 답변을 받았습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 또한, 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보를 확인했습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 사용자는 효창공원역 맛집 추천을 다시 요청했고, 첫 번째와 네 번째 맛집의 정보를 다시 확인했습니다. 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보도 확인했으며, 반포자이 114동에서 KT위즈파크까지의 소요 시간과 경로를 안내받았습니다.']INFO: 127.0.0.1:51812 - &quot;POST /api/v1/chat HTTP/1.1&quot; 200 OK 위와 같이 대화 내용과 summary_list가 모두 잘 저장되고 load되며 멀티턴 또한 잘 진행된다. User 별로 대화가 저장되도록 user_uuid 적용더미로 default_user라는 user_id로 통일하던 것을유저별로 대화 history와 summary가 적용될 수 있도록, 식별자인 user_uuid를 적용하였다. 헤더에 X-User-UUID를 달고 가는 방식이다. 12345678def generate_or_get_uuid(user_uuid: Optional[str]) -&gt; UUID: if user_uuid: try: # UUID가 유효한지 검사 return UUID(user_uuid) except ValueError: raise_error(&quot;E1006&quot;) else: # UUID가 없으면 새 UUID 생성 return uuid4() 유저 아이디에 맞게 redis key가 생성된다. 12345127.0.0.1:6379&gt; keys *1) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;2) &quot;summary_list:default_user&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;chat_history:default_user&quot; 테스트 세션1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;네, 판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n- 위도: 37.3933\\n- 경도: 127.1144\\n\\n더 필요한 정보가 있으면 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot; ]} 테스트 세션2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;효창공원 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;효창공원 주변의 맛집 정보는 다음과 같습니다:\\n\\n1. **효창원** \\n - **주소**: 서울 용산구 효창원로 97\\n - **특징**: 전통 한식과 다양한 메뉴를 제공하는 식당입니다.\\n\\n2. **카페 드 파리** \\n - **주소**: 서울 용산구 효창원로 85\\n - **특징**: 디저트와 커피를 전문으로 하는 아늑한 카페입니다.\\n\\n3. **고기리 막창** \\n - **주소**: 서울 용산구 효창원로 95\\n - **특징**: 고기 요리를 전문으로 하는 맛집으로, 막창이 유명합니다.\\n\\n이 외에도 더 많은 맛집이 있으니, 필요하시면 추가 정보를 요청해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot; ]} 테스트 세션3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;첫번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 49분입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot; ]} 테스트 세션4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원으로 이동한 후, 효창원에서 김포공항까지 이동하는 데 걸리는 총 소요 시간은 다음과 같습니다:\\n\\n- **판교 그랑블 아파트 → 효창원**: 약 41분 38초\\n- **효창원 → 김포공항**: 약 56분 4초\\n\\n따라서, 총 소요 시간은 약 1시간 37분 42초입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot; ]} 테스트 세션5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 1시간 10분이며, 효창원에서 인천공항까지의 소요 시간은 약 47분입니다. 따라서, 총 소요 시간은 약 1시간 57분입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다. 사용자가 인천공항으로 가는 시간을 묻자, 어시스턴트는 총 소요 시간이 약 1시간 57분이라고 안내합니다.&quot; ]} 서버에 배포프로젝트에서 requirements.txt 파일을 사용하고 있기 때문에, 이 파일에 redis 패키지를 추가했다. 1redis Redis가 실행되고 있는 호스트의 IP로 바꾸었다 12# host=os.getenv('REDIS_HOST', 'localhost'),host=os.getenv('REDIS_HOST', '10.71.176.93'), 1234# EC2 환경redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://10.71.176.93:6379/0&quot;)# 로컬 환경# redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://localhost:6379/0&quot;) Redis 서버가 외부 연결을 허용하는지 확인Redis 설정파일 위치 확인12# Redis 설정 파일 위치 확인find / -name &quot;redis.conf&quot; Redis 설정 파일 수정123456# 이렇게 있던 것을bind 127.0.0.1 -::1# 이렇게bind 0.0.0.0protected-mode no Redis 재시작123456789redis-cli shutdown# Redis 서버의 프로세스 ID 확인ps aux | grep redis-server# 프로세스 ID를 이용해 종료 (예: PID가 1234인 경우)kill 1234# Redis 서버 재시작redis-server /path/to/redis.conf 여전히 protected mode가 실행중으로 나타난다1234[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379-DENIED Redis is running in protected mode because protected mode is enabled and no password is set for the default user. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Set up an authentication password for the default user. NOTE:You only need to do one of the above things in order for the server to start accepting connections from the outside. Redis CLI를 통해 임시로 protected-mode 비활성화Redis 서버에 접근 가능한 경우, Redis CLI를 사용하여 protected-mode를 비활성화할 수 있다. 123456789# Redis CLI 접속redis-cli# protected-mode 비활성화CONFIG SET protected-mode no[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379 이번엔 연결이 되는 것 같다 서버 테스트테스트 1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n\\n- **위도**: 37.393299557105\\n- **경도**: 127.11436915566647\\n\\n이 정보가 도움이 되길 바랍니다! 추가적인 질문이 있으면 언제든지 말씀해 주세요.&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot; ]} 테스트 2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;포천 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;포천의 맛집 정보를 제공해드렸습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot; ]} 테스트 3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;영종도 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;영종도에서 추천하는 맛집 목록입니다:\\n\\n1. **선녀풍**\\n - 주소: 인천광역시 중구 을왕동 용유서로 272, 689-1\\n - 전화: 032-751-2121\\n - 주메뉴: 해물파전(15,000원), 선녀물회(20,000원), 낙지물회(28,000원)\\n - 영업시간: 매일 12:00 - 22:00\\n - 주차: 가능\\n - 좌표: [37.4444, 126.3787]\\n\\n2. **황해해물칼국수**\\n - 주소: 인천광역시 중구 덕교동 마시란로 37, 128-56\\n - 전화: 032-752-3017\\n - 주메뉴: 산낙지(15,000원), 전복 (4마리)(16,000원), 해물칼국수(10,000원)\\n - 영업시간: 매일 10:00 - 20:00\\n - 주차: 가능\\n - 좌표: [37.4262, 126.4212]\\n\\n3. **동해막국수**\\n - 주소: 인천광역시 중구 을왕동 용유서로479번길 16, 859-3\\n - 전화: 032-746-5522\\n - 영업시간: 매일 11:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4616, 126.3705]\\n\\n4. **미애네칼국수**\\n - 주소: 인천광역시 중구 덕교동 용유로21번길 51, 80-14\\n - 전화: 032-746-3838\\n - 주메뉴: 산낙지(18,000원), 전복회(15,000원), 바다속칼국수 (소)(35,000원)\\n - 영업시간: 매일 09:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4300, 126.4242]\\n\\n5. **을항**\\n - 주소: 인천광역시 중구 을왕동 선녀바위로55번길 39, 686-5\\n - 전화: 032-752-2227\\n - 주메뉴: 물회(1인)(25,000원), 물회(대)(75,000원), 물회(중)(55,000원)\\n - 좌표: [37.4436, 126.3782]\\n\\n맛집 선택에 도움이 되길 바랍니다!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot; ]} 테스트 4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;세번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 영종도의 세 번째 맛집인 동해막국수까지의 경로는 약 1시간 5분이 소요되며, 거리는 약 78.54 km입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot; ]} 테스트 5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 김포국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 김포국제공항**\\n - 소요 시간: 약 40분\\n - 거리: 약 45.51 km\\n\\n따라서 전체 소요 시간은 약 1시간 45분이며, 총 거리는 약 124.05 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot; ]} 테스트 6Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Request1234567891011121314151617{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 인천국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 인천국제공항**\\n - 소요 시간: 약 19분\\n - 거리: 약 13.88 km\\n\\n따라서 전체 소요 시간은 약 1시간 24분이며, 총 거리는 약 92.42 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트, 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다. 마지막으로 사용자가 인천공항으로 가는 경로를 묻자, AI가 소요 시간과 거리를 안내하였다.&quot; ]} Redis에서도 해당 사용자 id에 맞게 history와 summary가 잘 저장된다.123456789[root@ec2-ct01-dev-slm-app-01 ~]# redis-cli127.0.0.1:6379&gt; keys *1) &quot;summary_list:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;2) &quot;chat_history:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;127.0.0.1:6379&gt; lrange chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77 0 -11) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;user\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\uc6b0\\\\ub9ac\\\\uc9d1\\\\uc740 \\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc57c, \\\\uc6b0\\\\ub9ac\\\\uc9d1 \\\\uc88c\\\\ud45c \\\\uc54c\\\\uace0 \\\\uc788\\\\ub2c8?\\&quot;}&quot;2) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;assistant\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc758 \\\\uc88c\\\\ud45c\\\\ub294 \\\\ub2e4\\\\uc74c\\\\uacfc \\\\uac19\\\\uc2b5\\\\ub2c8\\\\ub2e4:\\\\n\\\\n- **\\\\uc704\\\\ub3c4**: 37.393299557105\\\\n- **\\\\uacbd\\\\ub3c4**: 127.11436915566647\\\\n\\\\n\\\\uc774 \\\\uc815\\\\ubcf4\\\\uac00 \\\\ub3c4\\\\uc6c0\\\\uc774 \\\\ub418\\\\uae38 \\\\ubc14\\\\ub78d\\\\ub2c8\\\\ub2e4! \\\\ucd94\\\\uac00\\\\uc801\\\\uc778 \\\\uc9c8\\\\ubb38\\\\uc774 \\\\uc788\\\\uc73c\\\\uba74 \\\\uc5b8\\\\uc81c\\\\ub4e0\\\\uc9c0 \\\\ub9d0\\\\uc500\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;}&quot; 123456789101112## 테스트 4### Request~~~json{ &quot;chat&quot;: { &quot;message&quot;: &quot;세번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 영종도의 세 번째 맛집인 동해막국수까지의 경로는 약 1시간 5분이 소요되며, 거리는 약 78.54 km입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot; ]} 테스트 5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 김포국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 김포국제공항**\\n - 소요 시간: 약 40분\\n - 거리: 약 45.51 km\\n\\n따라서 전체 소요 시간은 약 1시간 45분이며, 총 거리는 약 124.05 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot; ]} 테스트 6Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Request1234567891011121314151617{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 인천국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 인천국제공항**\\n - 소요 시간: 약 19분\\n - 거리: 약 13.88 km\\n\\n따라서 전체 소요 시간은 약 1시간 24분이며, 총 거리는 약 92.42 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트, 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다. 마지막으로 사용자가 인천공항으로 가는 경로를 묻자, AI가 소요 시간과 거리를 안내하였다.&quot; ]} Redis에서도 해당 사용자 id에 맞게 history와 summary가 잘 저장된다.123456789[root@ec2-ct01-dev-slm-app-01 ~]# redis-cli127.0.0.1:6379&gt; keys *1) &quot;summary_list:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;2) &quot;chat_history:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;127.0.0.1:6379&gt; lrange chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77 0 -11) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;user\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\uc6b0\\\\ub9ac\\\\uc9d1\\\\uc740 \\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc57c, \\\\uc6b0\\\\ub9ac\\\\uc9d1 \\\\uc88c\\\\ud45c \\\\uc54c\\\\uace0 \\\\uc788\\\\ub2c8?\\&quot;}&quot;2) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;assistant\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc758 \\\\uc88c\\\\ud45c\\\\ub294 \\\\ub2e4\\\\uc74c\\\\uacfc \\\\uac19\\\\uc2b5\\\\ub2c8\\\\ub2e4:\\\\n\\\\n- **\\\\uc704\\\\ub3c4**: 37.393299557105\\\\n- **\\\\uacbd\\\\ub3c4**: 127.11436915566647\\\\n\\\\n\\\\uc774 \\\\uc815\\\\ubcf4\\\\uac00 \\\\ub3c4\\\\uc6c0\\\\uc774 \\\\ub418\\\\uae38 \\\\ubc14\\\\ub78d\\\\ub2c8\\\\ub2e4! \\\\ucd94\\\\uac00\\\\uc801\\\\uc778 \\\\uc9c8\\\\ubb38\\\\uc774 \\\\uc788\\\\uc73c\\\\uba74 \\\\uc5b8\\\\uc81c\\\\ub4e0\\\\uc9c0 \\\\ub9d0\\\\uc500\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;}&quot;","link":"/2024/11/03/LangChain-RunnableWithMessage/"},{"title":"MS-AutoGen","text":"AutoGen은 서로 대화하여 작업을 해결할 수 있는 여러 에이전트를 사용하여 LLM 애플리케이션을 개발할 수 있는 프레임워크이다. AutoGen 에이전트는 사용자 정의가 가능하고 대화가 가능하며 인간의 참여를 원활하게 허용한다. LLM, 인간 입력 및 도구의 조합을 사용하는 다양한 모드에서 작동할 수 있다. AutoGen은 최소한의 노력으로 다중 에이전트 대화에 기반한 차세대 LLM 애플리케이션을 구축할 수 있게 해준다. 복잡한 LLM 워크플로의 오케스트레이션, 자동화 및 최적화를 간소화한다. LLM 모델의 성능을 극대화하고 약점을 극복한다. 복잡한 워크플로에 대한 다양한 대화 패턴을 지원한다. 사용자 정의 가능하고 대화 가능한 에이전트를 통해 개발자는 AutoGen을 사용하여 대화 자율성, 에이전트 수 및 에이전트 대화 토폴로지에 관한 광범위한 대화 패턴을 구축할 수 있다. 다양한 복잡성을 가진 작업 시스템 모음을 제공한다. 이러한 시스템은 다양한 도메인과 복잡성의 광범위한 애플리케이션을 포괄한다. 이는 AutoGen이 다양한 대화 패턴을 쉽게 지원할 수 있는 방법을 보여준다. Bert NER + 의도 분류멀티턴 chatbot NLU + DST Multi Agent Framework AutoGen, langgraph AutoGen (autogen Studio) AutoGen 플로우로 langchain 똑같이 agent를 만들어봐야 하지 않을까 promptFlow () AI Studio AI Agent Builder 등 의도 변환 리서치 AutoGen vs Langgraph","link":"/2024/07/02/MS-AutoGen/"},{"title":"LiteLLM","text":"LiteLLM은 다양한 대형 언어 모델(LLM) 제공자의 API를 간편하게 통합하고 관리할 수 있도록 설계된 라이브러리이다.OpenAI와 호환되는 API를 사용하여 이러한 모델들과의 상호작용을 표준화한다. 여러 LLM을 사용하는 프로젝트에서 LiteLLM은 통합, 관리, 그리고 비용 및 사용량 모니터링을 간편하게 만들어준다. LiteLLM의 주요 기능에는 Completions, Embeddings, Image generation을 지원하는 여러 provider들 간의 일관된 요청 처리가 포함된다.Completions : LLM이 주어진 입력(프롬프트)에 대한 응답으로 텍스트를 생성하는 작업Embeddings : 자연어 처리에서 텍스트를 수치화된 벡터로 변환하는 방법Image generation : 텍스트 입력을 기반으로 이미지를 생성하는 기술 OpenAI, Azure, AWS, HuggingFace 등 다양한 provider들을 지원하며, 로드 밸런싱, 속도 제한, 비용 추적 등의 기능도 제공하여 대규모 AI 애플리케이션 배포에 적합하다. LiteLLM의 두 가지 사용법LiteLLM Proxy Server - 100개 이상의 LLM을 호출하고, 로드 밸런싱하고, 프로젝트 전체에서 비용을 추적하는 서버(LLM 게이트웨이) LiteLLM은 다양한 LLM으로의 요청을 관리하고 라우팅할 수 있는 프록시 서버를 제공하며, 이 프록시는 개발자에게 통합된 인터페이스를 제공한다. 이 프록시는 로깅 및 가시성 기능도 제공하여 대규모 AI 배포를 제어하는데 중요한 역할을 한다. 일반적으로 Gen AI Enablement/ML 플랫폼에서 사용 LiteLLM 서버 (LLM 게이트웨이)는 아래의 것들을 관리한다.통합 인터페이스여러 LLM 제공자들의 API와 기능을 하나의 표준화된 인터페이스로 통합하여 제공하는 기능각각의 고유한 api를 따로 학습하거나 처리할 필요 없이, 하나의 일관된 방법으로 모든 모델에 접근하고 사용 가능OpenAI의 ChatCompletions &amp; Completions 포맷으로 Huggingface/Bedrock/TogetherAI 등의 100개 이상의 LLM 호출 Cost tracking여러 LLM 제공자와 상호작용할 때 발생하는 비용을 모니터링하고 관리할 수 있도록 도와주는 기능각 모델 사용 시 소모되는 토큰 수, 호출 빈도, 그리고 이에 따른 비용 추적, 전체적인 사용 비용을 투명하게 파악 가능 Open image-20240827-080122.pngimage-20240827-080122.png 스트리밍 비용, 사용량 import litellm track_cost_callbackdef track_cost_callback( kwargs, # kwargs to completion completion_response, # response from completion start_time, end_time # start/end time): try: response_cost = kwargs.get(“response_cost”, 0) print(“streaming response_cost”, response_cost) except: pass set callbacklitellm.success_callback = [track_cost_callback] # set custom callback function litellm.completion() callresponse = completion( model=”gpt-3.5-turbo”, messages=[ { “role”: “user”, “content”: “Hi 👋 - i’m openai” } ], stream=True) 로드밸런싱 : 여러 모델, 동일 모델의 여러 서버 배포 간 초당 1.5k 이상의 요청 처리 가능config 예시 : 요청은 model=gpt-3.5-turbo 여러 인스턴스로 라우팅 됨. model_list: model_name: gpt-3.5-turbolitellm_params: model: azure/ api_base: api_key: rpm: 6 # Rate limit for this deployment: in requests per minute (rpm) model_name: gpt-3.5-turbolitellm_params: model: azure/gpt-turbo-small-ca api_base: https://my-endpoint-canada-berri992.openai.azure.com/ api_key: rpm: 6 model_name: gpt-3.5-turbolitellm_params: model: azure/gpt-turbo-large api_base: https://openai-france-1234.openai.azure.com/ api_key: rpm: 1440routing_strategy: simple-shuffle # Literal[“simple-shuffle”, “least-busy”, “usage-based-routing”,”latency-based-routing”], default=”simple-shuffle” model_group_alias: {“gpt-4”: “gpt-3.5-turbo”} # all requests with gpt-4 will be routed to models with gpt-3.5-turbo num_retries: 2 timeout: 30 # 30 seconds redis_host: # set this when using multiple litellm proxy deployments, load balancing state stored in redis redis_password: redis_port: 1992 로깅 관찰성 : LLM 입력/출력 로딩LiteLLM은 Lunary, Langfuse, Helicone, Promptlayer, Traceloop, Slack에 데이터를 전송하기 위해 미리 정의된 콜백을 제공 from litellm import completion set env variables for logging toolsos.environ[“HELICONE_API_KEY”] = “your-helicone-key”os.environ[“LANGFUSE_PUBLIC_KEY”] = “”os.environ[“LANGFUSE_SECRET_KEY”] = “”os.environ[“LUNARY_PUBLIC_KEY”] = “your-lunary-public-key”os.environ[“OPENAI_API_KEY”] set callbackslitellm.success_callback = [“lunary”, “langfuse”, “helicone”] # log input/output to lunary, langfuse, supabase, helicone#openai callresponse = completion(model=”gpt-3.5-turbo”, messages=[{“role”: “user”, “content”: “Hi 👋 - i’m openai”}]) 가드레일, 캐싱을 사용자 정의LiteLLM은 다음을 지원한다메모리 캐시레디스 캐시Quadrant 의미 캐시Redis 의미론적 캐시S3 버킷 캐시 Proxy 엔드포인트프록시에 ChatCompletions 요청을 만들기 import openai # openai v1.0.0+client = openai.OpenAI(api_key=”anything”,base_url=”http://0.0.0.0:8000“) # set proxy to base_url request sent to model set on litellm proxy, litellm --modelresponse = client.chat.completions.create(model=”gpt-3.5-turbo”, messages = [ { “role”: “user”, “content”: “this is a test request, write a short poem” }])print(response) Swagger Open swagger.pngswagger.png LiteLLM python SDKLiteLLM Python SDK란?100개 이상의 LLM을 호출하고, 로드 밸런싱 및 비용 추적을 지원하는 Python 클라이언트OpenAI, Azure OpenAI, Anthropic, Cohere, Replicate, Bedrock, Vertex AI 등Python 코드에서 LiteLLM을 사용하여 여러 LLM에 액세스할 수 있는 통합 인터페이스 제공Azure, OpenAI 등 여러 배포 환경에서 Retry/fallback 로직을 적용하여 안정성 확보오픈소스 : GitHub에서 무료로 제공되어 개발자가 자신의 환경에서 배포하고 사용가능litellm/litellm at main · BerriAI/litellm가격(1) 오픈 소스 (무료)(2) 엔터프라이즈 기본 (월 $250)(3) 엔터프라이즈 프리미엄 (월 $1000)라우터 기능로드 밸런싱 및 fallback: 여러 배포 간 로드 밸런싱과 요청 우선 처리기본 안정성 로직: 쿨다운, fallback, 타임아웃, Retry(fixed + exponential backoff)프로덕션 환경 지원: Redis를 사용해 쿨다운 서버, 사용량 추적, tpm/rpm 제한 관리Callback: API 호출에 사용된 키, 엔드포인트, 모델 등을 추적모델 캐싱: Azure와 OpenAI 같은 다른 모델 그룹 간 캐싱이벤트 알림: LLM API 예외, 느린 응답 등의 이벤트를 Slack/웹훅 URL로 알림비용 및 배포 추적: 배포 비용 추적, 라우터 배포 및 디버깅기타 기능커스텀 설정: API 키, API 베이스, 버전, 타입, 프로젝트, 위치, 토큰 등 사용자 지정 가능완료 토큰 사용: 모든 완료 요청에서 토큰 사용량 반환사용자 정의 가격 측정: SageMaker, Azure 등의 가격 모델 지원비동기 임베딩 함수: embedding.aembedding 비동기 임베딩 지원조정 엔드포인트: OpenAI의 조정 엔드포인트 지원Budget Manager예산 관리: LLM API 호출 시 예산 초과를 방지.글로벌 예산 설정: litellm.max_budget으로 최대 예산(USD) 설정, 초과 시 BudgetExceededError 발생.BudgetManager 클래스: 사용자별 예산 설정 및 비용 관리LiteLLM Proxy Server: OpenAI 호환 엔드포인트로 LLM 호출, 예산 관리, 지출 추적, 부하 분산캐싱캐시 초기화: 메모리, Redis, S3 버킷, 디스크 캐시 등 다양한 캐시 옵션캐시 제어: no-cache, no-store, ttl, s-maxage 등 캐시 설정 가능캐시 컨텍스트 관리자: 캐시 활성화, 비활성화 및 매개변수 업데이트사용자 정의 캐시: 필요에 따라 캐시 설정을 커스터마이즈통합 기능Langchain: ChatLiteLLM()을 통한 통합Instructor: Function calling 기능 지원 우리꺼에서 어떻게 사용하는지, 유사 기술과 스택과 비교LangChain: LangChain은 LLM을 활용한 애플리케이션 개발을 위한 프레임워크로, 다양한 LLM 제공자와의 통합을 지원한다.LangChain은 특히 자연어 처리(NLP) 파이프라인을 구성하고, LLM의 출력을 후처리(post-processing)하거나 다양한 모델 간의 조합을 가능하게 하는 도구들을 제공한다.Haystack: Haystack은 주로 검색 엔진과 NLP 애플리케이션을 구축하는 데 사용되는 오픈소스 프레임워크이다.다양한 LLM을 포함한 모델을 손쉽게 통합할 수 있으며, 문서 검색, 질의응답, 텍스트 생성 등 다양한 기능을 제공한다.Haystack은 Elasticsearch, OpenSearch, Hugging Face 모델과의 통합을 통해 다양한 데이터 소스와 LLM을 활용할 수 있다.Rasa: Rasa는 주로 대화형 AI, 챗봇을 구축하기 위한 오픈소스 프레임워크로, 다양한 NLP 모델과 통합이 가능한다.Rasa는 자연어 이해(NLU)와 대화 관리(Dialogue Management)를 처리하며, 사용자 정의 가능한 파이프라인을 통해 여러 LLM과의 연동을 지원한다.OpenAI’s API: OpenAI는 자사의 API를 통해 GPT 시리즈 모델을 제공하며, 이를 다른 애플리케이션과 통합할 수 있는 다양한 도구들을 제공한다.여러 언어 모델을 사용한 텍스트 생성, 자연어 이해, 번역 등의 작업을 API를 통해 쉽게 수행할 수 있다.Hugging Face’s Transformers: Hugging Face는 다양한 LLM 모델을 포함한 Transformers 라이브러리를 제공하며, 여러 NLP 작업에 사용할 수 있는 모델들을 쉽게 통합하고 관리할 수 있는 도구를 제공한다.이 플랫폼은 모델 허브(Model Hub)와 함께 제공되어, 다양한 LLM 모델을 탐색하고 활용할 수 있다. Langchain과의 비교LangChain과 LiteLLM은 모두 LLM(대형 언어 모델)을 효과적으로 통합하고 활용하는 것을 목표로 하는 도구이지만, 그 목적과 사용 사례에 있어 차이가 있다.목적 및 사용 사례LangChain: 주로 LLM을 기반으로 한 복잡한 NLP 파이프라인 및 애플리케이션을 구축하는 데 중점을 둔다.LangChain은 데이터 소스 통합, 문서 검색, 질의응답, 대화형 에이전트와 같은 고급 애플리케이션에 강점을 가지고 있다.특히, 여러 모듈과 LLM을 조합하여 복잡한 워크플로우를 생성하는 데 유용하다​LiteLLM: 여러 LLM 제공자 간의 API 통합을 단순화하는 데 초점을 맞추고 있다.LiteLLM은 다양한 LLM 모델을 하나의 표준화된 API로 호출할 수 있게 하여, 여러 모델 간의 전환을 용이하게 한다.비용 추적, 로드 밸런싱, 요청 최적화 등 여러 모델을 효율적으로 관리할 수 있는 기능을 제공하며, 다양한 LLM을 사용하는 프로젝트에서 일관된 인터페이스를 제공하는 것이 주요 목적이다​.기능 및 특징LangChain:모듈성: LangChain은 여러 모듈로 구성되어 있으며, 각 모듈을 자유롭게 조합할 수 있다. 이로 인해 복잡한 대화형 애플리케이션이나 데이터 파이프라인을 구축하는 데 적합하다.다양한 작업 지원: 텍스트 생성, 문서 검색, 데이터 클러스터링 등 다양한 NLP 작업을 지원한다.데이터 소스와의 통합: LangChain은 다양한 데이터 소스와 통합할 수 있으며, 이를 통해 LLM을 활용한 고급 검색 및 정보 처리 기능을 구현할 수 있다.LiteLLM:통합 인터페이스: 여러 LLM 제공자의 API를 하나의 표준화된 인터페이스로 통합하여, 사용자가 다양한 LLM을 쉽게 전환하고 사용할 수 있도록 힌다.비용 추적 및 관리: 각 모델 사용에 따른 비용을 추적하고 관리할 수 있는 기능을 제공하여, 예산 관리 및 비용 최적화를 돕는다.간소화된 사용: 복잡한 설정 없이 여러 LLM을 일관되게 호출할 수 있어, 빠르고 쉽게 프로젝트에 통합할 수 있다.사용 편의성LangChain: 다소 복잡한 설정과 구성을 요구할 수 있으며, 다양한 모듈과 기능을 잘 이해하고 조합해야 한다.따라서, 강력한 커스터마이징이 필요하거나 복잡한 워크플로우를 구축하려는 사용자의 요구에 더 적합하다.LiteLLM: 비교적 간단한 설정과 사용을 제공하며, 여러 LLM을 쉽게 통합하고 관리할 수 있는 단순한 인터페이스를 제공힌다. 여러 모델을 효율적으로 관리하고자 하는 사용자에게 적합히다​결론:LangChain은 복잡한 NLP 애플리케이션을 구축하려는 사용자에게 더 적합하며, 다양한 모듈과 데이터를 통합하는 데 강점을 가지고 있다.반면, LiteLLM은 여러 LLM 제공자를 사용하는 프로젝트에서 통합과 비용 관리를 단순화하려는 사용자에게 더 유용하다. LiteLLM Python SDKAWS 예시 조합(다른 기능 조합 가능) : Lambda + API Gateway + Sagemaker + Step FunctionGoogle Cloud 예시 조합 : Functions + API Gateway + Vertex AIAzure 예시 조합 : Functions + API Management + Azure ML기능 / 솔루션LiteLLMAWSGoogle CloudAzureOpenAI API + LangChain + Redis + Prometheus/Grafana로드 밸런싱 및 폴백다중 배포 간 로드 밸런싱 및 폴백 제공Lambda와 API Gateway로 자동 처리Cloud Functions와 API Gateway로 지원Functions와 API Management로 지원LangChain에서 다양한 모델에 대한 로드 밸런싱 및 폴백 처리 가능기본 안정성 로직 (쿨다운, 타임아웃, 재시도)지원 (쿨다운, 타임아웃, Retry)Lambda + Step Functions로 워크플로우 제어 가능Functions + API Gateway로 제어 가능Functions + API Management로 제어 가능LangChain 및 Custom Logic으로 구현 가능프로덕션 환경 지원Redis로 쿨다운 및 사용량 추적 관리Redis, CloudWatch, Step Functions로 지원Cloud Memorystore, Cloud Monitoring으로 지원Azure Cache, Azure Monitor로 지원Redis, Prometheus/Grafana로 쿨다운 및 사용량 추적 가능Callback 기능지원SNS, SQS, Step Functions를 통한 호출Pub/Sub, Cloud Functions로 지원Event Grid, Logic Apps로 지원Webhooks 또는 Custom Logic으로 구현 가능모델 캐싱Redis 등 다양한 옵션 지원ElastiCache (Redis) 및 S3 버킷을 통해 캐싱Cloud Memorystore (Redis), Cloud Storage로 캐싱Azure Cache for Redis, Blob Storage로 캐싱Redis로 캐싱 가능이벤트 알림Slack/웹훅 URL로 알림 지원SNS, CloudWatch, Step Functions, Lambda로 알림 설정 가능Cloud Monitoring, Pub/Sub으로 알림 설정 가능Azure Monitor, Logic Apps로 알림 설정 가능Prometheus Alerts를 통해 Slack, 이메일 등으로 알림 가능비용 및 배포 추적비용 추적 및 라우터 배포 지원AWS Cost Explorer, Budgets, CloudWatch로 비용 추적 및 관리Google Cloud Billing, Budgets로 비용 추적 및 관리Azure Cost Management로 비용 추적 및 관리Prometheus/Grafana로 비용 추적 및 모니터링 가능커스텀 설정 (API 키, 토큰 등)사용자 지정 가능AWS Parameter Store, Secrets Manager로 설정 관리Secret Manager, Config Connector로 설정 관리Azure Key Vault, Configurations로 설정 관리환경 변수 및 Custom Scripts로 설정 관리완료 토큰 사용기본 지원Lambda 및 Sagemaker에서 API 호출 시 사용Cloud Functions 및 Vertex AI에서 API 호출 시 사용Functions 및 Azure ML에서 API 호출 시 사용OpenAI API, LangChain에서 기본 지원사용자 정의 가격 측정다양한 클라우드 가격 모델 지원AWS Cost Explorer로 사용자 정의 비용 분석 가능Google Cloud Billing에서 사용자 정의 비용 분석 가능Azure Cost Management에서 사용자 정의 비용 분석 가능Custom Scripts, Prometheus/Grafana로 비용 측정 가능비동기 임베딩 함수지원Lambda + Sagemaker에서 비동기 임베딩 함수 구현 가능Cloud Functions + Vertex AI에서 비동기 임베딩 함수 구현 가능Functions + Azure ML에서 비동기 임베딩 함수 구현 가능LangChain + OpenAI API에서 비동기 임베딩 함수 구현 가능조정 엔드포인트OpenAI Moderation API 지원Sagemaker 또는 Comprehend로 조정 기능 구현 가능Vertex AI 또는 Cloud Natural Language API로 구현 가능Azure Cognitive Services로 구현 가능OpenAI Moderation API로 직접 지원Budget Manager예산 관리 및 비용 추적 기능 지원AWS Budgets 및 Cost Explorer로 예산 관리 가능Google Cloud Budgets 및 Billing으로 예산 관리 가능Azure Budgets 및 Cost Management로 예산 관리 가능Custom Scripts, Prometheus로 예산 관리 가능캐싱 초기화 및 제어메모리, Redis, S3 등 지원Redis, S3, ElastiCache로 캐싱 초기화 및 제어 가능Cloud Memorystore, Cloud Storage로 캐싱 초기화 및 제어 가능Azure Cache for Redis, Blob Storage로 캐싱 초기화 및 제어 가능Redis, Local Disk Caching 등으로 초기화 및 제어 가능통합 기능 (Langchain, Instructor 등)통합 지원AWS Lambda, Sagemaker, API Gateway로 통합 가능Google Cloud Functions, Vertex AI, API Gateway로 통합 가능Azure Functions, Azure ML, API Management로 통합 가능LangChain, OpenAI API, Hugging Face로 통합 가능LiteLLM Python SDK 장점:다양한 LLM API 통합 관리로드 밸런싱 및 쿨다운, 재시도 로직 지원모델 캐싱, 예산 관리, 이벤트 알림 기능 제공유연한 커스터마이징 옵션LiteLLM Python SDK 단점:특정 LLM 제공업체에 의존추가 설정 및 구성이 필요함제공업체별 기능에 대한 제한된 사용자 정의 참고자료Simplifying Multi-Model LLM Development: A Developer’s Guide to LiteLLM and DatabricksLiteLLM | Technology Radar | Thoughtworks United StatesProjects built on LiteLLM | liteLLMLiteLLM - Getting Started | liteLLM💥 LiteLLM Proxy Server (LLM Gateway) | liteLLMRouter - Load Balancing, Fallbacks | liteLLM","link":"/2024/08/27/LiteLLM/"},{"title":"로지스틱 회귀","text":"로지스틱 회귀는 선형 방정식을 사용한 분류 알고리즘이다. 시그모이드 함수나 소프트맥스 알고리즘을 사용하여 클래스 확률을 출력할 수 있다. 이 책에서는, 구성품을 모른채 먼저 구매할 수 있는 럭키백이 있다고 가정하고 럭키백을 열어봐야 구성품을 알 수 있다고 한다.럭키백에 들어간 생선의 크기, 무게 등 특성이 주어졌을 때, 어떤 생선인지에 대한 확률을 출력해야 한다. 이를 확인할 수 있는 로지스틱 회귀를 알아보고, 이진분류에 필요한 시그모이드 함수와 다중 분류에 필요한 소프트맥스 함수를 알아본다 [출처 : 혼자 공부하는 머신러닝+딥러닝 4장. 다양한 분류 알고리즘] 데이터 준비하기csv파일을 pandas로 읽어와 타깃 데이터, 입력 데이터로 나눈다. 12345678import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)print(pd.unique(fish['Species'])) # 열에서 고유한 값 출력#fish의 종류를 타깃 데이터, 나머지 특성을 입력 데이터fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy() 훈련 세트와 테스트세트로 나눈 후 표준점수로 전처리한다. 1234567891011from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler#훈련세트와 테스트세트로 나눠주기train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)#입력 데이터 전처리ss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) k-최근접 이웃 분류기로 확률 예측사이킷런의 KneighborsClassifier 클래스로 모델을 훈련한다. 123from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target) 12&gt;&gt;&gt; print(kn.classes_)['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 사이킷런에서는 문자열로 된 타깃값을 그대로 사용할 수 있지만, 순서가 자동으로 알파벳 순서로 매겨진다. 훈련된 모델로 테스트 세트의 5개 샘플의 종류를 예측한다. 12&gt;&gt;&gt; print(kn.predict(test_scaled[:5]))['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] pridict_proba() 메서드는 클래스별 확률값을 반환한다.Numpy의 round()는 반올림 함수이며 decimals 매개변수는 유지할 소수점 아래 자리를 지정할 수 있다. 123import numpy as np# 클래스별 확률값 반환proba = kn.predict_proba(test_scaled[:5]) 1234&gt;&gt;&gt; print(np.round(proba, decimals=4)) #소숫점 4자리까지 반올림해 반환[[0. 0. 1. 0. 0. 0. 0. ][0. 0. 0. 0. 0. 1. 0. ][0. 0. 0. 1. 0. 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ]] 4번째 샘플의 경우 Perch일 확률이 2/3, Roach일 확률이 1/3이다. 1234distances, indexes = kn.kneighbors(test_scaled[3:4])&gt;&gt;&gt; print(train_target[indexes])[['Roach' 'Perch' 'Perch']] 4번째 샘플의 이웃은 Perch가 2개, Roach가 1개로, 구한 확률이 맞음을 보여준다. 단, 이 방법은 3개의 최근접 이웃만을 사용하기에 확률은 0, 1/3, 2/3, 1 뿐이라는 한계가 있다. 로지스틱 회귀로지스틱 회귀는 이름은 회귀이지만 분류 모델이다.선형 회귀와 동일하게 선형 방정식을 학습한다.z = a x Weight + b x length + ··· + fa, b, c, d, e는 계수이며 z는 어떤 값도 될 수 있다. 하지만 확률로 표현하려면 0~1 사이의 값이 되어야 하기 때문에z가 아주 큰 음수일때 0이 되고, 아주 큰 양수일 때 1이 되도록 바꾼다.이는 시그모이드 함수를 사용하면 가능하다. 시그모이드 함수 넘파이를 이용해서 간단하게 그려본다. 1234567import matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z))plt.plot(z, phi)plt.xlabel('z')plt.ylabel('phi')plt.show() 이진 분류를 먼저 수행해 볼 것이다.이진 분류에서 시그모이드 출력이 0.5보다 크면 양성클래스, 작으면 음성클래스로 판단한다. 로지스틱 회귀로 이진분류 수행하기불리언 인덱싱으로 도미와 빙어 데이터를 골라낸다. 123bream_smelt_indexes = (train_target == &quot;Bream&quot;) | (train_target == &quot;Smelt&quot;)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 이 데이터로 로지스틱 회귀 모델을 훈련한다. 123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt) 훈련한 모델로, 5개의 테스트 샘플을 예측해본다. 12&gt;&gt;&gt; print(lr.predict(train_bream_smelt[:5]))['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 각각의 샘플 확률을 예측해본다. 1234&gt;&gt;&gt; print(lr.predict_proba(train_bream_smelt[:5]))[[0.99759855 0.00240145][0.02735183 0.97264817][0.99486072 0.00513928][0.98584202 0.01415798][0.99767269 0.00232731]] 로지스틱 회귀가 학습한 계수도 볼 수 있다. 12&gt;&gt;&gt; print(lr.coef_, lr.intercept_)[[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]][-2.16155132] 계수들과 절편을 볼 수 있다. 따라서 이 로지스틱 회귀 모델이 학습한 방정식은 다음과 같다.z = -0.404 x 무게 + -0.576 x 길이 + ··· + -2.161 z값과 시그노이드 함수의 값 또한 볼 수 있다. 12345decisions = lr.decision_function(train_bream_smelt[:5])print(decisions)from scipy.special import expitprint(expit(decisions)) 1[-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 로지스틱 회귀로 다중 분류 수행하기LogisticRegression 클래스는 반복적인 알고리즘을 사용하며, max_iter 매개변수에서 반복값을 지정하며 기본값은 100 이다. 또한 릿지 회귀와 같이 계수의 제곱을 규제하며, L2 규제라고도 불린다. 릿지회귀에서 alpha로 규제의 양을 조절한 것과 달리, C 매개변수로 조절한다. C의 기본값은 1이며 작을수록 규제가 커진다. 123456lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target))print(lr.predict(test_scaled[:5])) # 샘플 5개의 종류 예측 1230.93277310924369750.925['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 과대적합이나 과소적합이 되지 않았다. 5개 샘플에 대한 예측 샘플도 볼 수 있다. 123print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) 123456['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'][[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 클래스 정보와 클래스 예측 확률을 볼 수 있다. 다중 분류에서의 예측 확률은 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다. 소프트맥스 함수소프트 맥스 함수는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다. 수식 넣을 방법 필요…","link":"/2024/03/28/Logistic-Regression/"},{"title":"LangSmith란","text":"LangSmith란 LLM 애플리케이션을 디버깅, 테스트, 평가, 모니터링할 수 있는 개발자 플랫폼이다. 프로젝트나 LangChain 학습을 시작한다면, LangSmith를 설정 후 진행하는 것이 좋다고 한다. LangSmith는 굳이 분류하자면 LLMOps 도구이다.DevOps의 LLM 버전이라고 이해하면 된다.LLM 어플리케이션을 잘 만들고 운영하기 위한 도구이고, 아래와 같은 기능들을 지원한다. 현재 사용자가 어떻게 채팅을 하고 있는지 추적하고, 피드백을 수집한다. 비용은 얼마나 차징이 되고 있는지, 응답 시간은 얼마나 걸렸는지를 실시간으로 추적한다. 데이터셋을 구성하여 LLM 어플리케이션을 자동 평가하고, 개발하는 일들을 도와준다. 이러한 핵심 기능들을 토대로 LLMOps라고 분류할 수 있으나, 개발사인 랭체인에서는 “Developer Platform”이라고 칭하고 LLMOps라는 용어를 사용하지는 않는다. LangSmith는 기본적으로 LLM을 넘어 멀티모달리티를 지원하고, LLM 자체를 잘 만드는 것 보다 LLM을 잘 사용하는 방법에 더 중점을 두고 있어서, LLMOps라는 용어가 좋은 용어라고 보기는 어렵다고 생각한다. 1. 추적과 디버깅 랭스미스에서는 위와 같이 각 중간 과정의 in/out 결과를 보여준다.Retrieve가 잘 안되었는지, 올바른 근거 자료를 찾아 줬음에도 LLM이 대답을 못하고 있는지, 추론이 가능하다. LLM 모델을 바꾸거나, Prompt를 바꾸거나, 다양하게 모두 돌린 후 간단히 비교하기도 좋다.비용도 나오고, 수행시간도 나와서 어디서 비용이 많이 드는지, 너무 느리지 않는지 모두 확인 가능하다. 개선을 진행할수록 소수의 질문에서만 문제가 생기는데, 이를 체계화해서 로깅을 남기기보단, 그 시간을 단축시켜주는 도구로서 유용하다. 2. 데이터 수집과 평가LLM 어플리케이션의 알파이자 오메가는 사실 데이터셋이다.데이터셋을 구성하고 사용하기 좋게 잘 구성이 되어있다. 허깅페이스처럼 연구/개발자가 직접 데이터셋을 올릴 수 있다. 연동된 서비스의 유저 로그를 데이터셋으로 자동 수집 가능하다. 연동된 서비스의 유저 피드백을 같이 수집 가능하다. 자동으로 데이터를 가공해서 추가수집 가능하다. 챗 서비스를 서빙한다고 하면, 유저의 질문, 시스템의 답변, 유저의 만족도 조사결과까지 맞물려 데이터 수집이 되니 매우 유용하다.모아다가 강화학습을 할 수도 있고, A/B 테스트를 할 수도 있고, 불만족스러웠던 데이터만 따로 백테스팅을 할 수도 있고, 사용하기 나름이다. 어떤 방식으로든 모아진 데이터는 평가에 활용을 하기 좋게 구성되어 있다. 아래와 같이 평가 로직을 등록할 수 있는데, API Key와 함께 프롬프트를 써주면, LLM이 알아서 평가를 해준다.아래 그림은 답변이 도움이 되었는지를 수치화하는 평가로직인데, 프롬프트를 템플릿화해서 다 만들어놔서 편하다. 평가로직을 등록만 해두면, 서비스에서 발생하는 내 답변들을 자동으로 평가해 데이터화 해준다.. LLM 어플리케이션은 “평가”가 어려운 경우가 많다. 정석적이기 때문이다. 이 부분을 해결하기 위해 양질읠 데이터, LLM을 다시 평가 판사 (LLM as judge)로 사용하기, 외 기타 여러가지 방법이 있다.이를 잘 지원하는 도구로서 유용하다. 3. 협업도메인 전문가들이 데이터를 분석하고, 평가해서 데이터 라벨링을 해주고, 의견을 주고 해야하는데, 협업하기가 쉽지 않다. 데이터를 어떤 형태로 뽑아야 할지, 도메인 전문가들이 어떻게 라벨링을 달아서 시스템에 다시 올려야할지, 이 부분을 잘 도와준다. 데이터들을 Annotation queue에 넣어서 사람을 지정해 줄 수가 있다. 그러면 도메인전문가들이 데이터를 보고 정성평가를 달아주고, 데이터셋에 연동이 된다. 모두 웹 인터페이스로 사용이 가능해서, 개발이 익숙하지 않은 사람들과 협업이 매우 편하다. 4. 사용법4.1. LangSmith 설치1pip install -U langsmith 4.2. API Key 발급4.3. 환경 설정12345export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=&lt;your-api-key&gt;# The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=&lt;your-openai-api-key&gt; 4.4. trace를 기록하기1234567891011121314151617import openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str): result = client.chat.completions.create( messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}], model=&quot;gpt-3.5-turbo&quot; ) return result.choices[0].message.contentpipeline(&quot;Hello, world!&quot;)# Out: Hello there! How can I assist you today? 4.5. Evaluation 실행하기12345678910111213141516171819202122232425262728293031323334from langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = &quot;Sample Dataset&quot;dataset = client.create_dataset(dataset_name, description=&quot;A sample dataset in LangSmith.&quot;)client.create_examples( inputs=[ {&quot;postfix&quot;: &quot;to LangSmith&quot;}, {&quot;postfix&quot;: &quot;to Evaluations in LangSmith&quot;}, ], outputs=[ {&quot;output&quot;: &quot;Welcome to LangSmith&quot;}, {&quot;output&quot;: &quot;Welcome to Evaluations in LangSmith&quot;}, ], dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example): return {&quot;score&quot;: run.outputs[&quot;output&quot;] == example.outputs[&quot;output&quot;]}experiment_results = evaluate( lambda input: &quot;Welcome &quot; + input['postfix'], # Your AI system goes here data=dataset_name, # The data to predict and grade over evaluators=[exact_match], # The evaluators to score the results experiment_prefix=&quot;sample-experiment&quot;, # The name of the experiment metadata={ &quot;version&quot;: &quot;1.0.0&quot;, &quot;revision_id&quot;: &quot;beta&quot; },) AWS에 Langsmith를 배포하여 사용 가능한가?Langsmith-Pricing 위 링크에 의하면, Enterprise 플랜을 사용하는 경우 데이터가 환경을 벗어나지 않도록 AWS, GCP 또는 Azure의 K8S 클러스터에서 실행되도록 Langsmith 제공 가능하다. 셀프호스팅 Architecture Overview 랭스미스는 사용자가 제어하는 클라우드 환경에서 Kubernetes(권장) 또는 Docker를 통해 실행할 수 있다. LangSmith 애플리케이션은 5개의 LangSmith 서버와 3개의 상태 저장 서비스로 구성된다. 랭스미스 프런트엔드 랭스미스 백엔드 랭스미스 플랫폼 백엔드 LangSmith Playground LangSmith Queue clickhouse db postgres redis 각각에 대한 설명은 Architectural overview를 참고 1. Kubernetes에 설치하기Self-hosting LangSmith on Kubernetes 2. Docker에 설치하기Self-hosting LangSmith with Docker Docker 설치, 최소 4개의 vCPU, 16GB 메모리, 충분한 디스크공간 필요 LangSmith 라이센스 키 필요 Docker compose를 사용하여 실행, 프로덕션에서는 k8s 권장 docker-compose.yaml 배포 검증12curl localhost:1980/info{&quot;version&quot;:&quot;0.5.7&quot;,&quot;license_expiration_time&quot;:&quot;2033-05-20T20:08:06&quot;,&quot;batch_ingest_config&quot;:{&quot;scale_up_qsize_trigger&quot;:1000,&quot;scale_up_nthreads_limit&quot;:16,&quot;scale_down_nempty_trigger&quot;:4,&quot;size_limit&quot;:100,&quot;size_limit_bytes&quot;:20971520}} 장단점 사용하기 매우 편하다, 문서도 잘 되어있고, 코드 구현도 쉽다. LangChain과 함께 사용하면 더 쉽고, 잘 연동 된다. 오픈소스가 아니라서 상업 수준으로 사용하려면, 비용을 내야한다. ReferencesLangSmith Quick StartLangSmith,사용후기","link":"/2024/09/25/LangSmith/"},{"title":"개체명 인식(Named Entity Recognition)","text":"NER(Named Entity Recognition)은 자연어 처리(Natural Language Processing, NLP)의 한 영역으로, 텍스트에서 사람, 조직, 위치, 날짜, 시간, 통화, 비율과 같은 명명된 엔티티(명사)를 식별하고 분류하는 기술이다. NER 시스템은 주어진 문서에서 중요한 정보를 추출하는 데 사용되며, 정보 검색, 질의 응답 시스템, 콘텐츠 요약, 고객 지원 시스템, 그리고 감정 분석 등 다양한 NLP 응용 프로그램에 활용된다. NER의 주요 기능: 엔티티 식별: 텍스트 데이터에서 사람의 이름, 기업 이름, 지리적 위치 등과 같은 엔티티를 식별합니다. 엔티티 분류: 식별된 엔티티를 사전 정의된 카테고리(예: 인물, 조직, 위치 등)로 분류합니다. 관계 추출: 텍스트 내에서 엔티티 간의 관계를 파악하고 추출합니다. 이는 텍스트 내에서 엔티티 간의 상호 작용을 이해하는 데 도움이 됩니다. NER의 활용 사례: 정보 추출: 뉴스 기사, 소셜 미디어 포스트, 문서에서 중요한 정보를 추출하여 구조화된 데이터로 변환합니다. 문서 분류: 엔티티 정보를 기반으로 문서를 자동으로 분류하고 정리합니다. 지능형 검색 엔진: 특정 엔티티에 관한 정보를 효율적으로 검색하기 위해 사용됩니다. 챗봇과 가상 비서: 사용자 질문에서 중요한 엔티티를 식별하여 보다 정확한 답변을 제공합니다. 감성 분석: 제품, 서비스, 브랜드에 대한 특정 엔티티의 언급을 분석하여 고객의 의견과 태도를 이해합니다.NER 기술의 발전은 딥러닝과 인공 신경망 모델의 진보 덕분에 크게 향상되었습니다. 이러한 모델들은 맥락과 의미를 더 잘 이해할 수 있게 해주며, NER의 정확도와 효율성을 높여줍니다.","link":"/2024/04/03/NER/"},{"title":"Microsoft Build 2024","text":"OpenAISLMsHuggingFacePhi-3 키노트 주요 영상 1 - ChatGPT 4o를 사용한 실시간 AICCChatGPT 4o와 대화를 함. 신발도 보여줌. 구매 내역이나 보여준 신발에 대하여 여러가지 데이터를 조회해서 답변을 함. 영어 - 스페인어 전환 시 chat GPT도 언어 전환 키노트 주요 영상 2 - Copilot + PC의 On-device 멀티 모달 기능Copliot PC는 인터넷이 끊겨도 작동 가능한 On-device 마인크래프트 게임 중 Copilot에게 게임 방법 물어보고, 가이드 받으며 게임 진행. 키노트 주요 영상 3 - Sovereign Cloud, 각국 AI Cloud Center 투자Multiple LLM + AI Eco.ND MI300X V5NPU에 대한 네트워크 위에 있는 소프트웨어 스펙과 칩에 대한 설명브라우저에서 llm이 돌아가는 기술에 대한 설명 Copilot+PC : 온디바이스에서 어떻게 돌아가는지 Azure AI model breadthCopilot StudioMicrosoft Copilot, Copilot + PC, Copilot stack 추후 공개될 기능들. AI Studio 내 ChatGPT-4o Open, ChatGPT 4 Fine Tuning 지원 등 ChatGPT 4o Open Phi-3-Vision 128K instruct Open Microsoft CopilotTeams의 Copilot에게 질문함 내 보스의 최근 활동 follow up 해줘 Copilot stack 산업, B2B, 특정기업 특화 Copilot을 위한 Data / API 연동 방법들 Declarative Copilots context를 이해함 Custom Copilot을 어떻게 만드는가 custom knowledge 추가 (confluence 등) 운영계 DB등을 등록 특정 agent 만든다고 할때, 기 등록된 knowledge 중 필요한 것 선택 action 선택 (결제, 팀즈 알람 등) deploy to channel Add Trigger (어떤 이벤트 발생 시 먼저 말을 거는 것도 가능) Agent Building, 프롬프트 엔지니어링 Prompt &amp; Agent, Rule base 분기 RAG, Image + Policy 연동 분기 지원 Multiple Agent RAG, Agent, FunctionCall 에이전트를 만드는 노코드 툴 AutoGen Document AI &amp; RAG 표 인식 가능 문서를 OCR로 가져올 수 있음 필기체도 가져올 수 있음 AI Security SovereigntyLLM에서의 새로운 보안 위협Prompt shields prompt define시 앞뒤로 define 하는 것 jailbreak 공격 block 및 모니터링 Purview / Defender DRM 걸린 파일들 권한 관리 등등 어떤식으로 핸들링하는지에 대한 SDK File SDK 엑셀파일 특정 칼럼 암호화 등 Respoinsible AI - AI Red Teaming Code level Github 주소 있음 Input Filter Output Filter Azure AI Content Safety Groundeness detection Evaluation &amp; Monitoring Data Monitoringm Incidents monitoring [결론]Apple / Microsoft의 AI 전략에서 바라보는 시사점 Multiple LLM으로 근본적인 안정성 보장 Server Big LLM과 Task sLM의 조화 직원용 PC, Kiosk, ATM, Edge Device … 주지할 것들 : 바깥 세상의 기술의 속도에 발 맞추어, 홀로 경쟁이 아닌, 그들과 발 맞추고 올라타야 하는 시점opensource 활용, partner Eco 활용 자체 AI Product + Total AI MSP","link":"/2024/07/14/Microsoft-Build/"},{"title":"크루스칼 알고리즘","text":"크루스칼 알고리즘에 대해 공부해보겠습니다.크루스칼 알고리즘은 가장 적은 비용으로 모든 노드를 연결하기 위해 사용하는 알고리즘입니다.즉 MST를 만들기 위한 대표적인 알고리즘 입니다. 언택트 시대를 맞아 저희 집의 MST를 구해보겠습니다. 위 그래프를 보면 노드의 갯수는 5개 이고, 간선의 갯수는 6개 입니다. 크루스칼 알고리즘의 핵심 개념은 간선의 거리가 짧은 순서대로 그래프에 포함 시키는 것 입니다. Greedy한 방법을 이용, 네트워크(가중치를 간선에 할당한 그래프)의 모든 정점을 최소 비용으로 연결하는 최적 해답을 구하기 MST (최소 비용 신장 트리) 최소 비용 간선으로 구성 사이클을 포함하지 않음 Kruskal 알고리즘의 동작 그래프의 간선들을 가중치의 오름차순으로 정렬한다. 정렬된 간선 리스트에서 순서대로 사이클을 형성하지 않는 간선을 선택한다 즉, 가장 낮은 가중치를 먼저 선택한다 사이클을 형성하는 간선을 제외한다. 해당 간선을 현재의 MST(최소 비용 신장 트리)의 집합에 추가한다. Kruskal 알고리즘을 이용하여 MST를 만드는 구체적인 과정은 아래와 같다 간선 선택 기반 알고리즘 이전 단계에서 만들어진 신장 트리와 상관 없이 무조건 최소 간선만 선택 밥먹거나 소파에 있다가 책상으로 오기전에 침대를 거치는 것이 효율적이군요 주의 해야할 점다음 간선을 이미 선택딘 간선들의 집합에 추가할 때 사이클을 생성하는지 체크사이클 생성 여부를 확인하는 방법 추가하고자 하는 간선의 양끝 정점이 같은 집합에 속해 있는지를 먼저 검사. ‘union-find 알고리즘’이용 Kruskal 알고리즘의 시간 복잡도 union-find 알고리즘을 이용하면 Kruskal 알고리즘의 시간 복잡도는 간선을 정렬하는 시간에 좌우 됨. 즉, 간선 e개를 퀵 정렬과 같은 효율적인 알고리즘으로 정렬 시간 복잡도는 O(elog2e) Prim 알고리즘의 시간 복잡도는 O(n^2) 그래프 내에 적은 숫자의 간선만을 가지는 Sparse Graph의 경우 kruskal이 적합하고 간선이 많이 존재하는 Dense Graph의 경우 Prim 알고리즘이 적합 연관 개념 최소 신장 트리 (MST) Prim 알고리즘 그래프 (Graph) 트리 (Tree)","link":"/2020/09/08/MST/"},{"title":"(프로젝트)EMG 데이터를 이용한 아바타","text":"이 프로젝트는 한양대학교 Conelab에서 학부생 프로그래머로써 개발한 프로젝트입니다.EMG 데이터와 optitrack 데이터를 이용하여 기계학습을 시킨 후, EMG 신호만으로 사용자의 표정을 실시간으로 따라하는 아바타를 만들었습니다.개발 기간은 2018년 12월 ~ 2020년 2월 입니다.Maya를 개발환경으로 python을 이용하여 만들었습니다. Marker_based_Facial_Mocap간략한 소개Optitrack으로 얻은 얼굴 움직임 데이터 (csv파일)을 이용하여 Maya 2018에서의 아바타에 적용.실시간으로도 구동 가능. 페이스 트래킹 마커 파일 설명include 폴더 FaceRig.ma 입력되는 데이터를 실제 표정으로 보여줄 아바타 Maya 2018ver 기준으로 Face Rig 됨. 최 상단의 사진이 이 아바타의 모습. csv_file.csv 실제 아바타 적용을 위한 가공된 csv파일 21개 마커 각각의 라벨 제거 프레임 넘버 제거 Calibration Ground Plane_01.csv Optitrak에서 얻은 raw한 csv파일 21개의 마커를 이용 하나의 마커는 각각 x,y,z 좌표값을 포함한다. marker_tag.txt 21개 마커의 순서를 적어놓은 파일 src 폴더 expTracker_python_Client.py 실시간 마커 트래킹 시 사용 Socket 통신을 위한 Client side 프로세스 샘플 데이터 expTracker_python_Clinet_Read_Csv.py csv파일을 이용하여 마커 트래킹 시 사용 Socket 통신을 위한 Client side 프로세스 csv_file.csv파일을 로드하여 사용 expTracker_python_server.py 실시간 및 csv파일 이용한 마커 트래킹 시 사용 Socket 통신을 위한 Server side 프로세스 포트넘버 7777사용, 이 포트를 다른 프로세스가 사용 시 포트 넘버 바꿀 것 Perseus_Marker.py 실시간 및 csv파일 이용한 마커 트래킹 시 사용 Maya 프로그램 내에서 포트를 열어 데이터를 수신 수신 된 데이터를 이용하여 아바타의 마커를 움직임 포트넘버 7777사용, 이 포트를 다른 프로세스가 사용 시 포트 넘버 바꿀 것 setting.py Optitrak의 영점이 맞는지 확인하기 위한 코드 Maya 프로그램 내에서 포트를 열어 데이터를 수신 수신 된 데이터를 이용하여 아바타의 마커를 움직임 포트넘버 7777사용, 이 포트를 다른 프로세스가 사용 시 포트 넘버 바꿀 것 프로그램 구동 순서 Maya 2018을 실행하여 open scene에서 include 폴더의 FaceRig.ma를 실행 Maya의 script Editor에서 src폴더의 Perseus_Parker.py를 실행 Start Real Time Expression 버튼 클릭 (포트 열고 데이터 받을 준비) src폴더의 expTracker_python_server.py를 실행 (idle이 pycharm보다 빠름) src폴더의 expTracker_python_Client_Read_Csv.py(csv파일로 트레킹) 또는 expTracker_python_Client.py(실시간) 실행 아바타의 표정을 감상 주의사항 Optitrak과 Maya의 좌표계가 다르다","link":"/2020/03/23/Marker-based-Avatar/"},{"title":"Pandas와 Matplotlib로  데이터 시각화 하기","text":"Matplotlib은 파이썬에서 데이터를 다양한 형태로 시각화해주는 라이브러리 입니다. Pandas를 이용하여 데이터를 수집하고 분석해 보겠습니다. COVID-19 확진자 수와 기타 데이터를 bar 그래프로 그려주는 코드를 사용해보겠습니다. 123456789101112131415161718192021222324252627282930313233343536373839404142import pandas as pdimport urllib.request as urllibimport matplotlib.pyplot as plturl = r'https://www.worldometers.info/coronavirus/'# urllib.request로 오프너 생성opener = urllib.build_opener()# User-Agent 값을 요청 헤더에 포함opener.addheaders = [('User-Agent', 'Mozilla/5.0')]# 크롤링 수행response = opener.open(url)# 크롤링한 첫 번째 테이블을 table 변수에 저장table = pd.read_html(response.read())[0]# Top 20개 국가만 출력하기 위해 head값을 21로 지정table = table.head(21)# 특정 칼럼값만을 조회하기 위해 데이터 프레임 설정df = pd.DataFrame(table, columns=['Country,Other', 'TotalCases', 'TotalDeaths', 'ActiveCases', 'TotalRecovered', 'Serious,Critical'])# 첫 번쨰와 두 번째 래코드값은 값이 너무 커 그래프를 왜곡하므로, 여기에서 버려야 함.df = df.drop(8)df = df.drop(1)# 인덱스는 국가로 설정한다. 인덱스는 그래프에서 x축 값으로 사용.df = df.set_index('Country,Other')# 데이터 프레임을 출력해서 확인하자print(df)# 격자를 넣고, X/Y 축의 칼럼명, 그래프 이름을 설정bar = df.plot.bar(grid = True)bar.set_xlabel(&quot;Country&quot;)bar.set_ylabel(&quot;Number&quot;)bar.set_title(&quot;COVID-19 Information per Country&quot;)# 준비되었다면 그래프를 그려보자plt.show() 실행 결과는 다음과 같습니다. “Country, Other”를 인덱스로 설정했기 때문에 기본 인덱스인 숫자 0,1,2… 대신 국가 이름을 사용합니다. 그래프는 위와 같이 나옵니다.격자가 추가된 그래프에는 그래프, x축, y축 이름이 각각 설정되었고, 데이터 프레임에 저장된 칼럼 중에 국가 정보는 인덱스로 설정되어 x축으로, 기타 값들은 y축으로 설정되어 막대그래프로 표현되었습니다.막대그래프에 커서를 대면 숫자를 읽을 수 있습니다. 이 코드는 이 포스트를 참고하여 작성하였습니다.","link":"/2020/09/24/Pandas/"},{"title":"Netflix-No-Rules-Rules","text":"이 글은 넷플릭스 창업주이자 CEO인 리드 헤이스팅스의 인터뷰가 담긴 책, &lt;규칙없음:넷플릭스, 지구상 가장 빠르고 유연한 기업의비밀&gt;의 요약 내용이다. 넷플릭스의 철학은 절차보다 사람을 소중히 여기고, 능률보다 혁신을 강조, 통제를 최대한 자제 인재밀도 (talent density)를 중시 Culture Deck : 넷플릭스 사내용 127개 슬라이드 - 정직성 중시적당한 성과를 내는 직원은 두둑한 퇴직금을 주어 해고하고, 새로운 스타를 맞이한다. 회사분위기가 편안하고 안전하게 바뀔수록 혁신은 더욱 활발해진다. 손실 회피(loss aversion) : 인간은 새로운 어떤 것을 얻으려는 욕구 이상으로 이미 소유하고 있는 것을 잃지 않으려고 애쓴다. 휴가 기회가 사라질 수 있으니 무슨 수를 써서든 기회를 잃지 않으려 휴가를 사용 애초에 정해진 휴가 기간이 없다면 휴가를 잃을 걱정이 없으므로 휴가를 아예 쓰지 않음 인재 밀도를 구축 일반적인 회사들이 규정과 통제 절차를 마련하는 이유는, 일 처리가 미숙하고 프로답지 못하거나 무책임한 직원들을 다루기 위함. 애초에 이런 사람들을 채용하지 않거나 내보낸다면 그런 규정은 필요 없음 인재 밀도가 높을수록 지원들에게 허용되는 자유는 더욱 커짐 솔직성을 키우기 재능 있는 직원들은 서로에게서 많은 것을 배움 재능 있는 직원들이 피드백을 습관처럼 서로 주고받게 되면 일을 더 잘하게 되고 동시에 서로 책임질 수 있는 행동을 하게 되어, 통제는 크게 필요하지 않게 됨. 통제를 줄여라 출장 규정, 지출 규정, 휴가 규정 등은 없앨 수 있는 것들 몇 가지 가이드라인만 주면 됨. 가이드라인 : 매니저에게는 ‘통제가 아닌 맥락으로 이끌 것’, 평사원에게는 ‘상사의 비위를 맞추려 들지 말 것’ 재능 있는 사람들은 서로 능률을 높인다.팀에 평범한 사람이 1~2명 섞여 있으면 팀 전체의 성과가 떨어짐 그룹 토의의 질을 떨어뜨려 팀 전반적 IQ를 낮춤 사람들이 싫어할 일을 하게 만들어 능률 떨어뜨림 남보다 탁월한 능력을 발휘하고 싶은 직원을 회사에서 나가게 만듬 평범한 사람도 받아준다는 사실을 보여줌으로써 문제를 복잡하게 만듬 -&gt; 빠르고 혁신적인 직장은 소위 말하는 ‘비범한 동료들’로 구성된다. 높은 성과 + 사심 없는 솔직함 = 대단히 높은 성과솔직한 문화가 조성되면 일을 잘하는 사람을 더욱 탁월한 인재로 만들 수 있다. 솔직한 피드백이 잦아지면 팀과 회사의 업무 속도와 능률이 기하급수적으로 증가한다. 4A 피드백 지침피드백을 줄 때 Aim to Assist (도움을 주겠다는 생각으로 하라) Actionable (실질적인 조치를 포함하라) 피드백을 받을 때 Appreciate(감사하라) Accept or Discard(받아들이거나 거부하라) 똑똑한 왕재수는 사절솔직한 문화라고 해서 다른 사람에게 미치는 영향을 고려하지 않고 아무렇게나 말해도 좋다는 뜻은 아니다.오히려 솔직하게 말하려면 4A 피드백 지침을 소중히 지켜야 한다.상대방의 기분도 헤아려야 한다. 그러려면 피드백을 주기 전에 꼼꼼히 따져보고 준비도 단단히 해야할 뿐 아니라, 책임을 맡은 사람들로부터 모니터링과 코치도 받아야 한다. 똑똑한 왕재수가 되지 않으려면 자신에게 물어라 회사 분위기를 안 좋게 만들 생각인가? 좀 더 품위있는 방법으로 문제를 처리할 수는 없는가? 1명의 슈퍼스타가 2명의 평범한 직원보다 낫다 항상 회사의 이익이 되는 방향으로 행동한다. 다른 사람의 목표 달성을 어렵게 하는 행위는 하지 않는다. 자신의 목표를 성취하기 위해 최선을 다한다. 이를 지킨다면 휴가기간을 정하는 문제는 각자 하고 싶은대로 해도 된다고 한다. 책임질 자유넷플릭스는 성과를 많이 내는 직원들이 스스로 생활을 통제할 수 있게 하는 한 가지 방법을 알아냈다.그리고 그런 통제가 오히려 모두를 더 자유롭게 해준다는 사실도 확인했다.인재 밀도가 높았기에, 직원들은 이미 양심과 책임 의식을 가지고 행동하고 있었다.솔직한 문화가 정착되었기에, 누군가가 제도를 역이용하거나 주어진 자유를 남용하기라도 하면, 주변 사람이 이를 지적해 상황을 바로 잡았다. 그와 동시에 직원들의 주인의식이 한층 높아졌다는 걸 피부로 느끼게 되었다고 한다.직원들에게 자유를 주면, 회사 일을 자기 일처럼 여기게 되어 더욱더 책임 있게 행동한다.자유는 책임의 대립 개념이 아니다. 오히려 자유는 책임을 향해가는 통로이다. 전 단계 맥락 (Context at the front end)선택이","link":"/2024/08/10/Netflix-No-Rules-Rules/"},{"title":"Route53을 이용해 도메인과 서버(EC2) 연결하기","text":"AWS Route53을 이용하여 웹 애플리케이션에 사용할 새 도메인 이름을 등록하고, DNS를 통해 해당 도메인 이름을 ALB가 제공하는 실행중인 웹 애플리케이션에 연결해보겠다. Route53에 접속하여, Dashboard에 등록하여","link":"/2024/07/30/Register-Domain-with-Route53/"},{"title":"SK AI SUMMIT 2024 후기","text":"2024년 Coex에서 열린 SK AI SUMMIT 1일차 세션들을 듣고 후기를 남긴다. ON device AI @에이닷 전화 강연 목적 및 핵심 주제 온디바이스 AI의 개념과 구현 사례를 통해 개인정보 보호 및 비용 절감을 위해 STT 서버를 온디바이스에서 처리하는 방법을 소개 핵심 내용 온디바이스 STT 서버 처리: 음성 파일 전송 없이 단말에서 음성 인식을 처리하여 개인정보 보호 및 서버 연산 비용 절감을 도모. 기존 서버 트래픽은 하루 약 천만 콜, 피크 시간대에는 한 시간당 120만 콜에 이르며, 이를 AWS에서 처리하면서 GPU 비용 문제가 발생 iOS와 Android의 차이: iOS는 2채널 통화 녹음을 지원해 모델 연산이 두 배로 필요하지만, Android는 화자 구별 작업이 추가로 필요 모델 학습 및 경량화 과정 Scratch Training → Knowledge Distillation → TensorFlow 모델 생성 → TF Lite &amp; 양자화 → Beamsearch → On-Device ASR의 단계로 최적화 모델 크기를 130MB로 줄이면서도 성능을 유지하며, Transformer에서 시작해 Conformer 및 Branchformer로 발전해 디코더 레이어를 줄여 속도를 약 20% 개선 지식 증류: 부모 모델의 특정 레이어를 선택하여 적용하며 선택 방식에 따라 성능 차이 발생 모델 성능 및 특성: 최종 모델은 0.15B, 133MB 크기로 서버용 LAS 구조와 유사하지만, On-device용으로 조정된 구조로서 상용 인식기보다 높은 인식 성능을 보임 장점 및 단점 온디바이스 모델 추론과 경량화 과정을 이해할 수 있음 데이터를 서버에 저장하지 않고 단말에 저장하기 때문에 개인 정보보호 문제 벗어날 수 있고 연산비용 절감가능 STT 관련 용어가 많아 초보자에게는 어려울 수 있음 주요 인사이트 온디바이스 모델로 개인정보 보호와 비용 절감이 가능 모델을 클라이언트에 탑재함으로써 서버 유지비용 감소할 것으로 보임 iOS와 Android 간의 처리 차이를 고려해야 함 On-Device 국방 경계 솔루션 혁신 강연 목적 및 핵심 주제 국방 도메인에서 실시간 비디오 온디바이스 처리 및 MLOps 파이프라인을 통한 국방 경계 솔루션의 혁신 사례 핵심 내용 On-Device AI 제약사항 : Transformer는 못 쓰고, YOLO, Resnet18로 사용해야 한다. 국방 경계이기 때문에 사람 놓치면 큰일남, 탐지 정확도 높아야 한다. 하지만 짙은 안개, 수풀 흔들림, 동물 등 오탐지 많음 전략 DET Inference Time Augmentation 입력해상도 1920 * 1080 Large Model로 딱 한번 추론하기 vs 입력해상도 640 * 640 Medium Model 분할 배치 추론하기 관심 영역(이전 프레임의 결과, 모션 등) 재추론 등 정책 결정, 경계 부분의 검출 고려한 Customized NMS 적용 필요 DET + CLS Model Ensemble Detector 모델로 전체 Recall을 올리고, Classification으로 전체 오탐지 수 줄인다. 학습 데이터를 더 많이 구하고, 만든다 국방 경계지역은 군사 보안구역이기 때문에, 학습데이터 구하기 힘듬 직접 촬영 + 생성 모델로 합성 데이터 만들기 똑똑한 과외 선생님 도움아래 못난 제자 없다. 똑똑한 과외 선생님 (Knowledge Distillation : Classification 성능 개선) 사용 학습 데이터 취득의 양과 질 확보가 어렵고, 가벼운 모델을 학습하는 경우에 매우 효과적 모든 Classification 모델과 다양한 국방 Test DB에서 정확도 개선효과 검증 성능 최적화 H/W 디코더가 S/W 디코더 대비 2.6배 빠르게 처리하며, Knowledge Distillation을 활용해 경량 모델의 성능을 개선하여 최종 93% 정확도를 달성. 장점 및 단점 Data Transfer Latency가 최소화 됨 도서 산간 어디든지 설치 가능한 솔루션 이미지 처리 관련 분야로 기술 도입 난이도가 높고, 세부 정보 부족 주요 인사이트 온디바이스 이미지 처리 기술이 국방 및 보안 분야에서 데이터 보안 및 실시간 처리가 가능함을 확인 향후 국방 VLM을 접목하여 긴급 상황에 자동 공지 시스템 개발 가능성을 제시 하드웨어를 이해하는 AI 모델 최적화 기술과 그 성공 사례 강연 목적 및 핵심 주제 NotaAI의 AI 모델 최적화 기술과 하드웨어 명령어 최적화를 통한 성능 개선 사례를 설명 핵심 내용 NotaAI는 LLM, GenAI, Computer Vision 등 AI 기술과 CPU, GPU, NPU, Memory 등 Semiconductor 간의 최적화를 돕는다. 칩셋 맞춤형 최적화 필요성 : HW는 2년에 2배정도 발전을 이루는데, AI Model Size는 2년에 410배 증가 추세 성능 향상 사례: 최신 모델이라도 특정 칩셋의 명령어가 최적화되지 않으면 성능 저하가 발생할 수 있으며, 이를 하드웨어 명령어 최적화를 통해 성능 저하 문제를 해결 모델 성능 향상 요소: 신경망 구조, 데이터셋, 하드웨어에 따라 성능이 달라지며, 하드웨어에 맞춘 최적화로 효율을 극대화할 수 있음 장점 및 단점 AI Model 최적화를 통해 한정된 HW 자원을 효율적으로 사용 가능 하드웨어 명령어 최적화의 중요성을 인식할 수 있음 기술적 설명보다 회사 소개에 다소 치중된 내용 주요 인사이트 HW 개발 속도가 AI Model 사이즈 증가속도를 따라가지 못하기에 모델 최적화는 중요해보임 AI 모델을 특정 하드웨어에 맞춰 최적화하면 성능이 향상될 수 있으며, 실무에서 다양한 칩셋에 맞춘 최적화가 필요함을 확인 어디 갈까? TMAP 데이터로 장소 추천 받기! 강연 목적 및 핵심 주제 티맵 주행 정보를 활용한 장소 추천 서비스. 핵심 내용 주행 목적지 데이터를 바탕으로 다양한 주행 패턴 중 여행 세션을 정의하고, 주행 데이터를 효과적으로 모델링하기 위해 활용한 그래프 구조 기반의 딥러닝 방식 소개 딥러닝한 모델을 실제 서비스에 적용하기 위한 MLOps 파이브라인 설명 추천의 개인화 및 고도화를 위한 LLM 적용 추천 서비스 방향성 여행이라는 패턴은 하루 내에 여행 관광과 관련된 장소를 3곳 이상 들렀고, 이 중 하나는 숙소에 들렀던 목적지의 시퀀스로 정의 Graph 구조를 활용한 딥러닝 - 모델링 주안점 시간적, 공간적 Locality를 위한 Graph 기반의 모델 사용 시간에 대해 고려하기 위해서 시간 임베딩을 추가 고차 연관성 학습을 위한 Multi-granularity Graph 사용 Popularity bias 완화를 위한 L2-norm 적용 시간대 별 이동거리를 고려한 장소 추천 이른 시간일수록 더 먼 장소 추천 시간대별 적합한 카테고리의 장소 추천 점심시간대에는 식당을 추천 저녁 시간대에는 호텔이나 캠핑장 추천 Light-weight 한 모델이 잘 working 한 이유 충분한 티맵의 데이터 그래프 모델의 Locality 학습 우위 빠른 서비스 배포를 위한 MLOps NEUF-CLI (Custom SDK) 사용 Jenkins 파이프라인 활용 장점 및 단점 시간적, 공간적 Locality를 고려하여 Graph 기반의 모델을 사용하였기에 추천의 퀄리티가 좋아보였음. 가벼운 모델을 사용했지만 데이터가 충분히 방대하여 괜찮은 추천 결과를 나타내 줌 아직 개인의 취향에 따라 추천해주는 부분은 없어서 아쉬움 주요 인사이트 검색한 POI를 기준으로 하면, 검색만 하고 실제로 해당 위치에 가지 않을수도 있기에, 실제로 이동하여 해당 위치에 간 경우만 데이터로 활용한 점이 효과적이라고 생각. 데이터 전처리 및 분석이 중요한 요소임을 확인 POI 검색 기준으로 실제 이동 데이터만 활용한 것이 효과적 장소를 추천하기 위해 모든 주행 데이터를 사용하는 것은 다양한 패턴이 존재하여 모델에 혼선을 유발하므로 조정해야 함 소규모 모델도 충분한 데이터를 활용해 높은 성능을 낼 수 있음 우리집 TV에도 AI가?! Btv 미디어 Agent 강연 목적 및 핵심 주제 AI가 접목된 TV의 개인화된 미디어 환경 핵심 내용 LLM 기반 검색 개선: 기존 키워드 기반 검색에서 LLM 기반 검색으로 개선 에이닷을 접목하여 미디어에 관련된 정보를 검색 키워드만 추출하는 NUGU SDK와 다르게 맥락을 추출하여 반영하고 검색 지식 그래프를 활용하여 컨텐츠간의 유사도 파악 임베딩 모델 어떤것을 사용했는지는 공개 불가 장점 및 단점 개인의 취향과 이력 패턴은 따로 저장하고 있지 않고, 사용할 때 취향과 맥락이 잘 반영되도록 입력해야 함 단순히 키워드를 추출하는 방식보다는 맥락이 반영되어 검색의 만족도가 높음 LLM 사용 이유와 구체적인 성능 설명 부족 주요 인사이트 개인화 이력 패턴은 개인정보 문제로 반영되지 못함 컨텐츠 메타정보는 컨텐츠 정보를 판매 회사에서 구매하고, 유튜브 댓글 등을 활용하여 임베딩을 통해 구성함 AI 여행 파트너, TGO: 일정부터 혜택까지 Agent가 설계하는 스마트 여행 강연 목적 및 핵심 주제 고객의 취향, 예산, 일정을 분석하여 최적의 여행 계획을 수립해주는 AI 여행파트너 TGO 핵심 내용 그래프 DB 기반 RAG를 활용하여 고객의 취향, 예산, 일정을 입력하면 데이터를 분석하여 최적의 여행 계획을 수립해 줌 MBTI도 입력하면 이를 반영하여 여행계획 수립 다중 에이전트 협력: 여러 에이전트가 협력하여 여행 일정을 설계하며 토큰 발생과 처리 시간 소요 문제를 인지 장점 및 단점 개인의 취향을 저장하고 있다가 이를 반영하지는 못하고, 분석 요청시 데이터를 입력해야 함 다중 에이전트 협력 구조를 이해할 수 있음 주요 인사이트 벡터 서치를 위한 데이터는 웹 크롤링을 통해 DB에 축적하며, POI 검색 고도화를 위한 웹서치 진행함 여러 에이전트의 응답시간은 여행 일정 수립에 중점을 두어 고려를 하지 않아 오래걸림","link":"/2024/11/05/SK-AI-SUMMIT-2024/"},{"title":"도쿄 여행을 위한 사전조사 1편 - 도쿄역, 긴자, 신바시, 롯폰기","text":"도쿄는 런던, 뉴욕과 함께 세계 3대 도시에 속한다고들 한다.또한 일본의 정치, 경제, 사회, 문화 등 모든 면에서 일본을 대표하는 최대 규모의 도시이다.과거와 현재가 공존하는 도쿄에서는 천년의 역사를 지닌 사찰부터 최신식 고층 빌딩까지 다채롭게 펼쳐지는 건축물들을 보는 즐거움이 있고 세련된 마천루 사이사이를 거닐거나 트렌디한 감성의 캐주얼한 골목에서 최신 유행 제품들을 둘러보는 재미도 있다고 한다. 매년 미쉐린 가이드 세스토랑이 가장 많이 선정되는 도시 중 하나로 저렴하면서도 맛있는 식당, 대를 이어 영업하는 노포부터 최고급 요리 전문점까지 아무리 까다로운 미식가라도 만족시킬 수 있는 다양한 옵션들이 존재하는 곳이다. 어쩌고 저쩌고 암튼 도쿄 여행에 대한 사전 조사를 시작하겠다. 첫번째로 도쿄역, 긴자, 신바시, 롯폰기에 대해 알아보겠다. 출처 : https://www.youtube.com/watch?v=ZzqN8lkNQ-I 도쿄 들어가기인천 - 나리타 소요시간은 약 2시간 20분이다.나리타에서 도쿄 도심까지는 최소 1시간이상 걸린다. 나리타에서는 케이세이 나리타 스카이 엑세스, 나리타 익스프레스, 스카이 라이너 등의 옵션이 있다.또한 최근 저가 리무진 버스도 노선을 확대하고 있다. 시내 교통시내 교통의 경우 이동하는 지역이 많은 여행계획을 가지고 있는 경우 도쿄 메트로와 은행잎 모양의 토에이선 등 13개 노선, 250여개 정류장을 무제한 이용할 수 있는 도쿄 메트로 패스 24/48/72 시간권이 가장 좋다고 한다.만약 핵심지역 몇 군데만 다닐것이다 하는 경우 도쿄, 시부야, 신주쿠 이케부쿠로, 아키하바라 등 핵심 관광명소를 지나는 JR 야카노테센을 이용하면 된다. JR 메트로 패스로는 이용이 불가한 점을 유념해야 한다. 마루노우치 니혼바시일왕이 현재 거주하는 도쿄 고쿄, 100년이 넘는 역사를 지닌 도쿄역이 위치한 마루노우치 지역일대는 관광지로서 엄청난 볼거리가 있다기보다는 도쿄의 심장과 같은 곳을 의미가 크다고 한다. 도쿄역을 중심으로 서쪽 마루노우치 방향 출구와 동쪽 야에스 방향 출구가 나뉘어 진다.마루토우치 방향으로 가면 고쿄(황거)를, 야에스 방향으로 가면 백화점거리인 주오도리와 니혼바시를 만날 수 있다.모두 도보로 이동할 수 있는 거리이다. 도쿄역 도쿄역 서편의 건물들 사이 마루노우치 광장은 생각보다 세련되고 예쁜 뷰를 자랑한다고 한다. 도쿄역 옆에 위치한 킷테 쇼핑몰 6층에서 조망 가능 중앙 우체국 건물을 리모델링하여 조성한 복합 쇼핑몰 구 우체국장실인 4층 레터룸 6층 킷테 가든 독특한 스팟 나카도오리 에비뉴마루노우치 광장에서 마루노우치 빌딩을 지나면 나카도오리 에비뉴가 나온다. 이곳은 마루노우치의 가로수길로 거리 양 옆의 초록 가로수길들과 다양한 브랜드 숍, 레스토랑들이 모여있다고 한다.천천히 산책하듯 걸으며 쇼핑하고 거리에서 커피 한 잔 즐기기 좋을것 같다. 고쿄내부 입장은 무료지만 반드시 투어를 통해 들어가야하고, 입장 인원 제한이 있다.그렇기에 인터넷으로 미리 예약하고 가는것이 권장된다. 일제강점기 우리나라 의사들의 의거지로도 유명한 니주바시 다리. 니주바시 다리와 이중교는 같은곳이다. 니혼바시 도쿄역에서 야에스 방면으로 나가면 다이마루 백화점을 시작으로 과거 에도시대에 상업의 중심지로 번창했던 니혼바시지역이 나온다. 이곳의 지명이 유래된 다리, 니혼바시를 기점으로 남북으로 뻗은거리를 주오거리라고 부른다.다이마루, 다카시마야, 미츠코시처럼 전통있는 박화점들이 줄지어 있다. 미츠코시 백화점은 외관도 멋지고 내부도 멋지다고 한다. 긴자 : 도쿄의 세련미 상징여전히 명실상부 도쿄 최고의 부촌 가운데 하나이다. 간자역 중심으로 펼쳐진 다양한 레스토랑, 카페, 상점들을 비롯해, 시계탑으로 유명한 와코백화점, 긴자식스, 도큐 플라자 긴자, 도쿄 미드타운 히비야 등이 볼만하다. 와코 백화점2차 세계대전 공습에도 살아남은 건물로, 역사가 느껴지는 우아함을 간직한 곳.이곳을 기점으로 펼쳐지는 사거리에서, 화려한 긴자 거리 전체를 조망할 수도 있다. 길을 건너 두 블럭 정도 걸어가면 긴자에서 가장 큰 쇼핑몰인 긴자식스가 있다. 긴자식스패션위주의 쇼핑몰답게 감각적인 내부 디자인이 돋보이는 건물이다. 인테리어 그 자체가 하나의 전시물을 감상하는 것처럼 느껴질정도로 보는 즐거움이 있고, 다양한 맛집과 함께 6층에는 츠타야 서점과 스타벅스가 있다. 도큐 플라자 긴자 긴자식스와 함께 긴자의 대표 쇼핑몰로 꼽히는 도큐 플라자 긴자는 컷팅유리로 이루어진 독특한 건물 외관을 지니고 있다. 내부는 알록달록한 차 매장(TWG)과 눈을 사로잡는 톡톡 튀는 패션 아이템들을 만나볼 수 있다. 도쿄 미드타운 히비야 2018년에 지어진 비교적 최신 쇼핑몰이지만생각보다 규모는 그리 크지 않다. 패션보단 맛집이 많은 것으로 유명하다.날씨가 좋을때는 쇼핑몰 앞 광장에서 문화 행사를 진행한다고 한다. 신바시신바시 부근 도심지역은 신주쿠, 시부야, 롯폰기에 등에 비해서는 여행자들 사이에서 인지도가 떨어지는 편이라고한다. 관광지로서 북적이는 곳이 아닌, 현지인들로 붐비는 곳에 가보고 싶다면 이곳을 추천한다. 시오도메 시오도메는 과거 화물역 부지를 재개발한 곳으로 현재 유수의 대기업과 미디어 방송사 등이 다수 입주해있는 활기찬 직장인 거리이다. 빌딩들이 밀집해있는 지역을 시오도메 시오사이트라고 부르는데, 사이트 내 빌딩을 연결하는 고가 육교를 걷다보면 뜻밖의 아름다운 전망을 감상할수도 있다. 미야자키 하야오의 시계 시오도메의 상징과도 같은 닛폰 텔레비전 건물의 커다란 시계인 미야자키 하야오의 시계도 볼만하다.세계에서 가장 큰 태엽 시계라고도 한다. 카렛타 시오도메근처의 카렛타 시오도메 쇼핑몰에서는 일본 내에서도 손꼽히는 일루미네이션을 볼 수 있고, 46층에는 무료 전망대가 있어서 야경을 감상하기에 좋다. 신바시시오도메와 인접한 신바시지역은 일본 현지인 샐러리맨들이 많은 지역이다. 신바시역에 내리면 서쪽 출구를 따라 1940년대 증기 기관차가 있는 신바시 만남의 장소, 니시구치 광장 거리가 있다. 광장에서 아래쪽으로 신바시 역사 주변, 교각 아래 다양한 식당과 이자카야 등이 줄지어 있는 거리가 있다. 이곳은 퇴근시간 이후 근처 직장인들이 회식이나 퇴근 후 스트레스를 풀며 한 잔할 장소로 많이 찾는다. 굴다리 아래 종기종기 모여있는 가게들이 독특한 풍경을 자아내는데, 일본인들의 일상을 엿볼 수 있는 일본스러운 밤거리 풍경을 볼 수 있다. 토라노몬 힐즈신바시에서 도보로 15분 거리에는 토라노몬 지역이 있는데, 대규모 복합시설인 토라노몬 힐즈가 있다. 토라노몬힐즈는 몰리타워를 중심으로 비즈니스 타워, 레지덴셜 타워, 스테이션 타워가 차례로 오픈하며 총 4개 타워가 있다. 도시속의 도시라는 지향점에 걸맞게 미래 도시에 와있는 듯한 느낌이 들게하는 곳이다. 여기서 남쪽으로 15분거리에는 도쿄타워가 있다. 도쿄타워도쿄 타워가 위치한 시바코엔의 탁 트인 잔디에서 타워를 볼 수 있다. 그 옆의 조죠d지 절을 배경으로 타워가 우뚝 선 모습도 멋지다. 오렌지색으로 반짝이는 타워와 야경이 어우러진 모습을 보려면 어두워진 후 전망대나 타워 근처로 이동하는게 좋지만, 낮 시간대에 시바공원, 조죠지 절과 함께 즐기는 것도 좋다. 롯폰기 아자부주반도쿄를 대표하는 번화가이자 긴자 못지않게 고급 상권가가 형성되어 있지만, 분위기를 비교하자면 좀 더 트렌디하고 힙한 느낌이 강한곳이다. 크게 롯폰기역 북쪽의 도쿄 미드타운 롯폰기, 남쪽의 롯폰기 힐스, 그리고 더 아래로 대사관들이 많아 한남동과 비슷한 분위기의 아자부주반이 있다. 롯폰기 힐스롯폰기는 지금의 롯폰기를 있게한 곳이다.20년 전 탄생했을 당시 기존에 보지 못했던 성격의 대규모 복합타운이었고, 그러다보니 등장 자체로 화제였다고 한다.쇼핑몰뿐만 아니라 호텔, 영화관, 방송국, 일반 주거지 등 무려 10여개의 건물이 모여 이루어져있고, 모리타워 전망대는 도쿄타워가 가장 아름답게 보이는 곳으로 도쿄 최고의 전망대 중 하나로 손꼽힌다. 롯폰기 힐즈의 성공으로 제2의 롯폰기 힐즈들이 연이어 생겨나게 되었는데, 그 중 하나가 롯폰기 랜드마크의 양대산맥인 도쿄 미드타운 롯폰기이다. 총 6개의 건물로 이루어진 미드타운은 호텔, 오피스, 미술관, 쇼핑센터, 주거공간 등이 부지의 40%를 차지하는 녹지와 어우러져 도심 한가운데 있으면서도 뭔가 힐링이 되는 휴식공간처럼 느껴진다. 출처 : https://m.blog.naver.com/ksm11015/222958660955 유리벽을 통해 자연 채광과 외부 조망이 가능한 가든테라스 식당가, 유리지붕과 나무로 이루어진 중앙광장 등이 대표적이고, 대나무숲으로 자연 친화적이고 따뜻한 느낌을 주는 인기 쇼핑몰 갤러리아와 산토리 미술관도 둘러보면 좋다. 이 롯폰기 일대에는 고급 맛집에서부터 전통있는 카페, 기념품 전문점, 캐주얼한 선술집, 그리고 가볍게 즐기는 베이커리나 브런치 식당에 이르기까지 다양한 종류의 가게들이 있으니 여유가 있다면 천천히 돌아보아도 좋다. 아자부주반아자부주반은 롯폰기와 도쿄타워 사이의 한적한 거리이자 고급주택가로 주변에 대사관들도 많아 외교관, 외국인들에게 인기 있는 지역이기도 하다. 롯폰기와 인접해있지만 화려함보다는 조용한 분위기이고 에도시대부터 번창해온 아자부주반 상점가를 비롯해 전통적인 상점, 국제적이며 고급스러운 카페, 레스토랑등이 구석구석 숨어있다. 사라시나호라이 - 냉소바 위치","link":"/2024/05/05/Tokyo-Travel-00/"},{"title":"순차데이터와 순환신경망","text":"RNN(Recurrent Neural Network, 순환 신경망)은 시퀀스 데이터를 처리하기 위해 설계된 인공 신경망의 한 종류이다. RNN은 시간에 따라 정보를 전달할 수 있는 내부 메모리를 가지고 있어, 시퀀스의 길이에 상관없이 입력데이터 사이의 장기 의존성을 학습할 수 있다. 이러한 특성으로 인해 RNN은 자연어 처리(NLP), 음성 인식, 시계열 예측 등 시퀀스 데이터를 다루는 다양한 분야에서 활용된다. 출처 : 혼자 공부하는 머신러닝+딥러닝 9장. 텍트를 위ㄴ 인공 신경망] 순차 데이터(Sequential Data)순서가 있는 데이터를 말한다. 이러한 데이터는 특정 순서대로 나ㅕㄹ되어 있으며, 각 데이터 포인트 사이에는 시간적 또는 공간적 연관성이 존재한다. 순차 데이터의 한 요소는 그 전후의 요소와 관련이 있으며, 이러한 연속성 때문에 데이터 전체를 통해 패턴이나 관계를 찾아낼 수 있다.ex) 글, 대화, 일자별 날씨, 일자별 판매 실적 순환 신경망일반적인 완전 연결 신경망과 거의 비슷하나, 이전 데이터의 처리 흐름을 순환하는 고리 하나가 추가된다. 뉴런의 출력이 다시 자기 자신으로 전달되는데, 즉 어떤 샘플을 처리할 때 바로 이전에 사용했던 데이터를 재사용하는 것이다. 이렇게 샘플을 처리하는 한 단계를 타임스텝이라고 부르며, 순환신경망은 이전 타임스텝의 샘플을 기억하지만, 타임스텝이 오래될수록 순환되는 정보는 희미해진다.순환 신경망에서는 특별히 층을 셀이라고 부른다. 한 셀에는 여러개의 뉴런이 있지만, 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 은닉 상태라고 부른다.입력에 어떤 가중치를 곱하고, 활성화 함수를 통과시켜 다음층으로 보내는 구조는 합성곱 신경망과 같으나, 층의 출력을 다음 타임 스텝에 재사용하는 것이 다르다.은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트(tanh)를 사용한다. 시그모이드 함수와는 달리 -1 ~ 1 사이의 범위를 가진다. RNN의 주요 특징 순환 구조 : RNN은 네트워크 내에서 정보를 순환시키는 구조를 가지고 있어, 이전의 계산 결과를 현재의 계산에 활용할 수 있다. 이를 통해 시퀀스 내의 정보를 시간적으로 연결하여 처리한다. 변동하는 시퀀스 길이 처리 : RNN은 입력 시퀀스의 길이가 변동적인 데이터를 처리할 수 있다. 이는 고정된 크기의 입력을 다루는 다른 신경망 모델과는 차별되는 특징이다. 파라미터 공유 : 시퀀스의 각 지점(time step)마다 동일한 가중치를 사용함으로써, 모델의 파라미터 수를 효율적으로 관리한다. 활용 분야 자연어 처리 : 문장, 문서분류, 기계 번역, 감성 분석 등 NLP의 여러 작업에서 사용된다. 음성 인식 : 오디오 시퀀스에서 음성을 텍스트로 변환하는 작업에 사용된다. 시계역 예측 : 주식 가격, 기상 상태 등 시간에 따라 변하는 데이터의 미래 값을 예측하는 데 사용된다. 한계 장기 의존성 문제 : RNN은 이론적으로는 시퀀스의 장기 의존성을 학습할 수 있지만, 실제로는 그레디언트 소실(vanishing gradient) 또는 폭발(exploding gradient) 문제로 인해 학습이 어려울 수 있다. 계산 비용 : 순환 구조로 인해 병렬 처리가 어렵고, 긴 시퀀스를 처리할 때 계산 비용이 높아질 수 있다. 이러한 한계를 극복하기 위해 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)와 같은 고급 RNN 구조가 개발되었다. 이들은 장기 의존성을 더 효과적으로 학습할 수 있는 메커니즘을 제공한다. IMDB 리뷰 분류1. 데이터 준비하기IMDB 리뷰 데이터셋을 적재한다. 리뷰를 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋인데, 총 50,000개의 샘플로 이루어져 있고 훈련 데이터와 테스트 데이터에 25,000개씩 나누어져 있다. 실제 IMDB 리뷰 데이터셋은 영어로 된 문장이지만, 텐서플로에는 이미 정수로 바꾼 데이터가 포함되어 있다. 여기에는 가장 자주 등장하는 단어 500개만 사용한다. 12345from tensorflow.keras.datasets import imdb(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)print(len(train_input[0]))print(len(train_input[1])) 123(25000, ) (25000, )218189 첫 번째 리뷰의 길이는 218개의 토큰, 두 번째는 189개의 토큰으로 이루어져있다. 1print(train_input[0]) 12[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, ... ] 텐서플로의 IMDB 리뷰 데이터는 정수로 변환되어 있다. num_words=500으로 지정했기 때문에 어휘 사전에 없는 단어는 모두 2로 표시된다. 1print(train_target[:20]) 1[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] 타깃 데이터는 0(부정)과 1(긍정)으로 나누어진다. 12from sklearn.model_selection import train_test_splittrain_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42) 훈련 데이터의 20% 정도를 검증세트로 떼어 놓는다. 2. 데이터 분석과 패딩평균적인 리뷰, 가장 짧은 리뷰, 가장 긴 리뷰의 길이를 확인하기 위해 먼저 각 리뷰의 길이를 계산해 넘파이 배열에 담아 그래프로 표현한다. 123456789import numpy as npimport matplotlib.pyplot as pltlengths = np.array([len(x) for x in train_input])plt.hist(lengths)plt.xlabel('length')plt.ylabel('frequency')plt.show() 대부분 리뷰 길이는 300개 미만인 것을 볼 수 있다. 리뷰는 대부분 짧기 때문에 이 예제에서는 100개의 단어만 사용하기로 한다.이 리뷰들의 길이를 맞추기 위해 패딩이 필요하다. pad_sequences()함수를 통해 시퀀스 데이터의 길이를 맞출 수 있다.짧은 리뷰는 앞에서부터 0토큰을 채우고, 긴 리뷰는 잘라내는데, 만약 pad_sequences()의 매개변수 padding을 기본값인 pre에서 post로 바꾸면 샘플의 뒷부분으로 패딩할 수 있다. 123456from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100)val_seq = pad_sequences(val_input, maxlen=100)print(train_seq.shape) 1(20000, 100) train_seq는 이제 (20000, 100) 크기의 2차원 배열임을 알 수 있다. 3. 원-핫 인코딩으로 데이터 바꾸기케라스는 여러 종류의 순환층 클래스를 제공하는데, 가장 간단한 것은 SimpleRNN 클래스이다. 이 문제는 이진 분류이므로 마지막 출력층은 1개의 뉴런을 가지고 시그모이드 활성화 함수를 사용한다. 1234from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500)))model.add(keras.layers.Dense(1, activation='sigmoid')) # 이진분류 뉴런 갯수를 8개로 지정하고, 샘플의 길이가 100이고 500개의 단어만 사용하도록 설정했기 때문에 input_Shape를 (100, 500)으로 둔다. 순환층도 활성화 함수를 사용하는데 기본 매개변수 activation의 의 기본값은 tanh로, 하이퍼볼릭 탄젠트 함수를 사용한다. 그러나 토큰을 정수로 변환한 데이터를 신경망에 주입하면, 큰 정수가 큰 활성화 출력을 만들게 된다. 이 정수들 사이에는 어떤 관련이 없기 때문에 정수값에는 있는 크기 속성을 없애고 각 정수를 고유하게 표현하기 위해 원-핫 인코딩을 사용한다. keras.utils 패키지의 to_categorical() 함수를 사용하여 훈련세트와 검증 세트를 원-핫 인코딩으로 바꾸어준다. 123train_oh = keras.utils.to_categorical(train_seq)val_oh = keras.utils.to_categorical(val_seq)print(train_oh.shape) 1(20000, 100, 500) 정수 하나마다 500차원의 배열로 변경되었다. 12print(train_oh[0][0][:12])print(np.sum(train_oh[0][0])) 12[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]1.0 첫 리뷰의 첫 단어를 원-핫 인코딩시킨 결과이다.모든 원소의 값을 더하면 1임을 알 수 있다. 4. 순환 신경망 훈련하기RMSprop의 기본 학습률 0.001을 사용하지 않기 위해 별도의 RMSprop 객체를 만들어 학습률을 0.0001로 지정한다.","link":"/2024/04/08/Recurrent-Neural-Network/"},{"title":"신경망 모델 훈련","text":"인공 신경망과 심층 신경망을 구성하고 다양한 옵티마이저를 통해 성능을 향상시킬 수 있는 방법에 대해 알아보았다.이번에는 과대적합을 막기 위해 신경망에서 사용하는 규제방법인 드롭아웃, 최상의 훈련 모델을 자동으로 저장하고 유지하는 콜백과 조기종료를 알아보겠다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망]","link":"/2024/04/02/Train-Neural-Network-Model/"},{"title":"(프로젝트)SSVEP 기반 홀로렌즈 BCI","text":"이 프로젝트는 한양대학교 Conelab에서 학부생 프로그래머로써 개발한 프로젝트입니다.개발 기간은 2018년 12월 ~ 2020년 2월 입니다.홀로렌즈 어플 인터페이스와 EMG신호(SSVEP), vuforia 영상 인식 기술을 통해 사용자의 의향을 파악하여 로봇청소기, 가습기, 공기청정기, 에어컨, 전구의 전원, 충전, 바람 세기 등 여러 기능들을 조정하는 BCI 시스템입니다.홀로렌즈 인터페이스는 Unity를 기반으로 C#과 vuforia를 이용하여 만들었습니다. 소스코드Hololens App source code repository 에 가시면 Unity와 C#, Vuforia를 이용해 개발한 홀로렌즈 어플의 소스코드를 볼 수 있습니다. 온라인 실험의 전체 절차(a) 제안된 가전 제어 시스템의 계층 구조. 반복되는 눈 깜박임으로 제어 시스템이 작동한다. 선택된 가전제품에 지정된 명령을 전달하고 나면 사용자의 반복적인 눈 깜빡임으로 제어 시스템이 꺼진다. (b) “기기 선택” 단계에 제시된 시각적 자극. (c), (d), (e)는 로봇 진공, 공기청정기, 가습기를 각각 제어하기 위해 “명령 선택” 단계에서 제시한 시각적 자극을 각각 보여준다. 제어 대상 기기 선택 방법 - 3종 이름 방식 선택 가능 기기 갯수 사용 시나리오 참고 그림 QR 코드 선택하고자 하는 기기에 부착된 QR코드 응시 제한 없음 눈앞에 있는 기기를 선택할 때 사용 SSVEP 기기에 해당하는 SSVEP 자극 응시 3개 눈앞에 없는 기기 중 자주 사용하는 기기를 등록해두고 사용 눈 움직임 패턴 눈으로 숫자 0~9에 해당하는 패턴 중 하나를 그려 선택 10개 눈앞에 없고 가끔 사용하는 기기를 선택할 때 사용 프로세스 절차 통신값 (BCI -&gt; Middleware) 기기별 명령 정리 기기 번호 기기명 명령 번호 제어 명령 기타 1 로봇청소기 1 전원 on/off 기존과 동일 2 터보모드 3 충전복귀 4 Back 2 공기청정기 1 전원 on/off 2 바람-자동 3 모드-취침 4 Back 3 가습기 공기청정기와 동일 4 에어컨 1 전원 on/off 신규 추가 2 온도조절(순환식(18도 -> 21 -> 24 -> 27 -> 18 -> ...)) 3 운전모드(순환식(auto -> cool -> dry -> auto -> ...)) 4 Back 5 전등 1 전원 on/off 2 색상(순환식(r -> g -> b -> r -> ... 3 밝기(순환식(33% -> 66 -> 99 -> 33 -> ... 4 Back 먼저 전구를 조종해보겠습니다전구를 쳐다보면 vuforia library를 통해 전구를 인식할 수 있습니다.전구가 선택 되었고, 전구의 기능중에 어떤 것을 수행할 지 고르는 화면입니다왼쪽 위의 아이콘을 쳐다보면 EOG 신호를 통해 전구 ON/Off기능을 선택할 수 있습니다.불이 켜졌고 이제 다시 다른 명령어를 고를 수 있습니다.오른쪽 위 아이콘을 쳐다보면 전구 색상을 바꿀 수 있습니다.전구가 빨간색으로 바뀌었습니다. 전구 색상은 흰색 - 빨간색 - 파란색 순으로 바뀝니다.왼쪽 아래 아이콘을 쳐다보면 전구의 밝기를 조절할 수 있습니다전구가 어두워졌고 이제 왼쪽 위 아이콘을 바라보아 전구를 끄고 메인화면으로 나가겠습니다. 오른쪽 아래 아이콘을 쳐다보면 메인으로 나갈 수 있습니다.이런식으로 로봇청소기와 공기청정기, 에어컨, 가습기도 조종할 수 있습니다.로봇 청소기를 컨트롤 해보겠습니다.참고로 눈을 깜박이면 시스템을 시작할 수 있습니다.vuforia 이미지 처리가 아닌 SSVEP 자극을 통한 시선 처리로도 기기를 고를 수 있습니다.원하는 기기를 쳐다보면 선택할 수 있습니다로봇 청소기를 골랐으니 이전 처럼 똑같이 원하는 기능을 SSVEP 자극이 깜박거릴 때 쳐다봐 골라주면 됩니다.로봇 청소기가 켜졌습니다.눈을 깜박여 SSVEP 모드 변경 화면을 켜 모드를 전환해주겠습니다.터보모드를 선택했습니다.나머지 기능들도 똑같이 사용할 수 있습니다.이제 가습기를 제어해보겠습니다.다른 기기들과 비슷하게 SSVEP 자극이 깜박일 때 시선으로 선택해 주면됩니다.이런식으로 Sleep 모드로 바꿔줄 수도 있습니다.이제 에어컨을 조종해보겠습니다.이번엔 눈글씨를 통해 기기를 골라보겠습니다.시선을 통해 9개 점에 원하는 기기의 번호를 그리면 됩니다.에어컨이 선택되었습니다.마찬가지로 모드, 온도조절 등 기능을 선택하여 조종하면 됩니다.오른쪽 위는 온도, 왼쪽 아래는 운전모드 변경입니다.마지막으로 공기청정기를 조종해보겠습니다.이번에도 눈글을 통해 기기를 선택합니다.공기청정기가 선택되었습니다.다른 기기와 비슷하게 기능을 선택하여 조종해 줍니다.","link":"/2020/03/22/SSVEP-based-Hololens-BCI/"},{"title":"Java StringBuilder란?","text":"String은 불변(immutable) 객체이다. Java 에서 String을 다루는 대표적인 클래스로 String, StringBuffer, StringBuilder가 있습니다. 연산이 많지 않을 때는 위에 나열된 어떤 클래스를 사용하더라도 이슈가 발생할 가능성은 거의 없습니다. 그러나 연산횟수가 많아지거나 멀티스레드, Race Condition 등의 상황이 자주 발생한다면 각 클래스의 특징을 이해하고 상황에 맞는 적절한 클래스를 사용해야 합니다. 그 중에서 이 글에서는 StringBuilder에 대해 적어 보겠습니다. 12String str1 = &quot;abc&quot;;String str2 = &quot;def&quot;; 2개의 String 객체가 있을 때, 만약 str1 + str2; 와 같은 연산을 하게 되면 새로운 String을 생성한다. 그래서 나온 것이 StringBuilder이다. StringBuilder는 String과 문자열을 더할 때 새로운 객체를 생성하는 것이 아니라 기존의 데이터에 더하는 방식을 사용하기 때문에 속도도 빠르며 상대적으로 부하가 적다. 따라서 긴 문자열을 더하는 상황이 발생할 경우 StringBuilder를 적극적으로 사용하면 좋다. 사용법1234StringBuilder sb = new StringBuilder();sb.append(&quot;ABC&quot;);sb.append(&quot;DEF&quot;);System.out.println(sb.toString()); StringBUilder에는 append()가 있는데, 이는 문자열을 더하는 역할을 한다. 만들어진 문자열을 출력하기 위해서는 StringBuilder의 인스턴스인 sb의 toString()을 부르자. 예제이므로 짧은 문자열을 더했지만, 빈번하게 긴 문자열을 더해야 할 경우 효과적이다.","link":"/2020/08/17/StringBuilder/"},{"title":"마이클 샌델의 The Case against Perfection을 읽고","text":"목차1장. 강화의 윤리학2장. 생체공학적 운동선수3장. 맞춤 아기를 설계하는 부모4장. 우생학의 어제와 오늘5장. 정복과 선물6장. 에필로그_배아 윤리학:줄기세포 논쟁7장. 해제_생명공학 시대와 마음의 습관 샌델이 말하고자 하는 바 샌델은 우리가 어떤 입장을 취하거나 행동을 할 때 마음에 불편함이 따를 경우, 그 감정의 근원이 무엇인지 물음. 마음의 불편은 어디에서 오는가? 왜 우리는 어떤 입장을 취할 때 망설이게 되는 것일까? 윤리적 불안감의 정체는 무엇일까? 왜 어떤 입장에 대해서는 윤리적 반감이 발생하는 것일까? 이러한 질문들이 우리안에서 생길 때, 우리는 생각하게 됨 생각을 통해 무엇이 옳은 것인지에 대한 판단으로 나아갈 수 있음. 샌델은 우리가 어떤 잘못된 입장을 취하게 되면 그 결과가 나쁠 수 밖에 없다는 것을 지적. 그 결과가 처음에 얻으려 했던 목적과 반대되는 것이라면 우리는 그런 행위를 취해서는 안됨. 소크라테스가 적극적으로 사용했던 귀류법의 논리를 윤리적으로 적용한 것. 샌델은 유전공학을 적용했을 때 그것이 가져다주는 단기적 성과에만 집착하여 유전공학이 유익하다고 판단하는데 그치지 말고, 그런 기술이 궁극적으로 어떤 사회를 만들어낼 것인지, 그런 사회가 과연 좋은 것인지 고민해볼 것을 권함. 샌델은 우리의 삶과 세계를 구성하고 있는 선한 것들의 본래적 가치에 주목해볼 것을 주문 우리가 소중하게 생각했더 책임의 가치, 서로 돕고 지원하는 사회적 연대의 가치 뮤지컬이나 스포츠가 왜, 그리고 어떻게 인간에게 감동을 주고 거기서 쾌감을 느껴 좋아하게 되는가를 들여다 보라고 권함. 우리가 새롭게 사용하려는 기술이 그러한 본래적 가치를 더하는 방향으로 변화를 주는지, 그것을 파괴하는 방향으로 변화를 주는지 살펴보라고 말함. 책을 읽은 동기 샌델이 말하는 완벽함과 완벽하다고 말하는 것에 대한 반론이 무엇인지에 대한 불편한 감정 해소 마이클 샌델이 말하는 윤리학과 생명공학에 대한 궁금증 과학클래식 과제라서… 3줄 요약 유전 공학을 통한 신체강화에서 당신이 느끼는 도덕적 불편함은 무엇인가? 이 도덕적 불편함은 사용하는 수단에 달려있는 것이 아니라 본질적으로 목적을 이루는 수단에 달려있다. 자유주의적 우생학은 국가가 강제하는 유전공학을 전혀 거부하지 않는다. 다만 유전공학으로 설계되는 아이의 자율성이 존중되기만을 요구할 뿐이다. 우리는 자유주의적 우생학의 삶을 살고 있다. 재능 역시 선택이 아닌 선물로 주어진 것이기에 생명공학기술을 통한 능력강화는 지나친 지배와 통제일 수 있다. 설계하고 유전자를 조작하는 우생학적 노력에 담긴 인간의 지배 심리에 대해 돌아보아야 한다. 계획적 장애 계획적으로 자녀를 청각장애로 만드는 사례. 듣지 못하는 것이 장애가 아니라 특별한 정체성이라 가정한다면, 부모가 자신이 가질 자식을 원하는 모습대로 선택하는 것은 잘못인가? 계획적 강화 불임 부부의 난자 제공자 찾는 광고 (키 175이상, 가족 병력 무, SAT 1400이상). 그들의 광고에 일부러 청각장애 아이를 낳은 커플의 경우와 달리 대중의 비난 없었음. 특정 유전적 특성을 가진 아이를 ‘주문’하려는 부모의 행동에는 아무런 문제가 없는가? 무엇에서 불편함을 느끼는가? 복제는 태어날 아이의 자율권을 손상시킨다. “부모가 아이의 유전적 구성을 미리 선택함으로써 이전에 살았던 누군가의 그림자와 같은 삶을 아이에게 부여하여, 스스로 미래를 열어갈 권리를 빼앗는 것이다.” 그러나 이는 유전적 구성을 선택하지 않으면 아이가 자신의 특성을 스스로 자유롭게 선택할 수 있다는 그릇된 가정을 함축. 자기 자신을 위한 유전적 강화에 대한 도덕적 망설임을 설명해주진 못함(예로, 성형수술) 자유주의 사회에서는 자율성과 공정함, 개인의 권리 같은 개념에 먼저 눈을 돌린다. 그러나 유전공학이 제기하는 어려운 문제를 해결할 만한 수단으로 제공해주지는 못한다. 유전공학적 강화근육 강화 : 떠오르는 이유 두 가지는 안전성과 공정성. 불편한 감정은 안정성과 공정성 때문은 아니다. 근육강화 유전자 치료법이 안전하면 금지해야 할 이유가 있는가? 부상당한 선수가 손상된 근육을 유전자 치료로 복구하는 것이 괜찮다면 건강한 근육을 강화해 과거보다 향상된 몸으로 경기에 출전하는 것은 어째서 잘못인가? 유전적으로 훌륭한 재능을 타고난 이들은 언제나 존재했다. 이런 선천적 불평등이 스포츠의 공정성을 훼손하지는 않는가? 기억력 / 신장 강화 : 도덕적 지위에 관한 선결문제의 오류. 기억력 / 신장 강화를 쉽게 접근하는 사람과 노화로 시들해지는 자연적 기억력에 만족하는 사람들. 기억력 / 신장 강화가 후세대까지 연결된다면, 왜 윤리적으로 불편한가? 가난한 자들이 생명공학 혜택을 누리지 못해서? 유전적 강화 기술을 누리는 부유층이 인간다움을 잃어버려서? 비판자들은 성장호르몬 선택적 사용을 성형 내분비학이라고 칭하며 비용이 매우 비싸며, 집단적 자기모순적 상황에 이르게 되는 것에 반대함. 왜 강화란 것은 치료와 건강회복에만 사용해야 하는가? 향후, 불공평함이 문제라면 공적 보조금을 제공한 불공평한 해소가 있겠으나. 자녀가 충분히 건강한데 추가적인 거금을 사용하는 사회에서 살고 싶은지에 대한 질문이 나타남. 스포츠에서의 경기력 강화의 수단 운동 장비 혁신도 일종의 강화의 수단이다. 그것이 경기에 꼭 필요한 능력을 더욱 완벽하게 만드는가, 아니면 그 능력의 ‘의미’를 퇴색시키는가에 있다. 운동 장비 혁신을 통해서 검증되어야 하는 재능과 실력을 환벽하게 한느가 아니면 왜곡하는가에 달려있다. 1970년 ~ 1980년대에 운동 선수들의 체중을 늘리는 방법에 기여한 것은 스테로이드의 사용이었다. 1990년 이를 금지했으나 선수들의 체중은 계속 증가했다. 주로 선수 명단에 오르기 위해 엄청난 양의 음식물을 섭취(대량의 빅맥을 섭취)하는 방법을 택했다.: 다량의 빅맥을 섭취하는 것은 최첨단 기술과 관련이 없지만, 스테로이드제나 성장호르몬, 유전공학 기술로 몸집을 불리는 것 못지않게 윤리적인 불편함을 자아낸다. 즉, 강화에 찬성하는 사람들의 주장은 ‘강화하는 행위의 적법성’이 ‘그들이 사용하는 수단에 달려있는 게 아니다’라는 뜻이며, ‘특정 목적에 달성하기 위한 본질적 목적 수단을 해치는가에 결정’된다고 한다. 그러나 이 본질적 목적 수단이란 것이 과연 무엇인가? 스포츠를 빗대어, 이 본질적인 목적 수단이 과연 자연적 재능과 능력의 의미인가? 스포츠에서 본질적이란 것이 단순히 오락을 위한 목적인가? 우생학 과거 우생학 : 과거 우생학의 창시자는 영국의 유전학자 골턴이었다. 1822년 인체측정학연구실을 창설하였고, 1904년 우생학 연구소를 설립하였다. 골턴의 사상은 20세기 초 미국으로 퍼져나갔고 독일에서는 미국의 우생학적 법안을 숭배하는 인물로 아돌프 히틀러가 등장했다. 히틀러는 자서전 “나의 투쟁”에서 우생학에 대한 신념을 이렇게 밝혔다. “유전적으로 열등한 사람들이 똑같이 열등한 자손을 번식하는 것을 막아야한다는 요구에는 매우 분명한 이유가 존재한다. 체계적으로 시행하기만 한다면 그것을 막는 일은 인류가 할 수 있는 가장 고상한 행동에 해당한다. 수백만의 불운한 자들이 부당한 고통을 당하는 것을 막을 수 있으며 결과적으로 인류 전체의 건강 수준이 높아질 것이기 때문이다.” 히틀러는 1933년 독일 최고 권력자의 자리에 오른 후 우생학에 근거한 단종법을 공포하여 미국 우생학자들로부터 찬사를 받았다. 콜드 스프링 하버 출판사에서 발간한 “우생학 뉴스”는 히틀러의 단종법을 영어로 번역한 것을 실으면서, 그 내용이 미국 우생학 운동가들이 제안한 단종법과 유사하다고 자랑스럽게 보도했다. 우생학 열풍이 고조되어 있던 캘리포니아에서는 “로스앤젤레스 타임스”에 1935년 나치의 우생학에 대한 긍정적인 논평이 실렸다. ‘히틀러는 왜 “유전적 열성자를 불임시켜라!”고 말하는가’라는 헤드라인 밑에 이런 내용이 적혀 있었다. “이것이야말로 미국과 전 세계가 비판할 수 없는 새로운 독일의 모습일 것이다.” 결국 히틀러는 우생학을 발판 삼아 미국의 불임수술에 그치지 않고 대량 집단 학살을 자행하기에 이르렀다. 제 2차 세계대전이 끝날 무렵, 나치의 잔혹 행위가 세상에 알려지면서 미국의 우생학 운동은 후퇴하기 시작했다. 자유시장 우생학:우생학의 그림자는 유전공학과 강화를 둘러싼 오늘날의 논쟁에도 드리워져 있다. 유전공학에 대한 비판자들은 인간 복제, 강화, 맞춤 아기에 대한 욕구가 “민간화된” 또는 “자유시장의” 우생학에 불과하다고 주장한다. 한편 유전공학 옹호자들은 자유로운 의사에 따른 유전학적 선택은 우생학과 다르다고, 적어도 우생학이라는 말에 담긴 경멸적인 의미는 적용되지 않는다고 응수한다. 강제성이 없는 최근의 한 우생학 정책1980년대에 싱가포르 총리 리콴유는 싱가포르 고학력 여성들의 출산율이 저학력 여성들보다 낮은 상황을 우려하면서 이렇게 말했다. “이처럼 한쪽으로 치우친 출산이 계속된다면 싱가포르는 현재의 국가 수준을 유지하지 못하게 될 것이다.” 그는 이후 세대에서 “재능 있는 인재가 고갈될 것”을 우려했다. 그러한 상황을 막기 위해 싱가포르 정부는 대졸 여성들의 결혼과 출산을 장려하기 위한 정책들을 만들었다. 국가가 운영하는 온라인 만남 주선 서비스, 출산하는 대졸 여성을 위한 재정 지원, 대학 커리큘럼에 이성교재 강의 개설, 미혼 대졸자들을 위한 무료 ‘사랑의 유람선’ 사업 등이 그것이다. 이와 동시에 고등학교 졸업장이 없는 저소득층 여성들이 불임수술을 받는 것에 동의하는 경우, 그들에게 저가 아파트의 계약금 4000달러를 지원했다. 싱가포르의 정책은 우생학을 자유시장에 맞는 버전으로 변형한 것이다. 사회적으로 환영받지 못하는 사람들에게 강제로 불임수술을 받게 한 것이 아니라 그에 대한 금전적 보상을 제공했기 떄문이다. 과거 우생학의 문제점은 그것에 수반되는 부담이 사회적 약자와 빈자들에게 지나치게 편중되기 제워져서, 그들만 부당하게 불임수술을 당했다는 점이다. 하지만 유전적 강화가 주는 이로움과 부담이 모두에게 공평하게 분배된다면 우생학적 조치들을 반대할 이유가 없고, 심지어 윤리적으로 필요할 수도 있다는 것이 이 생명윤리학자들의 주장이다. 자유주의적 우생학이 개인의 선택을 강조함에도 불구하고, 얼핏 보기와는 다르게 국가의 강요라는 요소들을 내포하고 있다. 자식의 행복을 증진하려고 노력하는 것이 부모의 의무임을 감안할 때, 그러한 강화는 허용 가능할 뿐만 아니라 의무적인 것이 된다. 부모가 아이를 학교에 보내는 것을 국가가 의무화 할 수 있는 것과 마찬가지로, (안전성이 보장된다면) 유전학 기술로 아이의 IQ를 높이는 것도 의무화 할 수 있다. 자유주의적 우생학은 국가가 강제하는 유전공학을 전혀 거부하지 않는다. 다만 유전공학으로 설계되는 아이의 자율성이 존중되기만을 요구할 뿐이다. 인류의 부족한 능력을 ‘강화’하는 행위는 비윤리적인가? 자녀의 유전 결함 = 부모의 책임 자율성 x &lt; 부모의 책임감 up 완벽하지 않으니 겸손하고 서로 친하다 -&gt; 삶은 우연적 개인의 한계, 운의 연속. 배려, 이해심, 연대 outstanding 뛰어나다는 뜻이지만 말그대로 홀로 밖에 서있다는 뜻이다 인생이란 정해진 운명을 개척하는게 아닐까? 1등하려고 학원에 보냄 (교육적 강화) vs 유전적 강화 어떤게 나쁘고 좋다고 할 수 있는가? 스포츠에서 유전적 강화를 반대하는 이유는 무엇인가? 스파르타 비결 - 산아 선별 훈련, 첨단 과학기술과 영양이 잡힌 식단 vs 스테로이드 캡틴 아메리카 약빨 타이거 우즈 - 라식수술 후 5회 우승. 라식은 괜찮으면 어디까진 안괜찮아? 유전적 ‘맞춤 설계’된 인류는 ‘인간’인가? 아닌가? 심장이식 수술, 팔 바꾼것(인공관절), 어느것 까지가 인간인지 기준이 무엇인가? 유전공학이 초래할 세 가지 재앙이런 변화가 인류에게 과연 축복이기만 한 것일까. 저자는 그렇지 않을 수 있다고 경고한다. 저자가 유전공학의 장밋빛 미래에 반론을 제기하는 데에는 크게 세 가지 이유를 들 수 있다. 첫째, 개인적 책임의 증폭유전공학으로 우리가 원하는 바를 더욱 쉽게 얻을 수 있게 된다면, 언뜻 생각하기에는 책임도 줄어들 것 같다. 사람은 힘들여 얻은 것을 더욱 소중히 여기는 법이니까. 그런데 정말 그럴까. 한 발짝 더 들어가서 생각해보면 그렇지 않다. 예를 들어서, 지금은 축구 선수가 경기 중에 실수해도 인간이기 때문에 그럴 수 있다고 넘어간다. 하지만 유전공학으로 신체 기능을 향상하는 것이 보편화된 이후에는 상황이 다르다. 유전공학으로 신체 기능을 향상시킬 수 있음에도 누군가 그렇게 하지 않아서 결과적으로 팀에 피해를 끼친다면, 그 모든 책임은 유전공학을 활용하지 않은 선수 개인에게 돌아갈 것이다. 다른 예를 들어보자. 지금은 학교에서 시험 성적이 좋지 않아도, 좀 더 노력하면 다음에는 좋은 성적을 얻을 수 있으리라 기대할 수 있다. 그런데 유전공학 기술을 활용하여 아이들의 지능을 향상시킬 수 있는 길이 열리면 어떨까. 자녀의 낮은 성적은 곧 부모의 무책임과 같은 의미를 갖게 될 것이다. 이처럼 우리가 삶에서 마주하는 과제에 대한 해결책이 유전공학으로 모이면, 그것을 활용하지 않는 사람은 무책임한 사람이 된다. 세상 모든 사람이 알고 있는 정답을 택하지 않았기 때문이다. 둘째, 사회적 연대감의 붕괴사회적으로 성공한 이들에게 성공의 이유가 무엇인지 물어보면, 생각보다 많은 이들이 자신의 ‘노력’보다는 ‘운’을 그 이유로 꼽는다. 이미 이룬 자의 겸손일 수도 있지만, 우리 삶의 향방이 때때로 운에 의해 결정되는 것도 엄연한 사실이다. 우리의 삶에 우연적 요소가 있다고 보는 것이 왜 중요할까. 지금 행복에 겨운 삶을 살고 있더라도 주위의 불운한 이들을 돌아보고 그들의 고통에 공감할 수 있기 때문이다. 삶의 우연적 요소를 인정하는 것은 누구나 다른 이의 처지에 놓일 수 있음을 이해하는 것이다. 하지만 유전공학에 의해 능력이 결정되면 어떨까. 타인의 불행에 공감할 이유가 사라진다. 운이 아니라 유전공학으로 향상된 능력 덕분에 성공한 것이기 때문이다. 자기 스스로 그럴 만한 자격이 있다고 여기기 때문이다. 따라서, 선택된 이들에게만 그들이 원하는 능력을 부여하는 유전공학은 사회 구성원 간의 연대감이 무너지는 결과를 초래하게 될 것이다. 셋째, 사회 불평등의 정당화능력의 차이가 오로지 개인의 책임으로 여겨지는 사회는 어떤 사회일까. 유전공학의 혜택을 누리는 자와 그렇지 못하는 자가 완전히 분리되어 서로의 세계에 발을 들여놓을 일이 없어지는, 그래서 서로 공감하기 어려운 사회는 어떤 곳일까. 축적한 부를 토대로 유전공학을 누릴 수 있는 이들은 더욱 건강하고 깨어있는 삶을 살게 된다. 시간이 갈수록 거듭 더 많은 부를 거머쥔다. 반면에 못 가진 자들은 점점 더 불리한 상황에서 그들의 고달픈 삶을 소진해야 한다. 시간이 흐르며 불평등에 대한 문제의식이 점차 희미해질 것이다. 불평등은 더 이상 ‘정의롭지 않기 때문에 개선되어야 할 그 무엇’이 아니라, ‘당연한 그 무엇’으로 여겨질 것이다. 이런 상황을 우리가 준비 없이 맞이한다면, 유전공학은 축복이 아니라 사회 구성원 사이의 불평등을 전례 없이 심화시키는 재앙이 될 수도 있다. 요컨대, 유전공학을 잘못 사용하면 사람들은 극도의 책임감 아래에서 신음하게 될 것이다. 동시에 사회 구성원 사이의 연대감은 허물어질 것이다. 결국, 사회적 불평등은 당연하게 받아들여질 것이다. 오늘날 부모의 재력에 따른 사교육으로 아이들에게 일어나고 있는 일과 비슷한 상황이 더욱 광범위하고 뿌리 깊게 나타날 것이다. 우월한 인류 vs 열등한 인류 폭력은 정당한가?태어날 아이의 자유권을 침해할 소지 이런 논리는 설득력이 떨어짐 부모가 유전적 구성을 미리 선택하지 않더라도 아이는 자신의 신체적 특성을 자유롭게 선택할 수 없다는 점 자기 자신을 위해 유전적 강화를 선택하기를 주저하는 사람들의 도덕적 망설임을 설명해줄 수 없다는 것 (모든 유전학적 개입의 결과가 자손에게 전달되는 것은 아님에도) 난자, 정자, 배아 등 대상의 유전적 개입만이 이후 세대에 영향을 미침 유전적 강화는 질병 치료, 신체적 손상 복구, 건강회복과 무관 문제1. 마이클 샌델은 계획적 강화 사례 (불임 부부의 난자 제공 찾는 광고)에서 복제는 아이의 어떠한 권리를 손상시킨다고 하였는가? 자율권 2. 이 책에서 마이클 샌델은 근육 강화에 대한 불편한 감정은 안전성과 공정성 때문이라고 하였다. (T/F)F 3. 이 책에서 마이클 샌델은 왜 강화란 것은 치료와 건강회복에만 사용해야 한다고 주장하였는가? 자녀가 충분히 건강한데 추가적인 거금을 사용하는 사회에서 살고 싶은지에 대해 질문함. 4. 마이클 샌델은 우생학과 유전공학의 문제점이 그것이 일방적 승리를 대변한다는 점이라고 하였다. 만일 생명공학이 선물로 주어진 삶에 대한 인식을 무너뜨린다면, 우리가 잃게되는 것은 무엇이라고 하였는가? 겸손과 책임과 연대 5. 신셰이머는 새로운 우생학은 강제성이 아니라 무엇을 특징으로 하여 우생학보다 더 인간적일 것으로 전망하였는가? 자발성 6. ‘유전적으로 열등한 사람들이 똑같이 열등한 자손을 번식하는 것을 막아야 한다는 요구에는 매우 분명한 이유가 존재한다. 수백만의 불운한 자들이 부당한 고통을 당하는 것을 막을 수 있으며 결과적으로 인류 전체의 건강 수준을 높아질 것이기 때문이다’라고 주장했던 인물은? 아돌프 히틀러 7. 고학력 여성들의 출산율이 저학력 여성들보다 낮은 상황을 우려하면서, 대졸 여성들의 결혼과 출산을 장려하기 위한 정책을 만드는 동시에 고등학교 졸업장이 없는 저소득층 여성들이 불임수술을 받는 것에 동의하는 경우, 그들에게 저가 아파트의 계약금 4000달러를 지원한 나라는? 싱가포르 8. 과거의 우생학의 문제점에 대하여 간단히 서술하시오. 우생학에 수반되는 부담이 사회적 약자와 빈자들에게 지나치게 편중되게 지워져서, 그들만 부당하게 차별과 불임 수술을 당했다는 점. 9. 신학자인 윌리엄 F.메이는 “선택하지 않은 것을 열린 마음으로 받아들이는 태도”를 미덕으로 여길 것을 주장했고, 비슷하게 마이클 샌델은 “선물로 받음(Giftedness)”라는 용어로 개념화 하고, 유전자 강화 문제를 하결하는데 사용한다. (T/F) T 10. 아이의 자율성을 제한하지 않는 비강제적인 유전적 강화를 의미하는 우생학은? 자유주의 우생학 11. “건강에는 본질적 가치가 아니라 도구적 가치만 있으며” 건강이 우리가 원하는 일을 할 때 활용하는 ‘자원’이라고 주장했으며 이러한 관점에서는 치료와 강화의 차이를 거부한 사람은? 줄리언 사불레스쿠 Julian Savulescu 12. 메이가 설명한 부모의 사랑 두 가지 측면은? 받아들이는 사랑 변화시키는 사랑 13. 강화 찬성론자들의 주장에서 가장 일리 있는 것은? 유전 공학으로 아이의 능력을 강화하는 것이 관도하게 관리하고 간섭하여 아이를 양육하는 요즘의 방식과 그 정신에서 비슷하다는 점. 14. 프랜시스 골턴이 제창은 우생학은 eugenics인데 이는 무엇을 뜻하는가? well born 저자에 대한 문제제기 해답이 다소 아쉬움. 뛰어난 통찰력을 통해 문제의 핵심을 꿰뚫고 독자를 생각하게 만들어 놓았다. 일부러 해답을 주지 않음으로써 오히려 더 생각할 공간을 남겨놓았다는 점에서 이렇나 접근도 좋지만, 그의 개인적인 해결방안 또한 같이 정리되었다면 더 좋지 않았을까? 사불레스쿠의 건강에 대한 의견에 대해 건강은 최대화 할 수 있는 종류의 선은 아니라고 하였으나, 적어도 가족력 등 유전적 질병을 치료하여서 최선의 건강상태를 만들 수 있다고 생각한다. 이러한 “치료”의 관점에서 볼 때 “강화”와 어떻게 다른지 기준을 어떻게 정할 것인지에 대한 의문이 있고, 자녀에게 최상의 상태의 삶을 주기 위한 치료와 강화는 같다고 생각한다.","link":"/2020/04/30/Perfection/"},{"title":"도쿄 여행을 위해 준비해야 할 것들","text":"도쿄 여행을 위해 사전에 예약 및 준비해야할 것들을 정리해보고자 한다. 항공권도쿄로 들어갈 수 있는 공항은 하네다, 나리타 공항이 있다. 간단히 정리하자면 하네다는 나리타에 비해 가격이 더 비싸고, 대한항공, 아시아나만 취항돼 있지만, 도쿄 도심과 거리가 모노레일로 20~30분으로 가깝다. 하네다는 김포공항, 나리타는 인천공항 포지션이라고 생각하면 쉽다. 공항 주차본인이 탑승할 항공사의 터미널에 맞게 주차를 예약해야 한다. 인천공항 기준 터미널별 취항 항공사는 아래 링크에서 확인할 수 있다. https://www.airport.kr/ap/ko/svc/airlinesTerInfoList.do 탑승할 항공기 항공사의 터미널을 확인 후 아래 링크에서 인천국제공항 주차 예약을 하면 된다. 또는 그냥 장기주차장을 이용해도 된다. https://www.airport.kr/ap_lp/ko/tpt/parinf/parinft1/parinft1.do 예약 주차는 아래와 같다. https://parking.airport.kr/reserve/6110_01 예약 주차장의 경우 예약보증금 10,000원을 결제 해 놓아야 예약이 가능하다.참고로 1터미널 예약주차장은 심야시간(00:00 ~ 05:00) 셔틀버스가 운행되지 않아 도보이동이 필요(약30분)하다고 한다.예약주차장과 주차대행(발레파킹)은 다른곳이다. 참고사항 눈, 비 악천후인 경우 단기주차장은 지하주차장, 장기주차장은 주차타워를 이용하는 것을 추천 예약주차장은 실외밖에 없다. 예약을 했더라도 장기주차장 주차타워에 먼저 주차를 한 후 예약 취소하는것이 나을수도 있다. 발렛도 가능하며 별도 비용 2만원이다. 주차장 혼잡도도 조회 가능하다. https://www.airport.kr/ap/ko/tpt/getParkingPlaceInfoT1Long.do 입국 전일본 여행 가기 전 미리 해놓으면 좋은 것 중 하나가 비짓재팬웹 (Visit Japan web) 신청이다. Visit Japan web 이란?비짓재팬웹은 입국 절차 (입국 심사, 세관 신고) 및 면세 구입에 필요한 정보를 등록할 수 있는 웹 서비스이다.미리 비짓재팬 웹으로 신청해 놓으면 입국심사 때 더 빠르다. 비짓재팬웹 등록 방법비짓재팬웹 등록 준비물 : 여권, 일본에서 묵는 숙소 정보, 비짓재팬웹 아이디 (처음 진행 시, 회원 가입 필요) 비짓 재팬웹 회원가입 및 로그인 비짓재팬웹 이용자 등록 본인 정보 등록, 자녀나 동반 가족이 있는 경우라면 입력 한국에서 관광목적으로 가는 사람들이라면 일본 정부가 발행한 여권을 가지고 있습니까? / 일본에 거주하고 있으며, 재입국허가를 받고 일본에 입국합니까?에 없음을 택해주면 된다. 면세 QR 코드 이용은 면세점에서 물건 살 때 여권 보여주는 것 대신에 QR코드로 대체할 수 있도록 새롭게 생긴 것이라고 한다. 여권정보를 등록하고, 기본정보 등록해준다. 비짓재팬웹 VISA 정보 확인 일본 입국 VISA를 가지고 계십니까? 없음을 선택한다. 입국, 귀국 예정 등록 일본은 무비자로 90일까지 체류가 가능하므로, 단순 관광목적이라면 인용하지 않고 등록 진행 선택하면 된다. 입국 귀국 예정 (일본 도착 예정일 / 출발지 / 탑승기 관련 항공사명, 편명) 입력해준다. 입국심사 및 세관신고 사전 등록해놓으면 공항 내 입국심사 카운터에서 QR코드를 제시해서 조금 더 빠르게 가능하다. 도항목적 : 관광 예정 기간 작성 입국심사를 위한 질문들 -&gt; 해당하는 경우가 거의 없을테니 아니오 누르고 넘어가면 된다. 다 등록한 후 QR코드를 볼 수 있는데, 휴대폰에 캡쳐해놓으면 입국심사때 기다리지 않고 더 빨리 들어갈 수 있으니 미리 준비해놓으면 된다. 환전트레블월렛,,, 트레블로그,,,트레블월렛 트레블로그 비교 : https://m.blog.naver.com/daun1217/223058129426 통신ESIM은 기존 유심과 다르게 심을 뻬고 새로 교체해줄 필요가 없다.전달 받은 링크로 들어가 QR 코드를 스캔한 뒤 체크인을 완료하면 준비가 끝난다. 다만 사용 가능한 기종인지 확인해야한다.아이폰의 경우설정 - 셀룰러 - 셀룰러 요금제 추가 (또는 eSIM추가) 탭이 있으면 사용 가능한 단말이다. 또는 “EID”라고 하는 값의 조회를 통해 알아볼 수 있다.스마트폰 다이얼 패드에서 *#06#을 입력하면 조회 가능하다. ESIM 사용법1 : https://m.blog.naver.com/ccll28/223159883500ESIM 사용법2 : https://blog.naver.com/today_hobby/223090987444이지 이심 : https://esimeasy.co.kr/product/JP 출국 1~3일전 등록하면 된다. 나리타 -&gt; 도쿄 도심 N’EX 스카이라이너 도쿄역 긴자 1300엔 버스 (https://blog.naver.com/rladbrud8791/223428551502) 케이세이 본선 교통파스모 패스포트?https://m.blog.naver.com/skli0612/223065650300 기타동전지갑멀티어댑터썬크림 도쿄역 https://m.blog.naver.com/e_eunel/223428769019지유가오카 https://m.blog.naver.com/shinolog/223425924553","link":"/2024/05/12/Tokyo-Travel-01/"},{"title":"RAG란 무엇인가?","text":"RAG는 “Retrieve, Generate, and Rank”의 약자로, 주로 자연어 처리(NLP)와 관련된 작업에서 사용되는 기술방법론이다. 이 접근 방식은 정보검색(IR: Information Retrieval)과 생성적 모델(Generative Models)을 결합하여, 복잡한 질문에 대해 더 정확하고 관련성 높은 답변을 생성하는데 사용된다. RAG 모델은 특히 대규모 텍스트 데이터셋에서 정확한 정보를 검색하고, 이를 기반으로 새로운 텍스트를 생성하여 사용자의 질문에 답변하는 데 효과적이다. 작동방식RAG 모델의 작동 방식은 크게 세 단계로 나눌 수 있다. 검색(Retrieve) : 사용자의 질문이 주어지면, 모델은 대규모 데이터셋에서 질문과 관련된 정보나 문서를 검색한다. 이 과정에서는 주로 효율적인 검색 알고리즘이나 기술이 사용된다. 생성(Generate) : 검색된 정보를 기반으로, 모델은 관련성 높은 답변을 생성한다. 이 단계에서는 생성적 딥러닝 모델, 특히 변형자(Transformer) 기반의 언어 모델이 사용될 수 있다. 순위 매기기(Rank) : 모델이 생성한 여러 답변 중에서 최종 사용자에게 제시될 가장 적합한 답변을 선택하기 위해, 답변들의 순위를 매긴다. 순위 매기기는 답변의 정확성, 관련성, 유용성 등 여러 기준을 고려하여 수행된다. 응용분야RAG 모델은 다양한 NLP 작업에 활용될 수 있으며, 특히 정보가 풍부한 오픈 도메인 질의응답(Open-Domain Question Answering), 자연어 이해(Natural Language Understanding), 챗봇(Chatbots) 등의 분야에서 유용하다. 장점 정확성과 관련성 향상 : RAG 모델은 관련 문서를 검색하여 정보를 기반으로 답변을 생성하기 때문에, 답변의 정확성과 관련성이 향상될 수 있다. 유연성 : 다양한 종류의 데이터와 질문에 적용될 수 있는 높은 유연성을 가진다. 지식 기반 학습 : RAG 모델은 대규모 데이터셋에서 검색한 정보를 활용하여 학습히기 때문에, 지식 기반 학습에 강점을 보인다. 한계 리소스 요구량 : 대규모 데이터셋에서 효율적인 검색과 순위 매기기를 수행하려면 상당한 계산 리소스가 필요할 수 있다. 검색 품질 : 최종 답변의 품질은 검색 단계에서 검색된 문서의 품질에 크게 의존한다. 검색 알고리즘의 성능이 중요하다. RAG는 AI와 NLP 분야에서 중요한 연구주제이며, 지속적인 기술 발전으로 인해 응용 가능성이 확대되고 있다.","link":"/2024/03/26/WhatIsRAG/"},{"title":"합성곱 신경망의 시각화","text":"이번에는 저번 편에서 저장한 합성곱 신경망 모델을 읽어 들인 후 모델의 가중치와 특성 맵을 시각화해본다. 또한 케라스의 함수형 API를 사용하여 모델의 조합을 자유롭게 구성해본다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] 1. 데이터 준비이전에 훈련한 합성곱 신경망 모델을 불러온다. 12from tensorflow import kerasmodel = keras.models.load_model('best-cnn-model.h5') 케라스 모델에 추가한 층은 layers 속성에 저장되어 있다. 1model.layers 12345678[&lt;keras.layers.convolutional.Conv2D at 0x7f803f8dad90&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc61459d0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7f7fc60eed10&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc60eee90&gt;, &lt;keras.layers.core.flatten.Flatten at 0x7f7fc6085c10&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc60eeb90&gt;, &lt;keras.layers.core.dropout.Dropout at 0x7f7fc608a450&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc607c990&gt;] 첫 번째 합성곱 층의 가중치를 조사해 본다. 층의 가중치와 절편은 층의 weights 속성에 저장되어 있다. 12conv = model.layers[0]print(conv.weights[0].shape, conv.weights[1].shape) 1(3, 3, 1, 32) (32,) 커널 크기가 (3,3,1)이며 필터 개수가 32개이므로 첫 번째 원소의 가중치의 크기는 (3,3,1,32)이다.필터마다 1개의 절편이 있으므로 두 번째 원소의 크기는 (32,0)이다. 한편, weights 속성은 텐서플로의 다차원 배열인 Tensor 클래스의 객체이다.numpy() 메서드로 넘파이 배열로 변환한 후 가중치 배열의 평균과 표준편차를 구해본다. 1234conv_weights = conv.weights[0].numpy()# 평균, 표준편차print(conv_weights.mean(), conv_weights.std()) 1출력 -0.019439656 0.23001778 이 가중치의 평균값은 0에 가깝고, 표준편차는 0.23 정도이다. 나중에 이 값을 훈련하기 전의 가중치와 비교해본다. 1. 가중치 시각화하기1-1. 가중치 분포 히스토그램으로 나타내기먼저 이 가중치의 분포를 히스토그램으로 그려본다. 12345import matplotlib.pyplot as pltplt.hist(conv_weights.reshape(-1,1))plt.xlabel('weight')plt.ylabel('count')plt.show() 0을 중심으로 종 모양으로 분포됨을 확인할 수 있다. 1-2. 커널 그림으로 나타내기이번에는 32개의 커널을 16개씩 두 줄에 출력해 본다. 123456789fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : # 32개의 커널을 16개씩 두 줄에 출력 axs[i, j].imshow(conv_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() vmin과 vmax 파라미터로 픽셀의 최댓값과 최솟값을 지정하여 컬러맵으로 표현할 범위를 지정한다.결과를 보면, 이 가중치 값이 무작위가 아닌 어떠한 패턴이 나타난 것을 볼 수 있다. 픽셀의 특정 부분이 밝다거나 하는 식이다. 1-3. 빈 합성곱 신경망의 가중치위와 같은 방법으로, 훈련되지 않은 합성곱 신경망의 가중치를 조사한다. 12345no_training_model = keras.Sequential()no_training_model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))no_training_conv = no_training_model.layers[0]print(no_training_conv.weights[0].shape) 1(3, 3, 1, 32) 3x3 커널을 32개 사용했다. 12no_training_weights = no_training_conv.weights[0].numpy()print(no_training_weights.mean(), no_training_weights.std()) 1-0.0010081615 0.07870515 위의 훈련된 합성곱 신경망과 비교하여 평균은 비슷하지만 표준편차는 매우 작은 것을 알 수 있다. 1234plt.hist(no_training_weights.reshape(-1,1))plt.xlabel('weights')plt.ylabel('count')plt.show() 대부분의 가중치가 -0.15 ~ 0.15 사이에 있고, 고른 분포를 보이는 것을 알 수 있다.텐서플로가 신경망의 가중치를 처음 초기화할 때, 균등 분포에서 랜덤하게 값을 선택하기에 이런 분포를 보인다. 이를 그림으로 시각화한다. 12345678fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : axs[i, j].imshow(no_training_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() 위와 달리 가중치가 밋밋하게 초기화된 것을 볼 수 있다. 이를 통해 합성곱 신경망이 데이터셋의 분류 정확도를 높이기 위해 유용한 패턴을 학습했다는 사실을 알 수 있다. 2. 함수형 API지금까지 신경망 모델을 만들 때 케라스 Sequential 클래스를 사용했다. 이는 층을 차례대로 쌓은 모델을 만드는데, 딥러닝에서는 좀 더 복잡한 모델이 많이 있어 이런 경우는 Sequential 클래스를 사용하기 어렵다. 대신 함수형 API를 사용한다. 함수형 API는 케라스의 Model 클래스를 사용하여 모델을 만든다.그 예로, Dense층 2개로 만들어진 완전 연결 신경망을 함수형 API로 구현해본다. 123456789inputs = keras.Input(shape=(784,))dense1 = keras.layers.Dense(100, activation='signoid')dense2 = keras.layers.Dense(10, activation='softmax')hidden = dense1(inputs)outputs = dense2(hidden)model = keras.Model(inputs, outputs) 케라스는 InputLayer 클래스 객체를 쉽게 다룰 수 있게 Input() 메서드를 별도로 제공한다. 두 개의 층 dense를 만든 뒤, inputs를 dense1에 통과시켜 출력값 hidden을 만들고, 이를 다시 입력값으로 dense2에 통과시켜 출력값을 만들어 이를 모델화한다. 한편, 특성 맵을 시각화하기 위해서는 첫 번째 층인 Conv2D의 출력이 필요하고, 이는 Conv2D 객체의 output 속성에서 얻을 수 있다.모델 객체의 input 속성으로 모델의 입력 또한 얻을 수 있다. 이것을 이용하여 model.input과 model.layers[0].output 을 연결하는 새로운 conv_acti 모델을 만들 수 있다. 1conv_acti = keras.Model(model.input, model.layers[0].output) model 객체의 predict() 메서드를 호출하면 입력부터 마지막 층까지의 계산을 수행한 후 최종 출력을 반환하므로, conv_acti의 predict() 메서드를 호출하여 Conv2D의 출력을 반환할 수 있다. 이를 통해 특성 맵을 시각화해 본다. 3. 특성 맵 시각화케라스 패션 MNIST 데이터셋으로 훈련 세트의 첫 번째 샘플을 그려본다. 123(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()plt.imshow(train_input[0], cmap='gray_r')plt.show() 이 샘플을 conv_acti 모델에 주입하여 Conv2D층이 만드는 특성 맵을 출력한다.입력 차원을 reshape()하고, 255로 나누어 표준화한다. 1234inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv_acti.predict(inputs)print(feature_maps.shape) 1(1, 28, 28, 32) 28x28 크기의 필터 32개로 구성되어 있다. 이를 시각화한다. 12345678fig, axs = plt.subplots(4, 8, figsize=(15,8))for i in range(4) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 32개의 필터로 인해 입력 이미지에서 강하게 활성화된 부분을 보여 준다.이전 가중치 시각화와 비교하여 어떤 부분이 크게 활성화되었는지 파악할 수 있다. 두 번째 합성곱 층이 만든 특성 맵도 같은 방식으로 시각화한다. 12345678910111213conv2_acti = keras.Model(model.input, model.layers[2].output)inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv2_acti.predict(inputs)fig, axs = plt.subplots(8, 8, figsize=(12,12))for i in range(8) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 시각적으로 이해하기 어렵다. 이를 통해, 처음의 합성곱 층은 이미지의 시각적인 정보를 감지하고, 뒤쪽에 있는 합성곱 층은 앞쪽에서 감지한 시각적인 정보를 바탕으로 추상적인 정보를 학습한다고 볼 수 있다.","link":"/2024/04/04/Visualization-Of-CNN/"},{"title":"비지도학습","text":"지도 학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.라벨링 되어있지 않은 데이터로부터 패턴이나 형태를 찾아야 하기 때문에 지도학습보다는 조금 더 난이도가 있다고 할 수 있다.실제로 지도 학습에서 적절한 피처를 찾아내기 위한 전처리 방법으로 비지도 학습을 이용하기도 한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 6장. 비지도 학습] 비지도학습비지도학습의 대표적인 종류는 클러스터링(Clustering)이 있다. 이 외에도 Dimentionality Reduction, Hidden Markov Model이 있다.예를 들어 여러 과일의 사진이 있고 이 사진이 어떤 과일의 사진인지 정답이 없는 데이터에 대해 색깔이 무엇인지, 모양이 어떠한지 등에 대한 피러르 토대로 바나나다, 사과다 등으로 군집화 하는 것이다. 지도/비지도 학습 모델(Semi-Supervised Learning)을 섞어서 사용할 수도 있다. 소량의 분류된 데이터를 사용해 분류되지 않은 더 큰 데이터 세트를 보강하는 방법으로 활용할 수도 있다. 최근 각광받고 있는 GAN(generative Adversarial Network) 모델도 비지도 학습에 해당한다. 과일 분류하기 예시1!wget https://bit.ly/fruits_300_data -O fruits_300.npy 코랩의 코드 셀에서 ‘!’ 문자로 시작하면 코랩은 이후 명령을 파이썬 코드가 아니라 리눅스 쉘 명령으로 이해한다. wget 명령은 원격 주소에서 데이터를 다운로드하여 저장한다. 1234import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy') npy 파일을 load() 메서드를 이용하여 로드한다. 12&gt;&gt;&gt; print(fruits.shape)(300, 100, 100) 첫 번째 차원(300)은 샘플의 개수 두 번째 차원(100)은 이미지 높이, 세 번째 차원(100)은 이미지 너비 1234567&gt;&gt;&gt; print(fruits[0, 0, :])[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 첫 번째 해에 있는 픽셀 100개에 들어 있는 값을 출력하면 위와 같다.이 넘파이 배열은 흑백 사진을 담고 있으므로 0~255까지의 정숫값을 가진다. 이 첫 번째 이미지를 배열과 비교하기 위해 그림으로 그리면 아래와 같다. 12plt.imshow(fruits[0], cmap='gray')plt.show() cmap : 사용할 컬러의 스케일을 지정해줄 수 있음 우리가 보는 것과 컴퓨터가 처리하는 방식이 다르기 때문에 위와 같이 흑백 이미지를 반전하여 사용한다.cmap 매개변수를 ‘gray_r’로 지정하면 다시 반전하여 우리 눈에 보기 좋게 출력 가능하다. 12plt.imshow(fruits[0], cmap='gray_r')plt.show() 이 그림에서 밝은 부분은 0에 가깝고 짙은부분은 255에 가깝다. 픽셀값 분석하기로드 한 데이터의 처음 100개는 사과, 그다음 100개는 파인애플, 마지막 100개는 바나나이다.각 과일 사진의 평균을 내서 차이를 확인해보겠다.사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나눠 보겠다. 123apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100) reshape() 메서드를 사용해 두 번째 차원(100)과 세 번째 차원(100)을 10,000으로 합친다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다. 이제 apple, pineapple, banana 배열의 크기는 100, 10000)이다. 각 배열에 들어 있는 샘플의 픽셀 평균값을 계산하기 위해 mean() 메서드를 사용하겠다.샘플마다 픽셀의 평균값을 계산해야 하므로 mean() 메서드가 평균을 계산할 축을 지정해야 한다.axis=0으로 하면 첫 번째 축인 행을 따라 계산한다.axis=1로 지정하면 두 번째 축인 열을 따라 계산한다. 1234567891011121314&gt;&gt;&gt; print(apple.mean(axis=1))[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] 사과 샘플 100개에 대한 픽셀 평균값을 계산한 것이다. 히스토그램을 그려보면 평균값이 어떻게 분포되어 있는지 한눈에 볼 수 있다. 히스토그램이란히스토그램은 값이 발생한 빈도를 그래프로 표시한 것이다. 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)이다. 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show() 사과와 파인애플은 90~100 사이에 많이 모여있다. 바나나는 픽셀 평균값만으로 사과나 파인애플과 확실히 구분된다. 사과와 파인애플은 많이 겹쳐져있어서 픽셀값만으로는 구분하기 쉽지 않다. 해결책으로 샘플의 평균값이 아니라 픽셀별 평균값을 비교하는 방법이 있다.전체 샘플에 대해 각 픽셀의 평균을 계산하는 것이다.픽셀의 평균을 계산하는 것은 axis=0으로 지정하면 된다. 12345fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show() 순서대로 사과, 파인애플, 바나나 그래프이다. 각 과일마다 값이 높은 구간이 다르다. 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하여 위 그래프와 비교하면 더 좋다.픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있다. 123456789apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show() 세 과일은 픽셀 위치에 따라 값의 크기가 차이난다.이 대표 이미지와 가까운 사진을 골라낸다면 사과, 파인애플, 바나나를 구분할 수 있을 것이다. 이처럼 흑백 사진에 있는 픽셀값을 사용해 과일 사진을 모으는 작업을 해 보았다. 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 한다. 군집은 대표적인 비지도 학습 작업 중 하나이고, 군집 알고리즘에서 만든 그룹을 클러스터(cluster)라고 부른다. k-means앞에서는 사과, 파인애플, 바나나에 있는 각 픽셀의 평균값을 구해서 가장 가까운 사진을 골랐다. 이 경우에는 사과, 파인애플, 바나나 사진임을 미리 알고 있었기 때문에 각 과일의 평균을 구할 수 있었다. 하지만 진짜 비지도 학습에서는 사진에 어떤 과일이 들어 있는지 알지 못한다.이런 경우 어떻게 평균값을 구할 수 있을까? 바로 k-평균(k-means) 군집 알고리즘이 평균값을 자동으로 찾아준다. k-means 알고리즘 작동방식 무작위로 k개의 클러스터 중심을 정한다. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다. 클러스터에서 속한 샘플의 평균값으로 클러스터 중심을 변경한다. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다. k-means 모델 만들기1. 데이터 준비하기1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) 준비된 넘파이 배열을 100*10000 크기로 재배열한다. 2. k-means 알고리즘으로 모델 학습하기사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈에 KMeans 클래스가 구현되어 있다.n-cluster 매개변수로 클러스터 갯수를 지정할 수 있다.3개로 지정 후 모델을 훈련시킨다. 123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruites_2d) 군집된 결과는 KMeans 객체의 labels_ 속성된 결과에 저장된다.클러스터 갯수가 3이기 때문에 배열의 값은 0,1,2 중 하나이다.단, 레이블값과 순서에 의미는 없다. 12345678910&gt;&gt;&gt; print(km.labels_)[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 이를 통해 각 클러스터의 샘플의 갯수를 알 수 있다. 12&gt;&gt;&gt; print(np.unique(km.labels_, return_counts=True))(array([0, 1, 2], dtype=int32), array([111, 98, 91])) 3. 각 클러스터의 그림 출력하기각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수를 만들어본다. 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() draw_fruits()는 (샘플갯수, 너비, 높이)의 3차원 배열을 받아 가로로 10개의 이미지를 출력하는 함수이다. figsize는 ratio 매개변수에 비례하여 커진다. draw_fruits()에 fruits 배열을 불리언 인덱싱을 통해 넣어준다. 1draw_fruits(fruits[km.labels_==0]) 레이블 0에는 파인애플과 바나나, 사과가 섞여있는 것을 볼 수 있다. k-means 알고리즘이 이 샘플들을 완벽하게 분류하진 못했지만, 비슷한 샘플을 잘 모은 것을 볼 수 있다. 사과를 완벽하게 분류 했다. 4. 클러스터 중심KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluser_centers_ 속성에 저장되어 있다.이를 그림으로 표현해보면 아래와 같다. 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 이전에 각 과일의 평균 픽셀값을 출력했던 것과 비슷함을 확인할 수 있다. 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transfor() 메서드와, 데이터를 예측하는 predict() 메서드가 있다.클러스터 중심이 가장 가까운 것이 예측 클래스로 출력된다. 12345&gt;&gt;&gt; print(km.transform(fruits_2d[100:101]))[[3393.8136117 8837.37750892 5267.70439881]]&gt;&gt;&gt; print(km.predict(fruits_2d[100:101]))[0] k-means 알고리즘은 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 과정을 반복하는데, 알고리즘이 반복한 횟수는 n_iter_에 저장된다. 12&gt;&gt;&gt; print(km.n_iter_)4 최적의 k 찾기 : 엘보우 방법k-means 알고리즘의 단점 중 하나는, 클러스터 갯수를 사전에 지정해야 한다는 것이다.군집 알고리즘에서 적절한 k값을 찾는 완벽한 방법은 없다. 저마다 장단점이 있지만 가장 대표적인 엘보우 방법을 알아보겠다. k-means 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이것의 제곱합을 이너셔(inertia)라고 한다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값인데, 클러스터 개수가 늘어나면 이너셔도 줄어든다. 엘보우 방법은 클러스터 갯수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터를 찾는 방법이다. 클러스터 갯수에 대한 이너셔를 그래프로 그리면 꺽이는 지점이 있는데, 그 지점이 바로 적절한 클러스터 갯수이다.KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다. 123456789inertia = []for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show() 주성분 분석차원 축소주성분 분석설명된 분산","link":"/2024/04/01/Unsupervised-Learning/"},{"title":"(프로젝트) 인공지능 스피커를 이용한 음성인식 키오스크 플랫폼","text":"SKT의 음성인식 스피커와 AWS EC2, S3, Node.js, React.js 를 이용, 음성인식 키오스크 플랫폼을 개발하였습니다.기술에 소외된 분들이 키오스크를 이용하는데 어려움을 겪는 것에 문제의식을 느껴 개발하게 되었습니다.또한 기존의 키오스크 플랫폼이 구축 비용이 큰 것에 비해 음성인식 키오스크는 구축 비용이 적습니다. Front-end repository Back-end repository 최종 발표 자료PDF 파일 AbstractVISQUIT는 인공지능 기반의 음성인식 기술로 구동되는 주문 서비스입니다. 요즘에는 많은 상점들이 KIOSK를 사용하고 있는데, 이를 통해 점원을 고용하는 비용과 개별 주문을 하는 데 드는 시간이 줄어들게 되었습니다. 하지만 문제는 키오스크 설치가 여전히 비싸다는 점입니다.이 비용 문제를 해결하기 위해, 우리는 음성 인식 기술을 이용한 NUGU 스피커를 사용할 것을 제안합니다. NUGU 스피커를 사용하면 고객이 음성으로 주문할 수 있습니다. 이 시스템이 한 사람의 발화를 이해하고 서로 소통 할 수 있다면, 이것은 이전의 “사람에 의해서 이루어지던” 주문 시스템을 대체할 수 있습니다. 그리고 더 나아가 KIOSK의 비용 부담 문제도 해결될 것입니다.SK텔레콤의 NUGU 플랫폼에 크게 의존해 음성인식 기능을 구현하고자 합니다. React.js 라이브러리로 UI를 개발하고, Node.js로 백엔드 구조를 구성하고, Docker와 Jenkins로 CI/CD 인프라를 구축할 것입니다. Intoduction1.1 Motivation최근 프랜차이즈 음식점들을 중심으로 키오스크의 설치 및 운영이 증가하는 추세입니다. 이를 통하여 주문 수납에 필요한 인력에 대한 인건비를 절약할 수 있고, 단위 시간에 보다 많은 주문을 처리할 수 있습니다. 하지만 키오스크의 등장으로 인하여 불편을 겪는 사람들도 있습니다. 바로 이러한 디지털 기술이 익숙하지 않은 계층입니다. 특히, 노년층에게 디지털 기술은 그야말로 낯선 괴물에 가깝습니다. 한국정보화진흥원(NIA)의 ‘2018 디지털정보격차 실태조사’에 따르면 만 55세 이상의 장노년층의 종합적인 ‘디지털정보화 수준’은 일반 국민의 63.1%에 불과합니다. 사람을 편리하게 만들고자 등장한 기술로 인하여 소외 받는 계층이 존재하는 모순적인 상황이 발생하는 것입니다. 또한 키오스크는 가격이 비싸기 때문에 자영업자들에게도 큰 비용적인 부담을 안겨준다는 문제 또한 가지고 있습니다. 1.2 Problem Analysis키오스크의 문제점 중에서 키오스크 내부에 탑재되는 주문/결제 소프트웨어의 개선에 초점을 두고자 합니다. 기존 방식은 주문시 모든 메뉴가 화면에 표시됨에 따라 정보량이 많아지고, UI에 익숙하지 않은 사람에게 불편을 초래합니다. 또한 표시되는 정보량을 최대화하기 위하여 글자와 이미지의 크기가 작고, 화면 해상도에 한계가 있어 선명하게 표시되지 않습니다. 또한, 터치 스크린의 감도가 물리 버튼에 비하여 정확하지 않아 사용자에게 혼란스러운 UX를 제공하게 됩니다. 1.3 SolutionSK Nugu Platform의 인공지능 음성 인식 기술을 활용하여, 사용자의 발화를 인식한 뒤 이를 기반으로 주문 과정을 진행합니다. 이를 통하여 디지털 기술에 익숙하지 않은 사용자에게 실제 사람과 대화하는 듯한 인터페이스를 제공하여 친숙하게 접근할 수 있습니다. 또한, 기존 방식에 비하여 복잡한 UI가 없이도 여러 가지 주문을 처리할 수 있습니다. 또한, 전반적으로 큰 이미지와 텍스트를 사용하여 컨텐츠의 선명도를 극대화하고, 시력이 낮은 사용자가 컨텐츠를 보다 잘 파악할 수 있도록 UI를 구성합니다. 컨텐츠의 크기가 증가함에 따라 한 화면에 담을 수 있는 정보량이 줄어들기 때문에, 줄어든 정보량에도 불구하고 기존과 동일한 비즈니스 로직을 구현할 수 있도록 UI를 설계합니다. 또한 기존의 키오스크와 다르게 NUGU 스피커만 있으면 되기 때문에 비용적인 부담도 줄여줄 수 있습니다. Structure전체 구조는 다음과 같습니다. User Interface for Client우리는 고객에게 메뉴 관리와 주문 확인 기능을 제공하고자 합니다. Contributors 김정모 (정보시스템/14) 이하민 (정보시스템/17) 이효식 (경영학부/14) 황성우 (정보시스템/16)","link":"/2020/06/08/VISQUIT/"},{"title":"Kubernetes에 Akka Cluster, Lagom 배포하기","text":"Akka와 Lagom을 Kubernetes 환경에 배포하는 방법을 설명합니다. MiniKube 환경에서 테스트하지만, 상용 Kubernetes 환경에서도 동일한 절차를 따를 수 있습니다. Kubernetes에 Akka Cluster 배포하기Akka Cluster는 JVM 기반의 분산 시스템을 구축할 수 있는 기술입니다. 이를 Kubernetes에서 운영할 경우, 두 가지 주요 이슈가 있습니다: 클러스터 초기 구성 초기 클러스터 구성 시, 어떤 노드가 클러스터를 시작할지 결정해야 합니다. Kubernetes에서는 고정된 IP를 사용할 수 없으므로, 동적으로 Seed Node를 찾아야 합니다. 클러스터 유지 관리 컨테이너 환경에서는 Pod가 언제든지 종료될 수 있으며, 동일한 Pod가 다시 실행되지 않을 수도 있습니다. 새로운 Pod가 기존 클러스터에 자동으로 합류해야 합니다. Kubernetes에서 Akka Cluster 설정 방법Seed Node 설정Kubernetes 환경에서는 시드 노드를 직접 고정할 수 없으므로, akka-management를 활용하여 자동으로 클러스터를 부트스트랩해야 합니다. application.conf 설정 예시1234567891011121314akka { loglevel = &quot;DEBUG&quot; actor.provider = cluster cluster { shutdown-after-unsuccessful-join-seed-nodes = 60s }}akka.management { cluster.bootstrap.contact-point-discovery { discovery-method = kubernetes-api required-contact-point-nr = ${REQUIRED_CONTACT_POINT_NR} }} Health Check 및 Readiness 설정Akka Cluster에서는 노드의 상태를 주기적으로 체크하여 유지 보수합니다. DemoHealthCheck.scala 예제12345678910111213package akka.sample.cluster.kubernetesimport scala.concurrent.Futureimport akka.actor.ActorSystemimport org.slf4j.LoggerFactoryclass DemoHealthCheck(system: ActorSystem) extends (() =&gt; Future[Boolean]) { private val log = LoggerFactory.getLogger(getClass) override def apply(): Future[Boolean] = { log.info(&quot;DemoHealthCheck called&quot;) Future.successful(true) }} Kubernetes 배포 설정akka-cluster.yaml1234567891011121314151617181920212223242526272829303132333435apiVersion: apps/v1kind: Deploymentmetadata: name: akka-cluster namespace: akka-systemspec: replicas: 3 selector: matchLabels: app: akka-cluster template: metadata: labels: app: akka-cluster spec: containers: - name: akka-cluster image: akka-sample-cluster-kubernetes:latest ports: - name: management containerPort: 8558 - name: http containerPort: 8080 readinessProbe: httpGet: path: /ready port: management initialDelaySeconds: 10 periodSeconds: 10 livenessProbe: httpGet: path: /alive port: management initialDelaySeconds: 20 periodSeconds: 10 참고 자료 Akka 공식 문서 Kubernetes 공식 문서 결론Kubernetes에서 Akka Cluster와 Lagom을 배포하는 것은 컨테이너 환경의 동적 속성을 고려해야 합니다. akka-management 및 kubernetes-api 기반의 자동 클러스터 부트스트랩을 통해 문제를 해결할 수 있습니다.","link":"/2022/03/28/akkaLagomOnKubernetes/"},{"title":"도커에 카산드라 올리기","text":"웹서비스를 운영하려면 보통 웹서버, WAS, DB를 배포해야 합니다.Akka나 Lagom을 활용할 경우, Cassandra와 Kafka가 기본적으로 필요합니다.이 글에서는 Docker를 이용하여 Cassandra를 배포하는 방법을 다룹니다. Cassandra Docker 배포 개요Cassandra를 Docker에 올리는 것은 로컬 개발 환경에서의 테스트나 클러스터 구축을 쉽게 만들기 위해 유용합니다. 주요 개념 Dedicate Server: 단독 물리 서버에서 실행되는 환경 Virtual Machine (VM): 여러 서비스가 같은 물리 서버에서 실행되는 환경 Container (Docker): 가벼운 가상 환경에서 독립적으로 실행되는 방식 Cassandra 설치 및 실행Docker에서 Cassandra 컨테이너 실행1docker run --name cassandra -p 9042:9042 -d cassandra:3.11.3 컨테이너 이름: cassandra 포트 포워딩: 9042:9042 (Cassandra 기본 클라이언트 포트) 이미지: cassandra:3.11.3 실행된 컨테이너 확인1docker ps 출력 예시: 12CONTAINER ID IMAGE COMMAND STATUS PORTS NAMES84e82de8f1a9 cassandra:3.11.3 &quot;docker-entrypoint.s…&quot; Up 17 seconds 7000-7001/tcp, 7199/tcp, 9160/tcp, 0.0.0.0:9042-&gt;9042/tcp cassandra CQLSH를 사용하여 Cassandra 접속1docker run -it --rm cassandra:3.11.3 cqlsh host.docker.internal 출력 예시: 12Connected to Cassandra Cluster at host.docker.internal:9042.[cqlsh 5.0.1 | Cassandra 4.0.1 | CQL spec 3.4.5 | Native protocol v5] 추가 설정 및 참고 자료 설정 파일 경로: /etc/cassandra/cassandra.yaml 데이터 저장 위치: /var/lib/cassandra Docker Hub - Cassandra Cassandra 공식 문서 결론Docker를 이용해 Cassandra를 로컬 개발 환경에 쉽게 배포할 수 있습니다.이를 통해 개발, 테스트 및 분산 데이터베이스 구축을 간편하게 진행할 수 있습니다.","link":"/2022/03/19/cassandraOnDocker/"},{"title":"큐로 스택 구현하기","text":"두 개의 큐를 이용해 스택을 구현해보자. 두 개의 큐가 존재한다.목적에 따라 메인 큐와 서브 큐로 생각해보자. 흐름은 아래와 같다. 메인큐에 item이 입력될 때. (입력 전) 기존 메인큐에 존재하는 아이템들은 순서대로 서브큐에 담는다. 메인큐에 item이 입력되고 서브큐에 있던 아이템들을 순서대로 메인큐에 다시 담는다. 1이 입력될 때 기존에 메인큐에 있던 Item이 없기 때문에 그대로 입력된다. 2가 입력될 때, 메인큐에 기존에 있던 item을 서브큐로 옮기고메인큐에 2가 입력된 후, 서브큐의 item들을 메인큐로 옮긴다. 3이 입력될 때, 기존에 있던 메인큐 item들을 서브큐로 옮기고3이 입력된 후, 서브큐에 존재하는 item들을 다시 메인큐로 옮긴다.","link":"/2020/10/21/constructStackByTwoQueue/"},{"title":"Why Hexo?","text":"Hexo framework 와 Git을 이용 간단한 블로그설정에 대한포스팅입니다. Why select hexo?다른 프레임워크간의 비교정적페이지 블로그 프레임워크 중에서 유명한것 중에 대표적으로 Hexo, Hugo, Jekyll 이 있다. Hexoㅡ Node js 기반ㅡ 한글 레퍼런스 + 사용자수 꽤 많음ㅡ 테마 여러개 많음ㅡ 플러그인과 확장성 좋다함ㅡ 근데 죄다 중국개발자님들… Hugoㅡ Go 기반ㅡ 빌드 빠르다고 함ㅡ 문서화 깔끔함ㅡ 한글 레퍼런스 거의 없음ㅡ 사실 뭐가 좋은지 모르겠다… Jekyllㅡ 루비 기반ㅡ 사용자수 제일 많음ㅡ 한글 레퍼런스도 제일 많음ㅡ 근데 느림… ( 10개 포스팅시 5분이상 걸렸음)ㅡ 윈도우 지원 이슈 많음… 라고 비교를 해봤는데결국 Jekyll 는 루비기반인데 포스팅속도나 윈도우 지원 이슈 때문에 포기Hugo 랑 Hexo 둘중 고민중 차라리 Hugo 의 커스텀을 위해선 Go Lang 으로 다루어야 하는데레퍼런스도 많이 없고 깃스타수는 많은데 보통 수정배포자가 아닌 이용자들만 많은 현황그래서 Hexo로 결정하였다. 12const var = &quot;Hi&quot;;console.log(var);","link":"/2020/03/17/Why-Hexo/"},{"title":"권한 관리(1) - chmod란","text":"유닉스 및 유닉스 유사 운영 체제에서 chmod는 파일 시스템 개체(파일 및 디렉토리)의 액세스 권한을 변경하는 데 사용되는 명령 및 system call이다.특수 모드 플래그를 변경할 때도 사용된다. 요청은 umask로 필터링된다. 이름은 Change mode의 약칭이다. 접근 권한 변경 (chmod) chmod 명령 : 기존 파일 또는 디렉토리에 대한 접근 권한 (파일 모드)을 변경할 때 사용 파일 모드의 변경은 파일 소유자나 슈퍼 유저만 가능 파일 모드는 기호(문자)나 8진수로 지정 주요 옵션 옵션 설명 -R 하위 디렉토리와 파일의 권한 까지 변경 (동일 권한을 디렉토리와 파일에 저장) 접근 권한을 기호 (문자)로 표현 문자 표기법은 변경할 사용자(대상), 수행할 명령(연산), 설정할 퍼미션(접근 권한) 세 부분으로 분류 됨 다른 대상의 속성을 건드리지 않고 한 대상 속성만 설정할 수 있는 장점을 가짐 복수 지정은 콤마(,)를 사용하여 구분 구분 기호 설명 사용자 (대상) u user의 약자, 파일이나 디렉토리 소유자를 의미 g 그룹 소유자 o others의 약자, 기타 사용자를 의미 a all의 약자, u, g, o의 조합 아무 문자를 사용하지 않으면 all로 추정 수행할 명령 (연산) + 추가 - 제거 = 지정, 기존의 속성 값은 사라짐 설정 할 퍼미션 (접근 권한) r 읽기 w 쓰기 x 실행 사용 예시 표기법 의 미 u+x 소유자에게 실행 권한을 추가 u-x 소유자의 실행 권한 제거 +x 모든 사용자 (소유자, 그룹, 기타 사용자)에게 실행 권한 추가 o-rw 기타 사용자의 읽기, 쓰기 권한 제거 go=rw 그룹, 기타 사용자에게 읽기. 쓰기 권한 지정, 이전에 실행 권한을 가지고 있다면 제거 됨 u+x,go=rw 소유자에게 실행 권한을 추가하고 그룹, 기타 사용자에게 읽기, 쓰기 권한 지정 a=rw 모든 사용자에게 읽기, 쓰기 권한 지정 g-w 그룹 소유자의 쓰기 권한 제거 g+w, o-x 그룹 소유자에게 쓰기 권한 추가, 기타 사용자의 쓰기 권한을 제거 접근 권한을 8진수로 표현 원하는 파일의 접근 권한을 설정하기 위해 8진수 표기법을 사용 3자리의 8진수로 소유자, 그룹 소유자, 기타 사용자를 위한 파일 모드를 설정 할 수 있음. r(읽기, 4), w(쓰기, 2), x(실행, 1) 2진법과 9진법의 파일 모드 8진법 2진법 파일모드 0 000 — 1 001 –x 2 010 -w- 3 011 -wx 4 100 r– 5 101 r-x 6 110 rw- 7 111 rwx chmod 사용 예시","link":"/2020/04/09/chmod-types/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Reactive Design Pattern Actors Cluster Cassandra Event Sourcing &amp; CQRS Persistence(Event Sourcing) Lagom core concepts Docker &amp; Kubernetes Cassandra, Zookeeper, Kafka on Docker Akka, Lagom on Kubernetes Programming in Scala Webflux","link":"/2019/09/01/hello-world/"},{"title":"ARM 및 그 응용 프로그램에서 하드웨어 지원 명령 레벨 분리 (2)","text":"이 포스트는 한양대학교 CSE 융합세미나의 조영필 교수님의 강연을 바탕으로 작성되었습니다. ARM architecture는 최근 스마트폰, IoT 디바이스 등에서 매우 폭발적으로 많이 사용되고 있는 아키텍쳐이다. 그리고 이 ARM 아키텍쳐에는 다양한 하드웨어 feature들이 있다. 이러한 feature들을 이용한 security system을 만들어 이를 이용해 어떻게 우리가 보안성을 향상시킬 수 있는가가 이 포스트의 포인트이다. 앞에서 본 바와 같이","link":"/2020/03/27/cse-Seminar1-1/"},{"title":"ARM 및 그 응용 프로그램에서 하드웨어 지원 명령 레벨 분리 (1)","text":"이 포스트는 한양대학교 CSE 융합세미나의 조영필 교수님의 강연을 바탕으로 작성되었습니다. ARM architecture는 최근 스마트폰, IoT 디바이스 등에서 매우 폭발적으로 많이 사용되고 있는 아키텍쳐이다. 그리고 이 ARM 아키텍쳐에는 다양한 하드웨어 feature들이 있다. 이러한 feature들을 이용한 security system을 만들어 이를 이용해 어떻게 우리가 보안성을 향상시킬 수 있는가가 이 포스트의 포인트이다. 우리는 다양한 보안 위협에 처해있다. 그렇기 때문에 Security Mechanism을 만들어 보안 위협을 대비해야 한다. 이러한 Security Mechanism들은 software only로 개발되어 왔는데 가장 큰 이유는 적용이 유리하기 때문이다. 하지만 이러한 software mechanisma들은 performance overhead가 상당하다. 우리가 시스템을 구현함에 있어서 5%만 느려져도 느려진것이라고 느끼는데 50%~300%의 상당히 overhead가 많다고 볼 수 있다.이 overhead를 줄이기 위해서는 security guarantee를 줄여야 하는데 그렇게 되면 보안성이 낮아진다. 그래서 이를 해결하기 위해서 Security Mechanism을 구현함에 있어서 hardware의 도움을 받아야 한다. Isolation Security Mechanism에서 효과적인 “code size”를 줄임으로써 공격 가능성을 줄인다. 왜 Code Size를 줄이는가? 시스템을 개발하는 과정에서 코드가 길어질 수록 code의 vulnerability가 발생할 가능성이 높기 때문. small SW entity의 경우 본인을 직접 공격하는 경우도 있겠지만 Large SW entity를 장악하고 small SW entity공격도 가능. 이 문제를 해결하기 위해 isolate 해야 한다. 이 isolate의 적용 예는 다음과 같다 intel SGX의 경우 Enclave가 완전히 격리 되어있다. 이러한 isolation을 Domain-level Isolation이라고 한다.un-authorized code와 Authorized code 두 가지로 구분해 둘 수 있다.두 코드는 서로 다른 access permision을 가진다.un-authorized로 접근하려면 Crossing isolation boundaty를 뛰어 넘어야 하는데 이는 매우 무거운 연산이다.그렇기 때문에 드물게 로그인 되는 경우에만 유용하다. Domain-level Isolation의 한계. 빈번하게 접속되는 데이터에 대해서는 민감하다 예) 스택의 리턴주소 heap objects들의 fuction pointer들 로그인 여부 flag variable 같은 환경 변수들 이렇게 빈번하게 access되는 return address attack을 막기 위한 방법에는 Shadow Stack이라는 방법이 있다. Shadow stack은 앞에서 나온 isolation을 이용한 security mechanism이다.원래 프로그램 내부에는 stack이 하나만 있지만, Shadow stack에서는 별도의 stack을 하나 더 둔다.요약하자면 shadow stack만 isolation 되어있으면 return address가 잘 보호된다.하지만 매번 함수호출마다 위처럼 네번의 crossing이 있다면 overhead가 커지게 될 것이다. Instruction-level Isolation은 위의 Domain-level Isolation과는 다르게 instruction 자체에 access permission을 부여.이 경우 instruction 사이는 특별히 isolation boundary를 넘어갈 필요가 없다.그렇기 때문에 Instructuon-level Isolation은 frequently 하게 access 되는 data 보호에 유리하다. 그렇다면 이러한 instruction-level isolation을 어떻게 구현할까?결국 키는 instruction별로 서로 다른 access permision을 줄 수 있느냐가 핵심이다.그래서 이것을 구현할 수 있는 한가지는 segment라는 것이다.이것은 x86-32bit에서만 지원이 된다.각각의 segment register 별로 access permission과 bit수와 limit을 설정할 수 있는 Segment Descriptor가 존재한다.그리고 instruction에 어느 segment register에 접근하는지 지정할 수 있다.instruction 간에는 특별히 isolation boundary를 넣지 않아도 자연스럽게 segment descriptor에 따라 access permission 차이가 날 것이다. SFI는 instruction 앞에 체크하는 코드를 넣는 것이다.이 방법의 단점은 하나만 보면 overhead가 크지 않지만 sensitive data 접속 빈도보다 normal data 접속 빈도가 더 많을 텐데 대부분의 instruction에 chk 코드를 넣어야 한다는 것이다.결과적으로 전체적인 performance overhead가 수십퍼센트 증가할 것이다. 조금 더 효율적인 방법이 Information hiding이다.Sensitive data를 임의의 장소에 위치를 시키고 그 위치를 특정 instruction에만 저장하는 것이다.그래서 그 위치를 알고있는 instruction은 접근이 가능하지만 그렇지 않은 경우는 접근 불가능하다.단점은 확률적인 보안이라는 것이다. 즉, normal instruction이 sensitive data의 위치를 모를것이라고 확신할 수 없다. 그래서 이 논문은 어떻게 ARM에서 instruction-level Isolation을 구현 할 것인가를 다루었다.메모리에 대해서 access를 하는데 일반적인 access permission과는 다른방법으로 access permission이 주어지는 그런 special한 memory instruction이 있을까에 대한 것이다. LSU를 이용해 보자는 것이 이 논문의 주장이다.왼쪽의 표가 ARM architecture에서 사용하는 memory instruction이다.일반적인 load, store와 unprivileged가 붙은것들의 차이점은 우리가 원래 memory access를 한다고 했을 때, 그 load store instruction이 access 할 수 있는 memory는 결과적으로 load store instruction이 수행 될 때 프로세스가 어떤 mode에 있었는가에 따라서 결정된다.가령 우리가 load store instruction을 application에서 실행했다면, 그러면 이 load, store instruction은 보통 프로세서가 unprivileged 모드로 작동한다. 쉽게 말하면 usermode이다. 그러니까 이 때는 user memory로 설정되어있는 memory만 access 할 수 있고, kernal의 memory는 access 불가능하다.반대로 이 instruction을 kernal에서 실행했다면 보통 privilleged 모드로 실행된다. 그러니까 이 때 load store instruction은 kernal 모드로 chk되어 있는 메모리에 access 가능하다. 뿐만 아니라 user space로 설정되어있는 메모리에도 접근 가능하다. 오른쪽 표는 instruction에 설정해줄 수 있는 memory access permission 여부이다. 이 access permission은 page table에 설정 할 수 있다. 여기서 알 수 있는 점은 privileged 모드와 unprivileged 모드 간에는 access permission에 차이가 나는 경우가 있다 load, store instruction은 어떤 mode에서 실행되느냐에 따라서 access permission이 결정.","link":"/2020/03/25/cse-Seminar1/"},{"title":"Insertion Sort란?","text":"삽입 정렬이란 선택한 요소에 알맞은 위치에 삽입하는 작업을 반복하여 정렬하는 알고리즘이다.자료 배열의 모든 요소를 앞에서부터 차례대로 선택하고이미 정렬된 배열 부분과 비교 후 알맞은 위치를 찾아 삽입함으로써 정렬을 완성하는 알고리즘이다. 동작 방식 Key값을 정렬된 배열 부분과 비교하여 알맞은 위치에 삽입.삽입 전, x는 정렬되지 않은 부분의 첫 번째 요소로 선택된 키 값이다. 삽입 후, 삽입 된 부분에서 x거 들어갈 알맞은 위치에 삽입하게 된다. 정렬되지 않은 항목 중 첫 번째 항목이 다음 key 값으로 지정된다. 특징 장점 간단한 구현 같은 값 사이에는 상대적 위치 변화가 없음 Stable한 정렬임 주어진 자료공간 이외의 공간을 사용하지 않음 단점 입력 데이터 배열의 길이가 길어질수록 비효율적 역으로 정렬되어 있는 입력 값에 대해서 최악의 성능을 보임 (모든 입력값에 대해 삽입이 필요) 시간복잡도 T(n) = c1n + c2(n-1) + c4(n-1) + c5(n(n+1)/2-1) + c6(n(n-1)/2) + c7(n(n-1)/2) + c8(n-1) T(n) = O(n^2) 예제 코드1234567891011121314151617181920212223242526272829303132333435import java.util.Scanner;// 단순 삽입 정렬class InsertionSort { // 단순 삽입 정렬 static void insertionSort(int[] a, int n) { for (int i = 1; i &lt; n; i++) { int j; int tmp = a[i]; for (j = i; j &gt; 0 &amp;&amp; a[j - 1] &gt; tmp; j--) a[j] = a[j - 1]; a[j] = tmp; } } public static void main(String[] args) { Scanner stdIn = new Scanner(System.in); System.out.println(&quot;단순 삽입 정렬&quot;); System.out.print(&quot;요솟수：&quot;); int nx = stdIn.nextInt(); int[] x = new int[nx]; for (int i = 0; i &lt; nx; i++) { System.out.print(&quot;x[&quot; + i + &quot;]：&quot;); x[i] = stdIn.nextInt(); } insertionSort(x, nx); // 배열 x를 단순 삽입 정렬 System.out.println(&quot;오름차순으로 정렬했습니다.&quot;); for (int i = 0; i &lt; nx; i++) System.out.println(&quot;x[&quot; + i + &quot;]＝&quot; + x[i]); }}","link":"/2020/10/18/insertionSort/"},{"title":"FastAPI를 이용하여 llm 모델 서빙하는 서비스 EC2에 배포하기","text":"fastAPI란 파이썬 3.6부터 제공되는 트랜디하고 높은 성능을 가진 파이썬 프레임워크이다. t5-small 모델 서빙하는 서버 테스트 server.py12345678910111213141516171819202122232425262728293031323334353637383940from fastapi import FastAPIfrom fastapi.responses import RedirectResponsefrom langserve import add_routesfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipelinefrom transformers import pipelinefrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom starlette.requests import Requestfrom starlette.responses import JSONResponse, Responsefrom dotenv import load_dotenvfrom transformers import AutoTokenizer, pipelineimport torchimport requestsapp = FastAPI()@app.get(&quot;/&quot;)async def redirect_root_to_docs(): return RedirectResponse(&quot;/docs&quot;)# T5 모델 사용pipe = pipeline(&quot;text2text-generation&quot;, model=&quot;t5-small&quot;)model = HuggingFacePipeline(pipeline=pipe)add_routes( app, model, path=&quot;/t5-small&quot;,)@app.get(&quot;/plugin/test&quot;)async def test_plugin(): return &quot;success&quot;if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=5002) Dockerfile123456789101112131415161718192021222324252627# 베이스 이미지 설정FROM python:3.11-slim# 작업 디렉토리 설정WORKDIR /app# 필요한 시스템 패키지 설치RUN apt-get clean &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# requirements 파일들 및 미리 다운로드한 패키지 복사COPY requirements1.txt .COPY requirements2.txt .COPY packages /app/packages# 첫 번째 requirements 파일 설치RUN pip install --no-cache-dir -r requirements1.txt --default-timeout=300 -i https://pypi.tuna.tsinghua.edu.cn/simple# 두 번째 requirements 파일 설치 (미리 다운로드한 패키지 포함)RUN pip install --no-cache-dir -r requirements2.txt --find-links=/app/packages --default-timeout=300 --extra-index-url https://pypi.nvidia.com# 애플리케이션 소스 코드 복사COPY . .# FastAPI 서버 실행CMD [&quot;uvicorn&quot;, &quot;app.server:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;5002&quot;] Requirements1.txt123456789fastapiuvicornlangservelangchain-communitylangchain-openailangchain-coretransformerspython-dotenvrequests Requirements2.txt123torchnvidia-cudnn-cu12sse_starlette 이미지 빌드하기1% docker build -t lm-test-server:20240724 . 도커로 실행하기123456789101112131415161718192021222324252627% docker run --name lm-test-server -p 5002:5002 lm-test-server:20240724/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`. warn_deprecated(INFO: Started server process [1]INFO: Waiting for application startup.INFO: Application startup complete.INFO: Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit) __ ___ .__ __. _______ _______. _______ .______ ____ ____ _______| | / \\ | \\ | | / _____| / || ____|| _ \\ \\ \\ / / | ____|| | / ^ \\ | \\| | | | __ | (----`| |__ | |_) | \\ \\/ / | |__| | / /_\\ \\ | . ` | | | |_ | \\ \\ | __| | / \\ / | __|| `----./ _____ \\ | |\\ | | |__| | .----) | | |____ | |\\ \\----. \\ / | |____|_______/__/ \\__\\ |__| \\__| \\______| |_______/ |_______|| _| `._____| \\__/ |_______|LANGSERVE: Playground for chain &quot;/openai/&quot; is live at:LANGSERVE: │LANGSERVE: └──&gt; /openai/playground/LANGSERVE:LANGSERVE: Playground for chain &quot;/t5-small/&quot; is live at:LANGSERVE: │LANGSERVE: └──&gt; /t5-small/playground/LANGSERVE:LANGSERVE: See all available routes at /docs/LANGSERVE: ⚠️ Using pydantic 2.8.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details. 브라우저로 접속하여 테스트하기http://localhost:5002/docs로 접속하여 테스트한다. EC2에 서비스 올리기EC2에 도커 설치12345678# sudo yum update -y# amazon-linux-extras install docker -y# service docker startRedirecting to /bin/systemctl start docker.service# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.# usermod -aG docker $USER# newgrp docker 이미지 옮기기ECR에 도커이미지를 push 한다. 도커 로그인123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 도커 이미지 푸시1234# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latestdocker push &lt;your-dockerhub-username&gt;/my-project:latest EC2에서 이미지 pull 하기1234567$ sudo yum install aws-cli -y$ aws configure$ export AWS_ACCESS_KEY_ID=[access_key_id]$ export AWS_SECRET_ACCESS_KEY=[aws_secret_access_key]$ export AWS_SESSION_TOKEN=[aws_session_token]$ aws ecr get-login-password --region [region] | docker login --username AWS --password-stdin [id].dkr.ecr.[region].amazonaws.com$ docker pull [id].dkr.ecr.[region].amazonaws.com/t5-small-fastapi:20240727 배포하는 EC2에 외부 인터넷이 연동되지 않기에 모델을 다운로드받아 이미지 업로드download_model.py 12345678910111213# download_model.pyfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizermodel_name = &quot;t5-small&quot;# 모델과 토크나이저 다운로드model = AutoModelForSeq2SeqLM.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)# 모델 저장model.save_pretrained(&quot;./model&quot;)tokenizer.save_pretrained(&quot;./model&quot;) 1% brew install transformers 도커 배포 시 참고 사항start.sh 스크립트에 Python 애플리케이션을 백그라운드에서 실행하도록 설정하면 Docker 컨테이너가 종료되는 원인이 될 수 있다. 따라서, 애플리케이션이 포그라운드에서 실행되도록 수정해야 한다. 수정된 start.sh1234567#!/bin/sh# Install dependenciespoetry install# Run the application in the foregroundpoetry run python3 src/main.py Docker 빌드 중에 “No space left on device” 오류Docker 빌드 중에 “No space left on device” 오류가 발생하는 경우, 이는 Docker 데몬이 실행되고 있는 호스트 머신의 디스크 공간이 부족하기 때문에 발생하는 문제 불필요한 Docker 이미지 및 컨테이너 삭제1234567891011# 중지된 모든 컨테이너 삭제docker container prune -f# 사용되지 않는 모든 이미지 삭제docker image prune -a -f# 사용되지 않는 모든 네트워크 삭제docker network prune -f# 사용되지 않는 모든 볼륨 삭제docker volume prune -f IPv4 바인딩 에러12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455Installing dependencies from lock fileNo dependencies to install or updateInstalling the current project: orchestrator (0.1.0)/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain/chat_models/__init__.py:31: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:`from langchain_community.chat_models import ChatOpenAI`.To install langchain-community run `pip install -U langchain-community`. warnings.warn(2024-07-29 11:42:49,494 - WARNING - WARNING! max_length is not default parameter. max_length was transferred to model_kwargs. Please make sure that max_length is what you intended.2024-07-29 11:42:49,504 - DEBUG - Starting new HTTPS connection (1): huggingface.co:4432024-07-29 11:42:49,738 - DEBUG - https://huggingface.co:443 &quot;GET /api/whoami-v2 HTTP/11&quot; 200 7572024-07-29 11:42:49,740 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False2024-07-29 11:42:49,741 - DEBUG - load_verify_locations cafile='/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/certifi/cacert.pem'2024-07-29 11:42:49,771 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False2024-07-29 11:42:49,772 - DEBUG - load_verify_locations cafile='/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/certifi/cacert.pem'INFO: Started server process [24]INFO: Waiting for application startup.INFO: Application startup complete.ERROR: [Errno 99] error while attempting to bind on address ('::1', 8002, 0, 0): cannot assign requested addressINFO: Waiting for application shutdown.INFO: Application shutdown complete.The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: fineGrained).Your token has been saved to /root/.cache/huggingface/tokenLogin successful main.py 수정src/main.py 파일에서 서버를 실행하는 부분을 확인하고, host 매개변수를 0.0.0.0으로 설정","link":"/2024/07/24/fastAPI/"},{"title":"instagram Clone하기 (Backend) 01-Prisma Setting하기","text":"instagram을 클론하는 프로젝트를 진행해보겠습니다.Backend는 node.js와 prisma, typescript 그리고 Frontend는 node.js, React-native, apollo를 이용하여 개발할 예정입니다.이 프로젝트 내용은 NomadCoders 를 참고하여 진행할 예정입니다. 0.0. What are we building (사용 기술 스택) node.js, react.js, react-native, prisma, graphql, apollo, typescript. node.js (서버 기술) prisma (데이터베이스 ORM) react.js (프론트엔드 기술), react-native를 앱을 만드는 데에 사용. ios, 안드로이드, 프론트엔드, 백엔드 모두 javascript로 할 것. 0.1. Requirements. 터미널에서 yarn이나 npm을 실행 해야 함. yarn 설치 할 것 (Homebrew 통해 설치). node version은 v10.15.2. visual studio를 에디터로 사용. Git 사용 prisma.io 사용. Google chrome. lot of coffee. 1.0. Setting up the project yarn init. yarn add graphql-yoga yarn add nodemon-D =&gt; src/server.js에 있는 코드를 실행하는 script를 작성하는데에 nodemon이 필요. yarn add babel-node =&gt; babel-node 설치. yarn add babel-cli -D nodemon –exec babel-node src/server.js =&gt; dev 명령어 추가. yarn dev를 실행하면, ‘nodemon –exec babel-node src/server.js’를 실행하게 되는 것. nodemon.json의 ext는 nodemon이 감시해야 할 파일의 확장자들을 지정할 수 있음. babel은 멋진 코드를 못생긴 코드로 바꿔주는 도구. 멋진 코드는 아직 지원되지 않는 곳들이 있어서, 못생긴 코드로 바꿔줘야 함. nodemon은 파일을 저장할 때마다 실행을 새로 해주는 도구, 서버를 껐다가 켤 필요가 없음. 1.1. Creating GraphQL Server 서버를 먼저 세우고, 그 다음에 prisma를 서버 코드에 추가할 것. dotenv 모듈은 .env 파일을 읽음. dotenv config에서 포트를 읽어오도록 할 수 있음. =&gt; .env 파일에 Port를 추가하면 됨. PORT 넘버 등 모든 설정값들을 env에 추가하는건 좋은 습관이다. node가 import를 인식하지 못한다면 .babelrc 파일 추가해야 함. .babelrc 파일에 presets를 입력. @babel/preset-env가 가장 최신의 프리셋. 같은 패키지에서 여러 모듈을 설치하고 싶으면 yarn add @babel/{node, preset-env} 이런식으로 할 수 있음. 1.2. Setting up the Server. morgan은 미들웨어이자 logger(로깅 전용 모듈)이다. GraphQLServer에는 express 서버가 내장되어 있다.typeDef와 resolvers를 추가하지 않고, 더 프로페셔널한 방식으로 추가하는 방법. src/api라는 폴더를 만들어 schema.js를 만들고 이 파일에 모든 파일들을 합침. api 폴더 안에 아주 많은 graphql 파일들이 추가될 것이고, 같은 위치에 resolvers파일들이 추가될 것. 그 파일들을 이 schema.js 파일로 밀어 넣을 것. 서버에는 schema.js 파일 하나만 입력해주면 됨. 서버에는 typeDefs와resolvers가 모두 입력 되어야 함. 아니면 그것을 schema 파일에 합쳐서 입력해도 됨. schema 파일이 typeDefs와 resolvers를 모두 가지고 있으면 됨. 폴더를 만들고 graphql 파일들과 resolver 파일들을 만들고, 그것들을 모아서 import해주는 방식! api 폴더 밑에 images, comments, likes, users 이렇게 폴더를 나눠서 만드는 것이 좋다. **는 모든 폴더고, *.graphql은 모든 .graphql 파일. api 폴더의 모든 파일들을 schema 파일에서 한번에 받음. server에는 typeDef와 resolvers를 따로 입력하거나, 하나로 합쳐진 schema를 입력하면 됨.","link":"/2020/03/24/instagramClone-1/"},{"title":"instagram Clone하기 (Backend) 03-Backend Resolvers (1)","text":"이번 포스팅에서는 로그인 시 인증, Like, follow, 유저 검색, 포스트 검색 그리고 댓글 달기 Resolver를 만들어 보겠습니다이 프로젝트 내용은 NomadCoders 를 참고하여 진행할 예정입니다. 3.6. Passport JWT part Three env.js 파일 생성. =&gt; passport와 utils에 dotenv를 불러오지 않고 process.env를 사용할 수 있음. passport는 쿠키와 세션 작업을 하기에 좋다. 쿠키를 가져오고 만들어주고 모든 일을 한다. express에서는 미들웨어를 지나서 라우트가 실행 됨. 토큰을 받아서 해석하고, 사용자를 찾고, 사용자가 존재한다면 req 객체에 사용자를 추가하고 나면 graphql 함수를 실행. 로그인 되어 있다면 모든 graphql 요청에 사용자 정보가 추가되어서 요청되는 것. context는 resolver 사이에서 정보를 공유할 때 사용. 예를 들어, prisma를 server.js에서 한 번만 import 한 후에 prisma를 context에 추가할 수 있음. 하지만 vscode가 이 방식을 이해하지 못한다. =&gt; 자동완성 사용 불가능. 많은 사람들이 prisma를 context에 db라는 이름으로 담아 사용한다.context로 담은 후에, context.db.user 처럼 호출. 서버에 전달되는 모든 요청은 authenticateJwt 함수를 통과함. confirmSecret으로 토큰 받기 command 123mutation { confirmSecret(email:&quot;ggamini7@gmail.com&quot;, secret:&quot;enchanting plantation&quot;)} result 12345{ &quot;data&quot;: { &quot;confirmSecret&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot; }} requestSecret으로 request 안에 user가 있는지 확인하기 command 123mutation{ requestSecret(email:&quot;ggamini7@gmail.com&quot;)} HTTP Header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} result 12345{ &quot;data&quot;: { &quot;requestSecret&quot;: false }} console.log(request.user) 1234567{ email: 'ggamini7@gmail.com', username: 'gigi', loginSecret: 'enchanting plantation', bio: '', lastName: 'Buffon', firstName: 'Gianluigi', id: 'ck77tw4291rqr0905yy5shirp' } passport workflow authenticateJwt 함수에서는 passport.authenticate(“jwt”) 함수를 실행 이 함수는 Stretegy를 활용해서 jwt 토큰을 추출 토큰이 추출되면 verifyUser를 payloa와 함께 실행. payload는 토큰에서 해석된 id를 받아서, user를 찾아서 리턴. 콜백 함수 authenticate(“jwt”, (error, user))가 실행되어, 사용자가 있으면 그 사용자를 req에 추가. sever.js에서 context에 request를 담아줌. 3.7. toggleLike Resolver toggleLike.js 및 toggleLike.graphql 파일 생성 middlewares.js 파일 생성 =&gt; graphql middleware 같은 것. 인증을 필요로 하는 Resolver들이 많은데 그 때마다 로그인 하라는 함수 만들 수 없으니 middleware 만들어서 한번에 처리. confirmSecret으로 로그인 하고 그 토큰을 받아 toggleLike 할 때 사용. toggleLike.js에서 만약 좋아요가 존재 하면, 이걸 지워야 하고, 좋아요가 존재하지 않는다면 이 사용자가 갖고 있는 그리고 이 포스트가 갖고 있는 좋아요를 만들 것. 3.8. addComment Resolver Like를 얻는 것과 지우는 것이 같은 방식이므로 toggleLike.js에 filterOptions라는 변수 만들어 줌. post에 like 하기 command 123mutation { toggleLike(postId:&quot;ck77phhmq3cdt0961jtbsz2yg&quot;)} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345{ &quot;data&quot;: { &quot;toggleLike&quot;: true }} 저 command를 한 번 더 하면 like가 사라진다. Comment 달기 command 123456mutation { addComment(postId:&quot;ck77phhmq3cdt0961jtbsz2yg&quot;, text:&quot;I'm hungry&quot;){ id text }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345678{ &quot;data&quot;: { &quot;addComment&quot;: { &quot;id&quot;: &quot;ck7bgja9p1fgd0984bcayfn7b&quot;, &quot;text&quot;: &quot;I'm hungry&quot; } }} 3.9. searchUser and searchPost Resolver search는 인증과정이 불필요하다. Prisma의 장점 중 하나는 검색이 매우 쉽다. searchUser 예시 command 12345678{ searchUser(term:&quot;gerrit&quot;){ id username firstName lastName }} Result 123456789101112{ &quot;data&quot;: { &quot;searchUser&quot;: [ { &quot;id&quot;: &quot;ck77oedm81kue0905wt78x0kx&quot;, &quot;username&quot;: &quot;gerrit&quot;, &quot;firstName&quot;: &quot;gerrit&quot;, &quot;lastName&quot;: &quot;Cole&quot; } ] }} searchPost 예시 command 12345678910111213{ searchUser(term:&quot;gigi&quot;){ id username firstName lastName } searchPost(term:&quot;americano&quot;){ id caption location }} Result 12345678910111213141516171819{ &quot;data&quot;: { &quot;searchUser&quot;: [ { &quot;id&quot;: &quot;ck77tw4291rqr0905yy5shirp&quot;, &quot;username&quot;: &quot;gigi&quot;, &quot;firstName&quot;: &quot;Gianluigi&quot;, &quot;lastName&quot;: &quot;Buffon&quot; } ], &quot;searchPost&quot;: [ { &quot;id&quot;: &quot;ck77phhmq3cdt0961jtbsz2yg&quot;, &quot;caption&quot;: &quot;Americano nyam&quot;, &quot;location&quot;: &quot;New york&quot; } ] }} searchPost 예시2 Command 12345678910111213{ searchUser(term:&quot;gigi&quot;){ id username firstName lastName } searchPost(term:&quot;New&quot;){ id caption location }} Result 12345678910111213141516171819{ &quot;data&quot;: { &quot;searchUser&quot;: [ { &quot;id&quot;: &quot;ck77tw4291rqr0905yy5shirp&quot;, &quot;username&quot;: &quot;gigi&quot;, &quot;firstName&quot;: &quot;Gianluigi&quot;, &quot;lastName&quot;: &quot;Buffon&quot; } ], &quot;searchPost&quot;: [ { &quot;id&quot;: &quot;ck77phhmq3cdt0961jtbsz2yg&quot;, &quot;caption&quot;: &quot;Americano nyam&quot;, &quot;location&quot;: &quot;New york&quot; } ] }} searchPost.js에서 caption_starts_with이므로 시작 부분을 비교하여 검색결과를 보여줌. 3.10. follow unfollow Resolverfollow 예시 Command 123mutation { follow(id:&quot;ck77oedm81kue0905wt78x0kx&quot;)} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345{ &quot;data&quot;: { &quot;follow&quot;: true }} unfollow 예시 Command 123mutation { unfollow(id:&quot;ck77oedm81kue0905wt78x0kx&quot;)} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345{ &quot;data&quot;: { &quot;unfollow&quot;: true }} Secret이 확인 되면 로그인 시크릿을 지우도록 confirmSecret.js에 코드 추가. 123456await prisma.updateUser({ where: { id: user.id }, data: { loginSecret: &quot;&quot; }});","link":"/2020/04/06/instagramClone-4/"},{"title":"Kafka를 Docker에 설치하기","text":"Kafka는 공식 Docker 이미지가 없으므로 직접 Dockerfile을 작성하여 설치해야 합니다. 이 글에서는 Kafka를 Docker에서 실행하는 과정을 설명합니다. Kafka 설치 및 실행Kafka 다운로드Kafka는 Apache Kafka 공식 사이트에서 다운로드할 수 있습니다. Kafka 2.0.0 다운로드 링크 Dockerfile 작성Kafka를 실행하는 Dockerfile을 작성합니다. 12345678FROM java:8RUN mkdir /appWORKDIR /appRUN curl https://archive.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz | tar xvzWORKDIR /app/kafka_2.12-2.0.0RUN sed -i &quot;s/zookeeper\\.connect=.*/zookeeper\\.connect=zookeeper:2181/g&quot; config/server.propertiesRUN sed -i &quot;s/#advertised\\.listeners=.*/advertised\\.listeners=PLAINTEXT:\\/\\/host.docker.internal:9092/g&quot; config/server.propertiesENTRYPOINT [ &quot;bin/kafka-server-start.sh&quot;, &quot;config/server.properties&quot; ] Docker 이미지 빌드1docker build -t my-kafka . Kafka 컨테이너 실행1docker run --name kafka -p 9092:9092 -d my-kafka Kafka 설정 설명Zookeeper 연결 설정 Kafka는 Zookeeper를 필요로 하므로 server.properties 파일에서 zookeeper.connect 값을 수정해야 합니다. Zookeeper는 별도의 컨테이너로 실행하는 것이 일반적입니다. Kafka 네트워크 설정 advertised.listeners 값을 host.docker.internal:9092로 설정하면, 로컬 머신에서도 Kafka에 접근할 수 있습니다. PLAINTEXT://host.docker.internal:9092 값은 Docker 내부에서 외부 네트워크를 통해 Kafka에 접근하도록 만듭니다. 참고 자료 Kafka 공식 문서 Docker Hub - Kafka (비공식) 결론Docker를 이용해 Kafka를 설치하면 로컬 개발 환경에서 분산 메시징 시스템을 쉽게 테스트할 수 있습니다.","link":"/2022/03/19/kafkaOnDocker/"},{"title":"Lagom Core Concepts","text":"이 글에서는 Lagom 프레임워크의 핵심 개념을 설명하고, 예제 코드로 MembershipService.scala를 분석합니다. Lagom 개요Lagom은 **마이크로서비스 아키텍처(MSA)**를 구축하기 위한 Scala 기반 프레임워크입니다. Lagom의 주요 특징 Service API 중심 개발: 서비스 인터페이스를 정의하고 구현을 분리할 수 있음. Event Sourcing &amp; CQRS 지원: 데이터 변경을 이벤트로 관리. 비동기, 반응형 시스템: Akka와 Play 기반으로 높은 확장성 제공. 개발 편의성: Lagom Dev Mode를 활용한 빠른 개발 사이클. MembershipService 예제아래는 Lagom에서 제공하는 MembershipService의 정의입니다. 12345678910111213141516171819202122package com.example.membership.apiimport akka.{Done, NotUsed}import com.lightbend.lagom.scaladsl.api.transport.Methodimport com.lightbend.lagom.scaladsl.api.{Descriptor, Service, ServiceCall}trait MembershipService extends Service { def join(name: String): ServiceCall[NotUsed, Done] def leave(name: String): ServiceCall[NotUsed, Done] def get(name: String): ServiceCall[NotUsed, String] override final def descriptor: Descriptor = { import Service._ named(&quot;membership&quot;) .withCalls( restCall(Method.PUT, &quot;/membership/:name&quot;, join _), restCall(Method.DELETE, &quot;/membership/:name&quot;, leave _), restCall(Method.POST, &quot;/membership/:name&quot;, get _) ) .withAutoAcl(true) }} 코드 설명 ServiceCall[NotUsed, Done]: 요청과 응답을 정의하는 Lagom의 핵심 개념. join, leave, get: REST 엔드포인트를 제공하는 메서드. descriptor: 서비스의 API 정의. restCall: HTTP REST 엔드포인트 매핑. 참고 자료 Lagom 공식 문서 Akka &amp; Lagom 개념 결론Lagom은 Scala 기반의 마이크로서비스 프레임워크로, 서비스 API 중심으로 개발을 진행할 수 있도록 돕습니다. 이 글에서는 MembershipService 예제를 통해 Lagom의 기본적인 구조와 특징을 살펴보았습니다.","link":"/2022/01/17/lagomCoreConcepts/"},{"title":"선형회귀 알고리즘","text":"지도 학습 알고리즘은 크게 분류와 회귀(regression)으로 나뉜다. 분류는 말 그대로 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이다. 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제이다. 예를 들면 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제이다. 회귀는 정해진 클래스가 없고 임의의 수치를 출력한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 3장. 회귀알고리즘과 모델규제] K-최근접 이웃 회귀(출처 : kNN 최근접 이웃 알고리즘) 녹색 영화는 액션영화일까? 로맨틱 영화일까?녹색 영화는 액션 영화와 로맨틱 영화 가운데 있다.그래서 상당히 답을하기 곤란한 상황이다.현실 세계에서 이게 액션 영화다 로맨틱 영화다 라고 딱 부러지게 얘기하기는 어렵다.이럴 경우에 머신러닝을 사용해 예측갑을 가지고 이야기 할 수가 있다. 그래서 기존의 데이터, 녹색 별을 제외한 기존의 데이터를 중심으로 이 녹색영화가 액션 영화다, 로맨틱 영화다라고 이야기 하는 방법이 knn 알고리즘이다. y축에 보이는 것처럼 발차기 횟수가 많을 경우에는 액션 영화의 가능성이 크고x축의 키스 횟수가 많을때는 로맨틱 영화다라고 볼 수가 있다. 그렇다면 여기서 knn 알고리즘을 간단하게 살펴보도록 하겠다.일단은 k를 정해줘야 한다.k는 최근접점을 우리가 몇개까지 볼것인지 정하는 것이다.일단 k=3으로 쓰겠다. 이것으로 한번 예측값을 내보도록 하겠다. k는 기본적으로 홀수를 쓴다. 왜냐하면 짝수로 쓰면 2:2와 같은 상황이 되어 답을 할 수 없는 상황이 되기 때문이다. 위 그래프에서 보이는 것처럼 최근접 거리에 있어 써클안에 액션 영화가 2개가 있고로맨틱 영화가 하나가 있다.그래서 원 안에 로맨틱 영화보다 액션 영화가 더 많기 때문에 녹색 영화는 액션 영화에요 라고 예측 값을 리턴할 수 있다.이것이 바로 knn 알고리즘의 핵심이다. 그렇다면 최근접점을 프로그램상에서 어떻게 구하는지 보자.피타고라스의 정리를 이용해서 구한다.두 정점의 거리를 구해서 가장 작은 거리의 점들부터 비교를 해나가는 것이다. 결정계수 (R^2)과대적합선형회귀모델 파라미터다항 회귀특성 공학과 규제다중 회귀특성 공학릿지라쏘하이퍼파라미터","link":"/2024/03/27/linearRegression/"},{"title":"Mac OsX에서 LaTeX 사용하기","text":"LaTeX (ˈleɪtɛk 레이텍 또는 ˈlɑːtɛk 라텍)은 문서 조판에 사용되는 프로그램입니다. 도널드 커누스가 만든 TeX을 쉽게 사용하기 위하여 1984년에 레슬리 램포트가 만든 매크로입니다. TeX을 직접 사용하기는 어렵기 때문에, 오늘날에는 LaTeX을 이용하여 문서를 만드는 경우가 많습니다.이 포스팅에서는 MacOSX에서 MacTex를 이용해 LaTeX를 사용하는 방법을 알아보겠습니다. 우선 MacTex 사이트에서 pkg를 다운로드 받습니다. 다운로드가 완료되면 pkg를 실행시켜 installation을 진행해 줍니다. 설치가 완료 되면 dock에 아래와 같은 아이콘이 생성됩니다. 저 icon을 클릭하면 TeXShop이 실행 됩니다. TeXShop editor에 다음과 같이 간단하게 작성하여 보았습니다. 이를 저장해 줍니다. 상단 메뉴 바에서 조판하기 (Typeset)에서 LaTeX를 선택해주면 아래와 같은 콘솔창이 뜹니다. 아래와 같은 pdf output 또한 나타납니다. TeXstudio 또한 사용 가능합니다.","link":"/2020/04/21/latex/"},{"title":"instagram Clone하기 (Backend) 02-Prisma Setting하기 (2)","text":"저번 포스팅에 이어서 Prisma 세팅을 마무리하고 Datamodel을 만들어 보도록 하겠습니다.이 프로젝트 내용은 NomadCoders 를 참고하여 진행할 예정입니다. 2.0. Introduction to Prisma prisma는 ORM (Object-relational mapping, 객체 관계 연결) prisma는 데이터베이스 관련한 어려운 문제들 해결해 줌. typeorm이나 djago orm, sequelize 같은 다른 ORM도 있음. prisma는 어플리케이션에 필요한 모델을 graphql로 정의할 수 있다는 점에서 특별함. prisma가 graphql로 정의한 모델을 이해한 후에 이런 코드들을 만들어 줌. User 모델에 ID, email, posts가 있음. npm install -g prisma prisma login -k mykey prisma init 기존 데이터베이스를 사용할 지 새로 만들지 선택 가능. 생성된 파일들을 깃허브에서 숨기고 싶다면 =&gt; .gitignore에 generated 추가. user model 추가 =&gt; prisma deploy. yarn global add prisma. prisma deploy =&gt; 서비스를 배포 시 자동으로 갱신. 데이터 모델만 변경하고 prisma deploy를 실행하면 됨. 2.1. Datamodel with Prisma 데이터 모델 추가 prisma 기능에는 unique 기능, default 기능 있음. directive나 naming convention에 관한 것도 있으니 도큐먼트 참고. relation기능으로 following과 followers 묶기. prisma deploy 명령어 맘에 들지 않으니 package.json에 script를 추가. 이제 yarn deploy로 배포. endpoint는 서버라고 보면 됨. graphql playground 서버가 us.prisma.sh/~~ 주소에 생성 됨. 이 주소는 사용자에게 알려주면 안됨. 옆의 Docs를 보면 모든 명령을 실행시켜볼 수 있음. 모든 user를 조회할 수 있고, 모든 Like도 조회 가능. fileConnection, createComment, createFile, update.. 모든 명령을 실행시킬 수 있음. subscription도 mutation도 create, delete, update deleteUser 다 있음. 이 주소를 절대 알려줘서는 안됨. 모든 명령어를 보고 실행해볼 수 있음.. prisma의 장점 =&gt; 데이터 모델만 작성하면 자동으로 관리 패널을 만들어 줌. 2.2. Testing Prisma create는 모든 모델에 할 수 있음. createUser, createPost 처럼 모든 모델에. delete는 User에만 가능. deletePost나 deleteLike는 실행 불가능. Room과 Message도 추가. messages는 message의 배열 타입. upsert는 update 혹은 insert 임. graphql 파일만 작성하면 resolver도 생성되고, 서버도 만들어지고 관리 패널도 만들어짐. createUser command 12345mutation { createUser(data:{username:&quot;Gerrit&quot;, email:&quot;gerrit@yankees.com&quot;}){ id }} result 1234567{ &quot;data&quot;: { &quot;createUser&quot;: { &quot;id&quot;: &quot;ck77oedm81kue0905wt78x0kx&quot; } }} 조회하기 command 12345{ user(where:{id:&quot;ck77ojlbt3a0t0961vm0dgsbb&quot;}){ username }} result 1234567{ &quot;data&quot;: { &quot;user&quot;: { &quot;username&quot;: &quot;paulo&quot; } }} update 하기 command 12345mutation { updateUser(data:{firstName:&quot;gerrit&quot;, lastName:&quot;Cole&quot;}where:{id:&quot;ck77oedm81kue0905wt78x0kx&quot;}){ username }} result 1234567{&quot;data&quot;: { &quot;updateUser&quot;: { &quot;username&quot;: &quot;gerrit&quot; } }} following 만들기 command 123456789101112131415161718mutation { updateUser(data:{following:{ connect: { id: &quot;ck77ojlbt3a0t0961vm0dgsbb&quot; } }}where:{id:&quot;ck77oedm81kue0905wt78x0kx&quot;}){ username firstName lastName following{ id } followers{ id } }} result 123456789101112131415{ &quot;data&quot;: { &quot;updateUser&quot;: { &quot;username&quot;: &quot;gerrit&quot;, &quot;followers&quot;: [], &quot;following&quot;: [ { &quot;id&quot;: &quot;ck77ojlbt3a0t0961vm0dgsbb&quot; } ], &quot;lastName&quot;: &quot;Cole&quot;, &quot;firstName&quot;: &quot;gerrit&quot; } }} 2.3. Integrating Prisma in our Server prisma.yml 파일 커밋하면 안됨. gitignore에 추가해 주어야. 만약 prisma에 문제가 생긴다면, url과 모든 서비스를 변경해야 함. prisma.yml에 generate라는게 있음. generator와 output이 있음. generated를 보면 prisma-schema 폴더에 index, prisma-schema 파일이 있는데 이 파일을 통해 prisma와 정보를 주고 받게 됨. 서비스를 배포할 때마다 prisma cloud에서 무언가를 생성하는데, 그것을 다운로드 받아야 함. prisma 서버와 정보를 주고받으려면 prisma client를 다운로드 받아야 함. package.json에 prisma 명령어 추가 =&gt; deploy 명령어를 수행하고 나면 generated 명령어를 수행하도록 할 것. yarn prisma를 해 주면 deploy가 실행되고 변경 사항들이 업로드 될 것. 그리고 나면 client가 generate 될 것. prisma client가 추가 됨. prisma와 상호작용 할 client는 javascript. prisma가 자동으로 client를 만들어 줌, 이 client는 사용자 정보를 체크할 수 있음. 이렇게 하면 데이터베이스에서 사용자 정보를 불러오고, prisma 엔드포인트를 보호할 수 있음. 사람들이 서버에 정보를 요청, 서버는 prisma를 요청. 이렇게 하는 것이 사람들이 서버에 직접 요청하도록 하는 것 보다 더 안전! 프론트 엔드가 서버에 요청을 하면 서버가 prisma에 요청. 서버가 중간에 있다고 볼 수 있다. 2.4. Resolvers with Prisma models.graphql 파일을 만들어 api 아래에 이동 (api 아래의 graphql 파일만 읽을 수 있도록 했기 때문에). graphql은 prisma 문법을 이해하지 못하기 때문에 두 개의 파일 모두 만들어야 함. (문법 약간 다름 @들어가는거 차이). userById.graphql에서 model을 import 해야 함. 그렇지 않으면 이 쿼리는 User가 무엇인지 모를 것. 명령 123456789{ allUsers { id firstName } userById(id:&quot;ck73au8pi2lkr0b45ejrt0j7q&quot;){ firstName }} 결과 1234567891011121314151617{ &quot;data&quot;: { &quot;allUsers&quot;: [ { &quot;id&quot;: &quot;ck733pwde25dp0b45ezb9iw9e&quot;, &quot;firstName&quot;: &quot;&quot; }, { &quot;id&quot;: &quot;ck73au8pi2lkr0b45ejrt0j7q&quot;, &quot;firstName&quot;: &quot;suhyun&quot; } ], &quot;userById&quot;: { &quot;firstName&quot;: &quot;suhyun&quot; } }} Prisma의 단점! =&gt; $fragment()라는 걸 추가해 줘야 posts 실행 가능.왜냐하면 공격받고 싶어하지 않기 떄문.아래처럼 무한한 코드를 만들 수 있고, Prisma 서버가 먹통이 될 수 있기 때문에.(post에서 post로 끝없는 코드가 나올 수 있다.)이런 공격을 막기 위해서 그렇다. 123456789101112131415161718192021222324252627282930{ allUsers { id firstName posts { id user { posts { id user { id posts { id user { posts { user { id } } } } } } } } } userById(id:&quot;ck73au8pi2lkr0b45ejrt0j7q&quot;){ firstName }}","link":"/2020/03/28/instagramClone-2/"},{"title":"instagram Clone하기 (Backend) 02-Prisma Setting하기 (3)","text":"저번 포스팅에 이어서 본격적으로 backend 기능을 구현해 보도록 하겠습니다.이 프로젝트 내용은 NomadCoders 를 참고하여 진행할 예정입니다. 3.0. Planning the API Create account Request Secret Confirm Secret (Login) Like / Unlike a photo Comment on a photo Search by user Search by location See user profile Follow / Unfollow User See the full photo Edit my profile Upload photo Edit the photo (Delete) See the feed 사용자들이 전송 받은 비밀값을 페이지에 붙여넣기 하면 로그인하게 되고, 그들이 이메일에 접근할 수 있다는 것을 확인! 3.1. Create Account Resolver graphql console에서 post 추가. Image/toggleLike 폴더 생성 =&gt; 이미 좋아요가 되어 있으면 좋아요 취소로 아이콘이 바뀔 것. createAccount 폴더 및 createAccount.graphql 파일 생성. 참고로 멀티 커서 키는 option + click. 그래서 resolver가 무엇인가??? createAccount로 user 추가 command 12345mutation { createAccount (username:&quot;gigi&quot;, email:&quot;gianluigi@juventus.com&quot;, firstName:&quot;Gianluigi&quot;, lastName:&quot;Buffon&quot;){ id }} 결과 1234567{ &quot;data&quot;: { &quot;createAccount&quot;: { &quot;id&quot;: &quot;ck77tw4291rqr0905yy5shirp&quot; } }} 궁금? 이거 왜 로컬 playground에서는 되는데 서버 playground에서는 에러남??? 근데 또 console db에는 추가 됨. 1234567891011121314{ &quot;data&quot;: null, &quot;errors&quot;: [ { &quot;message&quot;: &quot;Cannot query field 'createAccount' on type 'Mutation'. Did you mean 'createPost', 'createComment' or 'createRoom'? (line 2, column 3):\\n createAccount(username: \\&quot;gigi\\&quot;, email: \\&quot;gianluigi@juventus.com\\&quot;, firstName: \\&quot;Gianluigi\\&quot;, lastName: \\&quot;Buffon\\&quot;) {\\n ^&quot;, &quot;locations&quot;: [ { &quot;line&quot;: 2, &quot;column&quot;: 3 } ] } ] } 참고 : 삭제하기1234567mutation { deleteManyObjects(where: { id_not: 0 }){ count }} 3.2. requestSecret Resolver requestSecret.graphql과 requestSecret.js 파일 생성. utils.js 생성. 로그인하면 토큰 부여. 사용자가 페이지에 접속해서 이메일 입력하면, 비밀값이 전송 되고, 사용자들은 그 비밀값을 복사-붙여넣기 해서 접속. 보통은 비밀번호를 쓰지만 비밀번호는 진부해~~~! Math.random을 이용하여 랜덤으로 adjectives와 nouns를 조합하여 단어를 생성해 주는 기능 만들기. Math.floor는 소숫점 아랫자리 숫자 버림, Math.ceil은 반대로 올림. user의 email이 인자로 입력된 email과 같은 사용자를 where로 찾을 것. id, username, email로만 사용자를 찾을 수 있음. bio로 사용자를 찾아서 갱신할 수 는 없음. 유니크한 특성들로만 사용자를 찾아서 갱신요청 할 수 있기 때문. lastName이나 bio같은 @unique가 없는 것들로는 사용자를 검색할 수 없고 많은 사용자들을 필터는 할 수 있음. 유니크한 특성이 아니라면 삭제나 수정 요청을 할 수 없음. 3.3. sendMail Function with Nodemailer secretRequest를 실제로 메일로 보내주기 위해 nodemailer를 사용. yarn add nodemailer utils에 sendMail을 만들자. nodemailer는 몇몇 기본값들로 transport라는 것을 만들고, 그것으로 sendMail을 요청하면 끝. sendgrid를 사용하겠다. yarn add nodemailer-sendgrid-transport api_user와 api_key는 .env에 저장. server.js에 utils를 import. process.env를 가져오려고 함. sendMail은 외부에서 사용하지 않으니 export 하지 않아도 됨. sendSecretMail을 사용할 것. 참고로 보내는 메일 주소 도메인을 가짜로 설정 하는 것은 정말 쉬워 보인다. 3.4. Passport JWT part One requestSecret에 이메일 보내는 기능을 추가. utils에서 sendSecretMail은 sendMail을 리턴하고, sendMail은 Promise 함수를 리턴. command 123mutation { requestSecret(email:&quot;ggamini7@gmail.com&quot;)} result 12345{ &quot;data&quot;: { &quot;requestSecret&quot;: true }} confirmSecret을 만들어 보자. confirmSecret.js 및 confirmSecret.graphql파일 생성 confirmSecret.graphql의 함수는 jwt 토큰을 리턴할 것. secretConfirm 실행123mutation { confirmSecret(email:&quot;ggamini7@gmail.com&quot;, secret:&quot;great hate&quot;)} result 12345{ &quot;data&quot;: { &quot;confirmSecret&quot;: &quot;TOKEN&quot; }} 이제 토큰을 만들어 보자 passportjs에서 jwt를 사용할 것. yarn add passport-jwt passport passport는 인증 관련한 모든 일을 함. jwt 토큰이나 쿠키에서 정보를 가져와서 사용자 정보에 serialize(저장) 함. 토큰에서 정보를 가져와서 (express의) request에 붙여주는 것. 토큰을 가져와서 해독한 후에 사용자 객체를 request에 추가 해 줌. secret은 passport 정보를 암호화하는데 필요한 비밀 값. 확인용 callback 함수도 추가하여 옵션이 잘 맞게 적용되었을 때 JwtStrategy 함수가 토큰을 해석하도록 해야 함. 사용자 정보가 암호화 되어 토큰에 담기면 -&gt; JWT는 토큰을 입력 받아서 정보를 해석함 -&gt; 해석된 정보를 콜백 함수로 전달해 줌. 3.5. Passport JWT part Two JWT를 가져와서 해석하고 확인하는 작업들을 함. JWT 생성도 하도록 해야 함. jsonwebtoken 모듈이 필요함. =&gt; yarn add jsonwebtoken 암호화할 때와 해독할 때 같은 private key를 사용. jwt가 id를 암호화 해서 토큰을 만들어 줌. token 만들기 command 123mutation{ confirmSecret(email:&quot;ggamini7@gmail.com&quot;, secret:&quot;great hate&quot;)} result 12345{ &quot;data&quot;: { &quot;confirmSecret&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMwODA3OTl9.cZiSHChUpIxMnr21hP6H0_1W2KayZo3hViBzmOWlXbQ&quot; }}","link":"/2020/03/31/instagramClone-3/"},{"title":"Nmap으로 IP 스캔하기","text":"nmap을 사용하여 타겟 서버의 포트 개방 여부를 확인하고, 어떤 프로세서가 러닝되고 있는지 확인하려고 합니다.그러기 위해서는 ‘NMAP’ 이 설치가 되어 있어야 합니다.맥을 사용하시는 분들이라면 homebrew 를 설치하신 이후에 brew install nmap 을 하시면 설치 할 수 있습니다. 아래와 같이 인스톨 합니다. NMAP 으로 확인하려고 하는 결과를 먼저보여 드리면 아래 그림과 같습니다. 한양대학교의 아이피 주소인 166.104.177.24 을 넣어보니 http 프로토콜의 포트번호인 80번과, https의 포트번호인 443만 개방되어 있는것을 확인하실 수 있습니다. 12345678910import os# options인자는 CLI의 옵션을 받기 위해. def get_nmap(options, ip): command = &quot;nmap&quot; + options + &quot; &quot; + ip process = os.popen(command) results = str(process.read()) return resultsprint(get_nmap(' -F', '166.104.177.24')) 위의 CLI로 실행한 기능을 실행하는 python 코드 입니다. 이를 실행하면 위와 같이 실행됩니다.","link":"/2020/05/02/nmap/"},{"title":"(프로젝트) Pomodoro 할 일 관리 어플","text":"Pomodoro 기법을 활용하여 할 일을 효율적으로 관리할 수 있는 어플리케이션을 개발했습니다.React, Redux, Firebase 및 AWS Lambda를 활용하여 클라우드 기반의 빠르고 확장 가능한 서비스를 구축하였습니다. 프로젝트 개요문제 정의 사용자가 집중력을 유지하며 작업할 수 있도록 Pomodoro 기법 적용 직관적인 UI/UX를 제공하여 생산성 향상 실시간 동기화를 지원하는 클라우드 기반의 서비스 제공 솔루션 React 및 Redux 기반의 SPA (Single Page Application) 구축 Firebase를 활용한 실시간 데이터 동기화 AWS Lambda를 활용한 백엔드 로직 실행 기술 스택 영역 기술 스택 Frontend React.js, Redux, JavaScript Backend Firebase, AWS Lambda Database Firestore (NoSQL) CI/CD GitHub Actions, Firebase Hosting 주요 기능 할 일 등록 및 관리: 사용자가 할 일을 추가, 수정, 삭제 가능 Pomodoro 타이머 기능: 25분 집중 + 5분 휴식 타이머 지원 클라우드 동기화: Firebase 기반 실시간 데이터 저장 반응형 UI 지원: 모바일 및 데스크톱 환경 최적화 데모 및 실행 링크 프로젝트 Repository 서비스 이용하기 프로젝트 자료아래는 프로젝트의 주요 화면들입니다: 결론이 프로젝트를 통해 Pomodoro 기법을 활용한 할 일 관리 시스템의 효과를 확인하였습니다.향후 연구 방향으로는 사용자 맞춤형 알림 및 분석 기능 추가를 고려하고 있습니다.","link":"/2021/02/09/pomodoro/"},{"title":"instagram Clone하기 - Backend Resolvers (2)","text":"이번 포스팅에서는 editUser, seeUser, Computed Fields, upload, editPost Resolver를 만들어 보겠습니다이 프로젝트 내용은 NomadCoders 를 참고하여 진행할 예정입니다. 3.11. editUser and seeUser Resolver context에 Prisma를 넣어두었으므로 prisma만 import 하면 autocomplete 된다. 사실 isAuthenticated를 context에 추가할 수도 있다. server.js에 아래와 같이 해두면123import { isAuthenticated } from &quot;./middlewares&quot;;context: ({ request }) =&gt; ({ request, isAuthenticated }) edutUser.js에서 더 이상 아래의 것을 import하지 않아도 된다.1import { isAuthenticated } from &quot;./middlewares&quot;; editUser 예시 Command 1234567mutation { editUser(bio:&quot;I love to drink hot americano in the morning&quot;){ firstName lastName bio }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789{ &quot;data&quot;: { &quot;editUser&quot;: { &quot;firstName&quot;: &quot;Gianluigi&quot;, &quot;lastName&quot;: &quot;Buffon&quot;, &quot;bio&quot;: &quot;I love to drink hot americano in the morning&quot; } }} 만약 내가 firstname이나 lastname등 아무것도 보내지 않는다면 내 firstname이랑 lastname은 빈 string으로 올까?혹은 prisma가 이전의 것을 보존해 줄까?-&gt; 이전의 것을 보존해 준다. 위와 같이… editUser.js에서 return 할 때 await로 리턴하지 않아도 된다.-&gt; 왜냐면 return이 마지막 statement이기 때문이다.-&gt; 서버가 자동으로 이 promise가 resolve 되서 브라우저에게 결과를 전달하길 기다려주기 때문. seeUser 예시 Command 12345{ seeUser(id:&quot;ck77tw4291rqr0905yy5shirp&quot;){ firstName }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 1234567{ &quot;data&quot;: { &quot;seeUser&quot;: { &quot;firstName&quot;: &quot;Gianluigi&quot; } }} seeUser는 인증 필요하지 않아서 제거해버림 3.12. me Resolver + Prisma’s Limitations 내 정보를 확인하고 싶으면 이론적으로 playGround에서 me를 하면 됨.하지만 실제로는 못함 =&gt; Prisma Client는 강한 관계를 제공하지 않기 때문.내가 원하는 query에 직접 관계를 맺어줘야 함=&gt; 웹 해킹 (공격 벡터)를 최소화 시키기 위해서 이렇게 만들어진 것. me 조회하기 Command 1234567{ me { posts { id } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 1234567{ &quot;data&quot;: { &quot;me&quot;: { &quot;posts&quot;: [] } }} fragment.js에 있는 것만 가지고 올 수 있을까? 아니면 더 많은 정보를 가지고 오는게 가능할까?=&gt; 있는 것만 가져올 수 있음.=&gt; 엄청 깊은 관계를 가지고 있는 Query를 작성한다면, 아니면 relationship을 맺고 싶으면 =&gt; fragment 사용하기 싫다면, $fragment가 아니라 다른 구문을 사용해야 함. 가끔은 ORM 쓸떄 우리가 게을러져서, user, posts, 팔로워 권한을 모두 원하게 됨.=&gt; me.graphql처럼 하면 됨. me 사용 예시 2 Command 123456789101112{ me { user { firstName lastName email } posts { id } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789101112{ &quot;data&quot;: { &quot;me&quot;: { &quot;user&quot;: { &quot;firstName&quot;: &quot;Gianluigi&quot;, &quot;lastName&quot;: &quot;Buffon&quot;, &quot;email&quot;: &quot;ggamini7@gmail.com&quot; }, &quot;posts&quot;: [] } }} 3.13. See Full Posts. like 수 올리기 123mutation { toggleLike(postId:&quot;ck77phhmq3cdt0961jtbsz2yg&quot;)} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345{ &quot;data&quot;: { &quot;toggleLike&quot;: true }} seeFullPost 테스트 1234567891011121314{ seeFullPost(id: &quot;ck77phhmq3cdt0961jtbsz2yg&quot;) { post { location } comments { id } likeCount }}~~~HTTP header{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789101112131415{ &quot;data&quot;: { &quot;seeFullPost&quot;: { &quot;post&quot;: { &quot;location&quot;: &quot;New york&quot; }, &quot;comments&quot;: [ { &quot;id&quot;: &quot;ck7bgja9p1fgd0984bcayfn7b&quot; } ], &quot;likeCount&quot;: 1 } }} seeFullPost fragment로 comments의 text, user, username 넣는 예시 Command 123456789101112131415{ seeFullPost(id: &quot;ck77phhmq3cdt0961jtbsz2yg&quot;) { post { location } comments { id text user { username } } likeCount }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345678910111213141516171819{ &quot;data&quot;: { &quot;seeFullPost&quot;: { &quot;post&quot;: { &quot;location&quot;: &quot;New york&quot; }, &quot;comments&quot;: [ { &quot;id&quot;: &quot;ck7bgja9p1fgd0984bcayfn7b&quot;, &quot;text&quot;: &quot;I'm hungry&quot;, &quot;user&quot;: { &quot;username&quot;: &quot;gigi&quot; } } ], &quot;likeCount&quot;: 1 } }} 3.14. Computed Fields in PrismaComputed || custom filed me.js를 보면 UserProfile을 받게 되는데, 내가 내 User에 더하고 싶은게 있는 경우에, 예를 들면 fullName을 더해서 구하고 싶다고 해 보자. fullName은 firstName과 lastName에 의해 결정된다. 이건 database가 아니고, 사실은 User의 일부들이다. 예를들어, seeFullPost나 seeUser에서는 isLiked:Boolean 같은것을 더해주어야 한다. isLiked는 내가 사진에 좋아요를 눌렀는지 아닌지에 관한 것이다. 그리고 User에다가 예를 들어 amIFollowing도 더해줘야 한다. 이것이 무엇이냐면 우리가 userProfile을 갈 때마다 내가 이 사람을 팔로잉하는지 알고 싶을 것이다. 그래서 amIFollowing도 있고 isLiked도 있다. 그리하여 me.js에 user resolver를, custom resolver를 더하여 이건 하나의 field만을 위한 것이다. 이것은 다른 field들에 영향을 끼치지 않을 것이다. 이렇게 되면 먼저 Prisma로 가서 찾으려고 할 것이고, 만약 Prisma에서 찾을 수가 없다면, 자신의 서버에서 찾으려고 할 것이다. me를 이용하여 테스트 해 보기 Command 1234567 { me { user { fullName } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789{ &quot;data&quot;: { &quot;me&quot;: { &quot;user&quot;: { &quot;fullName&quot;: &quot;Gianluigi Buffon&quot; } } }} request에는 user가 있고, 기술적으로 내가 접근하고 싶은곳은 어디든 접근할 수 있다. Prisma에도 접근하고, request도 할 수 있다. 여기서 parent라는 것이 있다. 보통 parent가 하는것은 위에 있는 resolver를 준다. 1234567 { me { user { fullName } }} 예를 들어 fullName의 parent는 user이다. 만약 user의 parent를 console.log한다면 undefined가 나올 것이다. 왜냐하면 그것은 그냥 me로 query이기 때문이다. 즉 나를 call 한 resolver의 parent를 갖게 된다. 어느 resolver가 나를 call 했는지 알 수 있으니 유용하다. (참고로 _ 가 parent를 뜻한다, parent나 root라고 한다.) 아무튼 우리는 parent로 부터 return을 할 수 있다. 그 예는 아래와 같다. 12345 User: { fullName: parent =&gt; { return `${parent.firstName} ${parent.lastName}`; }} 이것을 me말고 seeUser에 해도 작동할까? =&gt; 그렇다, schema에다가 모든 resolver를 하나로 merging하고 있기 때문이다. 3.15. itsMe and amIFollowing Fields part One computed.js 파일 생성. amIFollowing -&gt; 이 유저가 그를 following 하고 있는지 확인 위해 만듬. amIFollowing 예시 Command 12345678{ me { user { amIFollowing itsMe } }} HTTP request 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345678910{ &quot;data&quot;: { &quot;me&quot;: { &quot;user&quot;: { &quot;amIFollowing&quot;: false, &quot;itsMe&quot;: true } } }} seeUser로 테스트 (다른 사람) Command 1234567{ seeUser(id:&quot;ck77oedm81kue0905wt78x0kx&quot;){ user { itsMe } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789{ &quot;data&quot;: { &quot;seeUser&quot;: { &quot;user&quot;: { &quot;itsMe&quot;: false } } }} 내가 나를 following 하는 지 알아보기. (amIFollowing 이용) Command 1234567{ me { user { amIFollowing } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789{ &quot;data&quot;: { &quot;me&quot;: { &quot;user&quot;: { &quot;amIFollowing&quot;: false } } }} 3.16. itsMe and amIFollowing Fields part Two. exits는 users를 위한 필터이므로 이것을 사용해보자. Command 123456789101112{ users( where: { AND: [ { id: &quot;ck77tw4291rqr0905yy5shirp&quot; } { followers_some: { id: &quot;ck77oedm81kue0905wt78x0kx&quot; } } ] } ){ id }} Result 12345{ &quot;data&quot;: { &quot;users&quot;: [] }} Prisma로 디버깅하는 방법은 (무언가 실수했는데 Prisma Client가 이를 삼켜버린다면) 코드를 복사해서 Prisma Endpoint에서 테스트 해보면 됨. itsMe =&gt; itSelf, amIFollowing =&gt; isFollowing 3.17. isLiked Computed File Post에 있는 “isLiked” =&gt; computed.js의 isFollowing이나 itSelf와 비슷. isLiked 테스트 Command 1234567{ seeFullPost(id:&quot;ck77phhmq3cdt0961jtbsz2yg&quot;){ post{ isLiked } }} Result 123456789{ &quot;data&quot;: { &quot;seeFullPost&quot;: { &quot;post&quot;: { &quot;isLiked&quot;: true } } }} 3.18. upload Resolverupload post 예시 command 123456789101112mutation { upload( caption: &quot;brewedCoffee&quot; files: [ &quot;https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fblog.bluebottlecoffee.com%2Fposts%2Fpro-tips-nel-drip-coffee&amp;psig=AOvVaw1gV7V1HDocPQG3ZkNs5RaD&amp;ust=1583590844673000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKDs6eKFhugCFQAAAAAdAAAAABAQ&quot; &quot;https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.123rf.com%2Fphoto_82666652_thai-people-use-drip-coffee-maker-or-dripper-made-hot-coffee-for-sale-for-people-at-shop-in-organic-.html&amp;psig=AOvVaw1gV7V1HDocPQG3ZkNs5RaD&amp;ust=1583590844673000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKDs6eKFhugCFQAAAAAdAAAAABAc&quot; ] ) { id caption }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345678{ &quot;data&quot;: { &quot;upload&quot;: { &quot;id&quot;: &quot;ck7g9xkwb8r980984i4isheai&quot;, &quot;caption&quot;: &quot;brewedCoffee&quot; } }} seeFullPost에서 파일들을 받도록 seeFullPost.graphql와 seeFullPost.js에 추가. upload한 포스트 seeFullPost로 보기 command 123456789101112{ seeFullPost(id: &quot;ck7g9xkwb8r980984i4isheai&quot;) { files { id url } post { caption location } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 1234567891011121314151617181920{ &quot;data&quot;: { &quot;seeFullPost&quot;: { &quot;files&quot;: [ { &quot;id&quot;: &quot;ck7g9xleof22u0a48s737b629&quot;, &quot;url&quot;: &quot;https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fblog.bluebottlecoffee.com%2Fposts%2Fpro-tips-nel-drip-coffee&amp;psig=AOvVaw1gV7V1HDocPQG3ZkNs5RaD&amp;ust=1583590844673000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKDs6eKFhugCFQAAAAAdAAAAABAQ&quot; }, { &quot;id&quot;: &quot;ck7g9xleof22v0a48av4n1z81&quot;, &quot;url&quot;: &quot;https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.123rf.com%2Fphoto_82666652_thai-people-use-drip-coffee-maker-or-dripper-made-hot-coffee-for-sale-for-people-at-shop-in-organic-.html&amp;psig=AOvVaw1gV7V1HDocPQG3ZkNs5RaD&amp;ust=1583590844673000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKDs6eKFhugCFQAAAAAdAAAAABAc&quot; } ], &quot;post&quot;: { &quot;caption&quot;: &quot;brewedCoffee&quot;, &quot;location&quot;: null } } }} 3.19. seeFullPost Refactoring. computed.js를 밖으로 옮김. (computed file이 공유 되야 하기 때문에) files, user, comments 이렇게 점점 커지기 때문에 fullPost를 위한 아주 큰 fragment를 만들었다.fragment.js를 정리하여 관계들을 분명하게 만들었다.그렇지 않으면, 엄청나게 많은 쿼리들 때문에 힘들어질 것이기 때문이다. 수정 후 seeFullPost로 예시 Command 12345678910111213141516{ seeFullPost(id: &quot;ck7g9xkwb8r980984i4isheai&quot;) { caption location likeCount files { id } user { id } comments { id } }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 123456789101112131415161718192021{ &quot;data&quot;: { &quot;seeFullPost&quot;: { &quot;caption&quot;: &quot;brewedCoffee&quot;, &quot;location&quot;: null, &quot;likeCount&quot;: 1, &quot;files&quot;: [ { &quot;id&quot;: &quot;ck7g9xleof22u0a48s737b629&quot; }, { &quot;id&quot;: &quot;ck7g9xleof22v0a48av4n1z81&quot; } ], &quot;user&quot;: { &quot;id&quot;: &quot;ck77tw4291rqr0905yy5shirp&quot; }, &quot;comments&quot;: [] } }} 3.20. editPost deletePost ResolvereditPost 예시 Command 123456mutation { editPost(id: &quot;ck7g9xkwb8r980984i4isheai&quot;, location: &quot;San Francisco&quot;, action: EDIT) { id location }} HTTP header 1{&quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNrNzd0dzQyOTFycXIwOTA1eXk1c2hpcnAiLCJpYXQiOjE1ODMyMDg4NTJ9.qAojVTOQnWH_z1Y-DvWUlZDsoJ41FfPA-UrIl-ggWNQ&quot;} Result 12345678{ &quot;data&quot;: { &quot;editPost&quot;: { &quot;id&quot;: &quot;ck7g9xkwb8r980984i4isheai&quot;, &quot;location&quot;: &quot;San Francisco&quot; } }}","link":"/2020/04/07/instagramClone-5/"},{"title":"Selection Sort란?","text":"선택 정렬은 가장 작은 요소부터 선택해 알맞은 위치로 옮겨서 순서대로 정렬하는 알고리즘이다 동작 방식 가장 작은 값의 요소인 1을 선택해 정렬을 시작하고, 6과 교환한다. 아직 정렬하지 않은 배열에서 가장 작은 요소인 3을 선택해 정렬을 시작하고, 배열의 첫 번째 요소인 4와 3을 교환한다. ~7 위의 그림처럼 이 과정을 반복한다. 특징 장점 자료 이동 횟수가 미리 결정된다. 단점 선택 정렬 알고리즘은 안정적이지 않다 즉, 요소값이 중복될 경우 상대적인 위치가 변경될 수 있다. 시간복잡도 비교 횟수 두 개의 for 루프의 실행 횟수 외부 루프 : (n-1)번 내부 루프(최솟값 찾기) : 0에서 n-2까지 변하는 i에 대하여 (n-1)-i번 (n-1, n-2, …, 2, 1번) 교환 횟수 외부 루프의 실행 횟수와 동일하다. 한 번 교환하기 위하여 3번의 이동이 필요하므로 전체 이동 횟수는 3(n-1)번 T(n) = (n-1) + (n-2) + … + 1 = n(n-1)/2 = O(n^2) 선택 정렬 Java 코드선택 정렬(Selection Sort)의 교환 과정 아직 정렬하지 않은 부분에서 가장 작은 키의 값(a[min])을 선택한다. a[min]과 아직 정렬하지 않은 부분의 첫 번째 요소를 교환한다. 예제 코드123456789101112// 단순 선택 정렬static void selectionSort(int[] a, int n) { for (int i = 0; i &lt; n - 1; i++) { int min = i; // 아직 정렬되지 않은 부분에서 가장 작은 요소의 인덱스를 기록한다. for (int j = i + 1; j &lt; n; j++) { if (a[j] &lt; a[min]) min = j; } if (i != min) swap(a, i, min); // 아직 정렬되지 않은 부분의 첫 요소와 가장 작은 요소를 교환한다. }} 단순 선택 정렬 알고리즘의 요소값을 비교하는 횟수는 (n^2 - n)/2회이다.최솟값이 자기 자신이면 자료 이동을 하지 않도록 한다. 일반적으로 비교 연산 1개가 이동 연산 3개보다 시간이 적게 걸리므로 효과적이다.","link":"/2020/10/18/selectionSort/"},{"title":"서버 대역과 포트 스캔하기","text":"오늘은 한양대학교 서버와 포트를 스캔해 보도록 하겠습니다. 우선 한양대의 서버 Ip를 알아봅시다. hanyang.ac.kr에 Ping을 날려봅시다 안타깝게도 패킷이 100% loss되는 군요… 왜 ping이 안 먹히는지는 다음에 알아보도록 하겠습니다. hanyang.ac.kr의 IP주소는 166.104.177.108이라는 군요 사실 166.104.177.108은 한양메일의 IP이고 한양대학교 홈페이지의 Ip주소는 166.104.177.24이긴 합니다. 왜 이렇게 나오는지도 다음에 알아보도록 하겠습니다. 그러면 이 두 IP주소에 nmap을 이용하여 스캔을 해보겠습니다. 어처구니 없게도 두 호스트가 다운되어 있다고 나오는군요… -Pn으로 해보라고 하니 해보겠습니다 166.104.177.108(한양 메일)의 경우 1000개의 포트를 스캔했고 모두 filtered 되어있다고 나오는군요 왜 filtered 되어있는걸까요? 하지만 몇시간 후에 해보니 이번엔 filtered가 안나오는군요… 왜그럴까요…? 166.104.177.24(한양 홈페이지)의 경우 80번과 443번 포트가 열려 있다고 나오는 군요. 참고로 다시 -F를 이용하여 해본결과 아래와 같이 이전과 다르게 스캔이 되었습니다 심지어 166.104.177.108은 어느 포트가 무엇을 하는지까지 보여줍니다. 94개만 filtered 되었다고 나오는 군요 왜 이전과 다른 결과가 나오는 걸까요..? 아무튼 이제 nmap을 이용하여 원하는 호스트의 원하는 포트를 스캔하는 파이썬 코드를 작성해보겠습니다. 그 코드는 아래와 같습니다 1234567891011121314151617181920212223242526272829303132333435363738import optparseimport socketimport nmapport = '21,22,23,25,42,53,70,79,80,88,110,118,143,156,161,220,443,993,8080'def nmapScan(tgtHost, tgtPort): nmScan = nmap.PortScanner() nmScan.scan(tgtHost, tgtPort) state = nmScan[tgtHost]['tcp'][int(tgtPort)]['state'] print(&quot; [*] &quot; + tgtHost + &quot; tcp/&quot; + tgtPort + &quot; &quot; + state)def main(): parser = optparse.OptionParser('usage %prog &lt;options&gt; \\n-H &lt;target host&gt; : input target host name \\n-p &lt;target port&gt; : input target port name \\n-A &lt;all port&gt; : target is all port') parser.add_option('-H', dest='tgtHost', type='string', help='specify target host') parser.add_option('-p', dest='tgtPort', type='string', help='specify target port[s] separated by comma') parser.add_option('-A', dest='allPort', action='store_true', help='target port is all') (options, args) = parser.parse_args() tgtHost = options.tgtHost tgtPorts = str(options.tgtPort).split(',') allPort = options.allPort if ((tgtPorts[0] == 'None') &amp; (allPort == None)) | (tgtHost == None) : print(parser.usage) exit(0) if (allPort == True): tgtPorts = str(port).split(',') for tgtPort in tgtPorts: nmapScan(tgtHost, tgtPort) else: for tgtPort in tgtPorts: nmapScan(tgtHost, tgtPort)if __name__ == '__main__': main() 그리고 이 코드를 실행 시켜보면 166.104.177.24(한양대 홈페이지)의 경우 아래와 같고 1234567891011121314151617181920(py36) ~/Documents/GitHub/Today_I_Learned/Network   master ●  python portScan11.py -H 166.104.177.24 -A [*] 166.104.177.24 tcp/21 filtered [*] 166.104.177.24 tcp/22 filtered [*] 166.104.177.24 tcp/23 filtered [*] 166.104.177.24 tcp/25 filtered [*] 166.104.177.24 tcp/42 filtered [*] 166.104.177.24 tcp/53 filtered [*] 166.104.177.24 tcp/70 filtered [*] 166.104.177.24 tcp/79 filtered [*] 166.104.177.24 tcp/80 open [*] 166.104.177.24 tcp/88 filtered [*] 166.104.177.24 tcp/110 filtered [*] 166.104.177.24 tcp/118 filtered [*] 166.104.177.24 tcp/143 filtered [*] 166.104.177.24 tcp/156 filtered [*] 166.104.177.24 tcp/161 filtered [*] 166.104.177.24 tcp/220 filtered [*] 166.104.177.24 tcp/443 open [*] 166.104.177.24 tcp/993 filtered [*] 166.104.177.24 tcp/8080 filtered 166.104.177.108(한양 메일)의 경우 아래와 같습니다 1234567891011121314151617181920(py36) ~/Documents/GitHub/Today_I_Learned/Network   master ●  python portScan11.py -H 166.104.177.108 -A [*] 166.104.177.108 tcp/21 filtered [*] 166.104.177.108 tcp/22 filtered [*] 166.104.177.108 tcp/23 filtered [*] 166.104.177.108 tcp/25 open [*] 166.104.177.108 tcp/42 filtered [*] 166.104.177.108 tcp/53 filtered [*] 166.104.177.108 tcp/70 filtered [*] 166.104.177.108 tcp/79 filtered [*] 166.104.177.108 tcp/80 open [*] 166.104.177.108 tcp/88 filtered [*] 166.104.177.108 tcp/110 open [*] 166.104.177.108 tcp/118 filtered [*] 166.104.177.108 tcp/143 open [*] 166.104.177.108 tcp/156 filtered [*] 166.104.177.108 tcp/161 filtered [*] 166.104.177.108 tcp/220 filtered [*] 166.104.177.108 tcp/443 open [*] 166.104.177.108 tcp/993 open [*] 166.104.177.108 tcp/8080 filtered 두 곳 모두 터미널에서 nmap을 사용했을 때와 같은 결과가 나오는군요! 그렇다면 이번에는 다른 웹 서버가 있는지 알아보겠습니다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 # 서버 스캔import optparsefrom socket import *from threading import *import ipaddressimport timecnt = 0# port-scan function, takes arguments tgtHost, tgtPorts def portScan(tgtHost, tgtPorts): global cnt try: tgtHost = str(tgtHost) # tries to get target IP address tgtIP = gethostbyname(tgtHost) except: # if unsuccesful, prints out following result print ('[-] cannot resolve ' + tgtHost + ': unknown host') return try: # tries to get target address tgtName = gethostbyaddr(tgtIP) cnt += 1 print ('\\n[+] scan results for: ' + tgtIP + &quot; Domain name is : &quot; + tgtName[0]) except: returndef main(): parser = optparse.OptionParser('usage %prog -t &lt;target-host&gt; -p &lt;target-port(s)&gt;') parser.add_option('-t', dest='tgtHost', type='string', help='specify target host') parser.add_option('-p', dest='tgtPort', type='string', help='specify target port(s), seperated by a comma, seperate ranges with a -') (options, args) = parser.parse_args() if (options.tgtHost == None) | (options.tgtPort == None): print (parser.usage) exit(0) else: tgtHost = options.tgtHost if tgtHost.endswith('.0'): hosts = ipaddress.ip_network(tgtHost+'/24') else: hosts = [tgtHost] if '-' in str(options.tgtPort): tgtPorts = options.tgtPort.split('-') tgtPorts = range(int(tgtPorts[0]),int(tgtPorts[1])) else: tgtPorts = str(options.tgtPort).split(',') for tgtHost in hosts: # print(tgtHost) portScan(tgtHost, tgtPorts) print('\\n Total number of web servers: ' + str(cnt)) print('\\n Scan duration : ', time.time() - start, 'sec')if __name__ == '__main__': start = time.time() # 시작 시간 설정 main() 위와 같이 nmap을 사용하여 측정하였습니다. 80포트와 8080포트를 기준으로 측정하여보면 결과는 아래와 같습니다. 우선 80포트를 기준으로 스캔하면 123456789101112131415161718192021222324(py36) ~/Documents/GitHub/Today_I_Learned/Network   master ●  python portScan13.py -t 166.104.177.0 -p 80[+] scan results for: 166.104.177.24 Domain name is : www.hanyang.ac.kr[+] scan results for: 166.104.177.62 Domain name is : nmail.hanyang.ac.kr[+] scan results for: 166.104.177.103 Domain name is : antispam1.hanyang.ac.kr[+] scan results for: 166.104.177.104 Domain name is : antispam2.hanyang.ac.kr[+] scan results for: 166.104.177.105 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.106 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.108 Domain name is : hanyang.ac.kr[+] scan results for: 166.104.177.109 Domain name is : antispam.hanyang.ac.kr[+] scan results for: 166.104.177.170 Domain name is : portal.hanyang.ac.kr[+] scan results for: 166.104.177.200 Domain name is : nf.hanyang.ac.kr Total number of web servers: 10 Scan duration : 0.28237009048461914 sec 8080포트를 스캔하면 12345678910111213141516171819202122232425(py36) ~/Documents/GitHub/Today_I_Learned/Network   master ●  python portScan13.py -t 166.104.177.0 -p 8080[+] scan results for: 166.104.177.24 Domain name is : www.hanyang.ac.kr[+] scan results for: 166.104.177.62 Domain name is : nmail.hanyang.ac.kr[+] scan results for: 166.104.177.103 Domain name is : antispam1.hanyang.ac.kr[+] scan results for: 166.104.177.104 Domain name is : antispam2.hanyang.ac.kr[+] scan results for: 166.104.177.105 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.106 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.108 Domain name is : hanyang.ac.kr[+] scan results for: 166.104.177.109 Domain name is : antispam.hanyang.ac.kr[+] scan results for: 166.104.177.170 Domain name is : portal.hanyang.ac.kr[+] scan results for: 166.104.177.200 Domain name is : nf.hanyang.ac.kr Total number of web servers: 10 Scan duration : 0.15810728073120117 sec 80포트와 8080포트 모두 스캔하면 12345678910111213141516171819202122232425 (py36) ~/Documents/GitHub/Today_I_Learned/Network   master ●  python portScan13.py -t 166.104.177.0 -p 80,8080[+] scan results for: 166.104.177.24 Domain name is : www.hanyang.ac.kr[+] scan results for: 166.104.177.62 Domain name is : nmail.hanyang.ac.kr[+] scan results for: 166.104.177.103 Domain name is : antispam1.hanyang.ac.kr[+] scan results for: 166.104.177.104 Domain name is : antispam2.hanyang.ac.kr[+] scan results for: 166.104.177.105 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.106 Domain name is : mail.hanyang.ac.kr[+] scan results for: 166.104.177.108 Domain name is : hanyang.ac.kr[+] scan results for: 166.104.177.109 Domain name is : antispam.hanyang.ac.kr[+] scan results for: 166.104.177.170 Domain name is : portal.hanyang.ac.kr[+] scan results for: 166.104.177.200 Domain name is : nf.hanyang.ac.kr Total number of web servers: 10 Scan duration : 0.16145086288452148 sec 이러한 결과들이 나옵니다. 이제 스레드를 사용하여 해보겠습니다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import optparsefrom threading import *from socket import *import ipaddressimport timescreenLock = Semaphore(value=1)cnt = 0def printResult(): print('\\n Total number of web servers: ' + str(cnt)) print('\\n Scan duration : ', time.time() - start, 'sec')def connScan(tgtHost, tgtPort): try: connSkt = socket(AF_INET, SOCK_STREAM) connSkt.connect((tgtHost, tgtPort)) # connect 까진 됨 #connSkt.send('Hi Hanyang\\r\\n') # send부터 안됨. #results = connSkt.recv(100) # print('[+] %d/tcp open' % tgtPort) result = str('[+] %d/tcp open' % tgtPort) return result except: result = str('[-] %d/tcp closed' % tgtPort) return result finally: connSkt.close()def portScan(tgtHost, tgtPorts): global cnt try: tgtHost = str(tgtHost) tgtIP = gethostbyname(tgtHost) except: print(&quot;[-] Cannot resolve '%s' : Unknown host&quot; %tgtHost) return try: tgtName = gethostbyaddr(tgtIP) cnt += 1 screenLock.acquire() print('\\n[+] Scan Results for: ' + tgtIP + &quot; Domain name is : &quot; + tgtName[0]) # for tgtPort in tgtPorts: # result = connScan(tgtHost, int(tgtPort)) # print(result) except: return finally: screenLock.release() def main(): parser = optparse.OptionParser('usage %prog -H &lt;target host&gt; -p &lt;target port&gt;') parser.add_option('-H', dest = 'tgtHost', type = 'string', help = 'specify target host') parser.add_option('-p', dest = 'tgtPort', type = 'string', help = 'specify target port[s] separated by comma') (options, args) = parser.parse_args() if (options.tgtHost == None) | (options.tgtPort == None): print (parser.usage) exit(0) else: tgtHost = options.tgtHost if tgtHost.endswith('.0'): hosts = ipaddress.ip_network(tgtHost+'/24') else: hosts = [tgtHost] if '-' in str(options.tgtPort): tgtPorts = options.tgtPort.split('-') tgtPorts = range(int(tgtPorts[0]),int(tgtPorts[1])) else: tgtPorts = str(options.tgtPort).split(',') for tgtHost in hosts: setdefaulttimeout(1) # 스레드 생성 t = Thread(target=portScan, args=(tgtHost, tgtPorts)) t.start() t.join() # 스레드가 종료될 때까지 기다림 printResult() if __name__ == '__main__': start = time.time() # 시작 시간 설정 main() 이를 실행하여 166.104.177.0/24의 모든 80포트를 스캔하면 아래와 같이 나옵니다. 또한 이를 실행하여 166.104.177.0/24의 모든 80과 8080포트를 스캔하면 아래와 같이 나옵니다.","link":"/2020/05/04/nmap2/"},{"title":"머신러닝을 위한 데이터 다루기","text":"머신러닝(machine learning)이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다.최근 머신러닝의 발전은 통계나 수학 이론보다 경험을 바탕으로 발전하는 경우도 많다. 컴퓨터 과학 분야가 이런 발전을 주도하고 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 사이킷런 라이브러리는 파이썬 API를 사용하는데 파이썬 언어는 배우기 쉽고 컴파일하지 않아도 되기 때문에 사용하기 편리하다.연구자들은 새로운 알고리즘을 끊임없이 개발하여 발표한다. 많은 사람이 이를 검증하고 사용해 본 다음 장단점을 파악하게 된다. 어느정도 시간이 지나서 이런 알고리즘이 유익하다고 증명되어 널리 사용하게 되면 사이킷런 라이브러리 개발자들이 이 알고리즘을 라이브러리에 추가한다. 그러므로 머신러닝 라이브러리에 포함된 알고리즘들은 안정적이며 성능이 검증되어 있다. 머신러닝에 쓰이는 용어 특성 : 데이터를 표현하는 하나의 성질ex) 생선 데이터 각각의 길이와 무게 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정, 사이킷런에서는 fit() 메서드가 하는 역할 K-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체를 모델이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함 정확도 : 정확한 답을 몇 개 맞췄는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨. matplotlib scatter()는 산점도를 그리는 맷플롯십 함수이다. 처음 2개의 매개변수로 x축과 y축 값을 전달한다. 이 값은 파이썬 list 또는 numpy 배열입니다. c 매개변수로 색깔을 지정합니다. RGB를 16진수로 지저하거나 색깔코드 ‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’ 중 하나를 지정합니다. 지저하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그립니다. maker 매개변수로 마커 스타일을 지정합니다. maker의 기본값은 o(circle, 원)입니다. scikit-learn KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스이다. n_neighbors 매개변수로 이웃의 개수를 지정한다. 기본값은 5이다. p 매개변수로 거리 재는 방법을 지정한다.(기본값 : 2) 1일 경우 : 맨해튼 거리 2일 경우 : 유클리디안 거리 n_jobs 매개변수로 사용할 CPU 코어를 지정한다. -1로 설정하면 모든 CPU 코어를 사용한다. 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다. 기본값은 1 fit() : 사이킷런 모델을 훈련할 때 사용하는 메서드이다. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달한다. predict() : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드이다. 특성 데이터 하나만 매개변수로 받는다. score() : 훈련된 사이킷런 모델의 성능을 측정한다. 처음 두 매개변수로 특성과 정답 데이터를 전달한다. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환한다. 지도학습과 비지도학습지도학습 (supervised learning) 지도학습에서는 데이터와 정답을 입력(input)과 타깃(target)이라고 하고, 이 둘을 합쳐 훈련 데이터(training data)라고 부른다. 입력으로 사용된 길이와 무게를 특성(feature)이라고 한다. 훈련세트와 테스트세트 훈련 세트 : 모델을 훈련할 때 사용하는 데이터. 보통 훈련 세트가 크면 클수록 좋다. 따라서 테스트 세트를 제외한 모든 데이터를 사용한다. 테스트 세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많다. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있다. 샘플링 편향(sampling bias) 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않은 경우 넘파이 (numpy) numpy는 파이썬의 대표적인 배열 라이브러리이다. 파이썬의 리스트로 2차원 리스트를 표현할 수 있지만 고차원 리스트를 표현하려면 번거롭다. 넘파이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. seed() : 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정한다. 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다. arrange() : 일정한 간격의 정수 또는 실수 배열을 만든다. 기본 간격은 1이다. 매개변수가 하나이면 종료 숫자를 의미한다. 0에서 종료 숫자까지 배열을 만든다. 종료 숫자는 배열에 포함되지 않는다. 12&gt;&gt;&gt; print(np.arange(3))[0 1 2] 매개변수가 2개이면 시작 숫자, 종료 숫자를 의미한다. 12&gt;&gt;&gt; print(np.arange(1, 3))[1 2] 매개변수가 3개면 마지막 매개변수가 간격을 나타낸다. 12&gt;&gt;&gt; print(np.arange(1, 3, 0.2))[1. 1.2 1.4 1.6 1.8 2. 2.2 2.4 2.6 2.8] shuffle() : 주어진 배열을 랜덤하게 섞는다. 다차원 배열일 경우 첫 번째 축(행)에 대하여만 섞는다.123456&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; np.random.shuffle(arr)&gt;&gt;&gt; print(arr)[[1 2] [5 6] [3 4]] 파이썬 리스트를 넘파이 배열로 바꾸는것은 쉽다. 넘파이 array() 함수에 파이썬 리스트를 전달하면 끝이다. 1input_arr = np.array(python_list) 넘파이는 슬라이싱 외에 배열 인덱싱(array indexing)이란 기능을 제공한다. 배열 인덱싱은 1개의 인덱스가 아닌 여러개의 인덱스로 한 번에 여러 개의 원소를 선택할 수 있다.예를 들면 아래처럼 input_arr에서 두 번째와 네 번째 샘플을 선택하여 출력 가능하다. 123print(input_arr[[1,3]])&gt;&gt; [[ 26.3 290. ] [ 29. 363. ]] 비지도학습 (unsupervised learning)타깃 데이터가 없다. 따라서 무엇을 예측하는것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 주로 활용한다. 데이터 전처리데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. (두 특성의 스케일이 다른 경우 등)알고리즘이 거리기반일 때 특히 그렇다. 여기에는 k-최근접 이웃도 포함된다. 이런 알고리듬들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰 주어야 한다. 이런 작업을 데이터 전처리(data preprocessing)라고 부른다. 표준점수데이터 전처리 방법 중 하는 표준점수이다(혹은 z 점수라고도 부른다). 표준점수는 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다. numpy의 std() 함수를 이용하여 표준점수를 계산할 수 있다.12mean = np.mean(train_input, axis=0)std = np.std(train_input, axis=0) 브로드캐스팅크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능이다. scikit-learntrain_test_split()훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수다. 여러 개의 배열을 전달할 수 있다. 테스트 세트로 나눌 비율은 test_size 매개변수에서 지정할 수 있으며 기본값은 0.25(25%)이다.shuffle 매개변수로 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞을지 여부를 결정할 수 있다. 기본값은 true이다. stratify 매개변수에 클래스 레이블이 담긴 배열(일반적으로 타깃 데이터)을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다. kneighbors()k-최근접 이웃 객체의 메서드이다. 이 메서드는 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로 이웃의 개수는 KNeighborClassifier 클래스의 객체를 생성할 때 지정한 개수를 사용한다. 하지만 n_neighbors 매개변수에서 다르게 지정할 수도 있다. return_distance 매개변수를 False로 지정하면 이웃 샘플의 인덱스만 반환하고 거리는 반환하지 않는다. 이 매개변수의 기본값은 True이다. 튜플이란? 파이썬 튜플은 리스트와 매우 비슷하다. 리스트처럼 원소에 순서가 있지만 한 번 만들어진 튜플은 수정할 수 없다. 튜플을 사용하면 함수로 전달한 값이 바뀌지 않는다는 것을 믿을 수 있기 때문에 매개변수 값으로 많이 사용된다. column_stack : 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결. 1234&gt;&gt;&gt; np.column_stack(([1,2,3],[4,5,6]))array([[1, 4], [2, 5], [3, 6]])","link":"/2024/03/25/machineLearning/"},{"title":"(프로젝트) 인공지능 스피커를 이용한 스포츠 질문 서비스","text":"인공지능과 응용 수업에서 했던 스포츠 매니아 프로젝트 입니다. MLB, La liga, EPL 등등 많은 스포츠 경기의 정보를 제공해주는 서비스 입니다. SK Nugu 스피커와 AWS Lambda, Django를 이용하여 만들었습니다. 여기로 가시면 서비스에 대해 자세히 알 수 있습니다. 시연 영상은 아래를 보시면 됩니다. https://www.youtube.com/watch?v=nPnfhiJ2C1A 제 목소리가 녹음되어 매우 민망하군요 (썸네일 사진 팀의 팬은 아닙니다)","link":"/2020/06/08/sportsMania/"},{"title":"Stack vs Queue","text":"선형구조란선형구조 (Linear Structure)는 데이터들이 일렬로 저장되어 있는 형태이다.일렬로 저장하는 방식은 리스트와 각 데이터가 다음 데이터의 위치를 가지는 연결 리스트 두 가지 방식이 있다. 일렬로 쭉 저장되어 있는 데이터를 사용하는 방법은 리스트와 연결 리스트 외에 사용 방법에 따라 스택 (Stack), 큐 (Queue) 데크가 추가된다. Stack 선형 자료구조의 일종으로 Last In First Out (LIFO). 즉, 나중에 들어간 원소가 먼저 나온다. 이것은 Stack의 가장 큰 특징. 차곡차곡 쌓이는 구조로 먼저 Stack에 들어가게 된 원소는 맨 바닥에 깔리게 된다. 그렇기 때문에 늦게 들어간 녀셕들은 그 위에 쌓이게 되고 호출 시 가장 위에 있는 녀석이 호출되는 구조이다. Queue 선형 자료구조의 일종으로 First In First Out (FIFO). 즉, 먼저 들어간 놈이 먼저 나온다. Stack과는 반대로 먼저 들어간 놈이 맨 앞에서 대기하고 있다가 먼저 나오게 되는 구조이다. 참고로 Java Collection에서 Queue는 인터페이스이다. 이를 구현하고 있는 Priority queue 등을 사용할 수 있다. 큐의 구현은 배열 (Array)을 이용하거나 순환 큐, 또는 연결 리스트 (Linked List)를 이용해도 된다. Stack은 언제 사용할까? 스택은 대표적으로 프로그램을 수행할 때 사용합니다Main 프로그램에서 함수 A를 호출하면 Main 프로그램 위에 함수 A가 쌓이고, 함수 A의 수행 중에 함수 B가 호출되면, 함수 A위에 함수 B가 스택처럼 쌓이게 된다.함수 B의 실행이 완료되면 함수 A가 실행되고, 함수 A의 실행이 완료되면 주 프로그램의 실행이 시작된다 (LIFO) Queue는 언제 사용할까? 컴퓨터 안에 여러 개의 프로세스가 수행 중인데, 새로운 프로세스가 수행되어야 하는 경우 기존에 수행되던 프로세스 중에서 가장 먼저 메모리에 올라온 프로세스가 아웃되고 (실행), 새로운 프로세스를 메모리에 올리게 됩니다. 이 경우 운영체제는 현재 수행 중인 프로세스를 큐의 형태로 관리합니다. Windows 운영체제를 사용하는 컴퓨터에서 수행 중인 프로그램에 이벤트 (버튼 누르기, 윈도우 크기 조정, 메뉴 선택하기 등)가 발생하면 발생한 이벤트가 큐에 저장되고, 수행중인 프로그램이 큐에 저장된 것을 앞에서부터 읽어 와서 처리한다 (FIFO) Stack 사용 방법 push : 스택에 자료 넣음 pop : 스택에서 자료 뺌 top : 스택의 가장 위에 있는 자료 empty : 스택이 비어있는지 아닌지 확인 size : 스택에 저장된 자료의 개수 확인 C++ : STL의 stack 사용 Java : java.util.Stack Queue 사용 방법 push : 큐에 자료 넣음 pop : 큐에서 자료 뺌 front : 큐의 가장 앞에 있는 자료 back : 큐의 가장 뒤에 있는 자료 empty : 큐가 비어있는지 아닌지 확인 size : 큐에 저장되어있는 자료 개수 확인 C++ : STL의 queue 사용 Java : java.util.Queue Deque 사용 방법 양 끝에서만 자료를 넣고 양 끝에서 뺄 수 있는 자료구조 push_front : 큐에 자료를 넣음 pop : 큐에서 자료를 뺌 front : 큐의 가장 앞에 있는 자료 back : 큐의 가장 뒤에 있는 자료 empty : 큐가 비어있는지 아닌지 확인 size : 큐에 저장되어있는 자료 개수 확인 스택과 큐의 java 코드 및 실행결과123456789101112131415161718192021222324252627import java.util.Stack;import java.util.Queue;public class stackQueue { public static void main(String[] args) { Stack&lt;String&gt; st = new Stack&lt;String&gt;(); Queue&lt;String&gt; q = new LinkedList&lt;String&gt;(); st.push(&quot;0&quot;); st.push(&quot;1&quot;); st.push(&quot;2&quot;); q.offer(&quot;0&quot;); q.offer(&quot;1&quot;); q.offer(&quot;2&quot;); System.out.println(&quot;-- Stack --&quot;); while (!st.isEmpty()) { System.out.println(st.pop()); } System.out.println(&quot;-- Queue --&quot;); while (!q.isEmpty()) { System.out.println(st.poll()); } }} 결과12345678-- Stack --210-- Queue --012 참고스택 활용 문제괄호 : https://www.acmicpc.net/problem/9012쇠막대기 : https://www.acmicpc.net/problem/10799에디터 : https://www.acmicpc.net/problem/1406 큐 활용 문제조세퍼스 문제 : https://www.acmicpc.net/problem/1158 덱 구현 문제https://www.acmicpc.net/problem/10866","link":"/2020/10/16/stackAndQueue/"},{"title":"Quick Sort란?","text":"퀵 정렬(Quick Sort)에 대해 공부해보겠습니다.퀵 정렬은 일반적으로 사용되고 있는 아주 빠른 정렬 알고리즘입니다.정렬 속도가 빠른데서 착안해 퀵 정랼이라는 이름이 붙었습니다.다른 원소와의 비교만으로 정렬을 수행하는 알고리즘 입니다. 1. 동작 방식동작을 수행할 때 각 그룹에 대해 pivot 설정과 그룹 나눔을 반복하게 되며모든 그룹이 1개가 되면 정렬을 마치게 됩니다. 이 과정을 분할정복 (Divide and Conquer)이라고 합니다. 1) Divide and ConquerDivide (PARTITION 과정) 정렬 해야 할 리스트 안에 있는 한 요소를 선택해 pivot으로 지정,pivot을 기준으로 작은 요소들을 왼쪽, 큰 요소들을 오른쪽으로 이동 Conquer pivot을 제외한 왼쪽, 오른쪽 리스트를 다시 정렬분할 된 부분 리스트에 대해 다시 pivot을 지정하고2개의 부분리스트로 나누는 과정 반복 Pivot 설정 2) 동작 과정 임의의 한 요소를 pivot으로 선택Pivot을 기준으로 작은 값을 왼쪽, 큰 값을 오른쪽으로 이동Pivot을 제외한 왼쪽, 오른쪽 리스트에 partition 과정 반복위의 과정을 반복해 각각 분할된 리스트의 크기가 0이나 1이 되면 정렬이 끝남 3) 특징 장점 평균적으로 빠른 수행 속도를 가짐 단점 Unstable한 정렬임 4) 시간 복잡도입력되는 자료에 따라 시간복잡도가 달라질 수 있다. Worst Case 완전하게 한쪽으로 몰려 그룹이 나뉘는 경우,left가 n-1개의 요소를 갖고 right가 0개의 요소를 가질 때 T(n) = 1+2+…+n-1+n = (n-1)/2T(n) = O(n^2) Best case pivot 값이 계속 중앙에 위치해 left, right가 균등하게 나뉠 때 T(n) = 2T(n/2) + O(n) (O(n)은 Divide 과정) T(n) = O(nlog_2n) Average Case best case에 보통 가까움 일정한 비율로 분할 된다면 O(nlog_2n)의 시간 복잡도를 가짐 극단적이게 분할되는 비율이 치우친 경우에도 동일함. 아래 증명에 의해 log_2n으로 표햔이 가능하다 따라서 T(n)은 아래와 같이 구할 수 있다. 예제 코드12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.Scanner;// 퀵 정렬class QuickSort { // 배열 요소 a[idx1]과 a[idx2]의 값을 바꿉니다. static void swap(int[] a, int idx1, int idx2) { int t = a[idx1]; a[idx1] = a[idx2]; a[idx2] = t; } // 퀵 정렬 static void quickSort(int[] a, int left, int right) { int pl = left; // 왼쪽 커서 int pr = right; // 오른쪽 커서 int x = a[(pl + pr) / 2]; // 피벗 do { while (a[pl] &lt; x) pl++; while (a[pr] &gt; x) pr--; if (pl &lt;= pr) swap(a, pl++, pr--); } while (pl &lt;= pr); if (left &lt; pr) quickSort(a, left, pr); if (pl &lt; right) quickSort(a, pl, right); } public static void main(String[] args) { Scanner stdIn = new Scanner(System.in); System.out.println(&quot;퀵 정렬&quot;); System.out.print(&quot;요솟수：&quot;); int nx = stdIn.nextInt(); int[] x = new int[nx]; for (int i = 0; i &lt; nx; i++) { System.out.print(&quot;x[&quot; + i + &quot;]：&quot;); x[i] = stdIn.nextInt(); } quickSort(x, 0, nx - 1); // 배열 x를 퀵 정렬 System.out.println(&quot;오름차순으로 정렬했습니다.&quot;); for (int i = 0; i &lt; nx; i++) System.out.println(&quot;x[&quot; + i + &quot;]＝&quot; + x[i]); }}","link":"/2020/10/18/quickSort/"},{"title":"도커란?","text":"서버 개발자들은 개발한 서버 프로그램을 배포할 환경을 세팅하는 것에 대해 잘 알아야 한다. 그 환경은 VM이 될 수도 있고, 물리 서버가 될 수도 있다. 최근에는 컨테이너 환경에서 배포하는 경우가 많다. 특히 쿠버네티스(Kubernetes) 환경을 사용하려면, 기본 컨테이너로 **도커(Docker)**를 이해해야 한다. Docker 개념Docker는 소프트웨어 설치, 실행, 배포, 제거를 단순화하는 도구이다. 이는 **컨테이너(Container)**라는 UNIX 기술을 활용하여 이루어진다. 컨테이너는 실행 환경을 격리하여 특정 자원에만 접근할 수 있도록 제한된 실행 환경을 제공한다. 직접 컨테이너를 설정하는 것은 어렵기 때문에, 이를 쉽게 다룰 수 있도록 Docker가 등장했다. 컨테이너는 하드웨어 가상화가 아니다. 컨테이너 내의 프로세스는 컨테이너 외부의 프로세스와 동일한 리눅스 커널 위에서 실행되지만, 서로 격리된 환경을 제공한다. Docker와 VM의 차이 VM (가상 머신): 각 인스턴스는 OS 전체를 포함하여 실행됨 → 무겁고 부팅 시간이 길다. Docker 컨테이너: 호스트 OS의 커널을 공유하며, 애플리케이션 및 필요한 라이브러리만 포함 → 가볍고 빠른 실행 가능. Docker 실습docker 설치 및 실행Docker를 설치한 후, busybox 컨테이너를 실행해 보자. 1docker run busybox echo &quot;Hello World&quot; 컨테이너 상태 확인1docker ps --all 컨테이너 삭제1docker rm &lt;CONTAINER_ID&gt; Docker 이미지 삭제1docker rmi &lt;IMAGE_ID&gt; Dockerfile 작성 및 실행Node.js 애플리케이션을 Docker로 실행하기아래와 같은 Dockerfile을 작성하여 Node.js 애플리케이션을 실행할 수 있다. 123FROM node:7ADD app.js /app.jsENTRYPOINT [ &quot;node&quot;, &quot;app.js&quot; ] Docker 이미지 빌드: 1docker build -t my-node-app . Docker 컨테이너 실행: 1docker run -p 8080:8080 -d my-node-app 결론Docker는 개발 및 배포 환경을 쉽게 관리할 수 있도록 돕는 컨테이너 기반 가상화 기술이다. 이를 통해 애플리케이션의 실행 환경을 격리하고, 어디서든 동일한 환경에서 실행할 수 있다.","link":"/2022/02/18/whatIsDocker/"},{"title":"쿠버네티스란?","text":"쿠버네티스(Kubernetes, k8s)는 컨테이너화된 애플리케이션을 자동으로 배포, 스케일링, 운영할 수 있도록 도와주는 오픈소스 컨테이너 오케스트레이션 플랫폼이다. 왜 쿠버네티스인가?최근에는 **마이크로서비스 아키텍처(MSA)**가 대두되면서, 하나의 거대한 애플리케이션을 여러 개의 독립적인 마이크로서비스로 분리하는 경우가 많다. 이로 인해 배포 주기가 달라지고, 서비스 간 조율이 필요하며, 복잡성이 증가하게 된다. 이를 해결하기 위해 컨테이너(Container) 기반의 배포 방식이 도입되었고, 컨테이너 관리 및 오케스트레이션을 위해 쿠버네티스가 등장했다. 쿠버네티스의 주요 개념Pod 컨테이너의 논리적 그룹으로, 쿠버네티스에서 배포 가능한 최소 단위. 같은 Pod 내 컨테이너는 네트워크와 스토리지를 공유. ReplicaSet 특정 수의 Pod가 항상 실행되도록 보장하는 역할. 장애 발생 시 새로운 Pod를 자동 생성. Service 여러 Pod를 하나의 네트워크 엔드포인트로 묶어 외부 또는 내부에서 접근 가능하게 함. 로드 밸런싱 역할 수행. ConfigMap &amp; Secret 환경 변수, 설정 정보 등을 저장하여 애플리케이션에서 동적으로 사용할 수 있도록 지원. Deployment 애플리케이션을 배포하고, 롤링 업데이트 및 롤백을 수행하는 핵심 오브젝트. 쿠버네티스의 장점 자동화된 배포 및 스케일링: 트래픽에 따라 Pod를 동적으로 조정 가능. 자원 효율성: 필요한 만큼의 리소스를 사용하여 운영 비용 절감. 셀프 힐링(Self-healing): 장애 발생 시 자동 복구 및 재시작. 멀티 클라우드 지원: AWS, GCP, Azure 등 다양한 환경에서 실행 가능. 참고 자료 쿠버네티스 공식 문서 Docker와 Kubernetes 차이 결론쿠버네티스는 컨테이너 기반 애플리케이션의 배포 및 운영을 자동화하는 필수 기술이다.이를 통해 복잡한 마이크로서비스 환경에서도 효율적인 애플리케이션 관리가 가능하다.","link":"/2022/03/08/whatIsKubernetes/"},{"title":"Zookeeper를 Docker에 설치하기","text":"Zookeeper는 공식 Docker 이미지를 제공하므로 이를 활용하여 컨테이너를 실행할 수 있습니다.또한, Kafka와 연동할 때 필수적으로 사용됩니다. Zookeeper 설치 및 실행공식 이미지 다운로드 및 컨테이너 실행1docker run --name zookeeper -d zookeeper:3.4.13 컨테이너 이름: zookeeper 백그라운드 실행: -d 사용 이미지: zookeeper:3.4.13 컨테이너 상태 확인1docker ps 출력 예시: 12CONTAINER ID IMAGE STATUS PORTS NAMES656fd86c8263 zookeeper:3.4.13 Up About an hour 2181/tcp, 2888/tcp, 3888/tcp zookeeper Zookeeper CLI 접속CLI로 Zookeeper에 접속1docker run -it --rm --link zookeeper:zookeeper zookeeper:3.4.13 zkCli.sh -server zookeeper --link zookeeper:zookeeper → 실행 중인 zookeeper 컨테이너와 연결 zkCli.sh -server zookeeper → CLI 실행 CLI 실행 결과12[zk: zookeeper(CONNECTED) 0] ls /[zookeeper] 참고 자료 Zookeeper 공식 Docker 이미지 Zookeeper 공식 문서 결론Zookeeper를 Docker로 간단하게 배포할 수 있으며, Kafka와의 연동에도 필수적입니다.Docker Compose를 활용하면 더욱 편리하게 관리할 수 있습니다.","link":"/2022/03/19/zookeeperOnDocker/"},{"title":"AWS의 Zelkova와 tiros란? AWS의 S3 Configuration Errors를 줄이려는 노력","text":"Tiros는 네트워크 연결 가능성 추론 도구입니다. 이 도구는 상용 자동 논리 검증 도구를 사용하여 보안 취약성의 원인이 되는 네트워크 구성 오류를 찾아냅니다. Tiros는 최근 Amazon inspector 서비스에 도입되어 현재 고객이 클라우드에서 애플리케이션을 빌드하는 데 사용 중인 네트워크 보안 분석 기능의 기초가 됩니다. 또한 Tiros는 AWS 내에서 기존 AWS 네트워킹 기능을 기반으로 구축 된 다양한 AWS 서비스의 규정 준수 검증과 보안 불변성 준수를 자동화하는 데에도 사용됩니다. Configuration Error로 인한 피해들.Amazon은 AWS S3의 Configuration errors의 가능성을 줄이기 위해 두 가지 새로운 도구인 Zelkova와 Tiros를 개발하여 누가 데이터와 리소스에 액세스 할 수 있고 이를 사용하여 무엇을 할 수 있는지를 명확하게 파악하고 있습니다. 이 툴은 액세스 제어를 분석 및 평가하고 클라우드 환경의 개방성을 매핑합니다. Aws는 고객이 클라우드 환경을 안전하게 보호하는하는 데 어려움을 겪고 있습니다. Misconfiguration Errors는 여전히 흔한 일이며, 엄청난 양의 민감한 데이터가 노출되도록 하였습니다. Verizon, BoossAllenHamiltion, WWE 재단, Alteryx, USNationalCredtiFederation, AustraliaBroadcastionCorporation, Accenture를 포함한 기관들은 모두 configuration Error로 인해 정보를 노출시켰습니다. AWS는 이전에 서비스 보안을 향상시키고 configuration Errors 발생 가능성을 줄이기 위해 노력하였습니다. 작년에 AWS에 저장된 중요한 데이터를 자동으로 검색하고 보호하기 위한 머신러닝 툴인 Macie와 기본 암호화, 상세 재고 보고서, 허가 확인 등 S3 보안을 개선하기 위한 일련의 새로운 기능을 출시하였습니다. 2018년 2월, FedEx가 여권, 운전 면허증, 고객 기록 등 10만 건 이상의 문서를 유출당했다는 뉴스가 전해진 지 불과 며칠만에 AWS는 S3 Bucket Permissions Check서비스를 모든 사용자에게 무료로 제공했습니다. AWS S3(SimpleStorageService)는 이 회사의 객체 스토리지 제품입니다. Skihigh Networks에 따르면 모든 S3 버킷의 7%는 제한 없이 공개 액세스 할 수 있으며 35%는 암호화되지 않은 상태로 남아 있습니다. 최근 노출되거나 보호되지 않은 데이터의 예로는 50만대 이상의 차량 추적 장치의 로그인 자격 증명, 2억명의 미국 유권자 기록, 미 육군 정보국에 속하는 민감한 데이터 등이 있습니다. 정보를 훔치는 것뿐만 아니라, 해커들은 랜섬 웨어로 데이터를 차단하고 암호를 알아내기 위해 컴퓨팅 자원을 사용하는 것이 발견되었습니다. 아마존 Zelkova와 Tiros가 하는 것.Zelkova와 Tiros는 Amazon 제품에 대한 검증 도구와 기술을 개발하는 AWS의 ARG(Automated Describe Group)에 의해 만들어졌습니다. 자동화된 추론은 공식적인 검증 방법으로서, semantic한 추론을 취하고 수학 공식을 특정 질문에 대한 대답과 정책이 예상대로 작동하는지 검증하는 것입니다. ARG는 AWS보안 팀에 속해 있으며 2년 이상 내부적으로 툴을 개발해 왔습니다. 2018년 6월에 처음 발표된 Zelkova는 자동화 된 추론을 사용하여 정책과 정책의 향후 결과를 분석합니다. 회사의 IAM(Identity and Access Management), S3 및 기타 리소스 정책과 호환되는 이 솔루션을 사용하면 조직에서 벤치 마크를 생성하고 현재 정책 설정의 결과를 사용자에게 알릴 수 있습니다. 예를 들어 S3정책에 대해 사용되는 경우 권한이 없는 사용자가 버킷을 읽거나 쓸 수 있는지 여부를 알려 줍니다. Tiros는 네트워크 간의 연결을 매핑 합니다. 예를 들어 인터넷에서 EC2 인스턴스에 연결 할 수 있는지 여부와 관련된 질문에 대답 할 수 있습니다. Zelkova와 Tiros는 Internal tools로 시작했습니다. Zelkova는 S3 대시 보드의 일부로 내부적으로 사용되며 AWS Macie 내에서 사용됩니다. 투자 관리 회사인 Bridgewater Associates는 초기에 이들을 테스트할 수 있는 권한을 부여 받았습니다. Bridgewater Associates의 수석 클라우드 보안 설계자인 DanPeebles는 “브리지 워터는 Zelkova를 사용하여 데이터 유출, Misconfiguration 및 기타 악의적이고 우발적인 바람직하지 않은 행동을 허용하지 않음을 확인하고 보장합니다.”라고 Zelkova announcement에서 말했습니다. “Zelkova를 통해 보안 전문가들은 이해한 내용을 한번에 인코딩한 다음 이를 관련 정책에 기계적으로 적용하여 오류가 발생하기 쉽고 느린 인적 검토를 방지하는 동시에 IAM 정책의 정확성과 보안에 대한 높은 신뢰도를 제공할 수 있습니다.” 현재 두 tool 모두 공개용으로 사용할 수 없습니다. Bridgewater는 그들이 특히 사용자 친화적이지 않은 “raw state”에 있다고 말한다. 아마존은 더 넓은 이용 가능성이나 가격에 관한 어떠한 정보도 공개하기를 거부했다. Amazon의 클라우드 Configuration challenges.Amazon 및 Microsoft Azure와 같은 클라우드 공급 업체는 서비스와 관련하여 일정 수준의 보안을 제공하고 권장 모범 사례를 제공하지만, 여전히 많은 책임이 고객에게 있는 공유 보안 모델 아래에서 운영되며, 이러한 모델에서는 문제가 자주 발생합니다. 영국에 본사를 둔 MSP Claranet의 선임 사이트 안정성 엔지니어인 SteveSmith는 “AWS S3버킷 주변에서 보고되고 있는 보안 문제는 플랫폼 자체와 거의 관련이 없다”며 “그러나 가장 큰 약점인 플랫폼을 사용하는 사람들과 관련된 모든 것”이라고 말했다. “AWS는 configuration을 support 하도록 설계 된 많은 합리적인 기본 값을 설정합니다. S3버킷은 기본적으로 private이지만, 안타깝게도 플랫폼을 사용하는 방법을 모르면 문제를 쉽게 야기할 수 있습니다.”라고 Smith는 덧붙였습니다. 교육이 부족하고 구축 및 관리 툴의 복잡성이 증가하고 관리자가 대처해야 하는 서비스의 수가 계속 증가하고 있으며, 클라우드 환경은 보안 팀의 시야에서 벗어나 쉽게 전환될 수 있으며, 데이터 유출이 계속해서 일상적인 문제가 되고 있습니다. 로그 관리 및 분석 공급 업체 (및 AWS고객) Sumo Logic의 George Gerchow는 “소비자는 shared responsibility model을 이해하고 데이터를 보호하기 위해 best practice를 적용하여 역할을 수행해야 합니다.”라고 말합니다. “AWS는 이러한 교육을 제공하기 위해 많은 것을 하지 않기 때문에 소비자들은 모든 것이 잘 이루어졌다고 믿기 쉽습니다.” AWS는 이러한 새로운 툴을 사용하여 사람이 실수할 가능성을 줄이고 데이터 누출 가능성을 줄이는 방안을 모색하고 있습니다. 하지만 그들이 도와 줄까요? “우리의 보안 목표는 AWS에서 데이터가 유출되는 것을 막는 것입니다.”라고 Bridgewater Associates의 보안 설계자 Greg Frascadore는 AWS New York Summit에서 Zelkova와 Toris에 관한 프레젠테이션에서 말했습니다. “우리가 찾고자 하는 것은 공식적인 분석과 우리가 설치한 보안 통제 장치가 우리가 생각하는 대로 작동하고 있는지 확인하는 체계적인 방법입니다.” Frascadore는 이러한 툴의 사용 사례에는 개별 보안 제어 검증, 보안 제어 관련 벤치 마크 생성, 다수의 고객에게 관련 제어 검색, 검증 자동화, 설계 단계에서 검증 등이 포함된다고 말했습니다. “ 이러한 도구의 가장 중요한 점은 설계 단계에서 확인할 수 있다는 것입니다. 실제로 AWS 인프라를 변경하기 전에 보안 검증을 통해 계정에 취약점을 적용할 수 있었으면 합니다.”라고 그는 말했습니다. 다른 이들은 이러한 새로운 도구가 장점과 잠재적인 단점을 모두 가지고 있다고 경고한다. Sumo Logic의 Gerchow는 시작은 좋지만 비용이 많이 들고 올바르게 구성해야 하므로 복잡성이 가중될 수 있으며, 멀티 클라우드 또는 하이브리드 구축에 도움이 되지 않는다고 말합니다. BTB Security의 수석 정보 보안 advisor인 MattWilson은 “Zelkova와 Tiros에게 엄청난 잠재적 이익이 있지만 이러한 팀은 데이터를 운영할 수 있어야 합니다. 그렇지 않으면 그들은 단지 도구를 사용하고 있을 뿐입니다.”라고 말합니다. “그들은 여전히 조직의 누군가가 그들을 execute하고, 그들의 output을 분석하고, 제공된 정보에 따라 행동할 것을 요구하고 있습니다. 고급 알고리즘과 매끄러운 인터페이스도 좋지만, 이것들 만으로는 충분하지 않습니다.” 출처 : CSO online US Paper : Reachability Analysis for AWS-based Networks","link":"/2020/04/21/tiros/"},{"title":"AI 데이터 전처리","text":"AI 모델 성능을 끌어올리는 데이터 전처리, 결측치, 이상치, 스케일링, 인코딩, 데이터 분리, 파이프라인 자동화까지 실무 위주로 정리. 데이터 편향성정의 데이터 편향성은 수집,처리,라벨링 과정에서 특정 집단, 속성, 상황이 과대 또는 과소대표되는 문제를 의미. 영향 모델의 예측 성능 불균형 특정 사용자 그룹에 대한 차별적 결과 서비스 신뢰도 하락 및 법적, 윤리적 문제 발생 가능성 대응 전략(1) 데이터 수집 단계 방법 설명 예시 다양한 출처 확보 지역, 성별, 연령, 기기, 시간대 등 다양한 상황·집단 포함 전국 단위 고객 데이터를 수집해 수도권 집중 현상 방지 대표성 확보 실제 서비스 대상 분포를 반영한 표본 설계 신용평가 모델 → 나이·직업군 분포를 실제 금융권 고객 비율과 일치시키기 희귀 사례 보강 소수 클래스, 드문 이벤트를 의도적으로 추가 사고율 낮은 보험 청구 사례, 사기 거래 패턴 보강 (2) 데이터 전처리 단계 방법 설명 예시 데이터 균형화 클래스 불균형 해소 → 오버샘플링, 언더샘플링 SMOTE로 소수 클래스 생성, 다수 클래스 축소 데이터 증강 데이터 크기와 다양성 확대 텍스트 동의어 치환, 이미지 회전·색상 변경 라벨링 품질 관리 주관적 편향 최소화 다수의 어노테이터, 합의 기반 라벨 결정, 가이드라인 강화 (3) 공정성 평가 단계 방법 설명 모델 편향 측정 Demographic parity, Equalized odds, Equal opportunity 등 공정성 메트릭 활용 시뮬레이션 테스트 특정 속성(성별·지역·연령)에 따른 출력 차이 분석 A/B 테스트 실제 서비스에서 집단별 성능 차이 확인 및 개선 방향 도출 (4) 반복 개선 단게 정기적인 데이터 업데이트 편향 지표 모니터링 모델,데이터,정책 동시 개선 사례 분야 편향 문제 대응 이미지 인식 서양인 얼굴 위주 학습 -&gt; 동양인 인식률 낮음 아시아인 데이터 보강, 데이터 증강 채용 AI 특정 성별,학력 우대 -&gt; 차별적 추천 성별,학력 비식별화, 공정성 메트릭 기반 모델 수정 금융 신용평가 나이,지역별 데이터 편중 -&gt; 특정 계층 신용 점수 불리 인구 통계 기반 샘플링, 리밸런싱 기법 적용 정규화(Normalization) &amp; 표준화(Standardization)1. 개념 비교 항목 정규화 (Normalization) 표준화 (Standardization) 목적 데이터 값을 일정 범위로 스케일링 데이터 분포를 평균 0, 표준편차 1로 변환 수식 $x’ = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$ → [0, 1] $x’ = \\dfrac{x - \\mu}{\\sigma}$ → 평균=0, 분산=1 전제 분포 가정 없음 가우시안(정규) 분포 가정 선호 주요 사용처 딥러닝 (신경망, 이미지/텍스트 임베딩 등) 통계/머신러닝 (SVM, PCA, K-Means, 회귀 등) 2. 활용 분야 분야 주로 쓰는 기법 이유 딥러닝 (신경망) 정규화 입력값 범위 고정 → 가중치 업데이트 안정화, Gradient Exploding/Vanishing 완화 머신러닝 (통계 기반) 표준화 공분산 기반 알고리즘(PCA, LDA, SVM 등)에서 변수 간 스케일 차이 제거 거리 기반 알고리즘 정규화 or 표준화 KNN, K-Means 등에서 변수 단위 차이에 따른 거리 계산 왜곡 방지 3. 주의할 점 학습 데이터로만 스케일링 기준($x_{min}$, $x_{max}$, $\\mu$, $\\sigma$)을 계산하고, 테스트 데이터는 같은 기준을 적용해야 함 → 데이터 누수(Data Leakage) 방지 극단값(Outlier) 존재 시 Min-Max 정규화는 민감 → 표준화(Z-score)나 로버스트 스케일링(Robust Scaler) 사용 고려 딥러닝에서도 Batch Normalization, Layer Normalization처럼 학습 과정 내에서 표준화 계열 기법을 사용하기도 함 텍스트 데이터 정제 (스페셜 토큰) 자연어 처리(NLP) 모델은 입력 텍스트를 토큰(token) 단위로 분리하여 처리 모델 학습 및 추론 과정에서 특별한 의미를 가지는 토큰(Special Token)을 활용 시퀀스 처리, 패딩, 마스킹, 문장 구분, 분류 등에 필수적 역할 주요 스페셜 토큰 토큰 의미 / 역할 사용 예시 &lt;s&gt; / [BOS] 문장 시작(Start of Sequence) 디코더가 문장 생성 시작을 알림 &lt;/s&gt; / [EOS] 문장 끝(End of Sequence) 문장 종료 지점 표시 &lt;pad&gt; 패딩(Padding) 시퀀스 길이 맞추기 → 배치 단위 처리 &lt;unk&gt; 알 수 없는 단어(Unknown) 사전에 없는 단어(OOV) 대체 &lt;sep&gt; 구분자(Separator) BERT 문장쌍 입력 시 두 문장 구분 &lt;cls&gt; 분류(Classification) 토큰 문장 전체 의미 요약 → 분류 태스크에 활용 (BERT) &lt;mask&gt; 마스킹(Masked token) MLM(Masked Language Model) 학습 시 단어 가려놓고 문맥으로 예측 모델별 처리 방식Sequence-to-Sequence (S2S) 모델 사용 예 : 기계번역(Transformer, LSTM 기반 Encoder-Decoder) 특징 길이 맞추기 위해 Padding 사용 (보통 EOS 뒤에 채움) 사전에 없는 단어는 &lt;unk&gt;로 치환 → 정보 손실 발생 예:12Inoput : &lt;s&gt; 나는 학생 입니다 &lt;/s&gt; &lt;pad&gt; &lt;pad&gt;Output : &lt;s&gt; I am a student &lt;/s&gt; &lt;pad&gt; &lt;pad&gt; Bert (Bidirectional Encoder Representations from Transformers) 사용 예:문장 분류, 개체명 인식, 질의응답 등 특징 입력 시 [CLS] / [SEP] 토큰으로 문장 전체 구조 표현 단어를 subword 단위로 분해 (WordPiece, SentencePiece) -&gt; OOV 문제 완화 학습 시 일부 토큰을 [MASK]로 가려 문맥 기반 예측 학습(MLM) 예:12Input : [CLS] 나는 [MASK] 입니다 [SEP]Model : 학생 -&gt; 문맥으로 예측 Pandas 핵심 기능 정리데이터 입출력 &amp; 기본 확인 범주 코드 예시 설명 데이터 불러오기 pd.read_csv('data.csv') CSV 파일 읽기 데이터 확인 df.head() 상위 5개 행 보기 df.info() 데이터 타입 및 결측치 정보 df.describe() 기초 통계 정보 확인 결측치 처리 코드 예시 설명 df.isnull().sum() 컬럼별 결측치 개수 확인 df.dropna() 결측치가 있는 행 제거 df['col'].fillna(df'['col']).mean() 평균값으로 결측치 채우기 df.dropna(axis=0, thresh=5) 5개 미만 값만 있는 행 삭제 df.dropna(axis=1) 결측치가 하나라도 있는 열 삭제 inplace=True df.dropna(inplace=True) -&gt; 원본 df 자체가 변경됨 기본값 False는 새로운 DataFrame 반환 -&gt; 원본 유지 중복 처리 코드 예시 설명 `df.drop_duplicates() 완전 동일한 행 제거 `df.drop_duplicates(subset=[‘user_id’,’date]) 특정 컬럼 기준 중복 제거 데이터 타입 변환 코드 예시 설명 `pd.to_datetime(df[‘data’]) 문자열 -&gt; 날짜 변환 `df[‘category’].astype(‘category’) 범주형 데이터로 변환 조건 필터링 / 새로운 컬럼 생성 코드 예시 설명 `df[df[‘age’] &gt; 30] age가 30 이상인 행 선택 `df[‘bmi’] = df[‘w’]/(df[‘h’]/100) ** 2 BMI 계산 후 컬럼 추가 그룹화 / 집계 코드 예시 설명 `df.groupby(‘category’)[‘sales’].sum() 카테고리별 판매 합계 정렬 코드 예시 설명 `df.sort_values(by=’date’, ascending=False) 날짜 기준 내림차순 정렬 이상치 탐지 (IQR 활용)1234Q1 = df['c'].quantile(0.25)Q3 = df['c'].quantile(0.75)IQR = Q3 - Q1outliers = df[(df['c'] &lt; Q1 - 1.5*IQR) | (df['c'] &gt; Q3 + 1.5*IQR)] 문자열 처리 코드 예시 설명 df[‘name’].str.lower() 소문자 변환 df[‘name’].str.strip() 공백 제거 df[‘email’].str.split(‘@’).str[1] 이메일 도메인 추출 날짜 / 시간 처리 코드 예시 설명 `df[‘date’] = pd.to_datetime(df[‘date’]) 날짜 변환 `df[‘year’] = df[‘date’].dt.year 연도 추출 `df[‘month’] = df[‘date’].dt.month 월 추출 `df[‘weekday’] = df[‘date’].dt.day_name() 요일 추출 `df[‘end’] - df[‘start’]).dt.days 날짜 차이 계산 범주형 인코딩 코드 예시 설명 df['cat'].astype('category').cat.codes 레이블 인코딩 `pd.get_dummies(df, columes=[‘cat’]) 원-핫 인코딩 데이터 병합 코드 예시 설명 `pd.merge(df1, df2, on=’id’, how=’inner’) 내부 조인 `pd.merge(df1, df2, on=’id’, how=’left’) 왼쪽 조인 피벗 테이블 코드 예시 설명 df.pivot_table(index=’cat’, columns=’year’, values=’sales’, aggfunc=’sum’) 피벗 테이블 생성 샘플링 코드 예시 설명 `df.sample(frac=0.1, random_state=42) 10% 무작위 샘플 추출 포인트 inplace=True -&gt; 메모리 절약, 체인 연산 어려움 -&gt; 실수 시 원복 힘듦 체인 연산 / 디버깅 편의성 위해 보통 기본값 False 사용 권장 후 필요한 경우 마지막에 덮어쓰기 대용량 데이터 : chunksize 옵션으로 CSV 읽기 (pd.read_csv(..., chunksize=100000)) 데이터 증강 (Data Augmentation) 정리 데이터 증강은 기존 데이터를 변형/합성하여 모델이 더 일반화되도록 돕는 기법 특히 이미지, 음성, 텍스트 등 비정형 데이터 처리에서 많이 사용 목적 데이터 부족 문제 해소 모델이 다양한 상황 (노이즈, 가림, 색 변화 등)에 강건해지도록 개선 과적합(Overfitting) 완화 주요 기법별 특징 기법 적용 분야 방법 장점 주의할 점 Gaussian Blur 이미지 가우시안 커널로 이미지 흐리게 해상도/화질 저하 상황 대응, 과적합 방지 디테일 손실 → 작은 물체 탐지 악영향 Random Erasing 이미지 임의 영역 제거 후 채움 가림(occlusion) 상황 대응, 정규화 효과 중요한 객체 삭제 시 성능 저하 가능 Mixup 이미지·오디오 두 샘플 입력+레이블 선형 혼합 일반화 향상, 과적합 완화 비현실적 샘플 → 해석 어려움, 경계 모호화 CutMix 이미지 한 이미지 일부를 다른 이미지에 붙임 Mixup보다 객체 형태 보존 원본과 다른 의미 생길 수 있음 Color Jitter 이미지 밝기·대비·채도·색조 변환 다양한 조명 상황 대응 과도 시 실제 데이터와 괴리 Noise Injection 이미지·오디오·탭울러 랜덤 노이즈 추가 모델이 노이즈에 강건 너무 심하면 원본 패턴 왜곡 SMOTE 탭울러(불균형 분류) 소수 클래스 주변 k-NN 보간 불균형 개선, 다양성 증가 이상치 주변 합성 → 성능 저하 ADASYN 탭울러(불균형 분류) 어려운 영역에 집중 합성 경계 학습 강화 노이즈도 강화 가능, 과적합 위험 SpecAugment 음성 스펙트로그램 구간/주파수 마스킹 음성 다양성 증가 정보 손실 주의 Back Translation 텍스트 번역 후 다시 원문 언어로 역번역 자연스러운 문장 다양화 번역 품질 따라 성능 좌우 EDA 텍스트 동의어 치환, 삭제, 삽입, 순서 변경 간단·빠른 증강 의미 왜곡 가능성 있음 그래프 데이터 증강 구분 기법 설명 장점 구조변형 Edge Dropping 무작위로 일부 엣지 제거 연결 희소화, 과적합 방지 구조변형 Edge Perturbation 엣지를 랜덤 추가/삭제 구조 다양성 증가 구조변형 Node Dropping 일부 노드와 연결 엣지 제거 중요 노드 의존 감소 구조변형 Subgraph Sampling Random Walk/BFS로 부분 그래프 추출 대규모 그래프 학습 효율 ↑ 노드 특성변형 Feature Masking 일부 노드 피처를 0으로 마스킹 정보 불완전성에 강건 노드 특성변형 Feature Perturbation 노드 피처에 노이즈 추가(예: Gaussian) 잡음 환경 대응 노드 특성변형 Attribute Shuffling 동일 클래스 내 노드 피처 섞기 데이터 다양성 증가 Mixup 계열 Graph Mixup 두 그래프/노드 임베딩을 선형 보간 결정 경계 부드러움, 일반화 ↑ Mixup 계열 Manifold Mixup latent space에서 보간 일반화 성능 ↑ Mixup 계열 GraphSMOTE 소수 클래스 근처 synthetic node 생성 클래스 불균형 완화 고급 기법 DropEdge 학습 중 일부 엣지 랜덤 drop over-smoothing 완화 고급 기법 GraphCL/GRACE 등 두 증강 뷰를 만들어 contrastive loss 학습 표현 학습 강화, 라벨 부족에 효과 고급 기법 Adversarial Augmentation 구조/피처에 adversarial perturbation 추가 모델 강건성(robustness) 강화 그래프 데이터 증강 (Generative Model)Diffusion Model (확산 모델) 원리 Forward: 데이터에 점진적으로 노이즈 추가 → 가우시안 분포 Reverse: 학습된 네트워크가 노이즈 단계적 제거 → 샘플 생성 특징 매우 고품질 샘플, 모드 붕괴 거의 없음 학습·샘플링 느림(최근 DDIM 등으로 개선) GAN (Generative Adversarial Network) 원리 Generator: 가짜 데이터 생성 Discriminator: 진짜/가짜 판별, 적대적 학습으로 Generator 개선 특징 학습/샘플링 빠름, 고품질 생성 가능 단점: 학습 불안정, 모드 붕괴(다양성 부족) VAE (Variational Autoencoder) 원리 Encoder: 데이터를 잠재 공간 확률분포로 인코딩 Decoder: 잠재 벡터에서 데이터 복원, KL Divergence로 정규화 특징 안정적 학습, 잠재 공간 해석 용이(인터폴레이션 적합) 단점: 생성 품질이 GAN·Diffusion 대비 낮고 흐릿한 경향","link":"/2025/08/29/AI-Data-Preprocessing/"},{"title":"AI-Model-Development","text":"AI 모델 개발 전 과정(문제정의, 데이터, 학습/평가, 배포/모니터링) 실무 가이드를 정리 Self-Supervised Learning 기법 유형 대표 방식 예시 모델 예측 기반 일부 마스킹/다음 단어 예측 BERT, GPT, MAE, wav2vec 대비 학습 positive/negative 쌍 학습 SimCLR, MoCo 프록시 태스크 회전/컬러화/위치 예측 RotNet, Colorization 비대비 학습 teacher-student, self-distill BYOL, SimSiam, DINO 멀티모달 학습 modality 간 alignment CLIP, ALIGN, VideoBERT 핵심 포인트 라벨 없이도 대규모 데이터에서 표현 학습을 수행 → 다운스트림 성능 향상 동일 데이터의 두 뷰(증강) 설계가 중요(대비/비대비 모두) 멀티모달에서는 텍스트-이미지/영상 정렬로 제로샷 성능 확보 손실 함수 설계 요약 대비 학습(Contrastive, 예: SimCLR) InfoNCE: 같은 원본에서 나온 positive 쌍의 유사도는 극대화, 다른 샘플(negative)과는 최소화 배치 내 모든 샘플을 negative로 활용(large batch가 유리), 또는 메모리 뱅크/큐(MoCo) 비대비 학습(Non-Contrastive, 예: BYOL/SimSiam/DINO) Teacher-Student 구조, stop-gradient로 collapse 방지 예측기(projector/predictor) 헤드가 student → teacher 표현을 추종하도록 학습 예측 기반(Masked/Auto-Regressive) MIM(Masked Image Modeling): MAE는 패치 마스킹 후 복원 L2 MLM(Masked Language Modeling): [MASK] 토큰 복원 cross-entropy 멀티모달 정렬(CLIP) Dual encoder 이미지/텍스트 임베딩 간 대각 정렬을 temperature-scaled softmax로 양방향 최적화 증강(Augmentation) 설계 팁 이미지: RandomResizedCrop, ColorJitter, GaussianBlur, HorizontalFlip, Solarize(특히 SimCLR/SimSiam/DINO) 음성: 시간 마스킹/주파수 마스킹(SpecAugment), 노이즈 주입, 피치 시프팅 텍스트: 토큰 드롭/셔플, 스팬 마스킹(T5 스타일), 동의어 치환은 의미 왜곡 주의 비전-텍스트: 동일 개체/장면을 보존하는 약한 증강 조합이 정렬 안정성에 유리 구현 체크리스트 대규모 배치 또는 모멘텀 큐(MoCo v2)로 충분한 negatives 확보(대비 학습) EMA(Exponential Moving Average)로 teacher 업데이트(BYOL/DINO) 학습 초기 온도(temperature)와 학습률 warmup, cosine decay 스케줄 권장 프로젝션 헤드 비선형(MLP, BN) 사용 시 수렴 안정화에 도움 언제 무엇을 쓸까 라벨이 거의 없음: BYOL/SimSiam/DINO/MAE로 표현 사전학습 후 파인튜닝 제로샷/텍스트 질의 필요: CLIP류 멀티모달 정렬 다운스트림이 분류/검색: Contrastive(InfoNCE) 임베딩이 전이 성능 우수 흔한 함정과 대응 Collapse(표현 상수화): stop-grad, predictor, centering(예: DINO)로 완화 과한 강증강으로 의미 소실: 약한+강한 증강을 균형 있게 혼합, 뷰간 일관성 유지 작은 배치로 성능 저하: gradient accumulation, MoCo 큐, 더 긴 학습으로 보완 자연어 처리 모델 (NLP) GPT (Generative Pre-trained Transformer) 목적: 다음 토큰(단어) 예측(Autoregressive LM) 특징: 문장 생성/요약/대화 등에 강함, 좌→우 단방향 컨텍스트 학습: 대규모 코퍼스 사전학습 → 지시튜닝/대화튜닝/정렬(RLHF) 강화 BERT (Bidirectional Encoder Representations from Transformers) 목적: 마스킹된 토큰 복원(MLM) + NSP(두 문장이 연속인지) 특징: 양방향 컨텍스트 인코딩, 분류/검색/NER 등 인코더 태스크에 강함 변형: RoBERTa(노 NSP, 긴 학습), ALBERT(파라미터 공유), DeBERTa(상대 위치 등) 상세 설명과 예시는 문제 해설 참고 Fine-Tuning 전략Full Fine-Tuning 개념: 모델 모든 파라미터를 태스크 데이터로 재학습 장점: 특정 태스크에 최적화된 최고 성능 단점: 초대형 모델에서 비효율(메모리/시간), 저장·재사용 어려움 LoRA (Low-Rank Adaptation) 개념: 가중치 업데이트를 저랭크 행렬로 근사, 추가 파라미터만 학습 장점: 성능 유지하며 메모리·연산량 절약, 모듈 병합/재사용 용이 단점: 일부 태스크에서 Full FT 대비 소폭 성능 손해 가능 PEFT (Parameter-Efficient Fine-Tuning) 개념: 전체가 아닌 일부 파라미터만 업데이트하는 기법 묶음 포함: LoRA, Adapter, Prompt/Prefix Tuning, BitFit 등 장점: 메모리/연산 효율 극대화, LLM 실무에서 표준화 추세 활용: HuggingFace peft 라이브러리로 다양한 방법 제공 Fine-Tuning 기법 비교 기법 특징 / 방식 장점 단점 Full Fine-Tuning 모델 전체 파라미터를 새 데이터에 맞게 업데이트 특정 도메인에 최적화된 최고 성능 연산량·메모리 소모 매우 큼, 데이터 많이 필요 Feature-based 사전학습 모델을 Feature Extractor로 사용, 최종 분류기만 학습 빠르고 단순, 데이터 적을 때 효과적 모델 전체를 활용하지 않아 성능 제한 Adapter Tuning 각 Transformer 블록에 작은 Adapter Layer 삽입 후 해당 모듈만 학습 여러 태스크 동시 적용 가능, 효율적 Full FT 대비 약간 성능 손실 Prompt Tuning 입력 앞에 학습 가능한 가상 토큰(프롬프트) 추가 초경량, 대형 모델에도 효율적 태스크 복잡할수록 성능 불안정 Prefix Tuning Attention Key/Value에 학습 가능한 Prefix 추가 대형 LLM에 적합, 다양한 태스크 적용 가능 Full FT보다 성능 낮을 수 있음 LoRA 가중치 업데이트를 저랭크 행렬로 근사해 추가 파라미터만 학습 성능·효율 균형, 여러 LoRA 모듈 결합 용이 일부 복잡 태스크에서 성능 한계 BitFit 모델의 Bias 항목만 학습 극도로 가벼움, 빠른 학습 성능 제한적, 단순 태스크에 적합 PEFT Parameter-Efficient FT 기법들의 총칭(LoRA/Adapter/Prompt 등) 대규모 LLM 실무 표준, 효율성 극대화 Full FT보다 성능 약간 낮을 수 있음 QLoRA 모델 가중치 4/8bit 양자화 후 LoRA 적용 저자원 환경에서도 초대형 모델 FT 가능, VRAM 절약 양자화로 인한 미세한 성능 손실 가능 CAM 모델CAM(Class Activation Map)은 CNN 분류 모델이 특정 클래스를 예측할 때, 해당 클래스와 관련된 중요한 이미지 영역을 시각화하는 기법입니다. 상세내용은 문제 해설 참고 기법 비교 기법 핵심 아이디어 장점 한계 / 단점 CAM CNN Feature Map × FC 가중치로 클래스별 중요 영역 시각화 간단·직관적, 초기 CNN 해석 기법 GAP(Global Avg Pooling) 구조 필요 → 특정 네트워크에만 적용 Grad-CAM Gradient + Feature Map으로 중요 영역 강조 구조 제약 거의 없음(ResNet/ Inception/ ViT 등) 해상도 낮고 객체 경계가 흐림 Score-CAM Gradient 대신 모델 출력 Score 기반으로 중요 영역 산출 Gradient 의존 제거 → 더 안정적 결과 연산 비용 큼(출력 반복 계산 필요) 블랙박스 모델블랙박스 모델은 내부 의사결정 과정을 직접 해석하기 어려운 고복잡도 모델(딥러닝, 앙상블 등)을 의미합니다. 실무에서는 높은 정확도와 함께 사후 설명(post-hoc explainability)을 병행합니다. 화이트박스 vs 블랙박스 비교 구분 화이트박스 모델 (White-Box) 블랙박스 모델 (Black-Box) 정의 내부 동작을 사람이 쉽게 이해 내부 동작 불투명, 해석 어려움 대표 모델 선형/로지스틱 회귀, 의사결정트리, 규칙기반 딥러닝(CNN/RNN/Transformer), 앙상블(RF/XGBoost), 비선형 SVM 해석 가능성 높음 → 예측 근거 설명 용이 낮음 → 입출력만 관찰 가능 예측 성능 단순 데이터에 충분 복잡 패턴·대규모 데이터에 강력 복잡도 낮음(식/규칙 기반) 높음(수백만~수십억 파라미터) 사용 사례 규제/의료/정책 등 설명 책임 중요 비전/음성/NLP/자율주행 등 정확도 중시 장점 투명성, 신뢰성 높은 정확도, 복잡 데이터 처리 단점 복잡 데이터에서 성능 한계 해석 곤란, 편향·책임성 이슈 블랙박스 해석(사후 설명) 기법 CAM/Grad-CAM/Score-CAM: 비전 모델 국소 중요도 시각화(상단 섹션 참조) SHAP: 샘플 수준 피처 기여도(일관성·지역선형 근사) LIME: 국소 영역 선형근사로 설명(간단, 안정성은 데이터/커널 의존) Integrated Gradients: 입력→baseline 경로 적분으로 기여도 산출 Occlusion/Feature Ablation: 피처/영역 마스킹 후 성능 변화 관찰 Counterfactuals: 최소 변경으로 예측이 바뀌는 사례 생성(what-if 분석) 운영 시 고려사항 데이터 편향·드리프트 모니터링, 모델카드/데이터시트로 문서화 설명 요청·이의제기 절차, 영향평가(금융·의료 등 규제 대응) 설명 가능성 vs 성능 트레이드오프: 규제 영역은 혼합 접근(화이트+블랙) Surrogate ModelSurrogate Model은 복잡한 블랙박스 모델을 근사(approximate)하여 설명력을 제공하는 간단하고 해석 가능한 모델입니다. 블랙박스(예: 딥러닝)에 다양한 입력을 넣어 출력(label)을 수집하고, 이 입력-출력 쌍으로 Decision Tree, Linear/Logistic Regression 등 화이트박스 모델을 학습해 블랙박스의 예측 경향을 사람 친화적으로 설명합니다. 목적: 블랙박스의 전역/국소 의사결정 경향을 해석 가능한 규칙/계수로 표현 전역(Global) 대체: 전체 입력 공간에서 근사(예: 트리 기반 규칙) 국소(Local) 대체: 특정 샘플 주변에서 근사(LIME의 국소 선형 모델 등) 주의: Surrogate의 설명은 근사치이며, 분포 밖(out-of-distribution)에서는 신뢰하기 어려움 Linear Regression vs Logistic Regression 구분 Linear Regression (선형 회귀) Logistic Regression (로지스틱 회귀) 목적 연속형 값 예측(회귀) 범주형 값 예측(이진/다중 분류) 출력값 범위 −∞ ~ +∞(실수) 0 ~ 1(확률) 활용 함수 단순 선형결합 시그모이드(이진) / 소프트맥스(다중) 손실 함수 MSE(Mean Squared Error) 로그 손실(Log Loss, Cross-Entropy) 활용 예시 집값, 온도, 매출 예측 스팸 분류, 질병 진단, 이탈 예측 Learning Curve 그림 예시는 문제 참고 Underfitting(과소적합) 개선 모델 복잡도 ↑: 더 큰 신경망, 더 많은 파라미터 특징 엔지니어링 강화: 더 정보력 높은 입력 feature 학습 더 오래: Epoch 증가, Learning rate 스케줄 조정 Overfitting(과적합) 완화 데이터 늘리기: 추가 수집 또는 데이터 증강(Data Augmentation) 정규화 강화: Dropout, L2/L1, Early Stopping 모델 단순화: 레이어/노드 수 축소 앙상블: 여러 모델 결합으로 일반화 성능 ↑ 교차검증(CV): 일반화 잘 되는 하이퍼파라미터 선택 학습곡선 해석 팁 훈련·검증 오류가 모두 높음 → 과소적합 훈련 낮고 검증 높음 → 과적합(데이터/정규화/단순화/증강 검토) AI 모델 평가(Confusion Matrix) 실제 Positive 실제 Negative 예측 Positive TP FP 예측 Negative FN TN 정확도(Accuracy): (TP + TN) / (TP + TN + FP + FN) 정밀도(Precision): TP / (TP + FP) 재현율(Recall, Sensitivity): TP / (TP + FN) 특이도(Specificity): TN / (TN + FP) F1-score: 2 × (Precision × Recall) / (Precision + Recall) 문제 요구 지표 확인 필수. 예: 14번 문제는 Recall이 정답. ROC-AUC / PR-AUC ROC-AUC ROC 곡선: FPR(False Positive Rate) = FP/(FP+TN) vs TPR(Recall) AUC는 임계값 전 범위에서의 평균 성능을 요약. 클래스 불균형이 심하지 않을 때 적합 PR-AUC PR 곡선: Precision vs Recall. 양성 클래스가 희소할수록 모델 구분력을 더 잘 반영 극단적 불균형(예: 사기·고장 예측)에서는 PR-AUC 사용 권장 선택 가이드: 불균형 심함 → PR-AUC, 그렇지 않음 → ROC-AUC. 두 지표를 함께 보고 trade-off 판단 임계값(Threshold) 조정 전략 비즈니스 비용 기반 최적화: 비용행렬(C_FP, C_FN)을 정의해 기대비용 최소화 임계값 선택 고정 재현율/정밀도 제약: 예) Recall ≥ 0.9를 만족하는 최소 임계값 선택 Youden’s J: J = TPR − FPR 최대화 임계값(의료 진단 등에서 사용) KS 통계량: 누적 TPR − 누적 FPR 최대점(신용평가) 검증셋/교차검증 기반 서치: Grid search로 임계값을 스윕하고 목표 지표 최대화 캘리브레이션 후 조정: Platt scaling/Isotonic으로 확률 보정 후 임계값 적용 Class 불균형 해소접근 방식별 요약 접근 방식 방법 장점 단점 데이터 수준 언더샘플링(Under-Sampling) 간단, 빠름 정보 손실 가능 데이터 수준 오버샘플링(SMOTE, ADASYN 등) 소수 클래스 보강, 다양성 증가 과적합 위험 데이터 수준 데이터 증강(Augmentation) 소수 클래스 다양성 확보 도메인 지식 필요 알고리즘 수준 클래스 가중치(Class Weighting) 단순 적용 가능, 효과적 가중치 튜닝 필요 알고리즘 수준 Cost-Sensitive Learning 잘못 예측 비용 반영 비용 설계 복잡 앙상블 EasyEnsemble, Balanced RF 강력한 성능, 불균형 완화 계산 비용 증가 손실 함수 Weighted Cross-Entropy 클래스별 가중치 반영, 단순·효과적 가중치 선택 어려움 손실 함수 Focal Loss Hard example 집중, 희귀 이벤트 강함 α, γ 튜닝 필요 평가지표 F1, ROC-AUC, PR-AUC 불균형에 적합한 평가 Accuracy보다 해석 복잡 실무 팁 우선 클래스 분포·오염 확인 → 데이터 질 개선 우선 소수 클래스 충분치 않으면 증강/합성 + 적절한 지표(PR-AUC/F1) 사용 비용민감·가중치·임계값 조정을 함께 튜닝(그리드/랜덤/베이지안) HyperParameter Tuning모델 학습에서 사람이 사전에 정해야 하는 값(데이터로 직접 학습되지 않음). 딥러닝: Learning Rate, Batch Size, Epoch, Dropout, Optimizer 등 트리 기반: max_depth, learning_rate, min_samples_split, n_estimators 등 SVM: C, γ, Kernel 문제 유형 주의: 국어형 설명 문항(정의/장단점/적용 상황) 확인 (1) Grid Search 후보를 격자(Grid)로 전수 조합해 탐색 장점: 단순, 재현 쉬움 단점: 차원↑ 시 계산량 폭증, 비효율적 (2) Random Search 범위에서 무작위 샘플링으로 탐색 장점: 고차원에서 효율적, 빠른 베이스라인 단점: 최적 보장 어려움, 운에 민감 (3) Bayesian Optimization 과거 결과로 확률모형(GP 등)을 구축 → 기대 개선이 큰 지점을 선택 장점: 시도 수 대비 효율 높음 단점: 구현 복잡, 계산량 있음 보완 전략 교차검증으로 일반화 성능 평가, 조기중단·러닝레이트 스케줄 병행 하이브리드: 랜덤 탐색으로 거친 영역 탐색 → 베이지안으로 미세 조정","link":"/2025/09/04/AI-Model-Development/"},{"title":"AI-System-Builiding-and-Operating","text":"AI 시스템 구축과 운영(아키텍처, 배포, 모니터링) 관련 정리 Model-in-Service vs Model-as-a-Service 구분 Model-in-Service Model-as-a-Service(MaaS) 정의 앱 내부에 모델 포함해 운영 모델을 독립 API 서비스로 배포, 여러 앱에서 호출 구조 모델·애플리케이션 강결합 모델은 독립 실행, 앱은 네트워크/API로 연동 배포 위치 앱 서버 내부(온프레미스/내장) 클라우드·온프레미스 API 서버 유지보수 모델 변경 시 서비스 전체 배포 모델만 교체/업데이트, 서비스는 그대로 확장성 서비스 단위 확장(모델 확장 비효율) 모델 서비스만 별도 스케일링 가능 장점 단순 구조, 네트워크 호출 없음 → 지연↓ 공용 사용, 교체·버전관리 용이, 확장성↑ 단점 앱 전체 수정/중복 배포 발생 네트워크 비용·별도 인프라 필요 활용 소규모 앱, 오프라인, 전용 기능 Azure ML, SageMaker, GCP AI Platform 등 대규모 환경 AI 모델 배포 프로세스 문제 해설 그림 참고(아키텍처/파이프라인) 핵심 단계 패키징: 컨테이너화(Docker), 종속성 잠금 서빙: REST/gRPC/스트리밍(배치/온라인), A/B·카나리·블루그린 배포 스케일링: 오토스케일(동시요청·지연 기준), GPU/CPU 혼합 관측성: 로깅/메트릭(지연, 에러율), 트레이싱, 모델/데이터 드리프트 모니터링 안전장치: 요청 검증, 레이트리미트, 안전필터, 롤백 전략 거버넌스: 모델카드, 버전/아티팩트 레지스트리, 재현성(Seed/환경) 딥러닝 방법 개요 Zero-shot: 학습에 없던 클래스를 설명/지식 기반으로 바로 처리 Few-shot: 소량 예시로 빠른 적응 One-shot: 예시 1개로 학습/적응 Transfer Learning: 사전학습 지식을 전이해 성능/수렴 개선 Meta-learning: “빨리 배우는 방법” 학습(태스크 전반에 빠른 적응) GZSL(Generalized Zero-Shot): seen+unseen을 동시에 다루는 설정 추가 메모 Zero/Few-shot은 LLM·비전-언어 모델에서 프롬프트/아답터로 활용 빈번 GZSL은 허구/허상 방지 위해 텍스트 속성/어트리뷰트 정렬 품질이 핵심 Transfer vs Meta-Learning 구분 Transfer Learning Meta-Learning 핵심 아이디어 기존 모델 지식 재사용 “빠르게 배우는 법” 자체 학습 접근 방식 Pre-train → Fine-tune 다양한 태스크 경험으로 빠른 적응 데이터 필요성 새 태스크에 일정량 필요 아주 적은 데이터로도 적응 대표 사례 BERT/ResNet 파인튜닝 MAML, ProtoNet, Few-shot LLM 적응 Zero/Few-shot과 관계 Zero-shot은 전이 모델에 프롬프트로 적용 Few/One-shot 잘하기 위한 핵심 기법 프롬프팅 방법 Zero-shot: 예시 없이 질문만 제공 Few-shot: 몇 개의 예시를 함께 제공 Chain-of-Thought(CoT): 단계별 추론 설명을 유도 Self-Consistency: 여러 CoT 경로 중 다수결 앙상블 ReAct(Reason+Act): 추론과 도구 호출 병행 Role/Persona Prompting: 역할 부여로 스타일/제약 제어 RAG(Retrieval-Augmented Generation): 외부 지식 검색 후 반영 AI 모델 아키텍쳐 설계(NAS)NAS (Neural Architecture Search) 정의 신경망 아키텍처(구조)를 자동으로 설계하는 방법론. 사람이 CNN, RNN 구조를 직접 설계하는 대신, 탐색 알고리즘이 자동으로 최적 구조를 찾음. NAS 구성 요소 탐색공간 (Search Space) 탐색 가능한 모든 아키텍처 후보들의 집합. 예: Conv3x3, Conv5x5, Skip Connection, Pooling 등을 어떻게 조합할 수 있는지. 상태공간 (State Space) 탐색 과정에서 “현재까지의 아키텍처 상태”를 표현. 예: 현재까지 몇 개의 레이어를 어떤 연산으로 쌓았는가? 보상공간 (Reward Space) 선택된 아키텍처를 평가했을 때 얻을 수 있는 보상(성능 지표)의 집합. 예: 검증 정확도, F1-score, Latency, FLOPs, 메모리 사용량. NAS 탐색 전략 (Search Strategy) 강화학습 기반: RNN Controller가 아키텍처를 생성 → 보상(성능)에 따라 학습 (예: NASNet). 진화 알고리즘 기반: 후보 아키텍처를 세대별로 진화시키며 성능 좋은 구조 선택 (예: AmoebaNet). Gradient 기반 (DARTS): 탐색 공간을 **연속화(differentiable)**하여 그라디언트 최적화로 탐색. Bayesian Optimization 기반: 확률 모델로 성능을 예측하며 탐색. One-Shot NAS (ENAS): 슈퍼넷(Supernet)을 학습한 뒤, 가중치 공유로 빠른 탐색. FLOPS, 모델 경량화FLOPs: 모델이 한 번의 추론(forward pass) 또는 학습 시 필요한 총 연산 횟수 “이 모델을 실행하려면 얼마나 많은 수학 연산(곱셈, 덧셈 등)을 해야 하는가?” 기법 설명 효과 Pruning 중요하지 않은 가중치/채널 제거 FLOPs ↓, 파라미터 ↓ Quantization FP32 → INT8/INT4 변환 메모리 ↓, 속도 ↑ Knowledge Distillation 큰 모델 → 작은 모델 학습 모델 크기 ↓, 성능 유지 Low-Rank Factorization 가중치 행렬을 저차원 행렬 곱으로 근사 연산량 ↓ Parameter Sharing 여러 레이어가 같은 가중치 공유 파라미터 ↓ Efficient Architecture MobileNet, ShuffleNet처럼 처음부터 효율적 구조 설계 FLOPs ↓, 속도 ↑ NAS 기반 Neural Architecture Search로 최적 구조 탐색 성능 대비 효율 ↑ Compound Scaling Depth·Width·Resolution을 균형 있게 동시에 확장 (EfficientNet) 같은 자원 대비 성능 ↑","link":"/2025/09/04/AI-System-Builiding-and-Operating/"},{"title":"AI-Trend-2025-09-04","text":"AI 트렌드 정리 https://pro222.postman.co/workspace/My-Workspace~5b9cbae5-e0d8-4aba-8ed9-589de1679a82/request/38822142-b6e2af94-fbcd-454c-9b9c-533af18573fc 1234567891011curl --location 'https://api.openai.com/v1/chat/completions' \\--header 'Authorization: Bearer ' \\--header 'Content-Type: application/json' \\--header 'Cookie: __cf_bm=gIxeFEPAHmWOTPqD.WQcVlr8qXg1S9FZsorCTn.RS_g-1756957753-1.0.1.1-uB8430vAYhN0HhCCrptH09ZPqAKzEB.XeEX3CiCD7.EVyoIDno2MB3aGvCdnO7kvUv4ngCuEqsp5msaIphVbCevJvTVEbS_WsFp_JE.Xt6o; _cfuvid=M7pcKQc55j1pocYBnOHV6hFVlOG3.bC0D0uqnt6OvbM-1756957753504-0.0.1.1-604800000' \\--data '{ &quot;model&quot;: &quot;gpt-4o&quot;, &quot;messages&quot;: [ { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot; }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;안녕 GPT! 오늘 날씨 알려줘.&quot; } ]}'","link":"/2025/09/04/AI-Trend-2025-09-04/"}],"tags":[{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"},{"name":"Backend","slug":"Backend","link":"/tags/Backend/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"SageMaker","slug":"SageMaker","link":"/tags/SageMaker/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Microsoft","slug":"Microsoft","link":"/tags/Microsoft/"},{"name":"Azure","slug":"Azure","link":"/tags/Azure/"},{"name":"Microsoft,Azure","slug":"Microsoft-Azure","link":"/tags/Microsoft-Azure/"},{"name":"MS Azure","slug":"MS-Azure","link":"/tags/MS-Azure/"},{"name":"AZ104","slug":"AZ104","link":"/tags/AZ104/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"AZ204","slug":"AZ204","link":"/tags/AZ204/"},{"name":"blockchain","slug":"blockchain","link":"/tags/blockchain/"},{"name":"Usenix","slug":"Usenix","link":"/tags/Usenix/"},{"name":"Security","slug":"Security","link":"/tags/Security/"},{"name":"Batch Server","slug":"Batch-Server","link":"/tags/Batch-Server/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"Multi-threading","slug":"Multi-threading","link":"/tags/Multi-threading/"},{"name":"Scalability","slug":"Scalability","link":"/tags/Scalability/"},{"name":"Programming","slug":"Programming","link":"/tags/Programming/"},{"name":"Cron","slug":"Cron","link":"/tags/Cron/"},{"name":"Agent","slug":"Agent","link":"/tags/Agent/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"JupyterLab","slug":"JupyterLab","link":"/tags/JupyterLab/"},{"name":"LangChain","slug":"LangChain","link":"/tags/LangChain/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"LLM,LangChain","slug":"LLM-LangChain","link":"/tags/LLM-LangChain/"},{"name":"LiteLLM","slug":"LiteLLM","link":"/tags/LiteLLM/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"LangSmith","slug":"LangSmith","link":"/tags/LangSmith/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Kruscal","slug":"Kruscal","link":"/tags/Kruscal/"},{"name":"MST","slug":"MST","link":"/tags/MST/"},{"name":"Prim","slug":"Prim","link":"/tags/Prim/"},{"name":"UnionFind","slug":"UnionFind","link":"/tags/UnionFind/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"maya","slug":"maya","link":"/tags/maya/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"machineLearning","slug":"machineLearning","link":"/tags/machineLearning/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Crawling","slug":"Crawling","link":"/tags/Crawling/"},{"name":"Netflix","slug":"Netflix","link":"/tags/Netflix/"},{"name":"SK","slug":"SK","link":"/tags/SK/"},{"name":"Travel","slug":"Travel","link":"/tags/Travel/"},{"name":"unity","slug":"unity","link":"/tags/unity/"},{"name":"c#","slug":"c","link":"/tags/c/"},{"name":"Hololens","slug":"Hololens","link":"/tags/Hololens/"},{"name":"vuforia","slug":"vuforia","link":"/tags/vuforia/"},{"name":"Books","slug":"Books","link":"/tags/Books/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"React.js","slug":"React-js","link":"/tags/React-js/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"Akka","slug":"Akka","link":"/tags/Akka/"},{"name":"Lagom","slug":"Lagom","link":"/tags/Lagom/"},{"name":"Cassandra","slug":"Cassandra","link":"/tags/Cassandra/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"DevOps","slug":"DevOps","link":"/tags/DevOps/"},{"name":"DataStructure","slug":"DataStructure","link":"/tags/DataStructure/"},{"name":"CS","slug":"CS","link":"/tags/CS/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"Unix","slug":"Unix","link":"/tags/Unix/"},{"name":"CLI","slug":"CLI","link":"/tags/CLI/"},{"name":"Insertion Sort","slug":"Insertion-Sort","link":"/tags/Insertion-Sort/"},{"name":"Sort","slug":"Sort","link":"/tags/Sort/"},{"name":"node.js","slug":"node-js","link":"/tags/node-js/"},{"name":"prisma","slug":"prisma","link":"/tags/prisma/"},{"name":"graphql","slug":"graphql","link":"/tags/graphql/"},{"name":"typescript","slug":"typescript","link":"/tags/typescript/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Distributed Systems","slug":"Distributed-Systems","link":"/tags/Distributed-Systems/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Microservices","slug":"Microservices","link":"/tags/Microservices/"},{"name":"LaTeX","slug":"LaTeX","link":"/tags/LaTeX/"},{"name":"nmap","slug":"nmap","link":"/tags/nmap/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"Frontend","slug":"Frontend","link":"/tags/Frontend/"},{"name":"Firebase","slug":"Firebase","link":"/tags/Firebase/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"Redux","slug":"Redux","link":"/tags/Redux/"},{"name":"Selection Sort","slug":"Selection-Sort","link":"/tags/Selection-Sort/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"Quick Sort","slug":"Quick-Sort","link":"/tags/Quick-Sort/"},{"name":"Container","slug":"Container","link":"/tags/Container/"},{"name":"Cloud","slug":"Cloud","link":"/tags/Cloud/"},{"name":"Zookeeper","slug":"Zookeeper","link":"/tags/Zookeeper/"},{"name":"DataPreprocessing","slug":"DataPreprocessing","link":"/tags/DataPreprocessing/"},{"name":"MLOps","slug":"MLOps","link":"/tags/MLOps/"},{"name":"FeatureEngineering","slug":"FeatureEngineering","link":"/tags/FeatureEngineering/"},{"name":"ModelDevelopment","slug":"ModelDevelopment","link":"/tags/ModelDevelopment/"},{"name":"System","slug":"System","link":"/tags/System/"}],"categories":[{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"API Gateway","slug":"AWS/API-Gateway","link":"/categories/AWS/API-Gateway/"},{"name":"EC2","slug":"AWS/EC2","link":"/categories/AWS/EC2/"},{"name":"SageMaker","slug":"AWS/SageMaker","link":"/categories/AWS/SageMaker/"},{"name":"LLM","slug":"LLM","link":"/categories/LLM/"},{"name":"BERT","slug":"LLM/BERT","link":"/categories/LLM/BERT/"},{"name":"Backend","slug":"Backend","link":"/categories/Backend/"},{"name":"EKS","slug":"AWS/EKS","link":"/categories/AWS/EKS/"},{"name":"AWS","slug":"Backend/AWS","link":"/categories/Backend/AWS/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"Cognito","slug":"AWS/Cognito","link":"/categories/AWS/Cognito/"},{"name":"DeepLearning","slug":"AI/DeepLearning","link":"/categories/AI/DeepLearning/"},{"name":"SageMaker","slug":"SageMaker","link":"/categories/SageMaker/"},{"name":"Microsoft","slug":"Microsoft","link":"/categories/Microsoft/"},{"name":"Aurora","slug":"AWS/Aurora","link":"/categories/AWS/Aurora/"},{"name":"papers","slug":"papers","link":"/categories/papers/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Batch Server","slug":"Batch-Server","link":"/categories/Batch-Server/"},{"name":"Software Engineering","slug":"Software-Engineering","link":"/categories/Software-Engineering/"},{"name":"Cron","slug":"Cron","link":"/categories/Cron/"},{"name":"Agent","slug":"Agent","link":"/categories/Agent/"},{"name":"blockchain","slug":"blockchain","link":"/categories/blockchain/"},{"name":"Toy Project","slug":"Toy-Project","link":"/categories/Toy-Project/"},{"name":"JupyterLab","slug":"AI/JupyterLab","link":"/categories/AI/JupyterLab/"},{"name":"Lambda","slug":"AWS/Lambda","link":"/categories/AWS/Lambda/"},{"name":"Computer Science","slug":"Software-Engineering/Computer-Science","link":"/categories/Software-Engineering/Computer-Science/"},{"name":"LangChain","slug":"LLM/LangChain","link":"/categories/LLM/LangChain/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Document","slug":"Agent/Document","link":"/categories/Agent/Document/"},{"name":"AI","slug":"Microsoft/AI","link":"/categories/Microsoft/AI/"},{"name":"LiteLLM","slug":"LiteLLM","link":"/categories/LiteLLM/"},{"name":"MachineLearning","slug":"AI/MachineLearning","link":"/categories/AI/MachineLearning/"},{"name":"LangSmith","slug":"LLM/LangSmith","link":"/categories/LLM/LangSmith/"},{"name":"NLP","slug":"AI/NLP","link":"/categories/AI/NLP/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Netflix","slug":"Netflix","link":"/categories/Netflix/"},{"name":"Route53","slug":"AWS/Route53","link":"/categories/AWS/Route53/"},{"name":"Travel","slug":"Travel","link":"/categories/Travel/"},{"name":"Performance Optimization","slug":"Software-Engineering/Computer-Science/Performance-Optimization","link":"/categories/Software-Engineering/Computer-Science/Performance-Optimization/"},{"name":"Books","slug":"Books","link":"/categories/Books/"},{"name":"Linux","slug":"Docker/Linux","link":"/categories/Docker/Linux/"},{"name":"Distributed Systems","slug":"Backend/Distributed-Systems","link":"/categories/Backend/Distributed-Systems/"},{"name":"Node.js","slug":"Backend/Node-js","link":"/categories/Backend/Node-js/"},{"name":"DevOps","slug":"Backend/DevOps","link":"/categories/Backend/DevOps/"},{"name":"CS","slug":"CS","link":"/categories/CS/"},{"name":"Javascript","slug":"Javascript","link":"/categories/Javascript/"},{"name":"CLI","slug":"CLI","link":"/categories/CLI/"},{"name":"Framework","slug":"Framework","link":"/categories/Framework/"},{"name":"Conelab","slug":"Projects/Conelab","link":"/categories/Projects/Conelab/"},{"name":"LaTeX","slug":"LaTeX","link":"/categories/LaTeX/"},{"name":"Prisma","slug":"Backend/Prisma","link":"/categories/Backend/Prisma/"},{"name":"nmap","slug":"nmap","link":"/categories/nmap/"},{"name":"Pandas","slug":"Python/Pandas","link":"/categories/Python/Pandas/"},{"name":"Backend","slug":"Projects/Backend","link":"/categories/Projects/Backend/"},{"name":"Sort","slug":"Algorithm/Sort","link":"/categories/Algorithm/Sort/"},{"name":"Book Review","slug":"Book-Review","link":"/categories/Book-Review/"},{"name":"Django","slug":"Backend/Django","link":"/categories/Backend/Django/"},{"name":"Japan","slug":"Travel/Japan","link":"/categories/Travel/Japan/"},{"name":"DevOps","slug":"Backend/Distributed-Systems/DevOps","link":"/categories/Backend/Distributed-Systems/DevOps/"},{"name":"Frontend","slug":"Frontend","link":"/categories/Frontend/"},{"name":"Distributed Systems","slug":"Backend/DevOps/Distributed-Systems","link":"/categories/Backend/DevOps/Distributed-Systems/"},{"name":"DataStructure","slug":"CS/DataStructure","link":"/categories/CS/DataStructure/"},{"name":"Node.js","slug":"Javascript/Node-js","link":"/categories/Javascript/Node-js/"},{"name":"Python","slug":"Framework/Python","link":"/categories/Framework/Python/"},{"name":"Messaging Systems","slug":"Backend/DevOps/Messaging-Systems","link":"/categories/Backend/DevOps/Messaging-Systems/"},{"name":"Software Architecture","slug":"Backend/Distributed-Systems/Software-Architecture","link":"/categories/Backend/Distributed-Systems/Software-Architecture/"},{"name":"maya","slug":"maya","link":"/categories/maya/"},{"name":"Network","slug":"Network","link":"/categories/Network/"},{"name":"Matplotlib","slug":"Python/Matplotlib","link":"/categories/Python/Matplotlib/"},{"name":"Frontend","slug":"Projects/Backend/Frontend","link":"/categories/Projects/Backend/Frontend/"},{"name":"SKT","slug":"Projects/SKT","link":"/categories/Projects/SKT/"},{"name":"Software Deployment","slug":"Backend/DevOps/Software-Deployment","link":"/categories/Backend/DevOps/Software-Deployment/"},{"name":"Unity","slug":"Unity","link":"/categories/Unity/"},{"name":"React.js","slug":"Frontend/React-js","link":"/categories/Frontend/React-js/"},{"name":"Security","slug":"Security","link":"/categories/Security/"},{"name":"Crawling","slug":"Crawling","link":"/categories/Crawling/"},{"name":"React.js","slug":"Projects/Backend/Frontend/React-js","link":"/categories/Projects/Backend/Frontend/React-js/"},{"name":"AWS","slug":"Projects/Backend/Frontend/React-js/AWS","link":"/categories/Projects/Backend/Frontend/React-js/AWS/"},{"name":"Lambda","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/"},{"name":"Toy Projects","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/"},{"name":"Firebase","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/"},{"name":"KT","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/KT","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/KT/"},{"name":"MLOps","slug":"AI/MLOps","link":"/categories/AI/MLOps/"}],"pages":[{"title":"","text":"ProjectsKT내비게이션 앱 내 챗봇 AI Agent 개발 기간 : 2024.6 ~ 2024.12 프로젝트 개요 멀티턴 대화를 지원하는 LLM 기반 챗봇 서비스 개발(PoC) 기여한 점 Master Agent 개발 경로탐색/ POI(Point Of Interest) Agent 등을 관리 및 발화 의도 분류 멀티턴 대화를 위한 대화 기록 및 대화요약 저장을 통한 개인화 (Redis + Langchain) pgvector 기반 개인화 고도화 GPU 서버 기반 JupyterLab 및 SageMaker 환경 구축하여 LLM 모델 학습 및 서빙 지원 LLM 서비스 API 제공을 위한 인프라 및 DevOps 체계 구축 활용 기술 Backend: Python, FastAPI, LangChain, Redis, pgvector Infra &amp; DevOps: AWS(Lambda, API Gateway, EC2, EKS), Jenkins 경로탐색엔진 성능 고도화 기간 : 2023.7 ~ 2024.6 프로젝트 개요 기존 내비게이션 엔진의 경로 탐색 속도를 서울역-제주공항 기준 7초 → 2초 미만으로 단축목표 양방향 다익스트라, Contraction Hierarchy(CCH) 알고리즘을 활용하여 효율적인 도로네트워크 그래프 모델을 구성하고, 이를 기반으로 빠른 속도의 경로탐색 구현 주요 역할 및 성과 Customizable Contraction Hierarchy(CCH) 알고리즘 적용(PoC) 상황 도로네트워크 링크 수 증가(2024년 3,757,898개)로 인한 경로탐색 속도 저하 문제 기존 양방향 다익스트라알고리즘에서는 모든 도로네트워크를 탐색해야 했음 기여한 점 Contraction Hierarchy 알고리즘을 통해 경로탐색 모델을 구축 그래프의 노드를 가중치(큰길우선, 무료도로우선 등) 우선순위에 따라 경로 탐색 시 불필요한 경로를 제외하는 전처리 작업을 통해 경로탐색 효율성 향상 성과 및 영향 경로탐색 속도 대폭 개선 (서울역 → 제주공항 / 7초 → 약 3초) 활용 기술 알고리즘: Bidirectional Dijkstra, Contraction Hierarchy(CCH) PostgreSQL(PostGIS), Java KT 내비게이션 서버 컨테이너화 기간 : 2023.1 ~ 2023.6 프로젝트 개요 기존 VM 기반으로 운영되던 서비스를 KT Cloud 컨테이너 환경으로 이전하여 리소스 최적화 주요 역할 및 성과 기존 VM 기반 서비스 → 컨테이너 기반 전환 상황 기존 서비스는 VM 환경에서 개별적으로 운영되어 서버 리소스 낭비가 발생, 배포 및 확장성 관리가 어려운 문제가 있었음 기여한 점 API Call 수 및 리소스 사용률 기반으로 필요 인프라 설계, 유관 부서들과 논의하여 인프라 구축 Docker &amp; k8s를 활용하여 기존 VM 기반 서비스를 컨테이너 환경으로 전환 필요 시 자동 확장(Auto Scaling)이 가능하도록 k8s Horizontal Pod Autoscaler(HPA) 적용 인프라 전환 시 높은 보안 수준을 달성하기 위해 보안단과 협업하여 보안성 검토 프로세스를 진행 성과 및 영향 VM 서버 30대 → VM 서버 8대(컨테이너 16대)로 축소, 운영 비용 절감 및 효율성 향상 k8s 기반으로 서비스 가용성이 99.99% 이상 유지 CI/CD 파이프라인 구축을 통한 개발 환경 생산성 증가 상황 기존 VM 환경에서는 수동 배포(커맨드) 방식으로 운영되어 배포 시간이 길고, 운영 중 장애 발생 시 롤백 절차가 복잡했음 기여한 점 Jenkins 기반 CI/CD 파이프라인 구축 (빌드 → 테스트 → 배포) 쿠버네티스 설정값 튜닝을 통한 배포 안정성 강화 성과 및 영향 배포시간 약 50% 단축, 장애 발생 시 롤백 시간 감소, 무중단 배포 활용기술 Docker, 쿠버네티스, Jenkins KT 내비게이션 통합로그 시스템 개발 기간 : 2022.9 ~ 2022.12 프로젝트 개요 내비게이션 앱에서 발생하는 GPS 정보, 사고제보, 그룹 주행 정보 등의 로그와, 관리자시스템에서 입력하는 유고 정보(사고 차량, 도로 통제 등) 를 효율적으로 수집하고 분석할 수 있도록 로그 집계 시스템 개발 기존에는 각 시스템에서 여러 유형의 로그를 모두 처리해야 했지만, 모든 로그를 집계하는 통합로그 시스템 구축을 통해 하나의 시스템에만 의존할 수 있도록 개선 기여한 점 관리자 시스템에서 유고정보 입력 및 데이터 흐름 개선 React 기반 관리자페이지에서 유고정보 입력 기능 개발 SpringBoot 서버에 유고정보 저장 및 Kafka 메시지 Pub 로직 추가 Kafka 기반의 통합로그 시스템 개발 내비게이션 앱에서 서버로 전송하는 GPS, 사고제보, 그룹주행 로그 등을 Kafka에 메시지 Pub Kafka 기반으로 유고정보를 메시지 Sub하고 내비게이션 앱으로 실시간 응답 성과 및 영향 로그 집계 및 메시징 시스템 도입으로 유지보수성 향상 로그의 원천으로부터 다른 시스템간의 결합도 감소 활용 기술 React, SpringBoot, Spring Webflux, Kafka KT 내비게이션 녹색교통 패턴 정책화 기간 : 2022.7 ~ 2022.8 프로젝트 개요 기존에는 변동이 거의 없는 녹색교통 정책이 Batch 방식으로 1시간마다 긴급정보 서버로부터 불필요하게 갱신 이를 개선하기 위해 KT내비게이션 관리자시스템에서 직접 녹색교통 정책을 등록 및 관리하고, 정책 변경이 발생할 때만 앱에 적용되도록 이벤트 기반으로 최적화. 기여한 점 관리자시스템에서 녹색교통 정책 입력 및 메시지 발행 기능 개발 React 기반 관리자페이지에서 녹색교통 입력 기능 개발 SpringBoot 서버에 녹색교통 저장 및 Kafka 메시지 Pub 로직 추가 Kafka 기반 메시징 시스템 구축 녹색교통 정책 Topic을 생성, 관리자시스템에서 Pub, 내비게이션 GW서버에서 Sub 내비게이션 GW서버의 2주치 정책 스케줄 생성 및 앱 활성화 시 정책 로딩 로직 개발 성과 및 영향 정책 변경 발생 시에만 유효한 데이터를 제공하여 불필요한 요청 제거 활용 기술 React, Spring Boot, Kafka, Scala, Lagom KT내비게이션 API GW 내재화 및 고도화 기간 : 2021.9 ~ 2022.6 프로젝트 개요 원내비 내재화 프로젝트에서 API Gateway 서버 개발 담당 내비게이션 서비스의 트래픽 처리 최적화, 사용자 데이터 관리 시스템 구축, 인증 시스템 구축 주요 역할 및 성과 동기 호출 개선 상황 출퇴근 시간과 명절과 같은 높은 트래픽 유입시 서비스 간 통신이 동기호출방식이 많아 병목 현상 발생, 장애 발생 시 서비스 전체가 영향 받음 기여한 점 비동기 프로세싱 적용 기존 동기 API 호출을 Akka Actor 기반 비동기 메시징 방식으로 변경, 서비스간 병목 현상 제거 Kafka를 활용, 일부 요청을 이벤트 기반 아키텍처로 변경 Circuit Breaker 패턴을 도입하여 특정 서비스 장애 발생 시 API GW의 정상 작동 보장 성과 일부 쓰기 작업에 대해 비동기 메시징 적용, API 응답속도 40% 단축 (평균 1.5초 -&gt; 900ms) Circuit Breaker 도입으로 장애 발생 시 자동 복구율 향상 데이터 정합성 개선 상황 기존시스템은 JPA, RDBMS를 사용하여 다중 인스턴스 환경에서 동일한 사용자 데이터가 여러 개의 노드에 존재, 특정 시점에서 다른 노드와 데이터 불일치 발생 기여한 점 Akka Persistence 및 이벤트 소싱 적용을 통한 데이터 일관성 및 확장성 강화 실시간 주행 기록, 상태 정보 등의 데이터를 이벤트 소싱으로 저장하여 최신 상태를 유지하고 트랜잭션 일관성 확보 데이터베이스 레벨에서 Read/Write 분리하고, 고속 데이터 쓰기/읽기 성능을 위해 Cassandra 도입 CQRS 패턴 도입하여 명령과 조회 로직을 분리하여 데이터 일관성 강화 성과 이벤트 소싱 적용 후 장애 발생 시 데이터 복구율 99.9% 데이터 정합성 문제로 인한 고객 문의 80% 감소 로드밸런싱 문제 개선 상황 기존시스템은 트래픽이 몰릴경우 특정 노드가 과부하 상태가 되는 문제 발생 로드밸런싱이 동기 방식 기반으로 적용되어 스케일링이 어려움 기여한 점 Akka의 Cluster Sharding 적용, 특정 노드에 부하가 집중되는 문제 해결, 인스턴스ID를 기반으로 노드 간 트래픽을 자동으로 분산 OpenResty, Kong을 활용하여 트래픽을 균등 분산 성과 서버 과부하 문제 해결 -&gt; 평균 CPU 사용률 5% 감소 서비스 다운타임 60% 감소 -&gt; SLA 개선 활용 기술 Scala, Lagom, Akka, Kafka, Cassandra, OpenResty 지니TV Android 네이티브 앱 전환 개발 기간 : 2020.12 ~ 2021.8 프로젝트 개요 웹뷰 기반으로 제공되던 기존 올레TV 서비스를 안드로이드 TV 플랫폼으로 전환 주요 역할 검색, 기기 설정, VOD 재생 화면 개발 및 서버 연동 상황 기존 올레TV 검색 기능은 웹뷰 기반이라 속도가 느리고 관련 VoC가 많았음 리모컨 포커스 손실 이슈, 예상치 못한 UI 비활성화 이슈 발생 기여한 점 안드로이드 TV 환경에 맞춰 Leanback 라이브러리 기반의 검색 UI 개발 AI스피커(기가지니)를 연동한 음성 검색기능 추가를 통한 사용자 편의성 강화 Leanback 라이브러리 및 커스텀 포커스 핸들링을 적용, Fragment 간 포커스 이동 경로 최적화 성과 및 영향 기존대비 UI 렌더링 속도 2배 향상, 속도관련 VoC 발생률 0%로 감소 검증팀 테스트 결과 포커스 사라지는 결함 0%로 감소 활용 기술 Android Studio, Java, Leanback Library","link":"/about/index_backup.html"},{"title":"Who am I","text":"Backend Engineer What I Interested inProgramming Language &amp; Framework Scala, Lagom, Akka Java, SpringBoot, WebFlux Python, FastAPI, LangChain Javascript, React, Node.js Infra Docker &amp; Docker Compose Kubernetes AWS, Azure LLM LangChain, LangSmith pgvector RAG ETC Cassandra, Redis, PostgreSQL, PostGIS Kafka Jenkins Android","link":"/about/index.html"}]}