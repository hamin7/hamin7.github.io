{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Reactive Design Pattern Actors Cluster Cassandra Event Sourcing &amp; CQRS Persistence(Event Sourcing) Lagom core concepts Docker &amp; Kubernetes Cassandra, Zookeeper, Kafka on Docker Akka, Lagom on Kubernetes Programming in Scala Webflux","link":"/2019/09/01/hello-world/"},{"title":"권한 관리 - chmod란","text":"유닉스 및 유닉스 유사 운영 체제에서 chmod는 파일 시스템 개체(파일 및 디렉토리)의 액세스 권한을 변경하는 데 사용되는 명령 및 system call이다.특수 모드 플래그를 변경할 때도 사용된다. 요청은 umask로 필터링된다. 이름은 Change mode의 약칭이다.","link":"/2020/04/08/chmod-types/"},{"title":"머신러닝을 위한 데이터 다루기","text":"머신러닝(machine learning)이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다.최근 머신러닝의 발전은 통계나 수학 이론보다 경험을 바탕으로 발전하는 경우도 많다. 컴퓨터 과학 분야가 이런 발전을 주도하고 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 사이킷런 라이브러리는 파이썬 API를 사용하는데 파이썬 언어는 배우기 쉽고 컴파일하지 않아도 되기 때문에 사용하기 편리하다.연구자들은 새로운 알고리즘을 끊임없이 개발하여 발표한다. 많은 사람이 이를 검증하고 사용해 본 다음 장단점을 파악하게 된다. 어느정도 시간이 지나서 이런 알고리즘이 유익하다고 증명되어 널리 사용하게 되면 사이킷런 라이브러리 개발자들이 이 알고리즘을 라이브러리에 추가한다. 그러므로 머신러닝 라이브러리에 포함된 알고리즘들은 안정적이며 성능이 검증되어 있다. 머신러닝에 쓰이는 용어 특성 : 데이터를 표현하는 하나의 성질ex) 생선 데이터 각각의 길이와 무게 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정, 사이킷런에서는 fit() 메서드가 하는 역할 K-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체를 모델이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함 정확도 : 정확한 답을 몇 개 맞췄는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨. matplotlib scatter()는 산점도를 그리는 맷플롯십 함수이다. 처음 2개의 매개변수로 x축과 y축 값을 전달한다. 이 값은 파이썬 list 또는 numpy 배열입니다. c 매개변수로 색깔을 지정합니다. RGB를 16진수로 지저하거나 색깔코드 ‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’ 중 하나를 지정합니다. 지저하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그립니다. maker 매개변수로 마커 스타일을 지정합니다. maker의 기본값은 o(circle, 원)입니다. scikit-learn KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스이다. n_neighbors 매개변수로 이웃의 개수를 지정한다. 기본값은 5이다. p 매개변수로 거리 재는 방법을 지정한다.(기본값 : 2) 1일 경우 : 맨해튼 거리 2일 경우 : 유클리디안 거리 n_jobs 매개변수로 사용할 CPU 코어를 지정한다. -1로 설정하면 모든 CPU 코어를 사용한다. 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다. 기본값은 1 fit() : 사이킷런 모델을 훈련할 때 사용하는 메서드이다. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달한다. predict() : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드이다. 특성 데이터 하나만 매개변수로 받는다. score() : 훈련된 사이킷런 모델의 성능을 측정한다. 처음 두 매개변수로 특성과 정답 데이터를 전달한다. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환한다. 지도학습과 비지도학습지도학습 (supervised learning) 지도학습에서는 데이터와 정답을 입력(input)과 타깃(target)이라고 하고, 이 둘을 합쳐 훈련 데이터(training data)라고 부른다. 입력으로 사용된 길이와 무게를 특성(feature)이라고 한다. 훈련세트와 테스트세트 훈련 세트 : 모델을 훈련할 때 사용하는 데이터. 보통 훈련 세트가 크면 클수록 좋다. 따라서 테스트 세트를 제외한 모든 데이터를 사용한다. 테스트 세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많다. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있다. 샘플링 편향(sampling bias) 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않은 경우 넘파이 (numpy) numpy는 파이썬의 대표적인 배열 라이브러리이다. 파이썬의 리스트로 2차원 리스트를 표현할 수 있지만 고차원 리스트를 표현하려면 번거롭다. 넘파이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. seed() : 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정한다. 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다. arrange() : 일정한 간격의 정수 또는 실수 배열을 만든다. 기본 간격은 1이다. 매개변수가 하나이면 종료 숫자를 의미한다. 0에서 종료 숫자까지 배열을 만든다. 종료 숫자는 배열에 포함되지 않는다. 12&gt;&gt;&gt; print(np.arange(3))[0 1 2] 매개변수가 2개이면 시작 숫자, 종료 숫자를 의미한다. 12&gt;&gt;&gt; print(np.arange(1, 3))[1 2] 매개변수가 3개면 마지막 매개변수가 간격을 나타낸다. 12&gt;&gt;&gt; print(np.arange(1, 3, 0.2))[1. 1.2 1.4 1.6 1.8 2. 2.2 2.4 2.6 2.8] shuffle() : 주어진 배열을 랜덤하게 섞는다. 다차원 배열일 경우 첫 번째 축(행)에 대하여만 섞는다.123456&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; np.random.shuffle(arr)&gt;&gt;&gt; print(arr)[[1 2] [5 6] [3 4]] 파이썬 리스트를 넘파이 배열로 바꾸는것은 쉽다. 넘파이 array() 함수에 파이썬 리스트를 전달하면 끝이다. 1input_arr = np.array(python_list) 넘파이는 슬라이싱 외에 배열 인덱싱(array indexing)이란 기능을 제공한다. 배열 인덱싱은 1개의 인덱스가 아닌 여러개의 인덱스로 한 번에 여러 개의 원소를 선택할 수 있다.예를 들면 아래처럼 input_arr에서 두 번째와 네 번째 샘플을 선택하여 출력 가능하다. 123print(input_arr[[1,3]])&gt;&gt; [[ 26.3 290. ] [ 29. 363. ]] 비지도학습 (unsupervised learning)타깃 데이터가 없다. 따라서 무엇을 예측하는것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 주로 활용한다. 데이터 전처리데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. (두 특성의 스케일이 다른 경우 등)알고리즘이 거리기반일 때 특히 그렇다. 여기에는 k-최근접 이웃도 포함된다. 이런 알고리듬들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰 주어야 한다. 이런 작업을 데이터 전처리(data preprocessing)라고 부른다. 표준점수데이터 전처리 방법 중 하는 표준점수이다(혹은 z 점수라고도 부른다). 표준점수는 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다. numpy의 std() 함수를 이용하여 표준점수를 계산할 수 있다.12mean = np.mean(train_input, axis=0)std = np.std(train_input, axis=0) 브로드캐스팅크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능이다. scikit-learntrain_test_split()훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수다. 여러 개의 배열을 전달할 수 있다. 테스트 세트로 나눌 비율은 test_size 매개변수에서 지정할 수 있으며 기본값은 0.25(25%)이다.shuffle 매개변수로 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞을지 여부를 결정할 수 있다. 기본값은 true이다. stratify 매개변수에 클래스 레이블이 담긴 배열(일반적으로 타깃 데이터)을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다. kneighbors()k-최근접 이웃 객체의 메서드이다. 이 메서드는 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로 이웃의 개수는 KNeighborClassifier 클래스의 객체를 생성할 때 지정한 개수를 사용한다. 하지만 n_neighbors 매개변수에서 다르게 지정할 수도 있다. return_distance 매개변수를 False로 지정하면 이웃 샘플의 인덱스만 반환하고 거리는 반환하지 않는다. 이 매개변수의 기본값은 True이다. 튜플이란? 파이썬 튜플은 리스트와 매우 비슷하다. 리스트처럼 원소에 순서가 있지만 한 번 만들어진 튜플은 수정할 수 없다. 튜플을 사용하면 함수로 전달한 값이 바뀌지 않는다는 것을 믿을 수 있기 때문에 매개변수 값으로 많이 사용된다. column_stack : 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결. 1234&gt;&gt;&gt; np.column_stack(([1,2,3],[4,5,6]))array([[1, 4], [2, 5], [3, 6]])","link":"/2024/03/25/machineLearning/"},{"title":"RAG란 무엇인가?","text":"RAG는 “Retrieve, Generate, and Rank”의 약자로, 주로 자연어 처리(NLP)와 관련된 작업에서 사용되는 기술방법론이다. 이 접근 방식은 정보검색(IR: Information Retrieval)과 생성적 모델(Generative Models)을 결합하여, 복잡한 질문에 대해 더 정확하고 관련성 높은 답변을 생성하는데 사용된다. RAG 모델은 특히 대규모 텍스트 데이터셋에서 정확한 정보를 검색하고, 이를 기반으로 새로운 텍스트를 생성하여 사용자의 질문에 답변하는 데 효과적이다. 작동방식RAG 모델의 작동 방식은 크게 세 단계로 나눌 수 있다. 검색(Retrieve) : 사용자의 질문이 주어지면, 모델은 대규모 데이터셋에서 질문과 관련된 정보나 문서를 검색한다. 이 과정에서는 주로 효율적인 검색 알고리즘이나 기술이 사용된다. 생성(Generate) : 검색된 정보를 기반으로, 모델은 관련성 높은 답변을 생성한다. 이 단계에서는 생성적 딥러닝 모델, 특히 변형자(Transformer) 기반의 언어 모델이 사용될 수 있다. 순위 매기기(Rank) : 모델이 생성한 여러 답변 중에서 최종 사용자에게 제시될 가장 적합한 답변을 선택하기 위해, 답변들의 순위를 매긴다. 순위 매기기는 답변의 정확성, 관련성, 유용성 등 여러 기준을 고려하여 수행된다. 응용분야RAG 모델은 다양한 NLP 작업에 활용될 수 있으며, 특히 정보가 풍부한 오픈 도메인 질의응답(Open-Domain Question Answering), 자연어 이해(Natural Language Understanding), 챗봇(Chatbots) 등의 분야에서 유용하다. 장점 정확성과 관련성 향상 : RAG 모델은 관련 문서를 검색하여 정보를 기반으로 답변을 생성하기 때문에, 답변의 정확성과 관련성이 향상될 수 있다. 유연성 : 다양한 종류의 데이터와 질문에 적용될 수 있는 높은 유연성을 가진다. 지식 기반 학습 : RAG 모델은 대규모 데이터셋에서 검색한 정보를 활용하여 학습히기 때문에, 지식 기반 학습에 강점을 보인다. 한계 리소스 요구량 : 대규모 데이터셋에서 효율적인 검색과 순위 매기기를 수행하려면 상당한 계산 리소스가 필요할 수 있다. 검색 품질 : 최종 답변의 품질은 검색 단계에서 검색된 문서의 품질에 크게 의존한다. 검색 알고리즘의 성능이 중요하다. RAG는 AI와 NLP 분야에서 중요한 연구주제이며, 지속적인 기술 발전으로 인해 응용 가능성이 확대되고 있다.","link":"/2024/03/26/WhatIsRAG/"},{"title":"선형회귀 알고리즘","text":"지도 학습 알고리즘은 크게 분류와 회귀(regression)으로 나뉜다. 분류는 말 그대로 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이다. 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제이다. 예를 들면 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제이다. 회귀는 정해진 클래스가 없고 임의의 수치를 출력한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 3장. 회귀알고리즘과 모델규제] K-최근접 이웃 회귀(출처 : kNN 최근접 이웃 알고리즘) 녹색 영화는 액션영화일까? 로맨틱 영화일까?녹색 영화는 액션 영화와 로맨틱 영화 가운데 있다.그래서 상당히 답을하기 곤란한 상황이다.현실 세계에서 이게 액션 영화다 로맨틱 영화다 라고 딱 부러지게 얘기하기는 어렵다.이럴 경우에 머신러닝을 사용해 예측갑을 가지고 이야기 할 수가 있다. 그래서 기존의 데이터, 녹색 별을 제외한 기존의 데이터를 중심으로 이 녹색영화가 액션 영화다, 로맨틱 영화다라고 이야기 하는 방법이 knn 알고리즘이다. y축에 보이는 것처럼 발차기 횟수가 많을 경우에는 액션 영화의 가능성이 크고x축의 키스 횟수가 많을때는 로맨틱 영화다라고 볼 수가 있다. 그렇다면 여기서 knn 알고리즘을 간단하게 살펴보도록 하겠다.일단은 k를 정해줘야 한다.k는 최근접점을 우리가 몇개까지 볼것인지 정하는 것이다.일단 k=3으로 쓰겠다. 이것으로 한번 예측값을 내보도록 하겠다. k는 기본적으로 홀수를 쓴다. 왜냐하면 짝수로 쓰면 2:2와 같은 상황이 되어 답을 할 수 없는 상황이 되기 때문이다. 위 그래프에서 보이는 것처럼 최근접 거리에 있어 써클안에 액션 영화가 2개가 있고로맨틱 영화가 하나가 있다.그래서 원 안에 로맨틱 영화보다 액션 영화가 더 많기 때문에 녹색 영화는 액션 영화에요 라고 예측 값을 리턴할 수 있다.이것이 바로 knn 알고리즘의 핵심이다. 그렇다면 최근접점을 프로그램상에서 어떻게 구하는지 보자.피타고라스의 정리를 이용해서 구한다.두 정점의 거리를 구해서 가장 작은 거리의 점들부터 비교를 해나가는 것이다. 결정계수 (R^2)과대적합선형회귀모델 파라미터다항 회귀특성 공학과 규제다중 회귀특성 공학릿지라쏘하이퍼파라미터","link":"/2024/03/27/linearRegression/"},{"title":"AWS Landing Zone 구축하기","text":"“Landing Zone”은 비행기가 안전하게 착륙할 수 있는 공간을 의미한다.즉, AWS Landing Zone을 설계한다는 것은 AWS 상에서 시스테믈 안정적으로 구축하기 위한 기본 사항을 준비하고 설계, 구축하는 일련의 과정이다. Landing Zone 이란?Multi Account의 확장 가능하고 안전한 환경을 제공하는 AWS 솔루션AWS의 컨트롤 타워는 ‘랜딩존’이라는 다중 계정의 AWS 환경을 쉽게 설정하고 관리하는 환경을 제공한다. 클라우드 세상의 컨트롤 타워클라우드를 잘 사용하기 위해서는 고려해야 할 사항이 많다. 성능, 보안, 안정성 등은 기본이고 가격 경쟁력이나 지속 가능성까지도 고려해야한다.서비스의 확장과 함께 탄탄한 아키텍처 기반에 대한 고민을 해결하기 위해 랜딩존을 사용할 수 있다고 한다.즉, 클라우드 환경 또는 확장 가능하고 유연한 아키텍처 설계를 위한 시작점으로써 랜딩존을 사용가능하다고 한다.랜딩 존은 아래와 같은 기능을 제공한다. 다계정 아키텍처가 있는 AWS 환경 초기 보안 기준 ID 및 액세스 관리 지배구조 데이터 보안 네트워크 설계 로그","link":"/2024/04/03/AWS-LandingZone/"},{"title":"로지스틱 회귀","text":"로지스틱 회귀는 선형 방정식을 사용한 분류 알고리즘이다. 시그모이드 함수나 소프트맥스 알고리즘을 사용하여 클래스 확률을 출력할 수 있다. 이 책에서는, 구성품을 모른채 먼저 구매할 수 있는 럭키백이 있다고 가정하고 럭키백을 열어봐야 구성품을 알 수 있다고 한다.럭키백에 들어간 생선의 크기, 무게 등 특성이 주어졌을 때, 어떤 생선인지에 대한 확률을 출력해야 한다. 이를 확인할 수 있는 로지스틱 회귀를 알아보고, 이진분류에 필요한 시그모이드 함수와 다중 분류에 필요한 소프트맥스 함수를 알아본다 [출처 : 혼자 공부하는 머신러닝+딥러닝 4장. 다양한 분류 알고리즘] 데이터 준비하기csv파일을 pandas로 읽어와 타깃 데이터, 입력 데이터로 나눈다. 12345678import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)print(pd.unique(fish['Species'])) # 열에서 고유한 값 출력#fish의 종류를 타깃 데이터, 나머지 특성을 입력 데이터fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy() 훈련 세트와 테스트세트로 나눈 후 표준점수로 전처리한다. 1234567891011from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler#훈련세트와 테스트세트로 나눠주기train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)#입력 데이터 전처리ss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) k-최근접 이웃 분류기로 확률 예측사이킷런의 KneighborsClassifier 클래스로 모델을 훈련한다. 123from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target) 12&gt;&gt;&gt; print(kn.classes_)['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 사이킷런에서는 문자열로 된 타깃값을 그대로 사용할 수 있지만, 순서가 자동으로 알파벳 순서로 매겨진다. 훈련된 모델로 테스트 세트의 5개 샘플의 종류를 예측한다. 12&gt;&gt;&gt; print(kn.predict(test_scaled[:5]))['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] pridict_proba() 메서드는 클래스별 확률값을 반환한다.Numpy의 round()는 반올림 함수이며 decimals 매개변수는 유지할 소수점 아래 자리를 지정할 수 있다. 123import numpy as np# 클래스별 확률값 반환proba = kn.predict_proba(test_scaled[:5]) 1234&gt;&gt;&gt; print(np.round(proba, decimals=4)) #소숫점 4자리까지 반올림해 반환[[0. 0. 1. 0. 0. 0. 0. ][0. 0. 0. 0. 0. 1. 0. ][0. 0. 0. 1. 0. 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ]] 4번째 샘플의 경우 Perch일 확률이 2/3, Roach일 확률이 1/3이다. 1234distances, indexes = kn.kneighbors(test_scaled[3:4])&gt;&gt;&gt; print(train_target[indexes])[['Roach' 'Perch' 'Perch']] 4번째 샘플의 이웃은 Perch가 2개, Roach가 1개로, 구한 확률이 맞음을 보여준다. 단, 이 방법은 3개의 최근접 이웃만을 사용하기에 확률은 0, 1/3, 2/3, 1 뿐이라는 한계가 있다. 로지스틱 회귀로지스틱 회귀는 이름은 회귀이지만 분류 모델이다.선형 회귀와 동일하게 선형 방정식을 학습한다.z = a x Weight + b x length + ··· + fa, b, c, d, e는 계수이며 z는 어떤 값도 될 수 있다. 하지만 확률로 표현하려면 0~1 사이의 값이 되어야 하기 때문에z가 아주 큰 음수일때 0이 되고, 아주 큰 양수일 때 1이 되도록 바꾼다.이는 시그모이드 함수를 사용하면 가능하다. 시그모이드 함수 넘파이를 이용해서 간단하게 그려본다. 1234567import matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z))plt.plot(z, phi)plt.xlabel('z')plt.ylabel('phi')plt.show() 이진 분류를 먼저 수행해 볼 것이다.이진 분류에서 시그모이드 출력이 0.5보다 크면 양성클래스, 작으면 음성클래스로 판단한다. 로지스틱 회귀로 이진분류 수행하기불리언 인덱싱으로 도미와 빙어 데이터를 골라낸다. 123bream_smelt_indexes = (train_target == &quot;Bream&quot;) | (train_target == &quot;Smelt&quot;)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 이 데이터로 로지스틱 회귀 모델을 훈련한다. 123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt) 훈련한 모델로, 5개의 테스트 샘플을 예측해본다. 12&gt;&gt;&gt; print(lr.predict(train_bream_smelt[:5]))['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 각각의 샘플 확률을 예측해본다. 1234&gt;&gt;&gt; print(lr.predict_proba(train_bream_smelt[:5]))[[0.99759855 0.00240145][0.02735183 0.97264817][0.99486072 0.00513928][0.98584202 0.01415798][0.99767269 0.00232731]] 로지스틱 회귀가 학습한 계수도 볼 수 있다. 12&gt;&gt;&gt; print(lr.coef_, lr.intercept_)[[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]][-2.16155132] 계수들과 절편을 볼 수 있다. 따라서 이 로지스틱 회귀 모델이 학습한 방정식은 다음과 같다.z = -0.404 x 무게 + -0.576 x 길이 + ··· + -2.161 z값과 시그노이드 함수의 값 또한 볼 수 있다. 12345decisions = lr.decision_function(train_bream_smelt[:5])print(decisions)from scipy.special import expitprint(expit(decisions)) 1[-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 로지스틱 회귀로 다중 분류 수행하기LogisticRegression 클래스는 반복적인 알고리즘을 사용하며, max_iter 매개변수에서 반복값을 지정하며 기본값은 100 이다. 또한 릿지 회귀와 같이 계수의 제곱을 규제하며, L2 규제라고도 불린다. 릿지회귀에서 alpha로 규제의 양을 조절한 것과 달리, C 매개변수로 조절한다. C의 기본값은 1이며 작을수록 규제가 커진다. 123456lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target))print(lr.predict(test_scaled[:5])) # 샘플 5개의 종류 예측 1230.93277310924369750.925['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 과대적합이나 과소적합이 되지 않았다. 5개 샘플에 대한 예측 샘플도 볼 수 있다. 123print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) 123456['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'][[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 클래스 정보와 클래스 예측 확률을 볼 수 있다. 다중 분류에서의 예측 확률은 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다. 소프트맥스 함수소프트 맥스 함수는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다. 수식 넣을 방법 필요…","link":"/2024/03/28/Logistic-Regression/"},{"title":"개체명 인식(Named Entity Recognition)","text":"NER(Named Entity Recognition)은 자연어 처리(Natural Language Processing, NLP)의 한 영역으로, 텍스트에서 사람, 조직, 위치, 날짜, 시간, 통화, 비율과 같은 명명된 엔티티(명사)를 식별하고 분류하는 기술이다. NER 시스템은 주어진 문서에서 중요한 정보를 추출하는 데 사용되며, 정보 검색, 질의 응답 시스템, 콘텐츠 요약, 고객 지원 시스템, 그리고 감정 분석 등 다양한 NLP 응용 프로그램에 활용된다. NER의 주요 기능: 엔티티 식별: 텍스트 데이터에서 사람의 이름, 기업 이름, 지리적 위치 등과 같은 엔티티를 식별합니다. 엔티티 분류: 식별된 엔티티를 사전 정의된 카테고리(예: 인물, 조직, 위치 등)로 분류합니다. 관계 추출: 텍스트 내에서 엔티티 간의 관계를 파악하고 추출합니다. 이는 텍스트 내에서 엔티티 간의 상호 작용을 이해하는 데 도움이 됩니다. NER의 활용 사례: 정보 추출: 뉴스 기사, 소셜 미디어 포스트, 문서에서 중요한 정보를 추출하여 구조화된 데이터로 변환합니다. 문서 분류: 엔티티 정보를 기반으로 문서를 자동으로 분류하고 정리합니다. 지능형 검색 엔진: 특정 엔티티에 관한 정보를 효율적으로 검색하기 위해 사용됩니다. 챗봇과 가상 비서: 사용자 질문에서 중요한 엔티티를 식별하여 보다 정확한 답변을 제공합니다. 감성 분석: 제품, 서비스, 브랜드에 대한 특정 엔티티의 언급을 분석하여 고객의 의견과 태도를 이해합니다.NER 기술의 발전은 딥러닝과 인공 신경망 모델의 진보 덕분에 크게 향상되었습니다. 이러한 모델들은 맥락과 의미를 더 잘 이해할 수 있게 해주며, NER의 정확도와 효율성을 높여줍니다.","link":"/2024/04/03/NER/"},{"title":"비지도학습","text":"지도 학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.라벨링 되어있지 않은 데이터로부터 패턴이나 형태를 찾아야 하기 때문에 지도학습보다는 조금 더 난이도가 있다고 할 수 있다.실제로 지도 학습에서 적절한 피처를 찾아내기 위한 전처리 방법으로 비지도 학습을 이용하기도 한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 6장. 비지도 학습] 비지도학습비지도학습의 대표적인 종류는 클러스터링(Clustering)이 있다. 이 외에도 Dimentionality Reduction, Hidden Markov Model이 있다.예를 들어 여러 과일의 사진이 있고 이 사진이 어떤 과일의 사진인지 정답이 없는 데이터에 대해 색깔이 무엇인지, 모양이 어떠한지 등에 대한 피러르 토대로 바나나다, 사과다 등으로 군집화 하는 것이다. 지도/비지도 학습 모델(Semi-Supervised Learning)을 섞어서 사용할 수도 있다. 소량의 분류된 데이터를 사용해 분류되지 않은 더 큰 데이터 세트를 보강하는 방법으로 활용할 수도 있다. 최근 각광받고 있는 GAN(generative Adversarial Network) 모델도 비지도 학습에 해당한다. 과일 분류하기 예시1!wget https://bit.ly/fruits_300_data -O fruits_300.npy 코랩의 코드 셀에서 ‘!’ 문자로 시작하면 코랩은 이후 명령을 파이썬 코드가 아니라 리눅스 쉘 명령으로 이해한다. wget 명령은 원격 주소에서 데이터를 다운로드하여 저장한다. 1234import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy') npy 파일을 load() 메서드를 이용하여 로드한다. 12&gt;&gt;&gt; print(fruits.shape)(300, 100, 100) 첫 번째 차원(300)은 샘플의 개수 두 번째 차원(100)은 이미지 높이, 세 번째 차원(100)은 이미지 너비 1234567&gt;&gt;&gt; print(fruits[0, 0, :])[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 첫 번째 해에 있는 픽셀 100개에 들어 있는 값을 출력하면 위와 같다.이 넘파이 배열은 흑백 사진을 담고 있으므로 0~255까지의 정숫값을 가진다. 이 첫 번째 이미지를 배열과 비교하기 위해 그림으로 그리면 아래와 같다. 12plt.imshow(fruits[0], cmap='gray')plt.show() cmap : 사용할 컬러의 스케일을 지정해줄 수 있음 우리가 보는 것과 컴퓨터가 처리하는 방식이 다르기 때문에 위와 같이 흑백 이미지를 반전하여 사용한다.cmap 매개변수를 ‘gray_r’로 지정하면 다시 반전하여 우리 눈에 보기 좋게 출력 가능하다. 12plt.imshow(fruits[0], cmap='gray_r')plt.show() 이 그림에서 밝은 부분은 0에 가깝고 짙은부분은 255에 가깝다. 픽셀값 분석하기로드 한 데이터의 처음 100개는 사과, 그다음 100개는 파인애플, 마지막 100개는 바나나이다.각 과일 사진의 평균을 내서 차이를 확인해보겠다.사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나눠 보겠다. 123apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100) reshape() 메서드를 사용해 두 번째 차원(100)과 세 번째 차원(100)을 10,000으로 합친다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다. 이제 apple, pineapple, banana 배열의 크기는 100, 10000)이다. 각 배열에 들어 있는 샘플의 픽셀 평균값을 계산하기 위해 mean() 메서드를 사용하겠다.샘플마다 픽셀의 평균값을 계산해야 하므로 mean() 메서드가 평균을 계산할 축을 지정해야 한다.axis=0으로 하면 첫 번째 축인 행을 따라 계산한다.axis=1로 지정하면 두 번째 축인 열을 따라 계산한다. 1234567891011121314&gt;&gt;&gt; print(apple.mean(axis=1))[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] 사과 샘플 100개에 대한 픽셀 평균값을 계산한 것이다. 히스토그램을 그려보면 평균값이 어떻게 분포되어 있는지 한눈에 볼 수 있다. 히스토그램이란히스토그램은 값이 발생한 빈도를 그래프로 표시한 것이다. 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)이다. 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show() 사과와 파인애플은 90~100 사이에 많이 모여있다. 바나나는 픽셀 평균값만으로 사과나 파인애플과 확실히 구분된다. 사과와 파인애플은 많이 겹쳐져있어서 픽셀값만으로는 구분하기 쉽지 않다. 해결책으로 샘플의 평균값이 아니라 픽셀별 평균값을 비교하는 방법이 있다.전체 샘플에 대해 각 픽셀의 평균을 계산하는 것이다.픽셀의 평균을 계산하는 것은 axis=0으로 지정하면 된다. 12345fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show() 순서대로 사과, 파인애플, 바나나 그래프이다. 각 과일마다 값이 높은 구간이 다르다. 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하여 위 그래프와 비교하면 더 좋다.픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있다. 123456789apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show() 세 과일은 픽셀 위치에 따라 값의 크기가 차이난다.이 대표 이미지와 가까운 사진을 골라낸다면 사과, 파인애플, 바나나를 구분할 수 있을 것이다. 이처럼 흑백 사진에 있는 픽셀값을 사용해 과일 사진을 모으는 작업을 해 보았다. 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 한다. 군집은 대표적인 비지도 학습 작업 중 하나이고, 군집 알고리즘에서 만든 그룹을 클러스터(cluster)라고 부른다. k-means앞에서는 사과, 파인애플, 바나나에 있는 각 픽셀의 평균값을 구해서 가장 가까운 사진을 골랐다. 이 경우에는 사과, 파인애플, 바나나 사진임을 미리 알고 있었기 때문에 각 과일의 평균을 구할 수 있었다. 하지만 진짜 비지도 학습에서는 사진에 어떤 과일이 들어 있는지 알지 못한다.이런 경우 어떻게 평균값을 구할 수 있을까? 바로 k-평균(k-means) 군집 알고리즘이 평균값을 자동으로 찾아준다. k-means 알고리즘 작동방식 무작위로 k개의 클러스터 중심을 정한다. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다. 클러스터에서 속한 샘플의 평균값으로 클러스터 중심을 변경한다. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다. k-means 모델 만들기1. 데이터 준비하기1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) 준비된 넘파이 배열을 100*10000 크기로 재배열한다. 2. k-means 알고리즘으로 모델 학습하기사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈에 KMeans 클래스가 구현되어 있다.n-cluster 매개변수로 클러스터 갯수를 지정할 수 있다.3개로 지정 후 모델을 훈련시킨다. 123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruites_2d) 군집된 결과는 KMeans 객체의 labels_ 속성된 결과에 저장된다.클러스터 갯수가 3이기 때문에 배열의 값은 0,1,2 중 하나이다.단, 레이블값과 순서에 의미는 없다. 12345678910&gt;&gt;&gt; print(km.labels_)[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 이를 통해 각 클러스터의 샘플의 갯수를 알 수 있다. 12&gt;&gt;&gt; print(np.unique(km.labels_, return_counts=True))(array([0, 1, 2], dtype=int32), array([111, 98, 91])) 3. 각 클러스터의 그림 출력하기각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수를 만들어본다. 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() draw_fruits()는 (샘플갯수, 너비, 높이)의 3차원 배열을 받아 가로로 10개의 이미지를 출력하는 함수이다. figsize는 ratio 매개변수에 비례하여 커진다. draw_fruits()에 fruits 배열을 불리언 인덱싱을 통해 넣어준다. 1draw_fruits(fruits[km.labels_==0]) 레이블 0에는 파인애플과 바나나, 사과가 섞여있는 것을 볼 수 있다. k-means 알고리즘이 이 샘플들을 완벽하게 분류하진 못했지만, 비슷한 샘플을 잘 모은 것을 볼 수 있다. 사과를 완벽하게 분류 했다. 4. 클러스터 중심KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluser_centers_ 속성에 저장되어 있다.이를 그림으로 표현해보면 아래와 같다. 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 이전에 각 과일의 평균 픽셀값을 출력했던 것과 비슷함을 확인할 수 있다. 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transfor() 메서드와, 데이터를 예측하는 predict() 메서드가 있다.클러스터 중심이 가장 가까운 것이 예측 클래스로 출력된다. 12345&gt;&gt;&gt; print(km.transform(fruits_2d[100:101]))[[3393.8136117 8837.37750892 5267.70439881]]&gt;&gt;&gt; print(km.predict(fruits_2d[100:101]))[0] k-means 알고리즘은 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 과정을 반복하는데, 알고리즘이 반복한 횟수는 n_iter_에 저장된다. 12&gt;&gt;&gt; print(km.n_iter_)4 최적의 k 찾기 : 엘보우 방법k-means 알고리즘의 단점 중 하나는, 클러스터 갯수를 사전에 지정해야 한다는 것이다.군집 알고리즘에서 적절한 k값을 찾는 완벽한 방법은 없다. 저마다 장단점이 있지만 가장 대표적인 엘보우 방법을 알아보겠다. k-means 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이것의 제곱합을 이너셔(inertia)라고 한다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값인데, 클러스터 개수가 늘어나면 이너셔도 줄어든다. 엘보우 방법은 클러스터 갯수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터를 찾는 방법이다. 클러스터 갯수에 대한 이너셔를 그래프로 그리면 꺽이는 지점이 있는데, 그 지점이 바로 적절한 클러스터 갯수이다.KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다. 123456789inertia = []for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show() 주성분 분석차원 축소주성분 분석설명된 분산","link":"/2024/04/01/Unsupervised-Learning/"},{"title":"인공 신경망 (Artificial Neural Network)","text":"인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 구조와 기능을 모방하여 만들어진 컴퓨팅 시스템이다. 인공 신경망은 데이터를 처리하고 학습하는데 사용되며, 주로 패턴 인식, 분류, 예측 등과 같은 다양한 머신러닝과 딥러닝 문제를 해결하는 데 활용한다. 생물학적 뉴런에서 영감 받아 만든 머신러닝 알고리즘이지만, 실제 우리 뇌를 모델링한 것은 아니다. 신경망은 기존 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있다. 인공 신경망 알고리즘을 종종 딥러닝이라고 부른다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 1. 패션 MNIST딥러닝을 배울 때 많이 쓰는 데이터셋이 MNIST 데이터셋이다.10종류의 패션 아이템으로 구성되어있다. keras.datasets.fashion_mnist모듈 아래 load_data() 함수로 훈련 데이터와 테스트 데이터를 얻을 수 있다. 12345from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()print(train_input.shape, traion_taret.shape)print(test_input.shape, test_target.shape) 12(60000, 28, 28) (60000,)(10000, 28, 28) (10000,) 28*28 사이즈의 이미지가 훈련세트에는 6만개, 테스트세트에는 1만개 들어있다. 훈련 데이터에서 몇 개의 샘플을 그림으로 표현해본다. 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10,10))for i in range(10): axs[i].imshow(train_input[i], cmap='gray_r') axs[i].axis('off')plt.show() 12import numpy as npprint(np.unique(train_target, return_counts=True)) 1(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) 12각 레이블 :0 티셔츠, 1 바지, 2 스웨터, 3 드레스, 4 코트, 5 샌달, 6 셔츠, 7 스니커즈, 8 가방, 9 앵클부츠 각 레이블마다 6000개의 샘플이 들어있다. 2. 로지스틱 회귀로 패션 아이템 분류하기이 훈련 샘플은 60,000개나 되기 때문에, 전체 데이터를 한꺼번에 사용하여 모델을 훈련시키기보다, 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 효율적이다. 이 상황에서 가장 잘 맞는 방법은 확률적 경사 하강법이다.SGDClassifier 클래스의 매개변수 loss='log'로 지정하여 확률적 경사 하강법 모델을 만든다. 2-1. 데이터를 표준화 전처리하기확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동한다. 특성마다 값의 범위가 다르다면 올바르게 손실 함수의 경사를 내려올 수 없다. 이 데이터셋의 각 픽셀은 0255 사이의 정수값을 가지기 때문에, 255로 나누어 01 사이의 값으로 정규화한다. 12train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28) SGDClassifier는 2차원 입력을 다루지 못하기 때문에 각 샘플을 1차원 배열로 만들어야 한다. reshape() 메서드의 두 번째 매개변수를 28*28 이미지 크기에 맞게 지정하면 척 번째 차원은 변하지 않고, 원본 데이터의 두 번째, 세 번째 차원이 1차원으로 합쳐진다. 1print(train_scaled.shape) 1(60000, 784) 2-2. 모델을 만들고 교차검증하기SGDClassifier 클래스와 cross_validate()로 교차검증을 진행한다. 123456from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)scores = cross_validate(sc, train_scaled, train_traget, n_jobs=-1)print(np.mean(scores['test_score'])) 10.8196000000000001 2-3. 로지스틱 회귀 공식 복습앞서 배운 로지스틱 회귀 공식은 이렇다.z = a * weight + b * length + ... + f 이를 MNIST 데이터셋에 적용하면 이렇다.z_티셔츠 = w1 * 픽셀1 + w2 * 픽셀2 + ... + w784 * 픽셀784 + bz_바지 = w1' * 픽셀1 + w2' * 픽셀2 + ... + w784' * 픽셀784 + b' 이와 같이 10개의 클래스에 대한 선형 방정식을 구한 뒤, 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻을 수 있다. 2-4. SGDClassifier 복습SGDClassifier는 사이킷런(Scikit-learn) 라이브러리에서 제공하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용한 선형 분류기이다. “SGD”는 반복적으로 주어진 데이터셋을 작은 배치로 나누어 모델의 가중치를 업데이트하는 최적화 알고리즘을 말하며, 이를 통해 분류 또는 회귀 문제를 해결할 수 있다. SGDClassifier는 특히 대규모 데이터셋에 대한 선형 분류 문제에 효과적인 방법으로 널리 사용된다. 이 클래스는 다양한 선형 모델, 예를 들어 로지스틱 회귀(Logistic Regression), 선형 서포트 벡터 머신(Linear Support Vector Machine) 등을 구현할 수 있도록 지원한다. 주요 기능 및 특징: 효율성: 대규모 데이터셋에 대해 효율적인 학습이 가능하다. 유연성: 다양한 손실 함수(Loss Function)를 지원하며, 이를 통해 다양한 선형 분류 문제를 해결할 수 있다. 예를 들어, loss=&quot;hinge&quot;는 선형 SVM을, loss=&quot;log&quot;는 로지스틱 회귀를 의미합니다. 정규화: l2, l1, elasticnet과 같은 정규화 옵션을 제공하여 모델의 복잡도를 조절하고, 과적합을 방지할 수 있다. 온라인 학습 지원: partial_fit 메서드를 사용하여 데이터가 순차적으로 도착할 때 모델을 점진적으로 업데이트할 수 있다. 사용 예시:123456789101112from sklearn.linear_model import SGDClassifierfrom sklearn.datasets import make_classification# 예제 데이터 생성X, y = make_classification(n_samples=1000, n_features=20, random_state=42)# SGDClassifier 인스턴스 생성 및 학습clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=1000)clf.fit(X, y)# 예측predictions = clf.predict(X[:5]) 이 코드는 먼저 가상의 분류 데이터셋을 생성하고, SGDClassifier를 사용하여 선형 SVM 모델 (loss-&quot;hinge&quot;)을 학습한 뒤, 몇 개의 샘플에 대해 예측을 수행한다.SGDClassifier는 특히 대규모 데이터셋을 다룰 때 그 장점이 두드러지며, 다양한 선형 분류 문제에 적용될 수 있다. 3. 인공신경망으로 모델 만들기3-1. 인공 신경망 가장 간단한 인공 신경망은 출력층 하나가 있는 인공 신경망이다.확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다.보통 인공 신경망을 이야기 할 때나 딥러닝을 이야기 할 때는 출력층 하나가 아니라 더 많은 층이 있는 경우를 이야기한다. 입력층 : 픽셀값 자체이고, 특별한 계산을 수행하지 않는다. 출력층 : z1 ~ z10을 계산하고 이를 바탕으로 클래스를 예측 뉴런 : z값을 계산하는 단위, 뉴런에서 일어나는 일은 선형 계산이 전부이다. 이제는 뉴런이란 표현 대신 유닛(unit)이라고 부르는 사람이 더 많다. 3-2. 텐서플로우와 케라스 텐서플로우 : 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리케라스 : 텐서플로의 고수준 API 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리 장치인 GPU를 사용하여 인공 신경망을 훈련한다는 것이다. GPU는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 인공 신경망에 큰 도움이 된다. 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다. 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다. 예를 들면 텐서플로가 케라스의 백엔드 중 하나이다. 이런 케라스를 멀티-백엔드 케라스라고 부른다. 케라스 API만 익히면 다양한 딥러닝 라이브러리를 입맛대로 골라서 쓸 수 있다. 3-3. 케라스 모델 만들기인공신경망에서는 교차 검증을 잘 사용하지 않고, 검증 세트를 별도로 덜어 내어 사용한다. 데이터셋이 충분히 크고, 교차검증에는 오랜시간이 걸리기 때문이다. 12345from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)print(train_scaled.shape, train_target.shape) #훈련세트print(val_scaled.shape, val_target.shape) #검증세트 12(48000, 784) (48000,)(12000, 784) (12000,) 가장 기본이 되는 층인 밀집층을 만들어본다.입력층은 784개의 뉴런으로 구성되며, 출력층은 10개의 뉴런으로 구성된다.밀집층은 각 뉴런이 모두 연결되어야 하기 때문에, 784*10 = 7840개의 선이 포함된다.이를 완전 연결층이라고도 부른다. 12dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))model = keras.Sequential(dense) # 밀집층을 가진 신경망 모델 뉴런의 갯수를 10으로 지정하고, 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해 소프트맥스 함수를 이용한다. 만약 이진분류라면 activation='sigmoid'로 입력한다.이후 밀집층을 가진 신경망 모델을 만들기 위해 Sequential클래스를 사용한다. 소프트맥스와 같이 뉴런의 선형방정식 계산 결과에 적용되는 함수를 활성화 함수라고 부른다. 케라스 모델은 훈련하기 전 설정 단계가 있다. 1model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') loss매개변수에 이진 분류라면 binary_crossentropy, 다중 분류라면 categorical_crossentropy를 사용한다. 정수로 된 타깃값을 원-핫 인코딩으로 바꾸지 않고 사용하려면 loss='sparse_categorical_crossentropy',타깃값을 원-핫 인코딩으로 준비했다면 loss='categorical_crossentropy'으로 지정한다. metrics 매개변수는 accuracy를 지정하면 에포크마다 정확도를 함께 출력해준다. 1model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 3s 1ms/step - loss: 0.6125 - accuracy: 0.7901Epoch 2/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4786 - accuracy: 0.8392Epoch 3/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8475Epoch 4/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4452 - accuracy: 0.8512Epoch 5/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4376 - accuracy: 0.8547 epochs 매개변수로 에포크 횟수를 지정할 수 있다.evaluate() 메서드로 모델의 성능을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 3ms/step - loss: 0.4630 - accuracy: 0.8458[0.46303632855415344, 0.8458333611488342] fit() 메서드와 비슷한 출력을 보여준다.","link":"/2024/04/02/Artificial-Neural-Network/"},{"title":"심층 신경망(Deep Neural Network)","text":"심층 신경망(Deep Neural Network, DNN)은 여러 개의 은닉층을 포함하는 인공 신경망의 한 종류이다. 인공 신경망은 입력층(input layer), 하나 이상의 은닉층(hidden layers), 그리고 출력층(output layer)으로 구성되며, 이 중에서 은닉층이 여러 개인 경우를 심층 신경망이라고 한다. 심층 신경망은 복잡한 데이터에서 높은 수준의 추상화와 패턴 인식을 수행할 수 있으며, 이미지 인식, 자연어 처리, 음성 인식 등 다양한 분야에서 광범위하게 활용된다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 심층 신경망의 특징: 다층 구조: 심층 신경망은 두 개 이상의 은닉층을 가진다. 은닉층의 수가 많을수록 네트워크는 더 복잡한 패턴과 관계를 학습할 수 있다. 비선형성: 심층 신경망은 비선형 활성화 함수를 사용하여 입력 데이터의 비선형 특성을 모델링한다. 이를 통해 선형 모델로는 표현할 수 없는 복잡한 패턴을 학습할 수 있다. 자동 특성 추출: 심층 신경망은 데이터로부터 중요한 특성을 자동으로 학습하고 추출할 수 있다. 이는 수동으로 특성을 설계하는 작업을 줄여준다. 범용 근사자: 이론적으로 심층 신경망은 어떤 함수도 근사할 수 있는 범용 함수 근사자(universal function approximator)로 간주된다. 활용 분야: 컴퓨터 비전: 이미지 분류, 객체 탐지, 이미지 생성 등에 활용된다. 자연어 처리: 기계 번역, 감성 분석, 텍스트 요약 등의 작업에 사용된다. 음성 인식: 음성을 텍스트로 변환하거나, 음성 명령을 인식하는 데 사용된다. 게임 및 로봇 공학: 자율 주행, 게임 AI, 로봇의 의사 결정 등에 활용된다. 도전 과제: 과적합(Overfitting): 모델이 훈련 데이터에 지나치게 최적화되어 새로운 데이터에 대한 일반화 능력이 떨어질 수 있다. 해석성(Interpretability): 심층 신경망의 결정 과정이 “블랙 박스”처럼 보일 수 있어, 모델의 예측을 해석하기 어려울 수 있다. 계산 비용: 심층 신경망의 학습은 대량의 데이터와 고성능의 컴퓨팅 자원을 요구한다. 이제 여러 개의 층을 추가하여 다층 인공 신경망, 즉 심층 신경망을 만들고, 은닉층에 사용하는 활성화 함수인 렐루 함수, 가중치와 절편을 학습하기 위한 옵티마이저를 알아본다. 1. 데이터 준비데이터를 표준화 전처리하고 훈련세트와 검증세트로 나눈다. 123456789from tensorflow import kerasfrom tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 시그모이드 함수로 밀집층 추가하기인공 신경망과 달리, 입력층과 출력층 사이에 밀집층을 추가한다. 이를 은닉층이라고 한다. 2-1. 은닉층의 활성화 함수 : 시그모이드인공 신경망에서 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다.단 출력층에서는 보통 이진 분류에서는 시그모이드 함수, 다중 분류에서는 소프트맥스를 사용한다.은닉층에도 활성화 함수가 적용되는데, 대표적으로 시그모이드 함수와 볼 렐루 함수가 있다. 은닉층 활성화 함수를 적용하는 이유는 선형 계산을 비선형으로 비틀어 주어 다음 층의 계산과 합쳐지지 않고 역할을 수행할 수 있기 때문이다. 아래 그림은 시그모이드 그래프이다. 이 함수는 뉴런의 출력 z값을 0과 1사이로 압축한다. 이를 사용해 은닉층을 만든다. 2-2. 시그모이드 활성화 함수로 심층 신경망 생성하기1234dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))#출력층에서 10개의 클래스를 분류하므로 10개의 뉴런, 소프트맥스 활성화함수dense2 = keras.layers.Dense(10, activation='softmax') activation='sigmoid'로 활성화 함수를 시그모이드로 지정할 수 있다.은닉층에서 100개의 뉴런을 지정했는데, 이는 특별한 기준이 없지만, 출력층의 뉴런보다는 많이 만들어야한다. 이제 위의 두 개층을 Sequential 클래스에 추가하여 심층 신경망을 만든다.두 개의 층을 리스트로 Sequential 클래스에 전달한다. 12model = keras.Sequential([dense1, dense2])model.summary() 1234567891011Model: &quot;sequential_2&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ summary() 메서드로 층에 대한 정보를 얻는다. dense의 출력 크기를 보면 (None, 100)으로, 첫번째 차원은 샘플 크기를 나타낸다. 샘플 갯수가 아직 정의되지 않아 None 이며, 후에 fit() 매서드에 훈련 데이터를 주입하면 미니배치 경사 하강법을 사용한다.케라스의 기본 미니베치 크기는 32개이며, fit() 메서드에서 batch_size 매개변수로 바꿀 수 있다. 두번째 100개 출력은, 784개의 특성이 은닉층을 통과하며 100개의 특성으로 압축됨을 뜻한다. 모델 파라미터 갯수는 입력픽셀 784개와 100개의 모든 조합에 대한 가중치, 100개의 절편이 있어 784*100 + 199 = 78500개 이다. 두번째 층의 파라미터 또한 100*10 + 10 = 1010개 이다. 2-3. 층을 추가하는 다른 방법Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 방법이 있다. 1234model = keras.Sequential([ keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'), keras.layers.Dense(10, activation='softmax', name=&quot;output&quot;)], name='패션 MNIST모델') 너무 많은 층을 추가하려면 생성자가 매우 길어지기 때문에, add() 메서드도 사용한다. 123model = keras.Sequential()model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax')) 2-4. 심층 신경망 모델 훈련12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 5s 3ms/step - loss: 0.5596 - accuracy: 0.8103Epoch 2/51500/1500 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8556Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3716 - accuracy: 0.8658Epoch 4/51500/1500 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8735Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3320 - accuracy: 0.8801 추가된 층이 성능을 항상시켰다는 것을 알 수 있다. 3. 렐루 활성화 함수초창기 인공 신경망의 은닉층에 많이 사용된 활성화 함수는 시그모이드 함수였다.다만 이 함수는 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다는 단점이 있다. 이는 층이 많은 신경망일수록 효과가 누적되어 학습을 어렵게 한다. 이를 개선하기 위해 렐루함수가 사용된다. 렐루 함수는 max(0,z)로 쓸 수 있다. 이는 특히 이미지 처리에 좋은 성능을 낸다. 3-1. 입력 차원을 일렬로 펼치는 Flatten 층렐루 함수를 적용하기 전, 입력차원을 일렬로 펼치는 Flatten 층을 알아본다.앞에서 reshape() 메서드를 사용하여 사진 데이터를 일렬로 펼쳤지만, 이를 입력층과 은닉층 사이에 추가할 수 있다. 1model.add(keras.layers.Flatten(input_shape=(28, 28))) 3-2. 렐루 함수를 이용한 밀집층 추가123456model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28, 28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() 123456789101112Model: &quot;sequential_4&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_4 (Dense) (None, 100) 78500 dense_5 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다. 3-3. 훈련 데이터로 모델 훈련123456(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5249 - accuracy: 0.8142Epoch 2/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3924 - accuracy: 0.8590Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3553 - accuracy: 0.8712Epoch 4/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3310 - accuracy: 0.8810Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.8868 검증 세트로 모델을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.8674[0.39478805661201477, 0.8674166798591614] 은닉층을 추가하지 않은 모델보다 성능이 몇 퍼센트 더 상승했다. 4. 옵티마이저 : 다양한 경사 하강 알고리즘신경망에는 모델이 학습되지 않아 사람이 지정해주어야 하는 하이퍼파라미터가 많다. 다양한 종류의 경사 하강법 알고리즘도 지정할 수 있는데, 이를 옵티마저라고 한다. 4-1. SGD : 확률적 경사 하강법compile() 메서드에서 케라스의 기본 경사 하강법 알고리즘은 RMSprop을 사용했다.확률적 경사 하강법인 SGD를 사용할 수 있는데, 이 역시 미니배치를 사용한다. 1model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 객체를 생성하여 옵티마저로 적용할 수 있다. 12sgd = keras.optimizers.SGD()model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 클래스의 학습률 기본값은 0.01이며, learning_rate 매개변수에 학습률을 지정할 수 있다. 1sgd = keras.optimizers.SGD(learning_rate=0.1) momentum 매개변수의 기본값은 0이고 0보다 큰 값으로 지정하면 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용할 수 있다. 보통 0.9 이상을 지정한다.nesterov 매개변수를 True로 바꾸면 네스테로프 모멘텀 최적화를 사용한다. 1sgd = keras.optimizers(momentum=0.9, nesterov=True) 네스테로프 모멘텀은 모멘텀 최적화를 두번 반복하여 구현한다. 대부분 기본 확률적 경사 하강법보다 나은 성능을 제공한다. 4-2. Adagrad, RMSprop : 적응적 학습률 사용모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있으며, 이를 통해 안정적으로 최적점에 수렴할 가능성이 높다. 12345adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accurac') 모멘텀 최적화와 RMSprop 장점을 접목한 것이 Adam이다. 4-3. Adam : 모멘텀 최적화와 RMSprop의 장점 접목Adam 클래스의 매개변수 기본값을 사용해 모델을 훈련한다. 12345678model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))# 옵티마이저를 Adam으로 훈련model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5218 - accuracy: 0.8183Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3916 - accuracy: 0.8586Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3544 - accuracy: 0.8711Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3248 - accuracy: 0.8809Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.8880 검증 세트에서의 성능도 확인해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3426 - accuracy: 0.8767[0.3426271080970764, 0.8767499923706055]","link":"/2024/04/02/Deep-Neural-Network/"},{"title":"합성곱 신경망의 구성요소와 이미지 분류","text":"합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이미지 인식, 영상 처리, 컴퓨터 비전 분야에서 사용되는 심층 신경망의 한 종류이다. CNN은 이미지로부터 패턴을 인식하고 이해하는 데 특화되어 있으며, 이를 위해 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer)을 포함한 특별한 구조를 가진다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] CNN의 주요 구성 요소: 합성곱 계층(Convolutional Layer): 이 계층은 이미지로부터 특성을 추출하는 데 사용된다. 여러 개의 필터(또는 커널)를 사용하여 이미지를 스캔하고, 이 과정에서 생성된 특성 맵(feature map)을 통해 이미지의 중요한 정보를 추출한다. 활성화 함수(Activation Function): 대부분의 CNN에서는 ReLU(Rectified Linear Unit) 함수가 활성화 함수로 사용된다. 이 함수는 비선형 변환을 제공하여 네트워크가 복잡한 패턴을 학습할 수 있게 한다. 풀링 계층(Pooling Layer): 특성 맵의 크기를 줄이거나 요약하여 계산량을 감소시키고, 과적합을 방지하는 역할을 한다. 최대 풀링(Max Pooling)과 평균 풀링(Average Pooling)이 일반적으로 사용된다. 완전 연결 계층(Fully Connected Layer): CNN의 마지막 부분에 위치하며, 앞서 추출된 특성을 바탕으로 최종적인 분류나 예측을 수행한다. CNN의 특징 및 장점: 공간적 계층 구조: CNN은 이미지의 공간적 계층 구조를 이해할 수 있으며, 이를 통해 이미지의 로컬 패턴(예: 가장자리, 질감 등)부터 복잡한 객체까지 인식할 수 있다. 매개변수의 공유: 합성곱 필터는 이미지 전체에 걸쳐 공유되므로, 전통적인 심층 신경망에 비해 훨씬 적은 수의 매개변수를 사용한다. 이동 불변성(Translation Invariance): CNN은 이미지 내 객체의 위치가 변해도 동일한 객체를 인식할 수 있다. 활용 분야: 이미지 분류: 사진 속 객체를 분류한다(예: 강아지, 고양이 분류). 객체 탐지: 이미지 내에서 객체의 위치와 종류를 탐지한다. 시맨틱 분할: 이미지를 픽셀 수준에서 분류하여, 각 픽셀이 어떤 객체에 속하는지 결정한다. 얼굴 인식, 자율 주행 자동차, 의료 영상 분석 등 다양한 분야에서 광범위하게 활용된다. 이번에는 텐서플로 케라스 API를 이용해 패션 MNIST 데이터를 합성곱 신경망 (Convolutional Neural Network, CNN)으로 분류한다. 모델을 만들며 합성곱, 패딩, 스트라이드, 풀링의 개념도 같이 알아볼 것이다. 1. 데이터 준비패션 MNIST 데이터를 불러오고 표준화 전처리 후 훈련세트와 검증세트로 나눈다.이때, 합성곱 신경망은 2차원 이미지를 그대로 사용하기 때문에 일렬로 펼치지 않는다.흑백 이미지이기 때문에 1차원 채널이 추가되며, 컬러 이미지는 3차원이 추가된다. 12345678from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()# 흑백 이미지에 채널 추가train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 합성곱 신경망 만들기2-1. 합성곱 층 추가하기합성곱 신경망의 구조는 합성곱 층에서 이미지의 특징을 감지한 후 밀집층에서 클래스에 따른 분류 확률을 계산한다. 먼저 Sequential 클래스 객체를 만들고, 첫 번째 합성곱 층인 Conv2D를 추가한다. Conv2D() 매개변수로 커널의 개수, 커널 사이즈, 활성화 함수, 패딩, 입력 데이터 크기가 필요하다. 123# 32개 필터, 커널 크기 3x3, 렐루 함수, 세임패딩model = keras.Sequential()model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1))) 커널의 개수를 32개로 지정하고, 커널 사이즈를 3으로 놓으면 (3, 3) 크기가 된다.렐루 함수를 활성화 함수로 지정하고, 세임패딩 적용, 인풋 데이터 크기를 지정한다. 세임 패딩과 밸리드 패딩패딩이란 입력 배열 주위를 가상의 원소로 채우는 것을 의미한다. 예로, (4, 4) 크기의 입력에 0을 1개 패딩하면 (6, 6)크기의 입력이 된다. 세임 패딩 : 합성곱 층의 출력 크기를 입력과 동일하게 만들기 위해 입력에 패딩을 추가하는 것이다. 밸리드 패딩 : 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 것, 특성 맵의 크기가 줄어든다. 만약 패딩이 없다면 원소들이 2번 이상 커널과 계산되는 것과 달리, 네 모서리에 있는 4개의 값은 커널에 한번만 계산되게 된다. 만약 이 입력이 이미지라면 모서리에 있는 중요한 정보가 특성 맵에 잘 전달되지 않을 가능성이 높다. 반대로 가운데 있는 정보는 잘 표현된다. 2-2. 풀링 층 추가하기풀링과 스트라이드 풀링 : 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행한다. 특성 맵의 개수는 줄이지 않는다. 최대 풀링 : 커널 영역에서 가장 큰 값을 고른다. 평균 풀링 : 커널 영역의 값을 평균화한다. 스트라이드 : 합성곱 층에서 필터가 입력 위를 이동하는 크기 예를 들어 (2,2,3) 크기의 특성 맵에 스트라이드가 1인 풀링을 적용하면 (1,1,3) 크기의 특성 맵이 된다. 많은 경우 평균 풀링보다 최대 풀링을 사용하는데, 평균 풀링은 특성 맵의 중요한 정보를 평균화하여 희석시킬 수 있기 때문이다. 케라스는 최대 풀링과 평균 풀링을 MaxPooling2D, AveragePooling2D 로 제공한다. 그 중에서 최대풀링을 사용하며, 풀링 크기를 (2,2)로 지정한다. 1model.add(keras.layers.MaxPooling2D(2)) 패선 MNIST 이미지가 (28,28) 크기에 세임 패딩을 적용하여 합성곱 층에서 출력된 특성 맵의 가로세로 크기는 입력과 동일하다. 이후 (2,2) 풀링을 적용하여 특성 맵의 크기는 절반으로 줄어들고, 합성곱 층에서 32개의 필터를 사용하여 최대 풀링을 통과한 특성 맵의 크기는 (14,14,32) 이다. 이제 두 번째 합성곱-풀링 층을 추가한다. 첫번째와 동일하지만, 필터 개수를 64개로 늘렸다. 12model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'))model.add(keras.layers.MaxPooling2D(2)) 이 층을 통과하면 특성 맵의 크기는 (7,7,64)가 된다. 2-3. Flatten, 은닉층, Drop, 출력층 구성하기이제, 마지막에 10개의 뉴런을 가진 출력층에서 확률을 계산하기 위해 3차원 특성 맵을 펼쳐야 한다. Flatten 층을 만들고, Dense은닉층, Dropout , Dense출력층 순서대로 층을 구성한다. 1234model.add(keras.layers.Flatten())model.add(keras.layers.Dense(100, activation='relu')) #은닉층model.add(keras.layers.Dropout(0.4)) # 40% 드롭아웃model.add(keras.layers.Dense(10, activation='softmax')) #출력층 은닉층과 출력층 사이에 드롭아웃을 넣어 은닉층의 과대적합을 막아 성능을 개선할 수 있다.은닉층에 100개의 뉴런을 사용하고 렐루 활성화 함수를 사용한다.클래스 10개를 분류할 다중 분류 문제이기 때문에 출력층의 활성화 함수는 소프트맥스 함수를 사용한다. 3. 모델 구조 확인하기summary() 메서드로 모델 구조를 확인할 수 있다. 1model.summary() 1234567891011121314151617Model: &quot;sequential&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D) (None, 14, 14, 32) 0 conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling2D) (None, 7, 7, 64) 0 flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 333,526Trainable params: 333,526Non-trainable params: 0_________________________________________________________________ 각 층의 파라미터의 개수를 계산할 수 있다. 첫 번째 합성곱 층은 32개의 필터를 가지고 있고 크기가 (3,3), 깊이가 1이다. 또 필터마다 하나의 절편이 있다. 3x3x1x32+32 = 320개의 파라미터가 있다. 두 번째 합성곱 층은 64개의 필터, 크기 (3,3), 깊이 32이다. 필터마다 하나의 절편이 존지하므로 3x3x32x64+64 = 18496개의 파라미터가 있다. Flatten 층에서 (7,7,64) 크기의 특성 맵을 1차원으로 펼치면 (3136,)이며, 은닉층에서는 3136개가 100개의 뉴런과 연결되어야 하고, 100개의 절편이 있으므로 3136x100+100 = 313700개의 파라미터가 있다. 마지막 출력층은 100개의 특성이 10개의 뉴런과 연결되고, 10개의 절편이 있으므로 100x10+10 = 1010개의 파라미터가 있다. keras.utils 패키지의 plot_model() 으로 층의 구성을 그림으로 볼 수 있다. 1keras.utils.plot_model(model, show_shapes=True, to_file='cnn-architecture.png', dpi=300) show_shapes=True 로, 입력과 출력의 크기가 표시되며, to_file 매개변수는 출력한 이미지를 파일로 저장한다. dpi 매개변수는 해상도를 지정한다. 4. 모델 컴파일과 훈련Adam 옵티마이저를 사용하고, ModelCheckpoint, EarlyStopping 콜백을 사용하여 조기 종료 법을 구현한다. 1234567model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5')early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) 123456789101112131415161718Epoch 1/201500/1500 [==============================] - 19s 7ms/step - loss: 0.5307 - accuracy: 0.8096 - val_loss: 0.3467 - val_accuracy: 0.8756Epoch 2/201500/1500 [==============================] - 10s 7ms/step - loss: 0.3584 - accuracy: 0.8720 - val_loss: 0.3089 - val_accuracy: 0.8859Epoch 3/201500/1500 [==============================] - 11s 7ms/step - loss: 0.3119 - accuracy: 0.8876 - val_loss: 0.2708 - val_accuracy: 0.8968Epoch 4/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2770 - accuracy: 0.8992 - val_loss: 0.2580 - val_accuracy: 0.9047Epoch 5/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2544 - accuracy: 0.9065 - val_loss: 0.2518 - val_accuracy: 0.9101Epoch 6/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2322 - accuracy: 0.9149 - val_loss: 0.2452 - val_accuracy: 0.9122Epoch 7/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2171 - accuracy: 0.9195 - val_loss: 0.2294 - val_accuracy: 0.9176Epoch 8/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2013 - accuracy: 0.9257 - val_loss: 0.2560 - val_accuracy: 0.9134Epoch 9/201500/1500 [==============================] - 10s 7ms/step - loss: 0.1881 - accuracy: 0.9299 - val_loss: 0.2308 - val_accuracy: 0.9186 이전보다 정확도가 훨씬 좋아진 것을 확인할 수 있다. 손실 그래프를 그려, 조기 종료가 잘 이루어졌는지 확인한다. 1234567import matplotlib.pyplot as pltplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() 일곱 번째 에포크가 최적임을 알 수 있다. predict() 메서드를 사용하여 데이터에 대한 예측을 만들어 본다. 123456import numpy as npclasses = ['티셔츠','바지','스웨터','드레스','코트','샌달','셔츠','스니커즈','가방','앵클 부츠']preds = model.predict(val_scaled[0:1])print(classes[np.argmax(preds)]) 1가방 테스트 세트로 합성곱 신경망의 일반화 성능을 가늠해본다. 12test_scaled = test_input.reshape(-1, 28, 28, 1) / 255.0model.evaluate(test_scaled, test_target) 12313/313 [==============================] - 2s 7ms/step - loss: 0.2460 - accuracy: 0.9108[0.24599412083625793, 0.9107999801635742] 약 91% 정도의 성능을 기대할 수 있다.","link":"/2024/04/03/Convolutional-Neural-Network/"},{"title":"신경망 모델 훈련","text":"인공 신경망과 심층 신경망을 구성하고 다양한 옵티마이저를 통해 성능을 향상시킬 수 있는 방법에 대해 알아보았다.이번에는 과대적합을 막기 위해 신경망에서 사용하는 규제방법인 드롭아웃, 최상의 훈련 모델을 자동으로 저장하고 유지하는 콜백과 조기종료를 알아보겠다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망]","link":"/2024/04/02/Train-Neural-Network-Model/"},{"title":"합성곱 신경망의 시각화","text":"이번에는 저번 편에서 저장한 합성곱 신경망 모델을 읽어 들인 후 모델의 가중치와 특성 맵을 시각화해본다. 또한 케라스의 함수형 API를 사용하여 모델의 조합을 자유롭게 구성해본다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] 1. 데이터 준비이전에 훈련한 합성곱 신경망 모델을 불러온다. 12from tensorflow import kerasmodel = keras.models.load_model('best-cnn-model.h5') 케라스 모델에 추가한 층은 layers 속성에 저장되어 있다. 1model.layers 12345678[&lt;keras.layers.convolutional.Conv2D at 0x7f803f8dad90&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc61459d0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7f7fc60eed10&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc60eee90&gt;, &lt;keras.layers.core.flatten.Flatten at 0x7f7fc6085c10&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc60eeb90&gt;, &lt;keras.layers.core.dropout.Dropout at 0x7f7fc608a450&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc607c990&gt;] 첫 번째 합성곱 층의 가중치를 조사해 본다. 층의 가중치와 절편은 층의 weights 속성에 저장되어 있다. 12conv = model.layers[0]print(conv.weights[0].shape, conv.weights[1].shape) 1(3, 3, 1, 32) (32,) 커널 크기가 (3,3,1)이며 필터 개수가 32개이므로 첫 번째 원소의 가중치의 크기는 (3,3,1,32)이다.필터마다 1개의 절편이 있으므로 두 번째 원소의 크기는 (32,0)이다. 한편, weights 속성은 텐서플로의 다차원 배열인 Tensor 클래스의 객체이다.numpy() 메서드로 넘파이 배열로 변환한 후 가중치 배열의 평균과 표준편차를 구해본다. 1234conv_weights = conv.weights[0].numpy()# 평균, 표준편차print(conv_weights.mean(), conv_weights.std()) 1출력 -0.019439656 0.23001778 이 가중치의 평균값은 0에 가깝고, 표준편차는 0.23 정도이다. 나중에 이 값을 훈련하기 전의 가중치와 비교해본다. 1. 가중치 시각화하기1-1. 가중치 분포 히스토그램으로 나타내기먼저 이 가중치의 분포를 히스토그램으로 그려본다. 12345import matplotlib.pyplot as pltplt.hist(conv_weights.reshape(-1,1))plt.xlabel('weight')plt.ylabel('count')plt.show() 0을 중심으로 종 모양으로 분포됨을 확인할 수 있다. 1-2. 커널 그림으로 나타내기이번에는 32개의 커널을 16개씩 두 줄에 출력해 본다. 123456789fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : # 32개의 커널을 16개씩 두 줄에 출력 axs[i, j].imshow(conv_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() vmin과 vmax 파라미터로 픽셀의 최댓값과 최솟값을 지정하여 컬러맵으로 표현할 범위를 지정한다.결과를 보면, 이 가중치 값이 무작위가 아닌 어떠한 패턴이 나타난 것을 볼 수 있다. 픽셀의 특정 부분이 밝다거나 하는 식이다. 1-3. 빈 합성곱 신경망의 가중치위와 같은 방법으로, 훈련되지 않은 합성곱 신경망의 가중치를 조사한다. 12345no_training_model = keras.Sequential()no_training_model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))no_training_conv = no_training_model.layers[0]print(no_training_conv.weights[0].shape) 1(3, 3, 1, 32) 3x3 커널을 32개 사용했다. 12no_training_weights = no_training_conv.weights[0].numpy()print(no_training_weights.mean(), no_training_weights.std()) 1-0.0010081615 0.07870515 위의 훈련된 합성곱 신경망과 비교하여 평균은 비슷하지만 표준편차는 매우 작은 것을 알 수 있다. 1234plt.hist(no_training_weights.reshape(-1,1))plt.xlabel('weights')plt.ylabel('count')plt.show() 대부분의 가중치가 -0.15 ~ 0.15 사이에 있고, 고른 분포를 보이는 것을 알 수 있다.텐서플로가 신경망의 가중치를 처음 초기화할 때, 균등 분포에서 랜덤하게 값을 선택하기에 이런 분포를 보인다. 이를 그림으로 시각화한다. 12345678fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : axs[i, j].imshow(no_training_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() 위와 달리 가중치가 밋밋하게 초기화된 것을 볼 수 있다. 이를 통해 합성곱 신경망이 데이터셋의 분류 정확도를 높이기 위해 유용한 패턴을 학습했다는 사실을 알 수 있다. 2. 함수형 API지금까지 신경망 모델을 만들 때 케라스 Sequential 클래스를 사용했다. 이는 층을 차례대로 쌓은 모델을 만드는데, 딥러닝에서는 좀 더 복잡한 모델이 많이 있어 이런 경우는 Sequential 클래스를 사용하기 어렵다. 대신 함수형 API를 사용한다. 함수형 API는 케라스의 Model 클래스를 사용하여 모델을 만든다.그 예로, Dense층 2개로 만들어진 완전 연결 신경망을 함수형 API로 구현해본다. 123456789inputs = keras.Input(shape=(784,))dense1 = keras.layers.Dense(100, activation='signoid')dense2 = keras.layers.Dense(10, activation='softmax')hidden = dense1(inputs)outputs = dense2(hidden)model = keras.Model(inputs, outputs) 케라스는 InputLayer 클래스 객체를 쉽게 다룰 수 있게 Input() 메서드를 별도로 제공한다. 두 개의 층 dense를 만든 뒤, inputs를 dense1에 통과시켜 출력값 hidden을 만들고, 이를 다시 입력값으로 dense2에 통과시켜 출력값을 만들어 이를 모델화한다. 한편, 특성 맵을 시각화하기 위해서는 첫 번째 층인 Conv2D의 출력이 필요하고, 이는 Conv2D 객체의 output 속성에서 얻을 수 있다.모델 객체의 input 속성으로 모델의 입력 또한 얻을 수 있다. 이것을 이용하여 model.input과 model.layers[0].output 을 연결하는 새로운 conv_acti 모델을 만들 수 있다. 1conv_acti = keras.Model(model.input, model.layers[0].output) model 객체의 predict() 메서드를 호출하면 입력부터 마지막 층까지의 계산을 수행한 후 최종 출력을 반환하므로, conv_acti의 predict() 메서드를 호출하여 Conv2D의 출력을 반환할 수 있다. 이를 통해 특성 맵을 시각화해 본다. 3. 특성 맵 시각화케라스 패션 MNIST 데이터셋으로 훈련 세트의 첫 번째 샘플을 그려본다. 123(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()plt.imshow(train_input[0], cmap='gray_r')plt.show() 이 샘플을 conv_acti 모델에 주입하여 Conv2D층이 만드는 특성 맵을 출력한다.입력 차원을 reshape()하고, 255로 나누어 표준화한다. 1234inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv_acti.predict(inputs)print(feature_maps.shape) 1(1, 28, 28, 32) 28x28 크기의 필터 32개로 구성되어 있다. 이를 시각화한다. 12345678fig, axs = plt.subplots(4, 8, figsize=(15,8))for i in range(4) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 32개의 필터로 인해 입력 이미지에서 강하게 활성화된 부분을 보여 준다.이전 가중치 시각화와 비교하여 어떤 부분이 크게 활성화되었는지 파악할 수 있다. 두 번째 합성곱 층이 만든 특성 맵도 같은 방식으로 시각화한다. 12345678910111213conv2_acti = keras.Model(model.input, model.layers[2].output)inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv2_acti.predict(inputs)fig, axs = plt.subplots(8, 8, figsize=(12,12))for i in range(8) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 시각적으로 이해하기 어렵다. 이를 통해, 처음의 합성곱 층은 이미지의 시각적인 정보를 감지하고, 뒤쪽에 있는 합성곱 층은 앞쪽에서 감지한 시각적인 정보를 바탕으로 추상적인 정보를 학습한다고 볼 수 있다.","link":"/2024/04/04/Visualization-Of-CNN/"},{"title":"순차데이터와 순환신경망","text":"RNN(Recurrent Neural Network, 순환 신경망)은 시퀀스 데이터를 처리하기 위해 설계된 인공 신경망의 한 종류이다. RNN은 시간에 따라 정보를 전달할 수 있는 내부 메모리를 가지고 있어, 시퀀스의 길이에 상관없이 입력데이터 사이의 장기 의존성을 학습할 수 있다. 이러한 특성으로 인해 RNN은 자연어 처리(NLP), 음성 인식, 시계열 예측 등 시퀀스 데이터를 다루는 다양한 분야에서 활용된다. 출처 : 혼자 공부하는 머신러닝+딥러닝 9장. 텍트를 위ㄴ 인공 신경망] 순차 데이터(Sequential Data)순서가 있는 데이터를 말한다. 이러한 데이터는 특정 순서대로 나ㅕㄹ되어 있으며, 각 데이터 포인트 사이에는 시간적 또는 공간적 연관성이 존재한다. 순차 데이터의 한 요소는 그 전후의 요소와 관련이 있으며, 이러한 연속성 때문에 데이터 전체를 통해 패턴이나 관계를 찾아낼 수 있다.ex) 글, 대화, 일자별 날씨, 일자별 판매 실적 순환 신경망일반적인 완전 연결 신경망과 거의 비슷하나, 이전 데이터의 처리 흐름을 순환하는 고리 하나가 추가된다. 뉴런의 출력이 다시 자기 자신으로 전달되는데, 즉 어떤 샘플을 처리할 때 바로 이전에 사용했던 데이터를 재사용하는 것이다. 이렇게 샘플을 처리하는 한 단계를 타임스텝이라고 부르며, 순환신경망은 이전 타임스텝의 샘플을 기억하지만, 타임스텝이 오래될수록 순환되는 정보는 희미해진다.순환 신경망에서는 특별히 층을 셀이라고 부른다. 한 셀에는 여러개의 뉴런이 있지만, 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 은닉 상태라고 부른다.입력에 어떤 가중치를 곱하고, 활성화 함수를 통과시켜 다음층으로 보내는 구조는 합성곱 신경망과 같으나, 층의 출력을 다음 타임 스텝에 재사용하는 것이 다르다.은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트(tanh)를 사용한다. 시그모이드 함수와는 달리 -1 ~ 1 사이의 범위를 가진다. RNN의 주요 특징 순환 구조 : RNN은 네트워크 내에서 정보를 순환시키는 구조를 가지고 있어, 이전의 계산 결과를 현재의 계산에 활용할 수 있다. 이를 통해 시퀀스 내의 정보를 시간적으로 연결하여 처리한다. 변동하는 시퀀스 길이 처리 : RNN은 입력 시퀀스의 길이가 변동적인 데이터를 처리할 수 있다. 이는 고정된 크기의 입력을 다루는 다른 신경망 모델과는 차별되는 특징이다. 파라미터 공유 : 시퀀스의 각 지점(time step)마다 동일한 가중치를 사용함으로써, 모델의 파라미터 수를 효율적으로 관리한다. 활용 분야 자연어 처리 : 문장, 문서분류, 기계 번역, 감성 분석 등 NLP의 여러 작업에서 사용된다. 음성 인식 : 오디오 시퀀스에서 음성을 텍스트로 변환하는 작업에 사용된다. 시계역 예측 : 주식 가격, 기상 상태 등 시간에 따라 변하는 데이터의 미래 값을 예측하는 데 사용된다. 한계 장기 의존성 문제 : RNN은 이론적으로는 시퀀스의 장기 의존성을 학습할 수 있지만, 실제로는 그레디언트 소실(vanishing gradient) 또는 폭발(exploding gradient) 문제로 인해 학습이 어려울 수 있다. 계산 비용 : 순환 구조로 인해 병렬 처리가 어렵고, 긴 시퀀스를 처리할 때 계산 비용이 높아질 수 있다. 이러한 한계를 극복하기 위해 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)와 같은 고급 RNN 구조가 개발되었다. 이들은 장기 의존성을 더 효과적으로 학습할 수 있는 메커니즘을 제공한다. IMDB 리뷰 분류1. 데이터 준비하기IMDB 리뷰 데이터셋을 적재한다. 리뷰를 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋인데, 총 50,000개의 샘플로 이루어져 있고 훈련 데이터와 테스트 데이터에 25,000개씩 나누어져 있다. 실제 IMDB 리뷰 데이터셋은 영어로 된 문장이지만, 텐서플로에는 이미 정수로 바꾼 데이터가 포함되어 있다. 여기에는 가장 자주 등장하는 단어 500개만 사용한다. 12345from tensorflow.keras.datasets import imdb(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)print(len(train_input[0]))print(len(train_input[1])) 123(25000, ) (25000, )218189 첫 번째 리뷰의 길이는 218개의 토큰, 두 번째는 189개의 토큰으로 이루어져있다. 1print(train_input[0]) 12[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, ... ] 텐서플로의 IMDB 리뷰 데이터는 정수로 변환되어 있다. num_words=500으로 지정했기 때문에 어휘 사전에 없는 단어는 모두 2로 표시된다. 1print(train_target[:20]) 1[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] 타깃 데이터는 0(부정)과 1(긍정)으로 나누어진다. 12from sklearn.model_selection import train_test_splittrain_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42) 훈련 데이터의 20% 정도를 검증세트로 떼어 놓는다. 2. 데이터 분석과 패딩평균적인 리뷰, 가장 짧은 리뷰, 가장 긴 리뷰의 길이를 확인하기 위해 먼저 각 리뷰의 길이를 계산해 넘파이 배열에 담아 그래프로 표현한다. 123456789import numpy as npimport matplotlib.pyplot as pltlengths = np.array([len(x) for x in train_input])plt.hist(lengths)plt.xlabel('length')plt.ylabel('frequency')plt.show() 대부분 리뷰 길이는 300개 미만인 것을 볼 수 있다. 리뷰는 대부분 짧기 때문에 이 예제에서는 100개의 단어만 사용하기로 한다.이 리뷰들의 길이를 맞추기 위해 패딩이 필요하다. pad_sequences()함수를 통해 시퀀스 데이터의 길이를 맞출 수 있다.짧은 리뷰는 앞에서부터 0토큰을 채우고, 긴 리뷰는 잘라내는데, 만약 pad_sequences()의 매개변수 padding을 기본값인 pre에서 post로 바꾸면 샘플의 뒷부분으로 패딩할 수 있다. 123456from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100)val_seq = pad_sequences(val_input, maxlen=100)print(train_seq.shape) 1(20000, 100) train_seq는 이제 (20000, 100) 크기의 2차원 배열임을 알 수 있다. 3. 원-핫 인코딩으로 데이터 바꾸기케라스는 여러 종류의 순환층 클래스를 제공하는데, 가장 간단한 것은 SimpleRNN 클래스이다. 이 문제는 이진 분류이므로 마지막 출력층은 1개의 뉴런을 가지고 시그모이드 활성화 함수를 사용한다. 1234from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500)))model.add(keras.layers.Dense(1, activation='sigmoid')) # 이진분류 뉴런 갯수를 8개로 지정하고, 샘플의 길이가 100이고 500개의 단어만 사용하도록 설정했기 때문에 input_Shape를 (100, 500)으로 둔다. 순환층도 활성화 함수를 사용하는데 기본 매개변수 activation의 의 기본값은 tanh로, 하이퍼볼릭 탄젠트 함수를 사용한다. 그러나 토큰을 정수로 변환한 데이터를 신경망에 주입하면, 큰 정수가 큰 활성화 출력을 만들게 된다. 이 정수들 사이에는 어떤 관련이 없기 때문에 정수값에는 있는 크기 속성을 없애고 각 정수를 고유하게 표현하기 위해 원-핫 인코딩을 사용한다. keras.utils 패키지의 to_categorical() 함수를 사용하여 훈련세트와 검증 세트를 원-핫 인코딩으로 바꾸어준다. 123train_oh = keras.utils.to_categorical(train_seq)val_oh = keras.utils.to_categorical(val_seq)print(train_oh.shape) 1(20000, 100, 500) 정수 하나마다 500차원의 배열로 변경되었다. 12print(train_oh[0][0][:12])print(np.sum(train_oh[0][0])) 12[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]1.0 첫 리뷰의 첫 단어를 원-핫 인코딩시킨 결과이다.모든 원소의 값을 더하면 1임을 알 수 있다. 4. 순환 신경망 훈련하기RMSprop의 기본 학습률 0.001을 사용하지 않기 위해 별도의 RMSprop 객체를 만들어 학습률을 0.0001로 지정한다.","link":"/2024/04/08/Recurrent-Neural-Network/"},{"title":"API Gateway Pattern에는 API Gateway가 없다.","text":"Microservices Architecture에 관해 이야기하면 왠지 API Gateway를 꼭 사용해야 할 것 같은 느낌이 든다. 과연 그럴까?왜 함부로 API Gateway를 사용하면 안 되는지, 그래도 사용해야 한다면 언제 사용할지에 대해 다뤄보려고 한다. 우아한형제들의 서버개발그룹장분이 잘 설명해주셔서, 두고두고 보려고 기록해두는 글이다. 출처 : https://www.youtube.com/watch?v=P2nM0_YptOA 다룰 내용 MSA에서 필수적인 API Gateway Pattern이 API Gateway Framework와 무관한 이유 API Gateway Pattern 이란? MSA의 API 애플리케이션의 역할 구분 하나씩 알아보는 API Gateway Framework을 사용하면 안되는 이유들 그래도 API Gateway Framework를 사용해도 되는 경우들 API Gateway Pattern 이란?API Gateway Pattern은 어디 나오나? 크리스 리처드슨은 본인이 생각한 MSA의 여러가지 패턴을 마이크로 서비스 패턴이라는 책으로 정리하였다.그 외에도 마이크로소프트 홈페이지에서도 API Gateway 패턴을 다루고 있다. API 게이트의 패턴은 여러 마이크로 서비스로 나뉜 것을 하나로 묶어 클라이언트가 호출하게 해주는 것으로 MSA에서는 필수적인 요소이다. 그렇다면 API 게이트웨이 패턴의 정의를 그림 한장으로 나타내면 무엇일까? 모바일 앱이나 웹브라우저 혹은 외부 애플리케이션 서버 등 외부의 클라이언트가 API 게이트웨이를 통해서 내부망에 있는 api를 그대로 호출한다.이때, API Gateway가 인증을 하고 클라이언트가 호출 가능한 api들만 공개함으로써 안전하게 내부 서비스를 보호해주게 된다고 생각할것이다.외부 개발자는 내부망의 모든 api 호출 엔드포인트를 알 필요가 없기 때문에 API 게이트웨이 엔드포인트 하나만 확인하고 편하게 호출할 수 있는 좋은 패턴이라고 생각될 것이다. 위의 구조는 API Gateway의 프레임워크를 통해 설정 정보만으로도 구성이 가능하다.안타깝게도 이것은 API Gateway 패턴이 전혀 아니다.그리고 계속 생각할것이다라고 말한 이유는 사실은 그렇지 못하기 때문이다. Microservice.io 혹은 마이크로서비스 패턴 책에 나오는 API Gateway 패턴의 정의는 위의 도식과 같다.이 API Gateway 패턴은 클라이언트의 특정 비즈니스 요청을 받아주는 서버 애플리케이션을 개발자가 직접 작성하고 내부 api들을 조합해서 호출한 뒤 응답을 클라이언트에게 필요한 것만 정리해서 내보내는 방식이다. 앞의 것은 설정만으로도 가능하고, 뒤의 실제 API Gateway 패턴은 개발자가 명백하게 코드로 작성해야만 하는 것이다.코드 작성은 쉽다. 자바 개발자라면 흔한 웹프레임워크인 스프링 웹 MVC나 Webflux를 사용하면 된다.다만 리액티브한 웹플럭스를 권한다. 1차 결론 외부에 API를 제공하기 위한 API Gateway Pattern은 Spring Cloud Gateway나 Netflix Zuul 같은 API Gateway Framework를 사용하는 것과는 무관 API Gateway Pattern은 클라이언트의 요청을 받아서 내부 마이크로서비스를 호출하는 로직을 직접 작성하고 응답 내용도 취사 선택하여 필요한 것만 내보내느 것 인증과 권한 기능을 붙였는지 여부와 무관하게 외부에 API를 제공해줄 용도로 API Gateway Framework를 사용한 요청/응답 Routing은 하면 안됨 특히 규모가 작다면 실제 MSA를 하면서 API Gateway Framework를 사용할 일은 사실 많지 않음 API Gateway FrameworkSpring Cloud Gateway, Netflix Zuul, AWS API Gateway 등을 이 글에서 부르는 용어 Gateway Routing PatternMS에서 정의한 API Gateway Framework를 통한 단순 요청/응답 라우팅에 관한 패턴 BFF - Backend For FrontendAPI Gateway Pattern과 유사하게 직접 구현하는 로직을 작성하는 방법과 API Gateway 애플리케이션을 클라이언트 기기 단위로 구분하고 소유권을 정하는 방식 조합 API Gateway 라는 단어의 의미는 현재 매우 심각하게 오염되어 있다. 이 단어를 듣는 순간 대부분 사람들이 떠올리는 그림은 위 두 그림 중 하나일 텐데 아마도 대부분 사람들은 첫 번째 보여 드린 요청응답을 단순 라우팅하는 그림을 떠올렸을 것이다. 바로 이 점 때문에 의사소통과 실제 구현에서 다양한 문제들이 발생하고 있다. 그래서 필자는 우리가 보통 API 게이트웨이라고 부르는 것들 즉 스프링클라우드 게이트의 넷플릭스 Zuul과 같은 것을 지금부터 API Gateway Framework 라고 부르겠다. 그리고 마이크로소프트 홈페이지에서는 이런 API Gateway Framework를 통한 단순 요청응답으로 구성하는 것을 게이트웨이 라우팅 패턴이라고 불렀다. 그래서 필자도 앞으로 Gateway Routing Pattern이라는 용어로 API Gateway Framework를 사용하는 단순 라우팅을 지칭하도록 하겠다. 왜냐하면 최근 API Gateway Framework는 단순 라우팅뿐만 아니라 코딩을 통해 API Gateway Pattern에서 정의한대로의 역할도 수행할 수 있기 때문이다. API Gateway Framework로도 API Gateway Pattern의 구현이 가능하지만, 필자는 권하지 않는다. 참고로 API Gateway Pattern이 API Gateway라는 용어의 경우 일부 다른 사람들은 엣지 마이크로 서비스 클라이언트 어댑터 등의 다른 용어로 부르는 것도 많이 보인다. 최근에는 BFF라는 용어도 대세가 되어가는 것 같다. MSA 자체의 역사가 길지 않다보니 많은 것들이 정리되지 못하고 다양한 오해를 일으키고 있어 보인다. 사실상 필자가 하고 싶은 말의 핵심은 여기서 끝이다. 위 두 글미을 보는 순간 딱 하고 그래 그렇지 않고 뭔가 떠오른 사람 혹은 이미 알고 있는 사람이라면 더 이상 이 글을 보지 않아도 좋을거 같다. 하지만 어쨋든 왜 API Gateway Pattern이라는게 어째서 단순히 API Gateway Framework를 통한 단순 라우팅을 하면 안되는 것인지 혹은 왜 비록 올바른 API Gateway의 구현이 가능하다 하더라도 필자가 API Gateway Framework의 사용을 권장하지 않는지 살펴보도록 하겠다. Monolithic Architecture : 기본으로 돌아가 모놀리식 아키텍처를 살펴보자. 일반적인 스프링 웹 MVC 기반 Monolithic 애플리케이션은 이 그림과 비슷한 형태를 띤다.애플리케이션은 크게 두 가지 계층으로 나눈다. 하나는 실제 비즈니스 로직을 처리하는 비즈니스 계층이다. 여기서 저장소에 접근하고 도메인 로직을 수행한다. 서비스 계층이라고 부르기도 한다. 그리고 프레젠테이션 계층이 외부 클라이언트의 요청을 받아서 비즈니스 계층을 호출하고 그 결과를 클라이언트가 사용하기 적합하게 변환해서 응답하게 된다. 프레젠테이션 계층에는 필터, 인터셉터가 있다. 비즈니스 계층에는 AOP가 있다. 이 둘은 역할이 비슷하다. 필터 혹은 인터셉터는 여러 컨트롤러의 공통 적용된 로직을 실행하고 컨트롤러 코드를 호출한다. AOP도 마찬가지인데 AOP는 보통 서비스나 그 외의 비즈니스 로직이 공통으로 적용할 로직을 수행한다. 둘이 하는 일을 횡단 관심사 처리라고 한다. 영어로는 Cross Cutting Concerns라고 한다. 프레젠테이션 계층에는 Facade라는 것도 보이는데 클라이언트의 요청이 하나의 비즈니스 로직이 아니라 여러 비즈니스 로직의 조합일 경우 작성한다. 이 Facade라는 용어를 모르고있더라도 혹은 별도의 클래스를 분리하지 않고 컨트롤러에 직접 Facade 코드를 작성하더라도 어찌됐든 여러분은 자기도 모르는 사이에 Facade를 작성하고 있을 것이다.계층형 아키텍처 혹은 계층형이 아니더라도 소프트웨어 개발에서 가장 기본 중 하나는 하위 모듈이 상위 모듈을 호출할 수 없고 하위 계층이 상위 계층을 호출할 수 없다는 것이다. 여기서 하위 계층은 비즈니스 계층이고 상위 계층은 프레젠테이션 게층이다. 따라서 비즈니스 계층은 절대로 프레젠테이션 계층을 호출하면 안된다. 쉽게 말해 서비스 클래스는 컨트롤러 클래스를 호출해서는 안된다. MSA를 이런 계층형 아키텍처로 매핑하면 어떤 모양이 될까? 우리가 보통 마이크로서비스라고 부르는 그것들은 비즈니스 계층에 속한다. 그래서 마이크로 서비스 아키텍처라고 부르는 것 같다. 그리고 API Gateway Framework가 아닌 API Gateway는 프레젠테이션 계층에 속하게 된다. 몇몇 분들은 아닌데 내가 만든 마이크로서비스는 프레젠테이션 계층인데라고 하는 사람이 있을 수 있는데 이것은 조금 이따가 알아보도록 하곘다. 혹시 이 그림을 보는 순간 어째서 API Gateway Framework로 라우팅하는 것이 문제가 되는지 곧바로 파악이 되는가?여기서 API Gateway를 API Gateway Framework를 통한 라우팅 기반으로 변경하면 어떤 형태가 되는 걸까? 바로 이렇게 컨트롤러 코드와 Facade 코드가 존재하지 않고 횡단 관심사만 처리하는 필터/인터셉터만 남은 프레젠테이션 계층이 되어버리는 것이다. 컨트롤러와 Facade 코드가 존재하지 않는 프레젠테이션 계층은 아주 심각한 문제들을 일으킨다. 2차 결론 API Gateway Framework를 통해 단순 Gateway Routing Pattern으로 내부 서비스 API를 외부로 노출시키는 경우에는 필터/인터셉터 역할만 하기 때문에 컨트롤러/Facade가 존재하지 않는 상태가 되어 많은 문제를 일으킴 프레젠테이션 계층 구성은 꼭 컨트롤러와 퍼사드 코드를 직접 개발자가 작성하는 API Gateway Pattern을 따라야 함. 다시 말하면 비즈니스 계층 마이크로서비스 API들을 프레젠테이션 계층으로 변환하는 용도로 API Gateway Framework를 사용해서는 안된다. 반대로 Filter/Interceptor/AOP 같은 횡단 관심사를 처리하는 데는 API Gateway Framework를 사용할 수 있다는 의미 - 추천하지는 않음 필자는 사실 API Gateway Framework를 사용할 일은 정말 드물다고 생각한다. 그것이 비록 횡단 관심사 처리 일지라도 말이다.사실상 이글의 핵심은 여기서 다 나왔다고 봐도 된다. 그래도 도대체 왜 컨트롤러와 퍼사드 코드를 직접 작성하지 않으면 문제가 되는지 어째서 횡단 관심사 처리에도 API Gateway Framework를 사용하지 말라는 것인지 궁금할 것이다. API Gateway Framework Gateway Routing 방식의 문제점 보안 취약성 Client 개발자 혹은 특정 Micro Service에게 Business 로직 전가 Business 계층과 Client 간의 강결합과 그로인한 보안 취약성 성능 저하 내부 Service들 간의 protocol 자유도 하락 계층형 아키텍처의 일반적인 원칙 위반 사례 횡단 관심사 처리 관점에서도 사용하지 않아야 하는 이유 Single Point Of Failure 성능 저하 관리 부담 문서화 AGF문제점 : 보안 취약성 인증 : 사용자의 로그인 여부 - 누구세요? 권한 : 당신은 이것을 사용할 수 있고 저것은 사용할 수 없습니다. “이것” 이란? 특정 API 해당 API가 처리하는 객체 API Gateway Framework를 사용하면 보안이 좋아진다는 착각이 많이 있다. 하지만 사실이 아니다.인증을 한다는 것과 특정 API 호출 권한을 제어한다는 것인 보안을 강화하는 것은 아니다. 그것은 프레젠테이션 계층이 당연히 해야할 일이다. 인증은 사용자의 로그인 여부를 체크해서 누구인지 확인한다. 여기서 문제는 권한에 대해 많이들 오해한다는 점이다.권한에는 특정 api를 호출해도 된다라는게 있다. 그리고 그 특정 api가 처리하는 객체에 대한 접근 가능 여부도 권한에 들어간다.Gateway Routing 방식으로 사용하면 위의 빨간색으로 된 객체 관련 권한 처리를 누락하게 된다. GET /users/{userId} : 허용 PUT /users/{userId} : 허용 POST /users : 금지 Login 사용자 1번이라면 GET /users/1 : 허용 PUT /users/1 : 허용 POST /users : 금지 GET /users/2 : 허용 GET /users/3 : 허용 GET /users/... : 허용 PUT /users/2 : 허용 PUT /users/3 : 허용 PUT /users/... : 허용 단순한 예로 로그인 사용자에게 API Gateway Framework에서 GET /users/id와 PUT은 허용하고 POST는 금지했다고 해보자.보안이 강화된 것처럼 보인다. 하지만 로그인 사용자 1번은 해당 api에서 1번 사용자의 개체만을 다루어야 한다. 그런데 2번 3번 객체를 다루는 것은 막지 못하고 있다.권한을 지정할 때 http 메소드와 url 패턴이 일치하면 허용했지 패턴에 들어가 있는 값에 대해서는 확인하지 않았기 때문이다. 다른사용자의 데이터를 조회하는 것뿐만 아니라 마음대로 수정까지 가능하다. 심각한 보안 위협이다. 이 문제를 해결하는 방법은 직접 컨트롤러 코드를 작성하는 수밖에 없다.지금 예제는 간단하다. 로그인 유저 아이디는 세션 역할의 쿠키에 저장되어 있고 이를 프레임워크가 알고 있으니까 왠지 단순하게 유저 id값과 로그인 사용자의 유저 아이디를 비교해서 호출하지 못하게 필터를 구성하면 될 것 같다. 하지만 문제가 해결된 것은 아니다. 오더나 리뷰처럼 이와 같이 오더 id와의 관계를 알 수 없는 수많은 api 호출들이 존재하기 때문이다. 이런 경우에도 로그인 사용자가 명백히 소유한 주문이 아니면 조회나 수정이 불가능해야 한다. 그러면 아래처럼 모든 api 호출에 대해 소유 관계를 넣으면 일부 문제는 해소해준다. 특정 프레젠테이션 계층에서는 이렇게 하면 성능도 좋아진다.하지만 사실 아주 많은 경우에 저렇게 소유 관계만으로 객체를 조회할 수 없다. 멀리 갈 것도 없이 오더 마이크로 서비스 입장에서는 본인들이 가지고 있지도 않은 유저에 대한 정보를 API url에 넣는 것 자체가 부담일 것이고 일반적인 비즈니스 로직에서는 오더 아이디만으로 조회해야 하는 경우가 태반일텐데 그렇제 못해서 오는 불편도 상당하다. 그리고 그런 것을 넘어서 가장 중요하게 이 방식이 안 되는 이유가 있다. 많은 착각 중에 하나가 프레젠테이션 계층이 하나일 거라는 것이다. 대부분의 엔터프라이즈 애플리케이션은 프레젠테이션 계층이 여러 개이다. 요즘 많이 회자되고 있는 어떤 아키텍처가 떠오를 것이다. Admin은 모든 사용자의 주문 정보에 접근이 가능해야 한다. 가가 업주는 본인의 가게에서 일어난 주문에 대해서는 그 사용자가 누구이든 조회할 수 있어야 한다. 필자는 Batch Job이나 이벤트 컨슈머도 프레젠테이션 계층으로 보는데 이것들은 시스템으로서 무제한의 데이터 접근권을 가진 프레젠테이션 계층에 속한다. 또한 타사의 서버 애플리케이션 호출 지점도 프레젠테이션 계층이다. 이 모든 경우를 미세하게 API Gateway Framework로 제어하는 것은 필자가 보기엔 불가능하다. 따라서 비즈니스 계층은 프레젠테이션 계층의 접근자가 누구인지 구분하지 않고 요청을 받아주고 프레젠테이션 계층에서 각자 코딩을 통해 권한을 미세하게 조정해야 한다. 아마도 비즈니스 계층 자체에 저 모든 처리르 넣고 API Gateway Framework를 붙이려는 욕구가 생길 수 있는데 비즈니스 계층 서비스가 100개이고 처음에는 두 개 정도의 프리젠테이션 계층으로 시작했는데 나중에 두 개의 또 다른 프리젠테이션 계층이 더 추가됐다고 상상해 보길 바란다… 당장 100개는 아니더라도 수많은 마이크로서비스에 자기들은 관심도 없는 프리젠테이션 계층의 권한 로직을 추가해야 하는 부담을 지게된다. 과연 또 다른 프리젠테이션 계층이 추가될 것 같은가? 지금 당장 생각나는 것은 배달원 프리젠테이션 계층 고객센터 직원 프리젠테이션 계층 고객사 직원 계층이 생각난다. 또 얼마나 많이 추가될지 상상도 안된다. 애초에 저런 권한 로직을 비즈니스 계층에 넣는 것 자체가 비즈니스 계층과 프리젠테이션 계층을 모호하게 만드는 잘못된 행동으로 보인다. AGF문제점 : 보안 취약성 IDOR IDOR : Insecure Direct Object Reference 그 중에서도 “수평적 권한상승” 개발자가 꼭 알아야 할 애플리케이션 보안 : 입문부터 놓치면 안될 트렌드까지?! (8월 우아한테크세미나 - 권현준) 수평적 권한상승쉽게 말해, 동일한 권한을 가진 다른 사용자의 객체에 접근할 수 있을때를 수평적 권한상승이라고 함 수직적 권한상승본인이 지닌 권한을 넘어서는 기능을 수행할 수 있을때를 수직적 권한상승이라 함 필자는 이 취약점이 최근에 급부상한 이유가 마이크로 서비스 아키텍처가 급부상하면서 함께 증가한 API Gateway Framework 사용 때문이라고 추측하고 있다.방금은 수평적 권한 획득의 예였고 잘못 설계된 마이크로 서비스 아키텍처에는 수직적 권한 상승 문제도 마찬가지로 발생한다. 그 예는 추후에 포스팅 하도록 하겠다. AGF 문제점 : 잘못된 로직 구현 전가정말로 어떻게 해서 절대로 보안 문제가 없는 경우라고 가정해도 단순 라우팅을 하게되면 Facade가 필요한 경우에 엉뚱한 곳에 로직을 전가하게 된다. 클라이언트(Mobile App, Web Browser)가 직접 스스로 비즈니스 계층의 로직을 다양하게 조합해서 호출하게 됨 혹은 반대로 Order Service가 엉뚱하게 Product Service를 호출해서 클라이언트에 필요한 정보를 조합해 내려주는 것처럼 본인의 비즈니스가 아닌 것에 대해 구현하고 의존하게 됨 이게 뭐가 문제일까? AFG 문제점 : Client에 구현 전가 일단 클라이언트 개발자가 UI 처리에 집중하지 못하게 됨 비즈니스 로직은 서버개발자와 클라이언트 개발자 둘 다 밀도 높게 파악해야 함 비즈니스 로직 하나하나는 무슨 수르 ㄹ써서 든 보안 요구를 충족할지라도 그것들을 자유롭게 조합할 권한을 해커에게 내준 것이라서 그 조합이 어떤 문제를 일으킬지 확신할 수 없음 버그가 존재할 경우 Mobile App은 각종 앱스토어 인증 절차에 걸리는 시간과 사용자들이 자발적으로 앱을 업데이트 할 때 까지의 시간동안 버그 해결이 불가능해짐 클라이언트 개발자에게 전가하기가 불가능 경우가 있거나 혹은 위에서 말한 문제들 떄문에 이 Facade 구현을 서버측에 전가하면 퍼사드의 역할을 결국은 누가 됐든 특정 마이크로서비스에서 처리해야한다. 그때도 문제가 심각해진다. AFG 문제점 : 각 마이크로서비스에 전가 특정 서비스 개발자들은 알지도 못하는 로직 구현 책임을 맡게되고 해당 서비스는 본 서비스가 아닌 다른 역할을 맡음으로써 불필요하게 복잡도 증가와 타 서비스들과의 결합도 증가 본 역할도 아닌 타 API Network 호출 증가로 인해 정작 본 서비스의 처리에 장애 유발 요인 증가 특정 프리젠테이션 계층 API Gateway에서 문제가 생겼다면 해당 API Gateway만 문제가 되지만 비즈니스 계층 Microservice에서 장애가 발생하면 이를 호출하는 다른 모든 계층에서 문제가 될 수 있음 - 장애 여파 훨씬 커지게 됨 이런식으로 전혀 엉뚱한 역할을 전가받은 서비스의 개발자들의 시선은 복잡해 개발만족도는 갈수록 떨어지게 된다. 그리고 애초에 마이크로 서비스로 나눈 이유 자체에도 반하게 되는 결정이 된다. 마이크로 서비스 패턴 책을 보면 API Gateway Pattern을 구현하는 방법을 스프링 웹 MVC, Webflux처럼 직접 웹플럭스처럼 직접 웹프레임워크로 개발하기와 스프링클라우드 게이트웨이처럼 라우팅과 직접 구현을 동시에 지원하는 API Gateway Framework를 사용하는 방법을 추천하고 있다. 실제 예제로도 스프링클라우드 게이트웨이를 사용한다. 즉 일부는 단순 라우팅, 나머지는 직접 구현하는 것을 섞어서 하고 있다. 하지만 필자는 앞서 API Gateway Framework를 통해 비록 API Gateway Pattern이 구현가능하고 일부 단순한 api는 간편하게 라우팅 처리를 할 수 있다 하더라도 그렇게 해서는 안된다고 말했다.이제 그 이유에 대해 다뤄보겠다. AFG 문제점 : Client / Business 계층간 강결합이제부터 Business 계층 개발자는 항상 Client의 변경사항을 주시해야 한다. 1234567{ &quot;userId&quot; : 1, &quot;name&quot; : &quot;강백호&quot;, &quot;address&quot; : &quot;서울시 동작구 ...&quot;, &quot;phone&quot; : &quot;010-1111-2222&quot;, &quot;ssn&quot; : &quot;221212-1234567&quot;} 배달원 앱용 Presentation AFG에서 User Microservice의 /users/{userId} API를 단순 라우팅 했고 그 응답 객체도 단순 반환했는데 나중에 User Microservice의 개발자가 별 생각없이 /users/{userId}의 응답에 고객의 전화번호와 주민번호를 추가한다면? -&gt; 배달원 앱을 통해 고객의 전화번호와 주민등록번호가 그대로 노출 이를 막으려면 User Microservice 측에서는 모든 Presentation 계층별로 서로 다른 사용자 정보 조회 API를 만드는 방법 밖에 없게 됨 Business 계층이 Presentation 계층에 의해서 계속 변화하게 됨 단순 요청/응답 라우팅은 어떠한 경우에도 사용하지 말고 항상 응답 내용을 프리젠테이션 게층에서 Client에 필요한 것만 정제해서 내보내기 배달앱에서는 어차피 새로 추가된 데이터를 보여주지 않을테니까 괜찮다는 식의 안심은 금물인 것은 당연히 알 것이다. 보다시피 게이트웨이 라우팅 패턴이 스며드는 순간 비즈니스 계층은 프리젠테이션 계층의 속박에서 벗어날 길이 없게 된다. AGF 문제점 : client 성능 저하Client는 느리다 여러 api 호출을 조합해야 하므로 network 호출이 여러번 일어나게 되고 api 호출 결과가 불필요한 데이터까지 포함하여 과하게 크거나 필요한 데이터가 없어서 추가 api 호출을 하게 됨 API Gateway Pattern을 Non Blocking IO / Reactive 하게 구현하여 성능 향상 추천 (Spring WebFlux 등) 이 그림은 넷플릭스 블로그에 나온 API 게이트웨이 예이다. 여기서 넷플릭스 api가 API GW라고 보면 되고 API GW의 서버가 다른 비즈니스 계층 서비스를 패러럴하게 호출하고 있다. 클라이언트는 단 한 번만 호출하고 있다. GraphQL로 이 문제를 해소 가능하지만, 비즈니스 계층 Micfoservice에 GraphQL을 구축하고 이를 그대로 프리젠테이션 계층까지 노출하면 앞서 말했던 보안 문제들이 그대로 발생함. GraphQL도 API Gateway Pattern에 따라 프리젠테이션 계층 서버 애플리케이션으로 적절한 보안 처리를 해서 별도 구축 AGF 문제점 : 내부 서비스의 프로토콜 제약 API Gateway Framework로 프리젠테이션 계층을 구현한다는 것은 곧 내부 비즈니스 계층 서비스가 모두 HTTP API(REST?)로 고정된다는 의미 내부 비즈니스 계층을 한 번 밖으로 노출하면 프로토콜 변경은 어려워짐 내부적으로 성능 향상과 다양한 요구 사항만족을 위해 gRPC, Message Queue, HTTP API, 기타 등등을 사용할 수 있는 자유도를 포기하지 말기 (출처 : https://microservices.io/patterns/apigateway.html) 계층형 아키텍처의 일반적인 원칙 무시 사례 계층형 아키텍처의 일반적인 원칙을 무시한 사례 중 마이크로 서비스를 나눌 때 비즈니스 계층으로 나눈게 아니라 애초에 프리젠테이션 계층으로 만들어 버리는 사례이다.예를 들어 주문 마이크로 서비스를 만드는데 애초에 앱클라이언트의 요청을 받게 만들어 버리는 것이다.인증도 붙이고 권한처리도 한다. 그리고 내부적으로 비즈니스 계층에서 비즈니스 로직을 처리하고 저장소 처리까지 합한다.사실 여기는까지는 괜찮다. 다만 많은 경우에 바로 이런 도식과 같은 형태를 띄게 버리는 것이다. 프리젠테이션 계층 api가 다른 프리젠테이션 계층 api를 호출하고 비즈니스 계층 api가 없다보니 프리젠테이션은 계층의 비즈니스 처리용 api를 만들어서 호출해버리는 것이다.이는 모놀리식 아키텍처에서 컨트롤러가 컨트롤러를 호출하고 서비스가 컨트롤러를 호출하는 형국이다.비즈니스 계층에 속하는 상품 서비스가 주문 프리젠테이션 계층을 호출하려면 필연적으로 주문 프리젠테이션 계층에서 인증을 풀어주거나 단순화한 형태의 api를 제공해줘야 한다.이는 IDOR의 수직적 권한 상승에 취약해지는 결과를 낳는다.Private 망의 상품 서비스가 Public 망의 api를 호출하려면 외부망으로 접속이 연결되면서 연결 지연이 발생한다.웹 방화벽에 의해 Private 망의 모든 호출이 단일 ip에서 오는 DDOS 공격으로 간주되어 차단될 수도 있다. 수직적 권한 상승에 취약해진다는것은 잘못하면 해커가 무제한의 권한을 가진 api를 알아낼 수 있다는 의미이다.계층형 아키텍처의 기본을 어김으로써 나타나는 문제들을 차치하고서라도 보안 문제와 네트워크 인프라적으로도 심각한 문제가 발생하는 것을 볼 수 있다. 하나의 메소드, 하나의 역할만 하듯 하나의 마이크로 서비스도 하나의 계층 역할만 해야한다.의존 관계는 항상 프리젠테이션 계층에서 비즈니스 계층으로 흐르거나 비즈니스 계층간에만 이루어져야 한다.비즈니스 계층간의 호출에 있어서도 Circular 호출이 일어나서는 안 됨 (A -&gt; B -&gt; A) 마이크로 서비스로 분할된 애플리케이션들간의 호출에 있어서도 우리가 보편적으로 코드를 짤 때 지켜야 하는 규칙을 철저히 지켜야한다. AGF를 횡단 관심사에서도 사용하지 말기진짜 진짜 필요한게 아니라면… Single Point Of Failure(단일 장애 지점)가 된다. 불필요하게 서버 관리 부담이 늘어나고 비용도 증가 Client가 하나의 end point만 바라본다고 해서 좋아질 것은 크게 없음 API Gateway pattern을 구현하게 되면 실제 client 개발자가 바라보는 API endpoint 개수는 현저하게 줄어들게 됨. ktNaviService.메소드수천개() : 정말 좋은가? 진짜 중요한 것은 필요한 API를 빠르게 찾아볼 수 있게 문서들을 잘 모아두기 API Gateway Framework를 사용할 만한 경우사실 대부분의 규모의 서비스에서 API Gateway Framework가 별로 필요 없다. 출처 : Announcing Zuul: Edge Service in the Cloud 이 그림은 넷플릭스가 2014년 Zuul API Gateway Framework를 발표한 내용을 가져온것이다.넷플릭스는 여기서 프리젠테이션 계층 API Gateway에 해당하는 부분을 클라이언트 어댑터라고 부르고 있으며 코드를 직접 짜서 구현하고 있다.넷플릭스는 Zuul을 비즈니스 계층 api를 프리젠테이션 계층으로 노출하는 용도로 사용한 것이 아니라 명백하게 각각의 프리젠테이션 계층에 대해서 횡단 관심사 처리를 위해 사용한 것이다.여기서 다양한 횡단 관심사를 보여주는데 흔히 생각하듯 인증 정적응답 처리 Canary testing, Stress testing, 동적 라우티을 위해서 사용하고 있다.만들 Micro Service에서 프리젠테이션 계층이 저정도 용도가 필요하다면 사용해도 좋을 것 같다. 참고로 넷플릭스는 위의 도식에 해당하는 아키텍처를 버린듯한 내용의 포스팅이 최근에 올라와있다.전반적으로 프리젠테이션 계층을 GraphQL로 전환하고 있는 것으로 보인다.자세한 것은 아래 참조 문서를 참고바란다. How Netflix Scales its API with GraphQL Federation (Part 1) 비즈니스 계층 횡단 관심사 처리 위 그림은 비즈니스 계층에 속하는 마이크로 서비스 API 서버에 API Gateway Framework를 적용한 경우로 이를 호출하는 프리젠테이션 계층 API Gateway들이 꼭 필요로 하는 API들만을 호출 가능하게 제약하고, 호출에 대한 로그를 남기는 API Gateway Framework를 두었다. 사실 실제로 이렇게까지 사용하는 경우는 드물다. 이것을 다시 모놀리식으로 비유하자면 서비스 코드에다가 AOP로, 어느 컨트롤러는 이 서비스 클래스의 메소드를 호출할 수 있고 어느 클래스는 호출 못하고 이런 것을 설정을 안할 것이다. 그래서 이런 API Gateway Framework도 별로 필요하지 않다고 말하는 것이다. 하지만 그래도 특정 프리젠테이션 계층 API Gateway에 대해 늘 철저히 보안 처리를 하고 싶다면 사용해도 좋을 것 같다. 여기서도 보다시피 절대로 API Gateay Framework는 비즈니스 계층을 프리젠테이션 계층으로 격상하는 용도로 사용한 것이 아니다. AOP처럼 횡단 관심사 처리를 위해 사용되었을 뿐이다. 여기서 내가 생각하거나 혹은 이미 마이크로서비스 패턴 책 등에 나와 있는 API Gateway Framework의 문제점들을 다양하게 제시했다. 마무리로 첨언하자면 여기서 다룬 모든 문제들이 모두 해결 가능하거나, 그런 문제들에도 불구하고 이를 사용해서 얻는 이득이 훨씬 큰 경우가 있다면 당연히 API Gateway Framework를 사용해도 괜찮을 것이다.","link":"/2024/04/14/API-Gateway/"},{"title":"도쿄 여행을 위한 사전조사 1편","text":"도쿄는 런던, 뉴욕과 함께 세계 3대 도시에 속한다고들 한다.또한 일본의 정치, 경제, 사회, 문화 등 모든 면에서 일본을 대표하는 최대 규모의 도시이다.과거와 현재가 공존하는 도쿄에서는 천년의 역사를 지닌 사찰부터 최신식 고층 빌딩까지 다채롭게 펼쳐지는 건축물들을 보는 즐거움이 있고 세련된 마천루 사이사이를 거닐거나 트렌디한 감성의 캐주얼한 골목에서 최신 유행 제품들을 둘러보는 재미도 있다고 한다. 매년 미쉐린 가이드 세스토랑이 가장 많이 선정되는 도시 중 하나로 저렴하면서도 맛있는 식당, 대를 이어 영업하는 노포부터 최고급 요리 전문점까지 아무리 까다로운 미식가라도 만족시킬 수 있는 다양한 옵션들이 존재하는 곳이다. 어쩌고 저쩌고 암튼 도쿄 여행에 대한 사전 조사를 시작하겠다. 첫번째로 도쿄역, 긴자, 신바시, 롯폰기에 대해 알아보겠다. 출처 : https://www.youtube.com/watch?v=ZzqN8lkNQ-I 도쿄 들어가기인천 - 나리타 소요시간은 약 2시간 20분이다.나리타에서 도쿄 도심까지는 최소 1시간이상 걸린다. 나리타에서는 케이세이 나리타 스카이 엑세스, 나리타 익스프레스, 스카이 라이너 등의 옵션이 있다.또한 최근 저가 리무진 버스도 노선을 확대하고 있다. 시내 교통시내 교통의 경우 이동하는 지역이 많은 여행계획을 가지고 있는 경우 도쿄 메트로와 은행잎 모양의 토에이선 등 13개 노선, 250여개 정류장을 무제한 이용할 수 있는 도쿄 메트로 패스 24/48/72 시간권이 가장 좋다고 한다.만약 핵심지역 몇 군데만 다닐것이다 하는 경우 도쿄, 시부야, 신주쿠 이케부쿠로, 아키하바라 등 핵심 관광명소를 지나는 JR 야카노테센을 이용하면 된다. JR 메트로 패스로는 이용이 불가한 점을 유념해야 한다. 마루노우치 니혼바시일왕이 현재 거주하는 도쿄 고쿄, 100년이 넘는 역사를 지닌 도쿄역이 위치한 마루노우치 지역일대는 관광지로서 엄청난 볼거리가 있다기보다는 도쿄의 심장과 같은 곳을 의미가 크다고 한다. 도쿄역을 중심으로 서쪽 마루노우치 방향 출구와 동쪽 야에스 방향 출구가 나뉘어 진다.마루토우치 방향으로 가면 고쿄(황거)를, 야에스 방향으로 가면 백화점거리인 주오도리와 니혼바시를 만날 수 있다.모두 도보로 이동할 수 있는 거리이다. 도쿄역 도쿄역 서편의 건물들 사이 마루노우치 광장은 생각보다 세련되고 예쁜 뷰를 자랑한다고 한다. 도쿄역 옆에 위치한 킷테 쇼핑몰 6층에서 조망 가능 중앙 우체국 건물을 리모델링하여 조성한 복합 쇼핑몰 구 우체국장실인 4층 레터룸 6층 킷테 가든 독특한 스팟 나카도오리 에비뉴마루노우치 광장에서 마루노우치 빌딩을 지나면 나카도오리 에비뉴가 나온다. 이곳은 마루노우치의 가로수길로 거리 양 옆의 초록 가로수길들과 다양한 브랜드 숍, 레스토랑들이 모여있다고 한다.천천히 산책하듯 걸으며 쇼핑하고 거리에서 커피 한 잔 즐기기 좋을것 같다. 고쿄내부 입장은 무료지만 반드시 투어를 통해 들어가야하고, 입장 인원 제한이 있다.그렇기에 인터넷으로 미리 예약하고 가는것이 권장된다. 일제강점기 우리나라 의사들의 의거지로도 유명한 니주바시 다리. 니주바시 다리와 이중교는 같은곳이다. 니혼바시 도쿄역에서 야에스 방면으로 나가면 다이마루 백화점을 시작으로 과거 에도시대에 상업의 중심지로 번창했던 니혼바시지역이 나온다. 이곳의 지명이 유래된 다리, 니혼바시를 기점으로 남북으로 뻗은거리를 주오거리라고 부른다.다이마루, 다카시마야, 미츠코시처럼 전통있는 박화점들이 줄지어 있다. 미츠코시 백화점은 외관도 멋지고 내부도 멋지다고 한다. 긴자 : 도쿄의 세련미 상징여전히 명실상부 도쿄 최고의 부촌 가운데 하나이다. 간자역 중심으로 펼쳐진 다양한 레스토랑, 카페, 상점들을 비롯해, 시계탑으로 유명한 와코백화점, 긴자식스, 도큐 플라자 긴자, 도쿄 미드타운 히비야 등이 볼만하다. 와코 백화점2차 세계대전 공습에도 살아남은 건물로, 역사가 느껴지는 우아함을 간직한 곳.이곳을 기점으로 펼쳐지는 사거리에서, 화려한 긴자 거리 전체를 조망할 수도 있다. 길을 건너 두 블럭 정도 걸어가면 긴자에서 가장 큰 쇼핑몰인 긴자식스가 있다. 긴자식스패션위주의 쇼핑몰답게 감각적인 내부 디자인이 돋보이는 건물이다. 인테리어 그 자체가 하나의 전시물을 감상하는 것처럼 느껴질정도로 보는 즐거움이 있고, 다양한 맛집과 함께 6층에는 츠타야 서점과 스타벅스가 있다. 도큐 플라자 긴자 긴자식스와 함께 긴자의 대표 쇼핑몰로 꼽히는 도큐 플라자 긴자는 컷팅유리로 이루어진 독특한 건물 외관을 지니고 있다. 내부는 알록달록한 차 매장(TWG)과 눈을 사로잡는 톡톡 튀는 패션 아이템들을 만나볼 수 있다. 도쿄 미드타운 히비야 2018년에 지어진 비교적 최신 쇼핑몰이지만생각보다 규모는 그리 크지 않다. 패션보단 맛집이 많은 것으로 유명하다.날씨가 좋을때는 쇼핑몰 앞 광장에서 문화 행사를 진행한다고 한다. 신바시신바시 부근 도심지역은 신주쿠, 시부야, 롯폰기에 등에 비해서는 여행자들 사이에서 인지도가 떨어지는 편이라고한다. 관광지로서 북적이는 곳이 아닌, 현지인들로 붐비는 곳에 가보고 싶다면 이곳을 추천한다. 시오도메 시오도메는 과거 화물역 부지를 재개발한 곳으로 현재 유수의 대기업과 미디어 방송사 등이 다수 입주해있는 활기찬 직장인 거리이다. 빌딩들이 밀집해있는 지역을 시오도메 시오사이트라고 부르는데, 사이트 내 빌딩을 연결하는 고가 육교를 걷다보면 뜻밖의 아름다운 전망을 감상할수도 있다. 미야자키 하야오의 시계 시오도메의 상징과도 같은 닛폰 텔레비전 건물의 커다란 시계인 미야자키 하야오의 시계도 볼만하다.세계에서 가장 큰 태엽 시계라고도 한다. 카렛타 시오도메근처의 카렛타 시오도메 쇼핑몰에서는 일본 내에서도 손꼽히는 일루미네이션을 볼 수 있고, 46층에는 무료 전망대가 있어서 야경을 감상하기에 좋다. 신바시시오도메와 인접한 신바시지역은 일본 현지인 샐러리맨들이 많은 지역이다. 신바시역에 내리면 서쪽 출구를 따라 1940년대 증기 기관차가 있는 신바시 만남의 장소, 니시구치 광장 거리가 있다. 광장에서 아래쪽으로 신바시 역사 주변, 교각 아래 다양한 식당과 이자카야 등이 줄지어 있는 거리가 있다. 이곳은 퇴근시간 이후 근처 직장인들이 회식이나 퇴근 후 스트레스를 풀며 한 잔할 장소로 많이 찾는다. 굴다리 아래 종기종기 모여있는 가게들이 독특한 풍경을 자아내는데, 일본인들의 일상을 엿볼 수 있는 일본스러운 밤거리 풍경을 볼 수 있다. 토라노몬 힐즈신바시에서 도보로 15분 거리에는 토라노몬 지역이 있는데, 대규모 복합시설인 토라노몬 힐즈가 있다. 토라노몬힐즈는 몰리타워를 중심으로 비즈니스 타워, 레지덴셜 타워, 스테이션 타워가 차례로 오픈하며 총 4개 타워가 있다. 도시속의 도시라는 지향점에 걸맞게 미래 도시에 와있는 듯한 느낌이 들게하는 곳이다. 여기서 남쪽으로 15분거리에는 도쿄타워가 있다. 도쿄타워도쿄 타워가 위치한 시바코엔의 탁 트인 잔디에서 타워를 볼 수 있다. 그 옆의 조죠d지 절을 배경으로 타워가 우뚝 선 모습도 멋지다. 오렌지색으로 반짝이는 타워와 야경이 어우러진 모습을 보려면 어두워진 후 전망대나 타워 근처로 이동하는게 좋지만, 낮 시간대에 시바공원, 조죠지 절과 함께 즐기는 것도 좋다. 롯폰기 아자부주반도쿄를 대표하는 번화가이자 긴자 못지않게 고급 상권가가 형성되어 있지만, 분위기를 비교하자면 좀 더 트렌디하고 힙한 느낌이 강한곳이다. 크게 롯폰기역 북쪽의 도쿄 미드타운 롯폰기, 남쪽의 롯폰기 힐스, 그리고 더 아래로 대사관들이 많아 한남동과 비슷한 분위기의 아자부주반이 있다. 롯폰기 힐스롯폰기는 지금의 롯폰기를 있게한 곳이다.20년 전 탄생했을 당시 기존에 보지 못했던 성격의 대규모 복합타운이었고, 그러다보니 등장 자체로 화제였다고 한다.쇼핑몰뿐만 아니라 호텔, 영화관, 방송국, 일반 주거지 등 무려 10여개의 건물이 모여 이루어져있고, 모리타워 전망대는 도쿄타워가 가장 아름답게 보이는 곳으로 도쿄 최고의 전망대 중 하나로 손꼽힌다. 롯폰기 힐즈의 성공으로 제2의 롯폰기 힐즈들이 연이어 생겨나게 되었는데, 그 중 하나가 롯폰기 랜드마크의 양대산맥인 도쿄 미드타운 롯폰기이다. 총 6개의 건물로 이루어진 미드타운은 호텔, 오피스, 미술관, 쇼핑센터, 주거공간 등이 부지의 40%를 차지하는 녹지와 어우러져 도심 한가운데 있으면서도 뭔가 힐링이 되는 휴식공간처럼 느껴진다. 출처 : https://m.blog.naver.com/ksm11015/222958660955 유리벽을 통해 자연 채광과 외부 조망이 가능한 가든테라스 식당가, 유리지붕과 나무로 이루어진 중앙광장 등이 대표적이고, 대나무숲으로 자연 친화적이고 따뜻한 느낌을 주는 인기 쇼핑몰 갤러리아와 산토리 미술관도 둘러보면 좋다. 이 롯폰기 일대에는 고급 맛집에서부터 전통있는 카페, 기념품 전문점, 캐주얼한 선술집, 그리고 가볍게 즐기는 베이커리나 브런치 식당에 이르기까지 다양한 종류의 가게들이 있으니 여유가 있다면 천천히 돌아보아도 좋다. 아자부주반아자부주반은 롯폰기와 도쿄타워 사이의 한적한 거리이자 고급주택가로 주변에 대사관들도 많아 외교관, 외국인들에게 인기 있는 지역이기도 하다. 롯폰기와 인접해있지만 화려함보다는 조용한 분위기이고 에도시대부터 번창해온 아자부주반 상점가를 비롯해 전통적인 상점, 국제적이며 고급스러운 카페, 레스토랑등이 구석구석 숨어있다. 사라시나호라이 - 냉소바 위치","link":"/2024/05/05/Tokyo-Travel-00/"},{"title":"GPU 서버에 도커 이미지로 JupyterLab 배포하기","text":"GPU서버에서 딥러닝모델 학습을 위해 JupyterLab을 도커파일로 만들어 실행하는 방법을 정리해보겠다. JupyterLab 이미지 파일 만들기Dockerfile 1234567891011121314# 사용할 기본 이미지FROM python:3.8-slim # 작업 디렉토리 설정WORKDIR /app # 필요한 패키지와 라이브러리 설치RUN pip install --no-cache-dir jupyterlab numpy pandas matplotlib scipy # 포트 8888 열기EXPOSE 8888 # Jupyter Lab 실행 명령CMD [&quot;jupyter&quot;, &quot;lab&quot;, &quot;--ip=0.0.0.0&quot;, &quot;--port=8888&quot;, &quot;--allow-root&quot;, &quot;--NotebookApp.token=''&quot;, &quot;--NotebookApp.password=''&quot;] python:3.8-slim 이미지를 기반으로 사용. /app 디렉토리를 작업 공간으로 설정. pip를 이용해 jupyterlab을 설치. 컨테이너의 8888번 포트를 열어 외부에서 접근할 수 있게 함. Jupyter Lab을 루트 권한으로 실행하며, 모든 IP에서 접근을 허용하고, 보안 토큰과 비밀번호를 비활성화. 도커파일 버전2 123456789101112131415161718192021222324252627282930# CUDA와 cuDNN이 포함된 TensorFlow 기반 이미지 사용FROM tensorflow/tensorflow:latest-gpu # 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y \\ wget \\ bzip2 \\ ca-certificates \\ libglib2.0-0 \\ libxext6 \\ libsm6 \\ libxrender1 \\ git \\ mercurial \\ subversion # Miniconda 설치하여 Python 환경 관리RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; \\ /bin/bash ~/miniconda.sh -b -p /opt/conda &amp;&amp; \\ rm ~/miniconda.sh ENV PATH /opt/conda/bin:$PATH # Jupyter Notebook 및 주요 라이브러리 설치RUN conda install -y jupyter notebook matplotlib scipy scikit-learn pandas &amp;&amp; \\ conda clean -ya # Jupyter Notebook 설정EXPOSE 8888CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--notebook-dir=/notebooks&quot;, &quot;--ip='*'&quot;, &quot;--port=8888&quot;, &quot;--no-browser&quot;, &quot;--allow-root&quot;] Jupyter notebook 이미지 Docker로 빌드 시 ‘pip.conf’ 파일을 포함시키기pip.conf 파일은 Python 패키지를 설치할 때 ‘pip’의 동작을 사용자 정의하기 위해 사용한다. 1. ‘pip.conf’ 파일 생성먼저, 로컬 시스템에 ‘pip.conf’ 파일을 생성한다. 파일 내용은 ‘pip’ 동작을 사용자화하기 위한 구성을 포함할 수 있다.에를 들어, 아래와 같이 설정할 수 있다. 123[global]trusted-host = pypi.orgindex-url = https://pypi.org/simple 이 설정은 ‘pip’이 PyPI의 메인 인덱스에서 패키지를 설치하도록 지시하고 SSL 인증서 검증 문제를 방지한다. 2. Dockerfile 작성Dockerfile에서 ‘pip.conf’ 파일을 적절한 위치에 복사하는 단계를 포함한다. Python 이미지에서는 사용자 레벨의 설정을 적용하기 위해 ‘~/.pip/pip.conf’ 경로를 사용할 수 있다. 1234567891011121314151617# 기본 이미지 선택FROM jupyter/base-notebook# 작업 디렉토리 설정WORKDIR /app# pip.conf 파일 추가COPY pip.conf /etc/pip.conf# 필요한 Python 라이브러리 설치RUN pip install --no-cache-dir numpy pandas matplotlib# 포트 8888 열기EXPOSE 8888# Jupyter Lab 실행 명령CMD [&quot;start-notebook.sh&quot;] ‘jupyer/base-notebook’을 베이스 이미지로 사용한다. ‘pip.conf’ 파일을 Docker 이미지의 ‘/etc/pip.conf’로 복사한다. (시스템 전역 설정을 적용하기 위함. 필요에 따라 위치 변경 가능) 필요한 Python 패키지 설치 8888 포트를 열어 외부에서 접근가능하도록 함. Jupyter Lab을 시작. Jupyter notebook 이미지 Docker로 빌드 시 vim 텍스트 편집기 포함하기Jupyter Notebook 이미지를 빌드할 때 vi와 vim 텍스트 편집기를 포함하려면 Dockerfile에 해당 패키지 설치 명령을 추가해야 한다. 대부분의 Linux 배포판에서 vim은 기본적으로 vi를 포함하고 있으며, 더 많은 기능을 제공한다. 아래의 지침은 vim을 설치하는 방법을 포함하고 있으며, 이를 통해 vi의 기능도 사용할 수 있다. Dockerfile 작성다음은 jupyter/base-notebook 이미지를 기반으로 하여 vim을 설치하는 Dockerfile 예시이다. 12345678910111213141516171819# 기본 이미지FROM jupyter/base-notebook# 작업 디렉토리 설정WORKDIR /app# vim 설치USER rootRUN apt-get update &amp;&amp; \\ apt-get install -y vim# 필요한 Python 라이브러리 설치RUN pip install --no-cache-dir numpy pandas matplotlib# Jupyter Lab 실행 명령CMD [&quot;start-notebook.sh&quot;]# 포트 8888 열기EXPOSE 8888 설명 이미지 선택: jupyter/base-notebook은 Jupyter Lab을 실행할 수 있게 미리 구성된 기본 이미지이다. 작업 디렉토리 설정: /app 디렉토리를 작업 디렉토리로 설정한다. 사용자 변경: root 사용자로 변경하여 시스템 레벨의 변경을 수행할 수 있다. vim 설치: apt-get update를 실행하여 패키지 목록을 최신 상태로 유지하고, apt-get install -y vim을 사용하여 vim을 설치한다. Python 라이브러리 설치: numpy, pandas, matplotlib와 같은 주요 Python 라이브러리를 설치한다. 포트 열기: 외부에서 Jupyter Lab에 접근할 수 있도록 8888 포트를 연다. 실행 명령: 컨테이너가 시작될 때 start-notebook.sh 스크립트를 실행하여 Jupyter Lab을 시작한다. 추가 팁 라이브러리 버전 지정: 특정 버전의 라이브러리가 필요한 경우, pip install 명령에 버전을 명시할 수 있습니다. 예: pip install numpy==1.18.5 requirements.txt 사용: 많은 수의 라이브러리를 관리해야 하는 경우, requirements.txt 파일을 만들고 그 안에 필요한 라이브러리와 버전을 목록화할 수 있습니다. 그리고 Dockerfile에서 이 파일을 참조하여 모든 라이브러리를 한 번에 설치할 수 있습니다. 12COPY requirements.txt /app/RUN pip install --no-cache-dir -r requirements.txt Docker 이미지 빌드1docker build -t [이미지이름/태그] -f [도커파일이름].dockerfile . GPU서버에서 이미지 실행이제 빌드한 이미지를 실행하여 Jupyter Notebook 서버를 시작한다. GPU를 사용하도록 설정하려면 –gpus all 옵션을 추가한다. 1docker run --gpus all -p 8888:8888 -v /path/to/your/notebooks:/notebooks my-jupyter-gpu –gpus all: 컨테이너가 호스트의 모든 GPU를 사용하도록 설정한다. -p 8888:8888: 호스트와 컨테이너 간의 포트 매핑을 설정하여 Jupyter Notebook이 사용하는 포트 8888을 열어준다. -v /path/to/your/notebooks:/notebooks: 호스트 시스템의 디렉토리를 컨테이너 내부의 /notebooks 디렉토리에 마운트하여 노트북 파일들을 저장한다. 또는 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ sudo docker run --gpus all -it -d -p 59999:8888 -v /home/mydirectory/jupyter-notebook-test/:/home/work [이미지이름]:[태그] /bin/bash$ sudo docker exec -it 5f0cb0da5743 jupyter lab --allow-root --ip='0.0.0.0'[I 2024-05-09 04:56:38.457 ServerApp] jupyter_server_fileid | extension was successfully linked.[I 2024-05-09 04:56:38.461 ServerApp] jupyter_server_ydoc | extension was successfully linked.[I 2024-05-09 04:56:38.466 ServerApp] jupyterlab | extension was successfully linked.[I 2024-05-09 04:56:38.469 ServerApp] nbclassic | extension was successfully linked.[I 2024-05-09 04:56:38.642 ServerApp] notebook_shim | extension was successfully linked.[I 2024-05-09 04:56:38.655 ServerApp] notebook_shim | extension was successfully loaded.[I 2024-05-09 04:56:38.655 FileIdExtension] Configured File ID manager: ArbitraryFileIdManager[I 2024-05-09 04:56:38.655 FileIdExtension] ArbitraryFileIdManager : Configured root dir: /[I 2024-05-09 04:56:38.655 FileIdExtension] ArbitraryFileIdManager : Configured database path: /root/.local/share/jupyter/file_id_manager.db[I 2024-05-09 04:56:38.657 FileIdExtension] ArbitraryFileIdManager : Successfully connected to database file.[I 2024-05-09 04:56:38.657 FileIdExtension] ArbitraryFileIdManager : Creating File ID tables and indices with journal_mode = DELETE[I 2024-05-09 04:56:38.659 ServerApp] jupyter_server_fileid | extension was successfully loaded.[I 2024-05-09 04:56:38.660 ServerApp] jupyter_server_ydoc | extension was successfully loaded.[I 2024-05-09 04:56:38.664 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.7/site-packages/jupyterlab[I 2024-05-09 04:56:38.664 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab[I 2024-05-09 04:56:38.673 ServerApp] jupyterlab | extension was successfully loaded. _ _ _ _ | | | |_ __ __| |__ _| |_ ___ | |_| | '_ \\/ _` / _` | _/ -_) \\___/| .__/\\__,_\\__,_|\\__\\___| |_| Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions. https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html Please note that updating to Notebook 7 might break some of your extensions. [I 2024-05-09 04:56:38.682 ServerApp] nbclassic | extension was successfully loaded.[I 2024-05-09 04:56:38.683 ServerApp] Serving notebooks from local directory: /[I 2024-05-09 04:56:38.683 ServerApp] Jupyter Server 1.24.0 is running at:[I 2024-05-09 04:56:38.683 ServerApp] http://5f0cb0da5743:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:38.684 ServerApp] or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:38.684 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 2024-05-09 04:56:38.695 ServerApp] No web browser found: could not locate runnable browser.[C 2024-05-09 04:56:38.695 ServerApp] To access the server, open this file in a browser: file:///root/.local/share/jupyter/runtime/jpserver-15-open.html Or copy and paste one of these URLs: http://5f0cb0da5743:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd1009839e97149705a[I 2024-05-09 04:56:50.087 ServerApp] 302 GET / (221.148.246.64) 1.40ms[I 2024-05-09 04:56:50.116 LabApp] 302 GET /lab? (221.148.246.64) 1.56ms[I 2024-05-09 04:57:04.161 ServerApp] 302 POST /login?next=%2Flab%3F (221.148.246.64) 6.76ms[W 2024-05-09 04:57:05.041 ServerApp] 404 GET /api/me?1715230246656 (221.148.246.64) 9.13ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 04:57:06.285 LabApp] Could not determine jupyterlab build status without nodejs[I 2024-05-09 04:57:28.961 ServerApp] 302 GET / (221.148.246.64) 1.07ms[W 2024-05-09 04:57:29.202 ServerApp] 404 GET /api/me?1715230270950 (221.148.246.64) 4.24ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 04:57:30.501 LabApp] Could not determine jupyterlab build status without nodejs[I 2024-05-09 05:02:43.977 ServerApp] 302 GET / (221.148.246.64) 1.02ms[W 2024-05-09 05:02:44.158 ServerApp] 404 GET /api/me?1715230585905 (221.148.246.64) 3.15ms referer=http://172.31.47.96:59999/lab?[W 2024-05-09 05:02:45.460 LabApp] Could not determine jupyterlab build status without nodejs Jupyter Notebook 접속 [GPU 서버의 VirtualIP]:59999로 접속 기동시 나온 토큰 입력 JupyterLab 재기동하기 실행 중인 컨테이너 조회 1sudo docker ps 실행 중인 컨테이너 중 59999포트로 실행 중인 JupyterLab 컨테이너를 종료 1docker stop container_id_or_name JupyterLab 이미지 실행 1docker run --gpus all -it -d -p 59999:8888 -v /directory/jupyter-notebook-test/:/home/work [이미지이름]:[태그] /bin/bash 이미지 실행 되면 해당 컨테이너에서 jupyter lab 프로세스 실행 12345678910111213docker exec -it 5f0cb0da5743 jupyter lab --allow-root --ip='0.0.0.0'~~~5. 실행 된 jupyter lab의 컨테이너 터미널 로그에 표시된 토큰을 이용하여 JupyterLab 접속예시)~~~bsh[C 2024-05-09 04:56:38.695 ServerApp] To access the server, open this file in a browser: file:///root/.local/share/jupyter/runtime/jpserver-15-open.html Or copy and paste one of these URLs: http://5f0cb0da5743:8888/lab?token=12f6cfeaabec704c03ef65ac230580440f7b32145c592698 or http://127.0.0.1:8888/lab?token=ba547f29f926ca0d1c1f03938f07afd10809839e97149705a 사용할 GPU의 수 설정하기 모든 GPU 사용: 컨테이너에 시스템의 모든 GPU를 할당하려면 all 키워드를 사용한다. 1docker run --gpus all nvidia/cuda:10.0-base nvidia-smi 특정 수의 GPU 사용: 특정 수의 GPU만 할당하려면 count 키워드를 사용한다. 예를 들어, 사용 가능한 GPU 중 2개만 할당하려면 다음과 같이 한다. 1docker run --gpus count=2 nvidia/cuda:10.0-base nvidia-smi 특정 GPU 사용: 특정 GPU를 지정하려면 device 키워드와 함께 GPU의 ID를 사용한다. 예를 들어, 첫 번째와 세 번째 GPU만 사용하려면 다음과 같이 명령한다. 1docker run --gpus '&quot;device=0,2&quot;' nvidia/cuda:10.0-base nvidia-smi 주의 사항 –gpus 옵션을 사용하기 전에, NVIDIA GPU 드라이버와 NVIDIA Docker 플러그인(NVIDIA Container Toolkit)이 설치되어 있어야 한다. 이 옵션은 CUDA와 같은 GPU 가속 라이브러리를 사용하는 애플리케이션에 특히 유용하다. Docker가 GPU를 사용하도록 설정하기 위해서는 Docker 엔진 설정에서 default-runtime을 nvidia로 설정하는 추가 설정이 필요할 수 있다.","link":"/2024/05/09/Deploy-JupyterLab-on-Gpu/"},{"title":"도쿄 여행을 위해 준비해야 할 것들","text":"도쿄 여행을 위해 사전에 예약 및 준비해야할 것들을 정리해보고자 한다. 항공권도쿄로 들어갈 수 있는 공항은 하네다, 나리타 공항이 있다. 간단히 정리하자면 하네다는 나리타에 비해 가격이 더 비싸고, 대한항공, 아시아나만 취항돼 있지만, 도쿄 도심과 거리가 모노레일로 20~30분으로 가깝다. 하네다는 김포공항, 나리타는 인천공항 포지션이라고 생각하면 쉽다. 공항 주차본인이 탑승할 항공사의 터미널에 맞게 주차를 예약해야 한다. 인천공항 기준 터미널별 취항 항공사는 아래 링크에서 확인할 수 있다. https://www.airport.kr/ap/ko/svc/airlinesTerInfoList.do 탑승할 항공기 항공사의 터미널을 확인 후 아래 링크에서 인천국제공항 주차 예약을 하면 된다. 또는 그냥 장기주차장을 이용해도 된다. https://www.airport.kr/ap_lp/ko/tpt/parinf/parinft1/parinft1.do 예약 주차는 아래와 같다. https://parking.airport.kr/reserve/6110_01 예약 주차장의 경우 예약보증금 10,000원을 결제 해 놓아야 예약이 가능하다.참고로 1터미널 예약주차장은 심야시간(00:00 ~ 05:00) 셔틀버스가 운행되지 않아 도보이동이 필요(약30분)하다고 한다.예약주차장과 주차대행(발레파킹)은 다른곳이다. 참고사항 눈, 비 악천후인 경우 단기주차장은 지하주차장, 장기주차장은 주차타워를 이용하는 것을 추천 예약주차장은 실외밖에 없다. 예약을 했더라도 장기주차장 주차타워에 먼저 주차를 한 후 예약 취소하는것이 나을수도 있다. 발렛도 가능하며 별도 비용 2만원이다. 주차장 혼잡도도 조회 가능하다. https://www.airport.kr/ap/ko/tpt/getParkingPlaceInfoT1Long.do 입국 전일본 여행 가기 전 미리 해놓으면 좋은 것 중 하나가 비짓재팬웹 (Visit Japan web) 신청이다. Visit Japan web 이란?비짓재팬웹은 입국 절차 (입국 심사, 세관 신고) 및 면세 구입에 필요한 정보를 등록할 수 있는 웹 서비스이다.미리 비짓재팬 웹으로 신청해 놓으면 입국심사 때 더 빠르다. 비짓재팬웹 등록 방법비짓재팬웹 등록 준비물 : 여권, 일본에서 묵는 숙소 정보, 비짓재팬웹 아이디 (처음 진행 시, 회원 가입 필요) 비짓 재팬웹 회원가입 및 로그인 비짓재팬웹 이용자 등록 본인 정보 등록, 자녀나 동반 가족이 있는 경우라면 입력 한국에서 관광목적으로 가는 사람들이라면 일본 정부가 발행한 여권을 가지고 있습니까? / 일본에 거주하고 있으며, 재입국허가를 받고 일본에 입국합니까?에 없음을 택해주면 된다. 면세 QR 코드 이용은 면세점에서 물건 살 때 여권 보여주는 것 대신에 QR코드로 대체할 수 있도록 새롭게 생긴 것이라고 한다. 여권정보를 등록하고, 기본정보 등록해준다. 비짓재팬웹 VISA 정보 확인 일본 입국 VISA를 가지고 계십니까? 없음을 선택한다. 입국, 귀국 예정 등록 일본은 무비자로 90일까지 체류가 가능하므로, 단순 관광목적이라면 인용하지 않고 등록 진행 선택하면 된다. 입국 귀국 예정 (일본 도착 예정일 / 출발지 / 탑승기 관련 항공사명, 편명) 입력해준다. 입국심사 및 세관신고 사전 등록해놓으면 공항 내 입국심사 카운터에서 QR코드를 제시해서 조금 더 빠르게 가능하다. 도항목적 : 관광 예정 기간 작성 입국심사를 위한 질문들 -&gt; 해당하는 경우가 거의 없을테니 아니오 누르고 넘어가면 된다. 다 등록한 후 QR코드를 볼 수 있는데, 휴대폰에 캡쳐해놓으면 입국심사때 기다리지 않고 더 빨리 들어갈 수 있으니 미리 준비해놓으면 된다. 환전트레블월렛,,, 트레블로그,,,트레블월렛 트레블로그 비교 : https://m.blog.naver.com/daun1217/223058129426 통신ESIM은 기존 유심과 다르게 심을 뻬고 새로 교체해줄 필요가 없다.전달 받은 링크로 들어가 QR 코드를 스캔한 뒤 체크인을 완료하면 준비가 끝난다. 다만 사용 가능한 기종인지 확인해야한다.아이폰의 경우설정 - 셀룰러 - 셀룰러 요금제 추가 (또는 eSIM추가) 탭이 있으면 사용 가능한 단말이다. 또는 “EID”라고 하는 값의 조회를 통해 알아볼 수 있다.스마트폰 다이얼 패드에서 *#06#을 입력하면 조회 가능하다. ESIM 사용법1 : https://m.blog.naver.com/ccll28/223159883500ESIM 사용법2 : https://blog.naver.com/today_hobby/223090987444이지 이심 : https://esimeasy.co.kr/product/JP 출국 1~3일전 등록하면 된다. 나리타 -&gt; 도쿄 도심 N’EX 스카이라이너 도쿄역 긴자 1300엔 버스 (https://blog.naver.com/rladbrud8791/223428551502) 케이세이 본선 교통파스모 패스포트?https://m.blog.naver.com/skli0612/223065650300 기타동전지갑멀티어댑터썬크림 도쿄역 https://m.blog.naver.com/e_eunel/223428769019지유가오카 https://m.blog.naver.com/shinolog/223425924553","link":"/2024/05/12/Tokyo-Travel-01/"},{"title":"실시간 추론을 위해 모델을 SageMaker에 배포하기","text":"SageMaker에 Gemma 2b 모델을 추론 모델로 배포하고 사용하기 빠른 설정 SageMaker 콘솔에 접속 왼쪽 탐색 창 중 Admin configurations(관리 구성)에서 Domains(도메인)를 선택 Create domain을 선택 빠른 설정 선택 사용자 지정 설정 인증방법(AWS Identity Center 또는 IAM) 선택 S3 버킷 기입 gemma-1.1-2b-it를 이용하여https://huggingface.co/google/gemma-1.1-2b-it?sagemaker_deploy=true 123456789101112131415161718192021222324252627282930313233343536373839import jsonimport sagemakerimport boto3from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uritry: role = sagemaker.get_execution_role()except ValueError: iam = boto3.client('iam') role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']# Hub Model configuration. https://huggingface.co/modelshub = { 'HF_MODEL_ID':'google/gemma-1.1-2b-it', 'SM_NUM_GPUS': json.dumps(1), 'HUGGING_FACE_HUB_TOKEN': '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'}assert hub['HUGGING_FACE_HUB_TOKEN'] != '&lt;REPLACE WITH YOUR TOKEN&gt;', &quot;You have to provide a token.&quot;# create Hugging Face Model Class# 2.0.2가 지원되지 않는 버전이라 하여 2.0.1로 바꿈huggingface_model = HuggingFaceModel( image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;,version=&quot;2.0.1&quot;), env=hub, role=role,)# deploy model to SageMaker Inferencepredictor = huggingface_model.deploy( initial_instance_count=1, instance_type=&quot;ml.c5d.4xlarge&quot;, container_startup_health_check_timeout=300,)# send requestpredictor.predict({ &quot;inputs&quot;: &quot;My name is Clara and I am&quot;,}) 에러 1ClientError: An error occurred (AccessDeniedException) when calling the CreateEndpointConfig operation: User: arn:aws:sts::058264433760:assumed-role/SageMaker-ExecutionRole-20240603T163452/SageMaker is not authorized to perform: sagemaker:CreateEndpointConfig on resource: arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint-config/huggingface-pytorch-tgi-inference-2024-06-04-02-00-16-823 because no identity-based policy allows the sagemaker:CreateEndpointConfig action 해결책 이 오류는 현재 사용 중인 IAM 역할에 sagemaker:CreateEndpointConfig 작업을 수행할 권한이 없어서 발생합니다. 이를 해결하기 위해서는 IAM 콘솔에서 해당 역할에 필요한 권한을 추가해야 합니다. 다음 단계에 따라 해결할 수 있습니다: IAM 콘솔 열기: AWS Management Console에서 IAM 콘솔을 엽니다.역할 찾기: 좌측 메뉴에서 “Roles”를 선택합니다.오류 메시지에 나오는 역할 이름(SageMaker-ExecutionRole-20240603T163452)을 검색하여 역할을 찾습니다.정책 추가: 역할을 클릭하여 역할의 세부 정보 페이지로 이동합니다.“Add permissions” 버튼을 클릭합니다.“Attach policies”를 선택합니다.검색 창에 AmazonSageMakerFullAccess를 입력하고, 검색 결과에서 해당 정책을 선택합니다.“Next” 버튼을 클릭하고 “Add permissions” 버튼을 클릭하여 정책을 역할에 추가합니다.커스텀 정책 생성 (선택사항): 만약 AmazonSageMakerFullAccess 정책을 사용하는 것이 너무 광범위하다면, 특정 권한만 포함된 커스텀 정책을 생성할 수 있습니다. “Policies” 메뉴를 선택하고 “Create policy” 버튼을 클릭합니다. “JSON” 탭을 선택하고 다음 JSON을 입력합니다: 12345678910111213141516{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;sagemaker:CreateEndpointConfig&quot;, &quot;sagemaker:CreateEndpoint&quot;, &quot;sagemaker:UpdateEndpoint&quot;, &quot;sagemaker:DeleteEndpoint&quot;, &quot;sagemaker:DescribeEndpoint&quot; ], &quot;Resource&quot;: &quot;*&quot; } ]} “Review policy” 버튼을 클릭하고, 정책 이름을 입력한 후 “Create policy” 버튼을 클릭하여 정책을 생성합니다. 역할로 돌아가서, 새로 생성된 정책을 역할에 추가합니다. 위 단계를 완료한 후, 다시 코드를 실행하여 필요한 권한이 부여되었는지 확인합니다. 권한 문제가 해결되면, CreateEndpointConfig 작업이 성공적으로 수행될 것입니다. 해결 후 새로운 에러 1UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2024-06-04-02-09-06-986: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html 이 오류는 생성된 엔드포인트의 기본 컨테이너가 헬스 체크를 통과하지 못했음을 의미합니다. 이를 해결하기 위해 다음 단계를 따르세요: CloudWatch 로그 확인: AWS Management Console에서 CloudWatch 콘솔을 엽니다. 로그 그룹에서 SageMaker 엔드포인트와 관련된 로그 그룹을 찾습니다. 일반적으로 aws/sagemaker/Endpoints 아래에 있습니다. 엔드포인트의 로그를 검토하여 실패 원인에 대한 구체적인 오류 메시지를 확인합니다. 인스턴스 유형 확인: SageMaker 엔드포인트가 사용하고 있는 인스턴스 유형이 적절한지 확인합니다. 인스턴스 유형이 모델 실행에 충분한 리소스를 제공해야 합니다. 필요한 경우, 더 많은 메모리와 CPU를 제공하는 인스턴스 유형으로 변경합니다. 모델 설정 확인: 모델 설정에서 올바른 이미지 URI 및 환경 변수가 설정되었는지 확인합니다. 이미지 URI와 관련된 모든 환경 변수가 올바르게 설정되었는지 다시 확인합니다. 트러블슈팅 가이드 참고: A- WS SageMaker 트러블슈팅 페이지를 참고하여 일반적인 문제와 해결책을 검토합니다. 다음은 CloudWatch 로그를 확인하고 문제를 진단하는 방법에 대한 예입니다: 12345678910111213141516171819import boto3# CloudWatch Logs 클라이언트 생성logs_client = boto3.client('logs')# 로그 그룹 이름과 스트림 이름을 설정log_group_name = '/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2024-06-04-02-09-06-986'log_stream_name = 'AllTraffic'# 로그 스트림에서 로그 이벤트 가져오기response = logs_client.get_log_events( logGroupName=log_group_name, logStreamName=log_stream_name, startFromHead=True)# 로그 이벤트 출력for event in response['events']: print(event['message']) 이 코드를 사용하여 CloudWatch 로그를 프로그램matically 가져와 확인할 수 있습니다. 로그를 통해 문제의 구체적인 원인을 파악한 후, 해당 문제를 해결하기 위한 조치를 취할 수 있습니다. 인스턴스 유형 변경 예제만약 인스턴스 유형을 변경하려는 경우, 아래 예제를 참고하세요: 1234567891011121314151617181920212223242526272829303132import jsonimport sagemakerimport boto3from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uritry: role = sagemaker.get_execution_role()except ValueError: iam = boto3.client('iam') role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']# Hub Model configuration. https://huggingface.co/modelshub = { 'HF_MODEL_ID': 'google/gemma-1.1-2b-it', 'SM_NUM_GPUS': json.dumps(1), 'HUGGING_FACE_HUB_TOKEN': 'your_actual_token_here'}assert hub['HUGGING_FACE_HUB_TOKEN'] != 'your_actual_token_here', &quot;You have to provide a token.&quot;# create Hugging Face Model Classhuggingface_model = HuggingFaceModel( image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;, version=&quot;2.0.1&quot;), # 지원되는 버전으로 변경 env=hub, role=role,)# 배포 시 인스턴스 유형을 변경predictor = huggingface_model.deploy( initial_instance_count=1, instance_type='ml.p3.2xlarge' # 더 큰 인스턴스 유형으로 변경) JumpStart로 배포하고 테스트하기https://www.youtube.com/watch?v=UigWJPfClcI https://www.youtube.com/watch?v=ZdOcrLKow3I https://www.youtube.com/watch?v=kcuyovwbznY https://www.youtube.com/watch?v=vQFuZUAFel4 inference-experience에 들어가서 Test inference 탭에서 테스트 진행 python SDK example code123456789101112131415161718192021from sagemaker.predictor import retrieve_defaultendpoint_name = &quot;jumpstart-dft-hf-llm-gemma-2b-20240609-080102&quot;predictor = retrieve_default(endpoint_name)payload = { &quot;inputs&quot;: &quot;Write me a poem about Machine Learning.&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256 }}response = predictor.predict(payload)print(response)payload = { &quot;inputs&quot;: &quot;Hello everyone, my name is &quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;top_p&quot;: 0.9, &quot;temperature&quot;: 0.2 }}response = predictor.predict(payload)print(response) Sample request Content type : application.json Json123456789101112{ &quot;body&quot;: { &quot;inputs&quot;: &quot;&lt;bos&gt;&lt;start_of_turn&gt;user\\nWrite me a poem about Machine Learning&lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;decoder_input_details&quot;: true, &quot;details&quot;: true } }, &quot;contentType&quot;: &quot;application/json&quot;, &quot;endpointName&quot;: &quot;jumpstart-dft-hf-llm-gemma-2b-instr-20240611-004431&quot;} request12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586{ &quot;body&quot;: [ { &quot;generated_text&quot;: &quot;&lt;bos&gt;&lt;start_of_turn&gt;user\\nWrite me a poem about Machine Learning&lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model**Machine Learning**\\n\\nAlgorithms dance, a digital ballet,\\nLearning from data, a never-ending tally.\\nData streams in, a torrent of bytes,\\nAlgorithms sift and sort, with tireless eyes.\\n\\nFrom images to speech, the patterns unfold,\\nPredicting the future, stories to be told.\\nSupervised, unsupervised, a spectrum of might,\\nMachine learning, a powerful light.\\n\\nUnsupervised, where patterns reside,\\nClustering data, a task with no guide.\\nReinforcement, a learning curve to ascend,\\nWith algorithms guiding, a new world is found.\\n\\nDeep learning, a neural net so vast,\\nLearning from data, a hidden past.\\nFrom medical diagnosis to financial sway,\\nMachine learning's impact cannot be swayed.\\n\\nA tool for good, or a path to despair,\\nThe ethical use of AI, a matter to share.\\nBias and fairness, a constant fight,\\nTo ensure that the machine's light shines bright.\\n\\nSo let the algorithms spin and the data flow,\\nA symphony of learning, a digital woe.\\nMachine learning, a journey without end,\\nShaping the future, a world without a friend.&quot;, &quot;details&quot;: { &quot;finish_reason&quot;: &quot;eos_token&quot;, &quot;generated_tokens&quot;: 248, &quot;seed&quot;: null, &quot;prefill&quot;: [ { &quot;id&quot;: 2, &quot;text&quot;: &quot;&lt;bos&gt;&quot;, &quot;logprob&quot;: null }, { &quot;id&quot;: 2, &quot;text&quot;: &quot;&lt;bos&gt;&quot;, &quot;logprob&quot;: -25 }, { &quot;id&quot;: 106, &quot;text&quot;: &quot;&lt;start_of_turn&gt;&quot;, &quot;logprob&quot;: -65.5 }, { &quot;id&quot;: 1645, &quot;text&quot;: &quot;user&quot;, &quot;logprob&quot;: -20.375 }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -4.78125 }, { &quot;id&quot;: 5559, &quot;text&quot;: &quot;Write&quot;, &quot;logprob&quot;: -8.6875 }, { &quot;id&quot;: 682, &quot;text&quot;: &quot; me&quot;, &quot;logprob&quot;: -5.5625 }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.044189453 }, { &quot;id&quot;: 19592, &quot;text&quot;: &quot; poem&quot;, &quot;logprob&quot;: -1.2421875 }, { &quot;id&quot;: 1105, &quot;text&quot;: &quot; about&quot;, &quot;logprob&quot;: -0.021606445 }, { &quot;id&quot;: 13403, &quot;text&quot;: &quot; Machine&quot;, &quot;logprob&quot;: -19.5 }, { &quot;id&quot;: 14715, &quot;text&quot;: &quot; Learning&quot;, &quot;logprob&quot;: -0.061523438 }, { &quot;id&quot;: 107, &quot;text&quot;: &quot;&lt;end_of_turn&gt;&quot;, &quot;logprob&quot;: -39 }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -4.875 }, { &quot;id&quot;: 106, &quot;text&quot;: &quot;&lt;start_of_turn&gt;&quot;, &quot;logprob&quot;: -42.75 }, { &quot;id&quot;: 2516, &quot;text&quot;: &quot;model&quot;, &quot;logprob&quot;: -20.75 } ], &quot;tokens&quot;: [ { &quot;id&quot;: 688, &quot;text&quot;: &quot;**&quot;, &quot;logprob&quot;: -0.24707031, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.13867188, &quot;special&quot;: false }, { &quot;id&quot;: 14715, &quot;text&quot;: &quot; Learning&quot;, &quot;logprob&quot;: -0.057128906, &quot;special&quot;: false }, { &quot;id&quot;: 688, &quot;text&quot;: &quot;**&quot;, &quot;logprob&quot;: -0.0073547363, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00007581711, &quot;special&quot;: false }, { &quot;id&quot;: 139972, &quot;text&quot;: &quot;Algorithms&quot;, &quot;logprob&quot;: -0.45507812, &quot;special&quot;: false }, { &quot;id&quot;: 11877, &quot;text&quot;: &quot; dance&quot;, &quot;logprob&quot;: -0.21875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.390625, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.05810547, &quot;special&quot;: false }, { &quot;id&quot;: 6403, &quot;text&quot;: &quot; digital&quot;, &quot;logprob&quot;: -0.6640625, &quot;special&quot;: false }, { &quot;id&quot;: 50455, &quot;text&quot;: &quot; ballet&quot;, &quot;logprob&quot;: -0.45898438, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000031471252, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000029087067, &quot;special&quot;: false }, { &quot;id&quot;: 26231, &quot;text&quot;: &quot;Learning&quot;, &quot;logprob&quot;: -0.796875, &quot;special&quot;: false }, { &quot;id&quot;: 774, &quot;text&quot;: &quot; from&quot;, &quot;logprob&quot;: -0.084472656, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.020507812, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.007873535, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.3359375, &quot;special&quot;: false }, { &quot;id&quot;: 2447, &quot;text&quot;: &quot; never&quot;, &quot;logprob&quot;: -1.2265625, &quot;special&quot;: false }, { &quot;id&quot;: 235290, &quot;text&quot;: &quot;-&quot;, &quot;logprob&quot;: -0.0047302246, &quot;special&quot;: false }, { &quot;id&quot;: 3002, &quot;text&quot;: &quot;ending&quot;, &quot;logprob&quot;: -0.00061416626, &quot;special&quot;: false }, { &quot;id&quot;: 78289, &quot;text&quot;: &quot; tally&quot;, &quot;logprob&quot;: -1.53125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000036478043, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000030398369, &quot;special&quot;: false }, { &quot;id&quot;: 1510, &quot;text&quot;: &quot;Data&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 24039, &quot;text&quot;: &quot; streams&quot;, &quot;logprob&quot;: -1.3984375, &quot;special&quot;: false }, { &quot;id&quot;: 575, &quot;text&quot;: &quot; in&quot;, &quot;logprob&quot;: -0.21679688, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0013275146, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.14355469, &quot;special&quot;: false }, { &quot;id&quot;: 61300, &quot;text&quot;: &quot; torrent&quot;, &quot;logprob&quot;: -0.67578125, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 15493, &quot;text&quot;: &quot; bytes&quot;, &quot;logprob&quot;: -0.41601562, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00062179565, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000039815903, &quot;special&quot;: false }, { &quot;id&quot;: 139972, &quot;text&quot;: &quot;Algorithms&quot;, &quot;logprob&quot;: -0.59375, &quot;special&quot;: false }, { &quot;id&quot;: 183807, &quot;text&quot;: &quot; sift&quot;, &quot;logprob&quot;: -0.41210938, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.39648438, &quot;special&quot;: false }, { &quot;id&quot;: 6728, &quot;text&quot;: &quot; sort&quot;, &quot;logprob&quot;: -0.099609375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.008056641, &quot;special&quot;: false }, { &quot;id&quot;: 675, &quot;text&quot;: &quot; with&quot;, &quot;logprob&quot;: -1.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 185614, &quot;text&quot;: &quot; tireless&quot;, &quot;logprob&quot;: -1.5546875, &quot;special&quot;: false }, { &quot;id&quot;: 4628, &quot;text&quot;: &quot; eyes&quot;, &quot;logprob&quot;: -2.109375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00008916855, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000047683716, &quot;special&quot;: false }, { &quot;id&quot;: 3604, &quot;text&quot;: &quot;From&quot;, &quot;logprob&quot;: -0.6875, &quot;special&quot;: false }, { &quot;id&quot;: 5191, &quot;text&quot;: &quot; images&quot;, &quot;logprob&quot;: -0.77734375, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.16210938, &quot;special&quot;: false }, { &quot;id&quot;: 11360, &quot;text&quot;: &quot; speech&quot;, &quot;logprob&quot;: -0.41015625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0045776367, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 12136, &quot;text&quot;: &quot; patterns&quot;, &quot;logprob&quot;: -0.43945312, &quot;special&quot;: false }, { &quot;id&quot;: 45411, &quot;text&quot;: &quot; unfold&quot;, &quot;logprob&quot;: -0.59765625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0004825592, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000021338463, &quot;special&quot;: false }, { &quot;id&quot;: 98951, &quot;text&quot;: &quot;Predic&quot;, &quot;logprob&quot;: -1.5859375, &quot;special&quot;: false }, { &quot;id&quot;: 1486, &quot;text&quot;: &quot;ting&quot;, &quot;logprob&quot;: -0.010681152, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 3936, &quot;text&quot;: &quot; future&quot;, &quot;logprob&quot;: -0.019897461, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0021514893, &quot;special&quot;: false }, { &quot;id&quot;: 8965, &quot;text&quot;: &quot; stories&quot;, &quot;logprob&quot;: -0.22265625, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.33203125, &quot;special&quot;: false }, { &quot;id&quot;: 614, &quot;text&quot;: &quot; be&quot;, &quot;logprob&quot;: -0.115234375, &quot;special&quot;: false }, { &quot;id&quot;: 4203, &quot;text&quot;: &quot; told&quot;, &quot;logprob&quot;: -0.07714844, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000036478043, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00006723404, &quot;special&quot;: false }, { &quot;id&quot;: 8437, &quot;text&quot;: &quot;Super&quot;, &quot;logprob&quot;: -0.60546875, &quot;special&quot;: false }, { &quot;id&quot;: 9470, &quot;text&quot;: &quot;vised&quot;, &quot;logprob&quot;: -0.00051116943, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.75390625, &quot;special&quot;: false }, { &quot;id&quot;: 195643, &quot;text&quot;: &quot; unsupervised&quot;, &quot;logprob&quot;: -0.0017166138, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0004749298, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.24511719, &quot;special&quot;: false }, { &quot;id&quot;: 18303, &quot;text&quot;: &quot; spectrum&quot;, &quot;logprob&quot;: -0.49023438, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 2613, &quot;text&quot;: &quot; might&quot;, &quot;logprob&quot;: -0.7734375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00002193451, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000027894974, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -1.2578125, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.005432129, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.61328125, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.27929688, &quot;special&quot;: false }, { &quot;id&quot;: 10276, &quot;text&quot;: &quot; powerful&quot;, &quot;logprob&quot;: -1.2890625, &quot;special&quot;: false }, { &quot;id&quot;: 2611, &quot;text&quot;: &quot; light&quot;, &quot;logprob&quot;: -0.5625, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00007581711, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000028252602, &quot;special&quot;: false }, { &quot;id&quot;: 18808, &quot;text&quot;: &quot;Uns&quot;, &quot;logprob&quot;: -0.39648438, &quot;special&quot;: false }, { &quot;id&quot;: 55760, &quot;text&quot;: &quot;uper&quot;, &quot;logprob&quot;: -0.02746582, &quot;special&quot;: false }, { &quot;id&quot;: 9470, &quot;text&quot;: &quot;vised&quot;, &quot;logprob&quot;: -0.0000072717667, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.28125, &quot;special&quot;: false }, { &quot;id&quot;: 1570, &quot;text&quot;: &quot; where&quot;, &quot;logprob&quot;: -0.51171875, &quot;special&quot;: false }, { &quot;id&quot;: 12136, &quot;text&quot;: &quot; patterns&quot;, &quot;logprob&quot;: -0.91015625, &quot;special&quot;: false }, { &quot;id&quot;: 53172, &quot;text&quot;: &quot; reside&quot;, &quot;logprob&quot;: -0.89453125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00034332275, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000037908554, &quot;special&quot;: false }, { &quot;id&quot;: 184568, &quot;text&quot;: &quot;Clustering&quot;, &quot;logprob&quot;: -0.33984375, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.38085938, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.140625, &quot;special&quot;: false }, { &quot;id&quot;: 6911, &quot;text&quot;: &quot; task&quot;, &quot;logprob&quot;: -1.4453125, &quot;special&quot;: false }, { &quot;id&quot;: 675, &quot;text&quot;: &quot; with&quot;, &quot;logprob&quot;: -1.8828125, &quot;special&quot;: false }, { &quot;id&quot;: 793, &quot;text&quot;: &quot; no&quot;, &quot;logprob&quot;: -1.2890625, &quot;special&quot;: false }, { &quot;id&quot;: 5608, &quot;text&quot;: &quot; guide&quot;, &quot;logprob&quot;: -0.52734375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0005226135, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00010585785, &quot;special&quot;: false }, { &quot;id&quot;: 75111, &quot;text&quot;: &quot;Rein&quot;, &quot;logprob&quot;: -0.47851562, &quot;special&quot;: false }, { &quot;id&quot;: 14707, &quot;text&quot;: &quot;forcement&quot;, &quot;logprob&quot;: -0.000053167343, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.083496094, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.33007812, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.7265625, &quot;special&quot;: false }, { &quot;id&quot;: 12942, &quot;text&quot;: &quot; curve&quot;, &quot;logprob&quot;: -0.54296875, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -1.1640625, &quot;special&quot;: false }, { &quot;id&quot;: 70806, &quot;text&quot;: &quot; ascend&quot;, &quot;logprob&quot;: -0.51171875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000017523766, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.0000104904175, &quot;special&quot;: false }, { &quot;id&quot;: 3192, &quot;text&quot;: &quot;With&quot;, &quot;logprob&quot;: -1.328125, &quot;special&quot;: false }, { &quot;id&quot;: 28514, &quot;text&quot;: &quot; algorithms&quot;, &quot;logprob&quot;: -0.640625, &quot;special&quot;: false }, { &quot;id&quot;: 55709, &quot;text&quot;: &quot; guiding&quot;, &quot;logprob&quot;: -0.33984375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.12402344, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.6328125, &quot;special&quot;: false }, { &quot;id&quot;: 888, &quot;text&quot;: &quot; new&quot;, &quot;logprob&quot;: -1.4375, &quot;special&quot;: false }, { &quot;id&quot;: 2134, &quot;text&quot;: &quot; world&quot;, &quot;logprob&quot;: -0.359375, &quot;special&quot;: false }, { &quot;id&quot;: 603, &quot;text&quot;: &quot; is&quot;, &quot;logprob&quot;: -0.31054688, &quot;special&quot;: false }, { &quot;id&quot;: 1942, &quot;text&quot;: &quot; found&quot;, &quot;logprob&quot;: -0.5703125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000011563301, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.000044822693, &quot;special&quot;: false }, { &quot;id&quot;: 26843, &quot;text&quot;: &quot;Deep&quot;, &quot;logprob&quot;: -0.6171875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.10546875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.026123047, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.028686523, &quot;special&quot;: false }, { &quot;id&quot;: 35544, &quot;text&quot;: &quot; neural&quot;, &quot;logprob&quot;: -0.5234375, &quot;special&quot;: false }, { &quot;id&quot;: 3117, &quot;text&quot;: &quot; net&quot;, &quot;logprob&quot;: -1.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 712, &quot;text&quot;: &quot; so&quot;, &quot;logprob&quot;: -0.37304688, &quot;special&quot;: false }, { &quot;id&quot;: 12380, &quot;text&quot;: &quot; vast&quot;, &quot;logprob&quot;: -0.68359375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0008125305, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.0000076293945, &quot;special&quot;: false }, { &quot;id&quot;: 26231, &quot;text&quot;: &quot;Learning&quot;, &quot;logprob&quot;: -0.7421875, &quot;special&quot;: false }, { &quot;id&quot;: 774, &quot;text&quot;: &quot; from&quot;, &quot;logprob&quot;: -0.18457031, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.703125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.020751953, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.7890625, &quot;special&quot;: false }, { &quot;id&quot;: 5915, &quot;text&quot;: &quot; hidden&quot;, &quot;logprob&quot;: -1.4765625, &quot;special&quot;: false }, { &quot;id&quot;: 3433, &quot;text&quot;: &quot; past&quot;, &quot;logprob&quot;: -0.4296875, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0004043579, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00014400482, &quot;special&quot;: false }, { &quot;id&quot;: 3604, &quot;text&quot;: &quot;From&quot;, &quot;logprob&quot;: -0.94921875, &quot;special&quot;: false }, { &quot;id&quot;: 6910, &quot;text&quot;: &quot; medical&quot;, &quot;logprob&quot;: -0.31640625, &quot;special&quot;: false }, { &quot;id&quot;: 19491, &quot;text&quot;: &quot; diagnosis&quot;, &quot;logprob&quot;: -0.3828125, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.0059509277, &quot;special&quot;: false }, { &quot;id&quot;: 6895, &quot;text&quot;: &quot; financial&quot;, &quot;logprob&quot;: -0.48046875, &quot;special&quot;: false }, { &quot;id&quot;: 46507, &quot;text&quot;: &quot; sway&quot;, &quot;logprob&quot;: -0.69921875, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000028133392, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00005865097, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.060546875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.005279541, &quot;special&quot;: false }, { &quot;id&quot;: 235303, &quot;text&quot;: &quot;'&quot;, &quot;logprob&quot;: -0.3046875, &quot;special&quot;: false }, { &quot;id&quot;: 235256, &quot;text&quot;: &quot;s&quot;, &quot;logprob&quot;: -0.0000011920929, &quot;special&quot;: false }, { &quot;id&quot;: 6418, &quot;text&quot;: &quot; impact&quot;, &quot;logprob&quot;: -0.78515625, &quot;special&quot;: false }, { &quot;id&quot;: 2952, &quot;text&quot;: &quot; cannot&quot;, &quot;logprob&quot;: -0.36523438, &quot;special&quot;: false }, { &quot;id&quot;: 614, &quot;text&quot;: &quot; be&quot;, &quot;logprob&quot;: -0.14355469, &quot;special&quot;: false }, { &quot;id&quot;: 150640, &quot;text&quot;: &quot; swayed&quot;, &quot;logprob&quot;: -0.96484375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.005554199, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00004863739, &quot;special&quot;: false }, { &quot;id&quot;: 235280, &quot;text&quot;: &quot;A&quot;, &quot;logprob&quot;: -1.1328125, &quot;special&quot;: false }, { &quot;id&quot;: 7217, &quot;text&quot;: &quot; tool&quot;, &quot;logprob&quot;: -0.65625, &quot;special&quot;: false }, { &quot;id&quot;: 604, &quot;text&quot;: &quot; for&quot;, &quot;logprob&quot;: -0.203125, &quot;special&quot;: false }, { &quot;id&quot;: 1426, &quot;text&quot;: &quot; good&quot;, &quot;logprob&quot;: -0.7734375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.3203125, &quot;special&quot;: false }, { &quot;id&quot;: 689, &quot;text&quot;: &quot; or&quot;, &quot;logprob&quot;: -0.6875, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.024414062, &quot;special&quot;: false }, { &quot;id&quot;: 3703, &quot;text&quot;: &quot; path&quot;, &quot;logprob&quot;: -0.64453125, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.12158203, &quot;special&quot;: false }, { &quot;id&quot;: 40813, &quot;text&quot;: &quot; despair&quot;, &quot;logprob&quot;: -0.045654297, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000044345856, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000035762787, &quot;special&quot;: false }, { &quot;id&quot;: 651, &quot;text&quot;: &quot;The&quot;, &quot;logprob&quot;: -0.7890625, &quot;special&quot;: false }, { &quot;id&quot;: 33970, &quot;text&quot;: &quot; ethical&quot;, &quot;logprob&quot;: -0.48828125, &quot;special&quot;: false }, { &quot;id&quot;: 1281, &quot;text&quot;: &quot; use&quot;, &quot;logprob&quot;: -0.5078125, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.15722656, &quot;special&quot;: false }, { &quot;id&quot;: 16481, &quot;text&quot;: &quot; AI&quot;, &quot;logprob&quot;: -0.26367188, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.13378906, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.4140625, &quot;special&quot;: false }, { &quot;id&quot;: 4391, &quot;text&quot;: &quot; matter&quot;, &quot;logprob&quot;: -0.9609375, &quot;special&quot;: false }, { &quot;id&quot;: 577, &quot;text&quot;: &quot; to&quot;, &quot;logprob&quot;: -0.40625, &quot;special&quot;: false }, { &quot;id&quot;: 4638, &quot;text&quot;: &quot; share&quot;, &quot;logprob&quot;: -0.27734375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000012755394, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00015258789, &quot;special&quot;: false }, { &quot;id&quot;: 79111, &quot;text&quot;: &quot;Bias&quot;, &quot;logprob&quot;: -0.8515625, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.60546875, &quot;special&quot;: false }, { &quot;id&quot;: 67512, &quot;text&quot;: &quot; fairness&quot;, &quot;logprob&quot;: -0.10205078, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0002193451, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.14941406, &quot;special&quot;: false }, { &quot;id&quot;: 6221, &quot;text&quot;: &quot; constant&quot;, &quot;logprob&quot;: -0.27929688, &quot;special&quot;: false }, { &quot;id&quot;: 5900, &quot;text&quot;: &quot; fight&quot;, &quot;logprob&quot;: -0.703125, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.000009536743, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000008940697, &quot;special&quot;: false }, { &quot;id&quot;: 1469, &quot;text&quot;: &quot;To&quot;, &quot;logprob&quot;: -0.17382812, &quot;special&quot;: false }, { &quot;id&quot;: 7433, &quot;text&quot;: &quot; ensure&quot;, &quot;logprob&quot;: -0.014953613, &quot;special&quot;: false }, { &quot;id&quot;: 674, &quot;text&quot;: &quot; that&quot;, &quot;logprob&quot;: -0.78515625, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.9453125, &quot;special&quot;: false }, { &quot;id&quot;: 6479, &quot;text&quot;: &quot; machine&quot;, &quot;logprob&quot;: -1.15625, &quot;special&quot;: false }, { &quot;id&quot;: 235303, &quot;text&quot;: &quot;'&quot;, &quot;logprob&quot;: -0.74609375, &quot;special&quot;: false }, { &quot;id&quot;: 235256, &quot;text&quot;: &quot;s&quot;, &quot;logprob&quot;: -3.5762787e-7, &quot;special&quot;: false }, { &quot;id&quot;: 2611, &quot;text&quot;: &quot; light&quot;, &quot;logprob&quot;: -1.2421875, &quot;special&quot;: false }, { &quot;id&quot;: 66308, &quot;text&quot;: &quot; shines&quot;, &quot;logprob&quot;: -0.08642578, &quot;special&quot;: false }, { &quot;id&quot;: 8660, &quot;text&quot;: &quot; bright&quot;, &quot;logprob&quot;: -0.041748047, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.0000067949295, &quot;special&quot;: false }, { &quot;id&quot;: 109, &quot;text&quot;: &quot;\\n\\n&quot;, &quot;logprob&quot;: -0.00061416626, &quot;special&quot;: false }, { &quot;id&quot;: 2339, &quot;text&quot;: &quot;So&quot;, &quot;logprob&quot;: -0.049804688, &quot;special&quot;: false }, { &quot;id&quot;: 2142, &quot;text&quot;: &quot; let&quot;, &quot;logprob&quot;: -0.62109375, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.359375, &quot;special&quot;: false }, { &quot;id&quot;: 28514, &quot;text&quot;: &quot; algorithms&quot;, &quot;logprob&quot;: -0.04321289, &quot;special&quot;: false }, { &quot;id&quot;: 11974, &quot;text&quot;: &quot; spin&quot;, &quot;logprob&quot;: -1.296875, &quot;special&quot;: false }, { &quot;id&quot;: 578, &quot;text&quot;: &quot; and&quot;, &quot;logprob&quot;: -0.45507812, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -1.3359375, &quot;special&quot;: false }, { &quot;id&quot;: 1423, &quot;text&quot;: &quot; data&quot;, &quot;logprob&quot;: -0.484375, &quot;special&quot;: false }, { &quot;id&quot;: 3781, &quot;text&quot;: &quot; flow&quot;, &quot;logprob&quot;: -0.7265625, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0000667572, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.000027179718, &quot;special&quot;: false }, { &quot;id&quot;: 235280, &quot;text&quot;: &quot;A&quot;, &quot;logprob&quot;: -1.171875, &quot;special&quot;: false }, { &quot;id&quot;: 113598, &quot;text&quot;: &quot; symphony&quot;, &quot;logprob&quot;: -0.03466797, &quot;special&quot;: false }, { &quot;id&quot;: 576, &quot;text&quot;: &quot; of&quot;, &quot;logprob&quot;: -0.0016555786, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -1.0234375, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.0076293945, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.1953125, &quot;special&quot;: false }, { &quot;id&quot;: 6403, &quot;text&quot;: &quot; digital&quot;, &quot;logprob&quot;: -1.2734375, &quot;special&quot;: false }, { &quot;id&quot;: 82647, &quot;text&quot;: &quot; woe&quot;, &quot;logprob&quot;: -1.125, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.00023078918, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00037765503, &quot;special&quot;: false }, { &quot;id&quot;: 24911, &quot;text&quot;: &quot;Machine&quot;, &quot;logprob&quot;: -0.8046875, &quot;special&quot;: false }, { &quot;id&quot;: 6044, &quot;text&quot;: &quot; learning&quot;, &quot;logprob&quot;: -0.0006828308, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00793457, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -0.029418945, &quot;special&quot;: false }, { &quot;id&quot;: 10734, &quot;text&quot;: &quot; journey&quot;, &quot;logprob&quot;: -1.15625, &quot;special&quot;: false }, { &quot;id&quot;: 2346, &quot;text&quot;: &quot; without&quot;, &quot;logprob&quot;: -0.111328125, &quot;special&quot;: false }, { &quot;id&quot;: 1580, &quot;text&quot;: &quot; end&quot;, &quot;logprob&quot;: -0.001083374, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.00002348423, &quot;special&quot;: false }, { &quot;id&quot;: 108, &quot;text&quot;: &quot;\\n&quot;, &quot;logprob&quot;: -0.00004673004, &quot;special&quot;: false }, { &quot;id&quot;: 3957, &quot;text&quot;: &quot;Sha&quot;, &quot;logprob&quot;: -1.265625, &quot;special&quot;: false }, { &quot;id&quot;: 10024, &quot;text&quot;: &quot;ping&quot;, &quot;logprob&quot;: -0.00013542175, &quot;special&quot;: false }, { &quot;id&quot;: 573, &quot;text&quot;: &quot; the&quot;, &quot;logprob&quot;: -0.37109375, &quot;special&quot;: false }, { &quot;id&quot;: 3936, &quot;text&quot;: &quot; future&quot;, &quot;logprob&quot;: -0.22558594, &quot;special&quot;: false }, { &quot;id&quot;: 235269, &quot;text&quot;: &quot;,&quot;, &quot;logprob&quot;: -0.002029419, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.2578125, &quot;special&quot;: false }, { &quot;id&quot;: 2134, &quot;text&quot;: &quot; world&quot;, &quot;logprob&quot;: -0.96875, &quot;special&quot;: false }, { &quot;id&quot;: 2346, &quot;text&quot;: &quot; without&quot;, &quot;logprob&quot;: -0.5625, &quot;special&quot;: false }, { &quot;id&quot;: 476, &quot;text&quot;: &quot; a&quot;, &quot;logprob&quot;: -1.828125, &quot;special&quot;: false }, { &quot;id&quot;: 4034, &quot;text&quot;: &quot; friend&quot;, &quot;logprob&quot;: -1.5234375, &quot;special&quot;: false }, { &quot;id&quot;: 235265, &quot;text&quot;: &quot;.&quot;, &quot;logprob&quot;: -0.000030517578, &quot;special&quot;: false }, { &quot;id&quot;: 1, &quot;text&quot;: &quot;&lt;eos&gt;&quot;, &quot;logprob&quot;: -0.000027060509, &quot;special&quot;: true } ] } } ], &quot;contentType&quot;: &quot;application/json&quot;, &quot;invokedProductionVariant&quot;: &quot;AllTraffic&quot;} 외부에서 curl을 날리면123456789101112% curl --location 'https://runtime.sagemaker.ap-northeast-2.amazonaws.com/endpoints/jumpstart-dft-hf-llm-gemma-2b-20240609-080102/invocations' \\--header 'endpointName: jumpstart-dft-hf-llm-gemma-2b-20240609-080102' \\--header 'Content-Type: application/json' \\--data '{ &quot;inputs&quot;: &quot;Write me a poem about Machine Learning.&quot;, &quot;parameters&quot;: { &quot;max_new_tokens&quot;: 256, &quot;decoder_input_details&quot;: true, &quot;details&quot;: true }}'{&quot;message&quot;:&quot;Missing Authentication Token&quot;} 외부에서 접속하기Amazon SageMaker에서 JumpStart를 사용하여 배포한 Gemma 추론 모델 엔드포인트는 기본적으로 AWS 내에서만 접근 가능하도록 설정됩니다. 하지만 외부망에서도 접근할 수 있도록 설정할 수 있습니다. 이를 위해서는 다음 단계를 따라야 합니다. 1. 엔드포인트 보안 그룹 설정엔드포인트가 속한 보안 그룹에서 인바운드 및 아웃바운드 규칙을 설정하여 외부 IP가 엔드포인트에 접근할 수 있도록 합니다. 인바운드 규칙 추가: 특정 IP 주소나 IP 범위에 대해 포트 443(HTTPS)을 열어야 합니다. 아웃바운드 규칙 확인: 기본적으로 모든 아웃바운드 트래픽이 허용되어야 합니다. 2. 엔드포인트에 대한 IAM 정책 설정외부 애플리케이션이 엔드포인트에 접근할 수 있도록 필요한 IAM 정책을 설정합니다. 이는 외부 애플리케이션이 올바른 인증 및 권한을 가지고 있는지 확인하기 위함입니다. 3. API Gateway 설정AWS API Gateway를 사용하여 SageMaker 엔드포인트를 퍼블릭 API로 노출할 수 있습니다. 이렇게 하면 외부 클라이언트가 API Gateway URL을 통해 SageMaker 엔드포인트에 접근할 수 있습니다. API Gateway 설정 단계: API Gateway 생성: AWS Management Console에서 API Gateway를 생성합니다. 유형은 REST API를 선택합니다. 새 리소스 생성: API에 새 리소스를 추가하고, HTTP 메서드(예: POST)를 설정합니다. 통합 유형 설정: 통합 유형으로 AWS 서비스(SageMaker)를 선택합니다. 엔드포인트 URL을 SageMaker 엔드포인트로 설정합니다. IAM 역할 설정: API Gateway가 SageMaker 엔드포인트에 접근할 수 있도록 필요한 권한을 가진 IAM 역할을 설정합니다. 배포: API를 배포하고, 사용할 스테이지(예: dev, prod)를 설정합니다. 4. 엔드포인트 호출외부 클라이언트가 API Gateway URL을 사용하여 SageMaker 엔드포인트에 접근하고, 추론 요청을 보낼 수 있습니다. 다음은 예시 코드입니다: 12345678910111213141516171819import requestsurl = &quot;https://your-api-gateway-id.execute-api.region.amazonaws.com/your-stage/your-resource&quot;headers = { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: &quot;Bearer your-auth-token&quot;}payload = { &quot;action&quot;: &quot;invoke&quot;, &quot;service&quot;: &quot;gemma&quot;, &quot;method&quot;: &quot;writePoem&quot;, &quot;parameters&quot;: { &quot;theme&quot;: &quot;love&quot;, &quot;style&quot;: &quot;sonnet&quot; }}response = requests.post(url, json=payload, headers=headers)print(response.json()) 요약외부망에서 SageMaker JumpStart로 배포한 Gemma 추론 모델에 접근하기 위해서는: 엔드포인트 보안 그룹 설정. IAM 정책 설정. API Gateway 설정을 통해 엔드포인트를 퍼블릭 API로 노출. API Gateway URL을 통해 추론 요청 전송. 예시https://devocean.sk.com/blog/techBoardDetail.do?ID=163528 https://boksup.tistory.com/33 Chalice를 통한 서버리스 API 작업 생성SageMaker 엔드포인트를 사용할 수 있게 되었다면, 최종 사용자에게 제공할 결과를 생성하기 위해 엔드포인트에 액세스할 수 있는 API 작업을 생성해야한다.그렇기에 Chalice 프레임워크를 사용하여 API Gateway에 간단한 Flask 유사 애플리케이션을 배포함으로써 SageMaker 엔드포인트와 상호 작용할 Lambda 함수를 트리거할 것이다. Chalice는 AWS를 위한 서버리스 마이클 프레임워크이다. 이를 사용하면 Amazon API Gateway 및 AWS Lambda를 사용하는 애플리케이션을 신속하게 생성하고 배포할 수 있다.Chalice를 사용하면 자체 Lambda 함수를 구축하는 것과 비교하여 빠르고 효율적으로 이를 수행 가능하다. 참고 : https://aws.amazon.com/ko/blogs/korea/build-a-serverless-frontend-for-an-amazon-sagemaker-endpoint/ 세팅가상 환경 생성12% python3 -m venv aws% source ./aws/bin/activate 관련 라이브러리 설치12% pip install chalice% brew install awscli Chalice 프로젝트 생성12% chalice new-project bert-text-classfication% cd bert-text-classfication Chalice 로컬 환경 테스트123% chalice local# localhost로 접근 시 기본 샘플 동작하는 것 확인 AWS CLI 설정 Access keys 눌러 키 값 및 명령어 입력 configure sso12345678910% aws configure sso# SSO session name : (임의로 입력해도 되는 것으로 추정)# SSO start URL: (Access keys에 있는 값 복사)# SSO Region: (Access Keys에 있는 값 복사)# SSO registration scopes: (기본 값이 있는 것 같아 Enter)# 이후 code를 입력하는 링크가 주어지는데, AWS 콘솔에 로그인한 환경에서 해다 링크에 접속하여 해당 code dlqfuraws s3 ls --profile [SSO session name] Access key 입력1234567# Access keys에 있는 값 복사하여 입력 (어느정도 시간이 지나면 키나 토큰값이 달라지는 것 같으므로 주의)export AWS_ACCESS_KEY_ID=&quot;&quot;export AWS_SECRET_KEY=&quot;&quot;export AWS_SESSION_TOKEN=&quot;&quot;# 가이드에는 없는데, deploy 시 에러 발생하여 추가로 실행export AWS_DEFAULT_REGION=&quot;&quot; boto3 사용1% pip install boto3 boto3 라이브러리를 requirements.txt 파일에 추가하여 chalice가 이를 인식할 수 있도록 한다. 추론 테스트Chalice Deploy 후 Postman 통해 추론 결과가 정상적으로 전달되는지 테스트 12345curl --location 'https://3joge41r35.execute-api.ap-northeast-2.amazonaws.com/api/invoke' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;This is a sample input text&quot;}' 에러남 123{ &quot;error&quot;: &quot;An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:sts::058264433760:assumed-role/bert-text-classfication-dev/bert-text-classfication-dev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint/jumpstart-dft-hf-tc-distilbert-base-20240718-065402 because no identity-based policy allows the sagemaker:InvokeEndpoint action&quot;} 이 오류 메시지는 Lambda 함수가 SageMaker 엔드포인트를 호출할 수 있는 권한이 없기 때문에 발생한다. Lambda 함수에 필요한 권한을 부여하기 위해서는 Lambda 함수의 실행 역할에 SageMaker 엔드포인트를 호출할 수 있는 권한을 추가해야 한다. 다음은 Lambda 실행 역할에 SageMaker 권한을 추가하는 방법이다: 1. IAM 역할에 정책 추가Lambda 함수에 할당된 IAM 역할에 sagemaker:InvokeEndpoint 권한을 추가해야 한다. 이를 위해 AWS Management Console 또는 AWS CLI를 사용할 수 있다. AWS Management Console 사용 IAM 콘솔로 이동: IAM 콘솔로 이동합니다. 역할(Role) 선택: Lambda 함수에 할당된 역할을 선택합니다. 예를 들어, bert-text-classfication-dev 역할을 찾습니다. 정책 추가: 역할의 “권한” 탭에서 “정책 추가” 버튼을 클릭합니다. 정책 연결: 아래 JSON 정책을 추가합니다. 정책 JSONIAM 콘솔에서 정책을 추가할 때, 아래 JSON 정책을 사용해라: 12345678910{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;, &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-northeast-2:058264433760:endpoint/jumpstart-dft-hf-tc-distilbert-base-20240718-065402&quot; } ]} Lambda 함수 재배포IAM 역할에 정책을 추가한 후, Lambda 함수가 새로운 권한을 인식할 수 있도록 Chalice를 사용하여 Lambda 함수를 다시 배포한다: 1chalice deploy 이제 테스트를 하면 요청12345curl --location 'https://3joge41r35.execute-api.ap-northeast-2.amazonaws.com/api/invoke' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;This is a sample input text&quot;}' 응답 { &quot;Inference&quot;: { &quot;Input text&quot;: &quot;This is a sample input text&quot;, &quot;Model prediction&quot;: [ 0.7710819840431213, 0.22891800105571747 ], &quot;Labels&quot;: [ &quot;LABEL_0&quot;, &quot;LABEL_1&quot; ], &quot;Predicted Label&quot;: &quot;LABEL_0&quot; } } ~~","link":"/2024/06/03/Deploy-Models-On-Sagemaker-For-Inference/"},{"title":"API Gateway, Lambda로 배포하기","text":"ML 모델을 서비스로 배포하려면 어떻게 하는게 좋을지 고민하던 중, AWS에 Amazon SageMaker를 이용하여 배포하기로 결정했다. Developer Guide의 Get Started를 읽으며 따라해보았다. ML inference를 서비스로 만드려면?AWS Docs를 따라 진행해보겠다. 가상환경 생성12345678% python3 -m venv aws% python3 --versionPython 3.12.3% source ./aws/bin/activate또는$ python3 -m venv venv37$ . venv37/bin/activate 관련 라이브러리 설치1234% python3 -m pip install chalice% chalice new-project gemma-inference-server# 이후 app.py 수정 directory로 들어가보면 아래와같이 여러 파일들이 생성되어있다. 1234567891011% ls -latotal 24drwxr-xr-x 9 leehamin staff 288 Jun 15 16:55 .drwxr-xr-x 63 leehamin staff 2016 Jun 15 16:50 ..drwxr-xr-x 3 leehamin staff 96 Jun 15 16:50 .chalice-rw-r--r-- 1 leehamin staff 37 Jun 15 16:50 .gitignoredrwxr-xr-x 9 leehamin staff 288 Jun 15 17:01 .ideadrwxr-xr-x 6 leehamin staff 192 Jun 15 16:51 .venvdrwxr-xr-x 3 leehamin staff 96 Jun 15 16:55 __pycache__-rw-r--r-- 1 leehamin staff 1427 Jun 15 16:52 app.py-rw-r--r-- 1 leehamin staff 31 Jun 15 16:54 requirements.txt 지금은 .chalice 디렉터리를 무시해도 된다. 우리가 집중할 두 가지 주요 파일은 app.py와 요구사항.txt입니다. Chalice 로컬 환경 테스트123% chalice local# localhost로 접근 시 기본 샘플 동작하는 것 확인 AWS CLI 설정configure sso123456789101112aws configure sso # SSO session name: (임의로 입력해도 되는 것으로 추정)# SSO start URL: (Access Keys에 있는 값 복사)# SSO Region: (Access Keys에 있는 값 복사)# SSO registration scopes: (기본 값이 있는 것 같아 Enter 누르고 넘김) # 이후 code를 입력하는 링크가 주어지는데, AWS 콘솔에 로그인 한 환경에서 해당 링크에 접속하여 해당 code 입력To use this profile, specify the profile name using --profile, as shown:aws s3 ls --profile [profile명] Access key 입력1234567# Access keys에 있는 값 복사하여 입력 (어느정도 시간이 지나면 키나 토큰값이 달라지는 것 같으므로 주의)export AWS_ACCESS_KEY_ID=&quot;&quot;export AWS_SECRET_ACCESS_KEY=&quot;&quot;export AWS_SESSION_TOKEN=&quot;&quot;# 가이드엔 없는데, deploy시 에러 발생하여 추가로 실행export AWS_DEFAULT_REGION=&quot;ap-northeast-2&quot; 배포1chalice deploy","link":"/2024/06/13/API-Gateway-Lambda/"},{"title":"AWS EC2에 pgvector 설치하고 접속하기","text":"AWS EC2에 PostgreSQL을 설치하고 접속해보겠다.사실 AWS에는 관계형 데이터베이스를 편하게 다룰 수 있는 서비스인 RDS를 제공하기는 하지만 과금이 많이 되는 경향도 있고 여러 이유로 PostgreSQL을 직접 EC2에 설치해서 사용하기로 했다. 1. postgresql 설치EC2에 접근하여 postgresql(이하 PG)를 설치한다. 12sudo amazon-linux-extras install postgresql10 epel -ysudo yum install postgresql-server postgresql-devel -y 에러 발생1234567891011121314151617181920212223242526272829303132333435363738Loaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00 One of the configured repositories failed (Unknown), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work &quot;fix&quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=&lt;repoid&gt; ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable &lt;repoid&gt; or subscription-manager repos --disable=&lt;repoid&gt; 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=&lt;repoid&gt;.skip_if_unavailable=trueCannot retrieve metalink for repository: epel/x86_64. Please verify its path and try again EC2가 yum 서버와 통신을 못해서 문제가 발생한 것이다.즉, EC2가 외 인터넷과 통신이 되지 않는 것이다. 이러한 경우 인터넷 게이트웨이를 생성하여 VPC및 서브넷과 연동해주면 된다. 참고 : https://garve32.tistory.com/63 AWS EC2 인스턴스에 파일 업로드 및 다운로드파일을 옮기기 위해 퍼블릭 IPv4 고정 IP 설정을 해준다. https://rypro.tistory.com/227 S3에 업로드한 pgvector 파일이용하여 설치하기S3와 EC2를 연동하는 방법은 다른 포스팅을 참고바란다. 12345678[root@ec2-ct01-dev-slm-app-03 ~]# aws s3 cp s3://aipin-bucket/pgvector.zip /rootdownload: s3://aipin-bucket/pgvector.zip to ./pgvector.zip[root@ec2-ct01-dev-slm-app-03 ~]# unzip /root/pgvector.zip -d /rootArchive: /root/pgvector.zip inflating: /root/README.md inflating: /root/docker-compose.yml extracting: /root/init_db.sql Docker 실행12$ sudo systemctl start docker$ sudo systemctl enable docker 1. amzn2extra 리포지토리 활성화123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566$ sudo amazon-linux-extras enable postgresql10 2 httpd_modules available [ =1.0 =stable ] 3 memcached1.5 available \\ [ =1.5.1 =1.5.16 =1.5.17 ] 6 *postgresql10=latest enabled [ =10 =stable ] 9 R3.4 available [ =3.4.3 =stable ] 10 rust1 available \\ [ =1.22.1 =1.26.0 =1.26.1 =1.27.2 =1.31.0 =1.38.0 =stable ] 18 libreoffice available \\ [ =5.0.6.2_15 =5.3.6.1 =stable ] 19 gimp available [ =2.8.22 ] 20 †docker=latest enabled \\ [ =17.12.1 =18.03.1 =18.06.1 =18.09.9 =stable ] 21 mate-desktop1.x available \\ [ =1.19.0 =1.20.0 =stable ] 22 GraphicsMagick1.3 available \\ [ =1.3.29 =1.3.32 =1.3.34 =stable ] 24 epel=latest enabled [ =7.11 =stable ] 25 testing available [ =1.0 =stable ] 26 ecs available [ =stable ] 27 †corretto8 available \\ [ =1.8.0_192 =1.8.0_202 =1.8.0_212 =1.8.0_222 =1.8.0_232 =1.8.0_242 =stable ] 32 lustre2.10 available \\ [ =2.10.5 =2.10.8 =stable ] 33 †java-openjdk11 available [ =11 =stable ] 34 lynis available [ =stable ] 36 BCC available [ =0.x =stable ] 37 mono available [ =5.x =stable ] 38 nginx1 available [ =stable ] 40 mock available [ =stable ] 43 livepatch available [ =stable ] 44 †python3.8 available [ =stable ] 45 haproxy2 available [ =stable ] 46 collectd available [ =stable ] 47 aws-nitro-enclaves-cli available [ =stable ] 48 R4 available [ =stable ] 49 kernel-5.4 available [ =stable ] 50 selinux-ng available [ =stable ] 52 tomcat9 available [ =stable ] 53 unbound1.13 available [ =stable ] 54 †mariadb10.5 available [ =stable ] 55 kernel-5.10 available [ =stable ] 56 redis6 available [ =stable ] 58 †postgresql12 available [ =stable ] 59 †postgresql13 available [ =stable ] 60 mock2 available [ =stable ] 61 dnsmasq2.85 available [ =stable ] 62 kernel-5.15 available [ =stable ] 63 †postgresql14 available [ =stable ] 64 firefox available [ =stable ] 65 lustre available [ =stable ] 66 †php8.1 available [ =stable ] 67 awscli1 available [ =stable ] 68 †php8.2 available [ =stable ] 69 dnsmasq available [ =stable ] 70 unbound1.17 available [ =stable ] 72 collectd-python3 available [ =stable ]* Extra topic has reached end of support.† Note on end-of-support. Use 'info' subcommand.Now you can install: # yum clean metadata # yum install postgresql 2. PostgreSQL 설치12345678910$ sudo yum --disablerepo=epel install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00No package postgresql10 available.No package postgresql10-server available.Error: Nothing to do 에러가 났다. amazon-linux-extras 리포지토리 확인123456789101112131415161718192021222324252627$ sudo amazon-linux-extras list# 출력에서 postgresql10이 활성화되어 있는지 확인합니다. 만약 활성화되어 있지 않다면, 다시 활성화.$ sudo amazon-linux-extras enable postgresql10# 리포지토리 캐시 갱신$ sudo yum clean all# 다시 설치$ sudo yum --disablerepo=epel install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00(1/9): amzn2-core/2/x86_64/group_gz | 2.7 kB 00:00:00(2/9): amzn2-core/2/x86_64/updateinfo | 935 kB 00:00:00(3/9): amzn2extra-epel/2/x86_64/primary_db | 1.8 kB 00:00:00(4/9): amzn2extra-postgresql10/2/x86_64/updateinfo | 55 B 00:00:00(5/9): amzn2extra-postgresql10/2/x86_64/primary | 15 kB 00:00:00(6/9): amzn2extra-docker/2/x86_64/primary_db | 102 kB 00:00:00(7/9): amzn2extra-epel/2/x86_64/updateinfo | 76 B 00:00:00(8/9): amzn2extra-docker/2/x86_64/updateinfo | 16 kB 00:00:00(9/9): amzn2-core/2/x86_64/primary_db | 68 MB 00:00:00No package postgresql10 available.No package postgresql10-server available.Error: Nothing to do 계속 epel이 문제인듯 하니 epel enabled = 0을 바꿔서 비활성화 시켜버림 123456789[epel]name=Extra Packages for Enterprise Linux 7 - $basearch#baseurl=http://download.fedoraproject.org/pub/epel/7/$basearchmirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchmetalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 yum cache 갱신12345678910111213141516171819202122232425$ sudo yum clean all$ sudo yum makecacheLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00(1/17): amzn2-core/2/x86_64/group_gz | 2.7 kB 00:00:00(2/17): amzn2-core/2/x86_64/updateinfo | 935 kB 00:00:00(3/17): amzn2-core/2/x86_64/filelists_db | 61 MB 00:00:00(4/17): amzn2-core/2/x86_64/primary_db | 68 MB 00:00:00(5/17): amzn2extra-docker/2/x86_64/filelists_db | 34 kB 00:00:00(6/17): amzn2extra-docker/2/x86_64/updateinfo | 16 kB 00:00:00(7/17): amzn2extra-docker/2/x86_64/primary_db | 102 kB 00:00:00(8/17): amzn2extra-docker/2/x86_64/other_db | 32 kB 00:00:00(9/17): amzn2-core/2/x86_64/other_db | 21 MB 00:00:00(10/17): amzn2extra-epel/2/x86_64/updateinfo | 76 B 00:00:00(11/17): amzn2extra-epel/2/x86_64/filelists_db | 882 B 00:00:00(12/17): amzn2extra-epel/2/x86_64/primary_db | 1.8 kB 00:00:00(13/17): amzn2extra-epel/2/x86_64/other_db | 507 B 00:00:00(14/17): amzn2extra-postgresql10/2/x86_64/updateinfo | 55 B 00:00:00(15/17): amzn2extra-postgresql10/2/x86_64/primary | 15 kB 00:00:00(16/17): amzn2extra-postgresql10/2/x86_64/filelists | 121 kB 00:00:00(17/17): amzn2extra-postgresql10/2/x86_64/other | 6.2 kB 00:00:00Metadata Cache Created 다시 설치1234$ sudo yum install postgresql10 postgresql10-serverLoaded plugins: extras_suggestions, langpacks, priorities, update-motdNo package postgresql10 available.No package postgresql1 여전히 안됨 명령어가 틀린듯1234567yum install postgresqlLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00Package postgresql-10.21-1.amzn2.0.1.x86_64 already installed and latest versionNothing to do 3. PostgreSQL 데이터베이스 초기화123$ sudo /usr/pgsql-10/bin/postgresql-10-setup initdb# 안먹음 계속 psql에 문제가 있어서 PostgreSQL을 설치하기 위해 필요한 RPM 파일을 인터넷이 되는 외부 시스템에서 다운로드한 후, 폐쇄망 서버로 전송하여 설치하는 방법으로 진행 docker-compose 설치12345678910$ curl -L &quot;https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 57.9M 100 57.9M 0 0 14.5M 0 0:00:03 0:00:03 --:--:-- 21.4M# 옮기기$ aws s3 cp s3://aipin-bucket/docker-compose /rootdownload: s3://aipin-bucket/docker-compose to ./docker-compose Docker Compose 파일을 적절한 위치로 이동시키고 실행 권한을 부여 12$ sudo mv docker-compose /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 잘 설치되었는지 확인 12$ docker-compose --versionDocker Compose version v2.20.2 docker-compose로 pgvector 올리기12$ docker-compose up -d[+] Running 1/1 postgre를 다시 다른 방법으로 설치해보자123456789$ aws s3 cp s3://aipin-bucket/postgresql-9.6.16.tar.gz /rootdownload: s3://aipin-bucket/postgresql-9.6.16.tar.gz to ../postgresql-9.6.16.tar.gz# 패키지 파일 압축해제 $ tar -zxvf postgresql-9.6.16.tar.gz# 설치 디렉토리는 디폴트로 함, --prefix=/usr/local/pgsql 이렇게 하는 경우도 있는듯$ ./configure --without-readline$ make &amp;&amp; make check$ make install PostgreSQL 사용자 및 디렉토리 설정12345sudo useradd postgressudo mkdir /usr/local/pgsql/datasudo chown postgres /usr/local/pgsql/datasudo mkdir /usr/local/pgsql/logssudo chown postgres /usr/local/pgsql/logs 데이터베이스 초기화1234567891011121314151617181920212223242526272829su - postgres[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/initdb -D /usr/local/pgsql/dataThe files belonging to this database system will be owned by user &quot;postgres&quot;.This user must also own the server process.The database cluster will be initialized with locale &quot;en_US.UTF-8&quot;.The default database encoding has accordingly been set to &quot;UTF8&quot;.The default text search configuration will be set to &quot;english&quot;.Data page checksums are disabled.fixing permissions on existing directory /usr/local/pgsql/data ... okcreating subdirectories ... okselecting default max_connections ... 100selecting default shared_buffers ... 128MBselecting default timezone ... Asia/Seoulselecting dynamic shared memory implementation ... posixcreating configuration files ... okrunning bootstrap script ... okperforming post-bootstrap initialization ... oksyncing data to disk ... okWARNING: enabling &quot;trust&quot; authentication for local connectionsYou can change this by editing pg_hba.conf or using the option -A, or--auth-local and --auth-host, the next time you run initdb.Success. You can now start the database server using: /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l logfile start PostgreSQL 서버 시작PostgreSQL 서버를 시작합니다. PostgreSQL 사용자로 전환하여 PostgreSQL 서버를 백그라운드에서 실행 12[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfile startserver starting PostgreSQL 서버 자동 시작 설정 (Optional)시스템 부팅 시 PostgreSQL 서버가 자동으로 시작되도록 설정할 수 있습니다. /etc/systemd/system/postgresql.service 파일을 생성하고 다음 내용을 추가합니다. 123456789101112131415sudo tee /etc/systemd/system/postgresql.service &lt;&lt;EOF[Unit]Description=PostgreSQL database serverAfter=network.target[Service]Type=forkingUser=postgresExecStart=/usr/local/pgsql/bin/pg_ctl start -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfileExecStop=/usr/local/pgsql/bin/pg_ctl stop -D /usr/local/pgsql/dataExecReload=/usr/local/pgsql/bin/pg_ctl reload -D /usr/local/pgsql/data[Install]WantedBy=multi-user.targetEOF PostgreSQL 접속1234567891011$ /usr/local/pgsql/bin/psqlpsql (9.6.16)Type &quot;help&quot; for help.# 비번 설정postgres=#postgres=# ALTER USER postgres WITH PASSWORD 'new1234';ALTER ROLE# 종료postgres=# \\q 잘 돌고있는지 확인1234567891011ps aux | grep postgresroot 16797 0.0 0.0 200768 3912 pts/1 S 22:04 0:00 su - postgrespostgres 16798 0.0 0.1 124744 4012 pts/1 S 22:04 0:00 -bashpostgres 17004 0.0 0.4 277808 16760 pts/1 S 22:06 0:00 /usr/local/pgsql/bin/postgres -D /usr/local/pgsql/datapostgres 17009 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: checkpointer processpostgres 17010 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: writer processpostgres 17011 0.0 0.0 277808 2748 ? Ss 22:06 0:00 postgres: wal writer processpostgres 17012 0.0 0.1 278208 5504 ? Ss 22:06 0:00 postgres: autovacuum launcher processpostgres 17013 0.0 0.0 132824 2300 ? Ss 22:06 0:00 postgres: stats collector processpostgres 17840 0.0 0.1 162296 3960 pts/1 R+ 22:14 0:00 ps auxpostgres 17844 0.0 0.0 119420 912 pts/1 S+ 22:14 0:00 grep --color=auto postgres 123456789101112[postgres@ec2-ct01-dev-slm-app-03 ~]$ /usr/local/pgsql/bin/psqlpsql (9.6.16)Type &quot;help&quot; for help.postgres=# SELECT version(); version----------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.16 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17),64-bit(1 row) 다시 앞의 docker-compose로 pgvector 올리기1234$ docker-compose up -d[+] Running 1/1 ✘ db Error 15.0sError response from daemon: Get &quot;https://registry-1.docker.io/v2/&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 흐음… postgres 종료$ sudo su - postgres Last login: Mon Jun 24 22:16:42 KST 2024 on pts/0 $ /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/logs/logfile stop waiting for server to shut down.... done server stopped - references https://inblog.ai/guri-tech-blog/ec2%EC%97%90-postgressql-96-%EC%84%A4%EC%B9%98-%ED%8C%A8%ED%82%A4%EC%A7%80%EC%84%A4%EC%B9%98-tar-7393 https://wldnjd2.tistory.com/m/95","link":"/2024/06/17/AWS-Install-Postgresql-On-EC2/"},{"title":"AWS S3 버킷과 EC2 Instance 연결","text":"프로젝트 배포에 사용한 ec2와 S3을 연동해 보았다. https://celdan.tistory.com/40 추가로 ec2-s3 역할에AmazonEC2FullAccess, AmazonSSMManagedInstanceCore, CloudWatchAgentServerPolicy이 세가지 정책도 추가하였다. 12$ aws s3 ls aipin-bucket2024-06-21 15:24:24 1185 pgvector.zip 이런식으로 s3에 업로드한 파일이 보인다. 12$ aws s3 cp s3://aipin-bucket/pgvector.zip /root$ unzip /root/pgvector.zip -d /root","link":"/2024/06/23/AWS-Connect-EC2-S3/"},{"title":"폐쇄망 EC2에 Docker 설치하기","text":"docker를 사용하려면 기본적으로 외부망(인터넷이 되는 환경)이 되는 환경이어야 하는데 폐쇄망에서 docker를 설치해야 하는 경우도 있다. 이럴 경우 어떻게 docker를 어떻게 설치하는지 알아보자. 1234567891011wget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/fuse3-libs-3.6.1-4.el7.x86_64.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpmwget http://mirror.centos.org/centos-7/7/extras/x86_64/Packages/slirp4netns-0.4.3-4.el7_8.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.6.21-3.1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-compose-plugin-2.18.1-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-buildx-plugin-0.10.5-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-cli-24.0.1-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-rootless-extras-24.0.2-1.el7.x86_64.rpmwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-24.0.1-1.el7.x86_64.rpm 이제 S3를 통해 EC2로 옮긴다 12345678910111213141516$ aws s3 cp s3://aipin-bucket/docker-installer.zip /rootdownload: s3://aipin-bucket/docker-installer.zip to ./docker-installer.zip$ unzip docker-installer.zip -d /rootArchive: docker-installer.zip creating: /root/docker-installer/ inflating: /root/docker-installer/docker-ce-cli-24.0.1-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-ce-rootless-extras-24.0.2-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-ce-24.0.1-1.el7.x86_64.rpm inflating: /root/docker-installer/docker-compose-plugin-2.18.1-1.el7.x86_64.rpm inflating: /root/docker-installer/slirp4netns-0.4.3-4.el7_8.x86_64.rpm inflating: /root/docker-installer/fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpm inflating: /root/docker-installer/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm inflating: /root/docker-installer/docker-buildx-plugin-0.10.5-1.el7.x86_64.rpm inflating: /root/docker-installer/fuse3-libs-3.6.1-4.el7.x86_64.rpm inflating: /root/docker-installer/containerd.io-1.6.21-3.1.el7.x86_64.rpm yum이 필요하니 설치123456789$ aws s3 cp s3://aipin-bucket/CentOS-7-x86_64-Everything-2009.iso /rootdownload: s3://aipin-bucket/CentOS-7-x86_64-Everything-2009.iso to ./CentOS-7-x86_64-Everything-2009.iso$ cd /etc/yum.repos.d$ ll$ mkdir backup$ mv *.repo backup/$ cd backup$ ll$ vi local_repository 12345[Cento OS7 Repository]name=CentOS Local Repositorybaseurl=file:///root/local_repo/CentOS-7/gpgcheck=0enabled=1 1234567891011$ yum clean allLoaded plugins: extras_suggestions, langpacks, priorities, update-motdThere are no enabled repos. Run &quot;yum repolist all&quot; to see the repos you have. To enable Red Hat Subscription Management repositories: subscription-manager repos --enable &lt;repo&gt; To enable custom repositories: yum-config-manager --enable &lt;repo&gt;$ yum repolistLoaded plugins: extras_suggestions, langpacks, priorities, update-motdrepolist: 0 https://pkgs.org/ 여기서 필요한 패키지를 다운로드 한다. S3를 통해 S3로 옮긴다. 12$ aws s3 cp s3://aipin-bucket/createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpm /root$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpm -i 옵션은 설치를 의미합니다. -v 옵션은 상세 출력을 의미합니다. -h 옵션은 진행 상태를 해시 마크로 표시합니다. 에러가 났다. 1234$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpmerror: Failed dependencies:createrepo_c-libs = 0.12.2-2.amzn2.0.2 is needed by createrepo_c-0.12.2-2.amzn2.0.2.x86_64libcreaterepo_c.so.0()(64bit) is needed by createrepo_c-0.12.2-2.amzn2.0.2.x86_64 의존성 문제를 해결하기 위해 필요한 패키지를 함께 설치해야 합니다. 이 경우에는 createrepo_c-libs와 libcreaterepo_c.so.0 라이브러리를 포함하는 패키지를 함께 설치해야 합니다. 마찬가지로 rpm 파일을 다운로드 후 s3 통해 EC2에 추가하였다. 12345678$ rpm -ivh createrepo_c-libs-0.12.2-2.amzn2.0.2.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:createrepo_c-libs-0.12.2-2.amzn2.################################# [100%]$ rpm -ivh createrepo_c-0.12.2-2.amzn2.0.2.x86_64.rpmPreparing... ################################# [100%]package createrepo_c-0.12.2-2.amzn2.0.2.x86_64 is already installed yum-utils 설치123$ aws s3 cp s3://aipin-bucket/yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm/rootdownload: s3://aipin-bucket/yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm to ./yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm$ rpm -ivh yum-utils-1.1.31-45.amzn2.0.1.noarch.rpm 다 때려치우고 amzn2extra-docker를 사용하여 Docker를 설치하겠다. 1. amzn2extra 리포지토리 활성화1$ sudo amazon-linux-extras install docker 하지만 에러가 났다 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647sudo amazon-linux-extras install dockerInstalling dockerLoaded plugins: extras_suggestions, langpacks, priorities, update-motdCleaning repos: amzn2-core amzn2extra-docker amzn2extra-epel amzn2extra-postgresql10 epel : nexusrepo12 metadata files removed0 sqlite files removed0 metadata files removedLoaded plugins: extras_suggestions, langpacks, priorities, update-motdamzn2-core | 3.6 kB 00:00:00amzn2extra-docker | 2.9 kB 00:00:00amzn2extra-epel | 3.0 kB 00:00:00amzn2extra-postgresql10 | 2.9 kB 00:00:00 One of the configured repositories failed (Unknown), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work &quot;fix&quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=&lt;repoid&gt; ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable &lt;repoid&gt; or subscription-manager repos --disable=&lt;repoid&gt; 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=&lt;repoid&gt;.skip_if_unavailable=trueCannot retrieve metalink for repository: epel/x86_64. Please verify its path and try againInstallation failed. Check that you have permissions to install. 현재 문제는 epel 리포지토리가 활성화되어 있지만 해당 리포지토리에 접근할 수 없어서 발생하는 것이다. 일시적으로 epel 리보지토리를 비활성화하고 진행하곘다. 1$ sudo yum --disablerepo=epel install docker 잘 설치되었다. 2. Docker 서비스 시작 및 자동 시작 설정Docker가 설치된 후, Docker 데몬을 시작하고 시스템 부팅 시 자동으로 시작되도록 설정해야 한다. 12$ sudo systemctl start docker$ sudo systemctl enable docker 3. Docker 버전 확인12$ docker --versionDocker version 25.0.3, build 4debf41 4. 현재 사용자에게 Docker 권한 부여기본적으로 Docker 명령어는 루트 사용자 권한이 필요하다.그러나 일반 사용자로 Docker를 사용하려면 해당 사용자를 docker 그룹에 추가해야 한다. 1sudo usermod -aG docker $USER 이 명령어를 실행한 후, 변경 사항을 적용하려면 로그아웃했다가 다시 로그인해야 한다. 5. Docker 설치 확인1docker run hello-world references:https://dev-luna-archive.tistory.com/36https://oingdaddy.tistory.com/134https://velog.io/@hognod/Docker-Install-Offlinehttps://finai.tistory.com/2","link":"/2024/06/23/AWS-Install-Docker-On-EC2/"},{"title":"Mac에서 yum 사용하기 (Docker 에서 리눅스 사용)","text":"Docker를 사용하면 Linux 환경을 Mac에서 쉽게 사용할 수 있습니다. 이를 통해 yumdownloader를 실행할 수 있습니다. Docker 설치: Mac에 Docker가 설치되어 있어야 합니다. Docker for Mac을 설치합니다. CentOS Docker 컨테이너 실행: 12코드 복사docker run -it centos:latest /bin/bash yum-utils 설치 및 yumdownloader 사용:12yum install -y yum-utilsyumdownloader &lt;패키지명&gt; 다운로드한 패키지를 Mac으로 복사:다운로드한 파일을 Docker 컨테이너에서 Mac으로 복사합니다. 1docker cp &lt;container_id&gt;:/path/to/downloaded/rpm /path/to/local/directory","link":"/2024/06/24/Linux-Use-In-Mac/"},{"title":"AWS EC2에 remote yum repository 설정하기","text":"CentOS 리눅스 시스템에서 사용되는 YUM 리포지토리 설정 파일의 예 리포지토리 설정 파일 생성먼저, 제공된 설정을 리포지토리 설정 파일에 저장해야 합니다. 이 파일은 일반적으로 /etc/yum.repos.d/ 디렉토리에 위치합니다. 예를 들어, 파일 이름을 nexus.repo로 할 수 있습니다. 1sudo vi /etc/yum.repos.d/nexus.repo 1234567[nexusrepo]name=Nexus Repositorybaseurl=https://{id}:{pw}{remote-repo-url}/repository/yum-dspace-group/7/x86_64enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-amazon-linux-2priority=1 YUM 캐시 클리어YUM 캐시를 클리어하여 최신 리포지토리 정보를 불러옵니다. 1sudo yum clean all 리포지토리 리스트 확인리포지토리가 제대로 추가되었는지 확인합니다. 1sudo yum repolist enabled 이 명령은 활성화된 모든 리포지토리의 리스트를 보여줍니다. nexusrepo가 리스트에 나타나야 합니다. 패키지 검색 테스트nexusrepo 리포지토리에서 특정 패키지를 검색해보아 테스트 할 수 있습니다. 1sudo yum --disablerepo=&quot;*&quot; --enablerepo=&quot;nexusrepo&quot; list available 패키지 설치 테스트리포지토리에서 패키지를 설치하여 테스트할 수 있습니다. 예를 들어 example-package를 설치하려면 다음 명령을 사용합니다. 1sudo yum --disablerepo=&quot;*&quot; --enablerepo=&quot;nexusrepo&quot; install example-package 이렇게 함으로써, 리포지토리 설정이 올바르게 작동하는지 확인할 수 있습니다. 패키지 이름은 해당 리포지토리에서 제공하는 실제 패키지 이름으로 교체해야 합니다.","link":"/2024/06/25/AWS-Change-Yum-Repo/"},{"title":"AWS EKS에 FastAPI 프로젝트 배포하기","text":"AWS EKS에 프로젝트를 배포하려면 Docker 이미지를 만들어야 한다. 이를 위해 Dockerfile을 작성하고, Docker 이미지를 빌드 및 푸시한 후, Kubernetes 매니페스트 파일을 사용해 EKS에 배포하는 과정을 따르게 된다. 단계1 : Dockerfile 작성1234567891011121314151617181920212223242526# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# Poetry 설치RUN pip install poetry# 로컬 패키지 설치 (project-name_beta_~.whl 위치를 /app/library로 복사)COPY ../aipin_library/aipin_beta_~.whl /app/library/RUN poetry add /app/library/aipin_beta_~.whl# 프로젝트 파일 복사COPY . .# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 단계 2: Docker 이미지 빌드위의 Dockerfile을 프로젝트 루트 디렉토리에 저장한 후, Docker 이미지를 빌드합니다. 터미널에서 다음 명령어를 실행한다. 1docker build -t my-project:latest . 하지만 에러가 난다. 12345678910111213141516171819202122% docker build -t aipin-orchestrator:latest .[+] Building 1.9s (9/12) =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 765B 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/python:3.9-slim 1.8s =&gt; CANCELED [1/8] FROM docker.io/library/python:3.9-slim@sha256:451832461e0c1587078511b0a6ad35c3c0a0106 0.0s =&gt; =&gt; resolve docker.io/library/python:3.9-slim@sha256:451832461e0c1587078511b0a6ad35c3c0a010656b660f4f 0.0s =&gt; =&gt; sha256:ac14bdcdeeab6f08dc915a5c5971f998797127c47e2e860ace34090b3886746e 1.94kB / 1.94kB 0.0s =&gt; =&gt; sha256:b4045d7da52ebacb256e6843ae8b6eaedca3fa0486f343d4916e52e8e7320cab 6.88kB / 6.88kB 0.0s =&gt; =&gt; sha256:451832461e0c1587078511b0a6ad35c3c0a010656b660f4f26ebfac0e723f7bf 10.41kB / 10.41kB 0.0s =&gt; [internal] load build context 0.0s =&gt; =&gt; transferring context: 1.59kB 0.0s =&gt; CACHED [2/8] WORKDIR /app 0.0s =&gt; CACHED [3/8] RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends build-essential 0.0s =&gt; CACHED [4/8] RUN pip install poetry 0.0s =&gt; ERROR [5/8] COPY ../aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/ 0.0s------ &gt; [5/8] COPY ../aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/:------failed to compute cache key: &quot;/aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl&quot; not found: not found 이 상황에서 Docker가 컨텍스트 외부 파일을 찾지 못하는 문제는 Docker의 기본 동작과 관련이 있다. Docker는 빌드 컨텍스트 외부에 있는 파일을 복사할 수 없다. 이를 해결하기 위해 두 가지 접근 방식을 사용할 수 있다 Docker 빌드 컨텍스트를 변경하여 상위 디렉토리를 포함 빌드 컨텍스트 내에 필요한 파일을 복사 필자는 두 번째 방법을 사용하여 Docker 빌드가 성공하도록 한다. 하지만 이번에는 아래와 같은 에러가 난다. 12 &gt; [4/8] RUN pip install poetry:#7 1.579 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1133)'))': /simple/poetry/ 이 문제는 SSL 인증서 문제로 인해 pip이 패키지를 설치하지 못하는 경우이다. Docker 이미지 안에서 SSL 인증서를 제대로 인식하지 못하는 경우가 종종 있다. PIP 환경변수 설정인증서 검사를 비활성화하도록 환경 변수를 설정할 수 있다. 이는 보안상 좋은 방법은 아니지만, 일시적인 해결책이 될 수 있다. 123456789101112131415161718192021222324252627282930# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 프로젝트 파일 복사COPY . .# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 하지만 아래와 같은 에러가난다. 1234567 ERROR [6/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl 0.6s ------ &gt; [6/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl:#10 0.507 #10 0.507 Poetry could not find a pyproject.toml file in /app or its parents------executor failed running [/bin/sh -c poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl]: exit code: poetry는 pyproject.toml 파일이 있는 디렉토리에서 실행되어야 한다. Dockerfile에서 poetry를 실행하기 전에 pyproject.toml 파일이 있는 디렉토리로 이동해야 한다. 현재 Dockerfile의 구조를 보면, pyproject.toml 파일은 /app 디렉토리로 복사되기 전에 poetry add 명령이 실행되고 있는 것 같다. 이를 해결하려면 다음과 같이 수정할 수 있다. 1234567891011121314151617181920212223242526272829303132# 베이스 이미지로 Python 3.9 사용FROM python:3.9-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 프로젝트 파일 복사 (pyproject.toml 파일 포함)COPY . .# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/# Poetry를 사용하여 로컬 패키지 추가RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 이렇게 바꿔서 실행하면… 이번엔 이런 에러가 나타난다. 12345678910 ERROR [7/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl 0.7s------ &gt; [7/8] RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl:#11 0.623 Path /aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl for aipin-beta-2024 does not exist#11 0.644 The currently activated Python version 3.9.19 is not supported by the project (^3.10).#11 0.644 Trying to find and use a compatible version. #11 0.677 #11 0.677 Poetry was unable to find a compatible version. If you have one, you can explicitly use it via the &quot;env use&quot; command.------executor failed running [/bin/sh -c poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl]: exit code: 1 이 에러는 두 가지 문제를 나타낸다: pyproject.toml 파일에서 Python 버전이 ^3.10로 설정되어 있고, 현재 Docker 이미지에서 사용 중인 Python 버전은 3.9.19이다. Poetry가 해당 .whl 파일을 찾지 못하고 있다. 이 문제들을 해결하기 위해 다음과 같이 진행한다 Python 버전 변경: Dockerfile에서 Python 3.10 이미지를 사용하도록 변경한다. Poetry 환경 설정: Poetry가 Python 버전을 강제로 사용하도록 설정합니다. 파일 경로 문제 해결: COPY 명령어의 경로를 올바르게 설정합니다. 1234567891011121314151617181920212223242526272829303132333435# 베이스 이미지로 Python 3.10 사용FROM python:3.10-slim# 작업 디렉토리 설정WORKDIR /app# 시스템 패키지 업데이트 및 필수 패키지 설치RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ ca-certificates \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# 인증서 문제 해결을 위한 환경 변수 설정ENV PIP_CERT /etc/ssl/certs/ca-certificates.crt# Poetry 설치RUN pip install poetry --trusted-host pypi.org --trusted-host files.pythonhosted.org# 프로젝트 파일 복사 (pyproject.toml 파일 포함)COPY . .# 로컬 패키지 설치 (aipin_beta_~.whl 위치를 /app/library로 복사)COPY aipin_library/aipin_beta_2024-0.1.25-py3-none-any.whl /app/library/# Poetry 환경 설정: Python 3.10 사용 강제RUN poetry env use python3.10# Poetry를 사용하여 로컬 패키지 추가RUN poetry add /app/library/aipin_beta_2024-0.1.25-py3-none-any.whl# 스크립트 실행 권한 부여RUN chmod +x start.sh# 컨테이너 시작 시 실행할 명령어CMD [&quot;./start.sh&quot;] 이렇게 수정하여 이미지를 빌드하였고, 이미지가 잘 만들어졌다. ECR애 Docker 이미지 푸시단계1:AWS CLI 설정먼저 AWS CLI가 설정되어 있어야 한다. 설정되지 않았다면, 다음 명령을 사용하여 AWS CLI를 설정한다. 단계2:ECR 리포지토리 생성단계3: 도커 로그인ECR에 로그인해야 한다. 다음 명령을 사용하여 로그인힌다. 123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 단계4: Docker 이미지 푸시12345# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latest# Docker 이미지 푸시docker push &lt;your-dockerhub-username&gt;/my-project:latest Cloud9 구성하기1234$ curl --silent --location &quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz&quot; --output eksctl.tar.gz$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.22.6/bin/linux/amd64/kubectl$ curl -LO https://github.com/derailed/k9s/releases/download/v0.26.7/k9s_Linux_x86_64.tar.gz$ curl -sSL -o ~/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 위 패키지들 S3에 업로드 후 Cloud9 으로 옮기기 12$ aws s3 cp s3://aipin-bucket/eksctl_Linux_amd64.tar.gz .... 나머지도 비슷하게 옮기기 1234567891011$ tar -zxvf eksctl_Linux_amd64.tar.gz$ sudo chmod +x eksctl$ sudo mv eksctl /usr/local/bin/$ tar -zxvf k9s_Linux_x86_64.tar.gz$ sudo chmod +x k9s$ sudo mv k9s /usr/local/bin/$ chmod +x kubectl$ sudo mv kubectl /usr/local/bin/kubectl$ aws eks update-kubeconfig --name [클러스터 명 예 : eks-prod-ct01-tap-01] --region [region명 예 : ap-northeast-2]$ sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd$ rm argocd-linux-amd64 Cloud9 -&gt; EKS 접근 시 Token invalid 에러12$ aws eks update-kubeconfig --region [region명 예 : ap-northeast-2] --name [클러스터 명 예 : eks-prod-ct01-tap-01]An error occurred (UnrecognizedClientException) when calling the DescribeCluster operation: The security token included in the request is invalid 해결책Cloud9 우측 최상단의 톱니바퀴 클릭 &gt; AWS Settings &gt; Credential 부분을 X로 변경 &gt; 터미널 종료 후 다시 시도 namespace 생성12$ kubectl create ns aipinnamespace/aipin created Kubernetes 매니페스트 파일 작성이제 EKS에 배포하기 위한 Kubernetes 매니페스트 파일을 작성한다. deployment.yaml과 service.yaml 파일을 준비한다. AWS의 Elastic Container Registry에서 이미지 탭에 들어가 배포할 이미지 목록 리스트를 선택 후 URL 복사를 눌러 이미지 태그를 복사한다. ex) 058264433760.dkr.ecr.ap-northeast-2.amazonaws.com/aipin-orchestrator:latest Cloud9로 돌아가서 생성한 유저폴더로 이동하여 Deployment.yaml과 Service.yaml 파일을 생성한다. nano 명령어를 이용하여 deployment.yaml 파일을 생성한다. 1vim aipin-orchestrator.deployment.yaml 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name: aipin labels: app: aipin-orchestrator namespace: aipinspec: replicas: 3 selector: matchLabels: app: aipin-orchestrator template: metadata: labels: app: aipin-orchestrator spec: containers: - image: 058264433760.dkr.ecr.ap-northeast-2.amazonaws.com/aipin-orchestrator:latest imagePullPolicy: Always name: aipin-orchestrator ports: - containerPort: 8080 protocol: TCP 동일하게 service.yaml 파일을 생성한다. 1$ vim aipin-service.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: aipin-orchestrator annotations: alb.ingress.kubernetes.io/healthcheck-path: &quot;/healthy&quot; namespace: orchestratorspec: selector: app: aipin-orchestrator type: NodePort ports: - port: 80 protocol: TCP targetPort: 8080 파일 생성 후 apply 명령어를 이용해 pod를 생성하여 등록 및 배포를 진행한다. 1234$ kubectl apply -f aipin-orchestrator.deployment.yaml deployment.apps/aipin created$ kubectl apply -f aipin-orchestrator.service.yamlservice/aipin-orchestrator created 생성 후 이전에 만들어놓은 네임스페이스를 이용해 파드들을 조회해보겠다. 1kubectl get all -n aipin 배포 시 libpq 에러kubectl apply로 배포 시 아래와 같은 에러가 난다. 1234567891011121314151617181920212223242526272829303132333435363738394041Installing dependencies from lock fileNo dependencies to install or updateInstalling the current project: orchestrator (0.1.0)Traceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import controller File &quot;/app/src/controller.py&quot;, line 2, in &lt;module&gt; from orchestrator import orchestrator File &quot;/app/src/orchestrator.py&quot;, line 1, in &lt;module&gt; from aipin.Plugin.plugin_manager import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/__init__.py&quot;, line 1, in &lt;module&gt; from .orchestrator import Orchestrator File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/orchestrator.py&quot;, line 1, in &lt;module&gt; from aipin.Plugin import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/__init__.py&quot;, line 2, in &lt;module&gt; from .plugin_manager import PluginManager File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/plugin_manager.py&quot;, line 3, in &lt;module&gt; from aipin.Plugin.plugin import * File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/Plugin/plugin.py&quot;, line 5, in &lt;module&gt; from aipin.data import ChatHistory, ChatRequest File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/data/__init__.py&quot;, line 3, in &lt;module&gt; from .vector_retriever import PGVectorRetriever, PGVectorConfig File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/aipin/data/vector_retriever.py&quot;, line 2, in &lt;module&gt; from langchain_postgres import PGVector File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain_postgres/__init__.py&quot;, line 3, in &lt;module&gt; from langchain_postgres.chat_message_histories import PostgresChatMessageHistory File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain_postgres/chat_message_histories.py&quot;, line 13, in &lt;module&gt; import psycopg File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/__init__.py&quot;, line 9, in &lt;module&gt; from . import pq # noqa: F401 import early to stabilize side effects File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/pq/__init__.py&quot;, line 118, in &lt;module&gt; import_from_libpq() File &quot;/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/psycopg/pq/__init__.py&quot;, line 110, in import_from_libpq raise ImportError(ImportError: no pq wrapper available.Attempts made:- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'- couldn't import psycopg 'python' implementation: libpq library not found 로그에 나타난 에러 메시지를 분석해 보면, 문제는 psycopg 패키지가 필요한 libpq 라이브러리를 찾을 수 없기 때문에 발생하는 것으로 보인다.이는 PostgreSQL과 상호작용하는 Python 라이브러리인 psycopg가 제대로 설치되지 않았거나, 필요한 시스템 종속성이 누락된 경우에 발생한다. 해결법기본 이미지에 libpq 설치 libpq 라이브러리가 누락된 경우, 이를 설치해 주어야 합니다. Dockerfile에 해당 라이브러리를 설치하는 명령을 추가한다. 1 포트포워딩 진행참고https://greenhead.blog/blog/kubernetes/2024-04-27/https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/setting-up.htmlhttps://velog.io/@judemin/EKS-CICD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B5%AC%EC%B6%95https://devnoong.tistory.com/entry/MiniKube-%EC%8B%A4%EC%8A%B5-Cloud9-ECR-%EC%97%85%EB%A1%9C%EB%93%9C-%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0#article-4--3--deployment-yaml--service-yaml-%ED%8C%8C%EC%9D%BC-%EC%83%9D%EC%84%B1-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-(%EC%83%88-%EC%9C%A0%EC%A0%80-%EA%B6%8C%ED%95%9C%EC%9C%BC%EB%A1%9C-%EC%A7%84%ED%96%89)","link":"/2024/07/03/AWS-Deploy-FastAPI-project-On-EKS/"},{"title":"MS-AutoGen","text":"AutoGen은 서로 대화하여 작업을 해결할 수 있는 여러 에이전트를 사용하여 LLM 애플리케이션을 개발할 수 있는 프레임워크이다. AutoGen 에이전트는 사용자 정의가 가능하고 대화가 가능하며 인간의 참여를 원활하게 허용한다. LLM, 인간 입력 및 도구의 조합을 사용하는 다양한 모드에서 작동할 수 있다. AutoGen은 최소한의 노력으로 다중 에이전트 대화에 기반한 차세대 LLM 애플리케이션을 구축할 수 있게 해준다. 복잡한 LLM 워크플로의 오케스트레이션, 자동화 및 최적화를 간소화한다. LLM 모델의 성능을 극대화하고 약점을 극복한다. 복잡한 워크플로에 대한 다양한 대화 패턴을 지원한다. 사용자 정의 가능하고 대화 가능한 에이전트를 통해 개발자는 AutoGen을 사용하여 대화 자율성, 에이전트 수 및 에이전트 대화 토폴로지에 관한 광범위한 대화 패턴을 구축할 수 있다. 다양한 복잡성을 가진 작업 시스템 모음을 제공한다. 이러한 시스템은 다양한 도메인과 복잡성의 광범위한 애플리케이션을 포괄한다. 이는 AutoGen이 다양한 대화 패턴을 쉽게 지원할 수 있는 방법을 보여준다. Bert NER + 의도 분류멀티턴 chatbot NLU + DST Multi Agent Framework AutoGen, langgraph AutoGen (autogen Studio) AutoGen 플로우로 langchain 똑같이 agent를 만들어봐야 하지 않을까 promptFlow () AI Studio AI Agent Builder 등 의도 변환 리서치 AutoGen vs Langgraph","link":"/2024/07/02/MS-AutoGen/"},{"title":"Microsoft Build 2024","text":"OpenAISLMsHuggingFacePhi-3 키노트 주요 영상 1 - ChatGPT 4o를 사용한 실시간 AICCChatGPT 4o와 대화를 함. 신발도 보여줌. 구매 내역이나 보여준 신발에 대하여 여러가지 데이터를 조회해서 답변을 함. 영어 - 스페인어 전환 시 chat GPT도 언어 전환 키노트 주요 영상 2 - Copilot + PC의 On-device 멀티 모달 기능Copliot PC는 인터넷이 끊겨도 작동 가능한 On-device 마인크래프트 게임 중 Copilot에게 게임 방법 물어보고, 가이드 받으며 게임 진행. 키노트 주요 영상 3 - Sovereign Cloud, 각국 AI Cloud Center 투자Multiple LLM + AI Eco.ND MI300X V5NPU에 대한 네트워크 위에 있는 소프트웨어 스펙과 칩에 대한 설명브라우저에서 llm이 돌아가는 기술에 대한 설명 Copilot+PC : 온디바이스에서 어떻게 돌아가는지 Azure AI model breadthCopilot StudioMicrosoft Copilot, Copilot + PC, Copilot stack 추후 공개될 기능들. AI Studio 내 ChatGPT-4o Open, ChatGPT 4 Fine Tuning 지원 등 ChatGPT 4o Open Phi-3-Vision 128K instruct Open Microsoft CopilotTeams의 Copilot에게 질문함 내 보스의 최근 활동 follow up 해줘 Copilot stack 산업, B2B, 특정기업 특화 Copilot을 위한 Data / API 연동 방법들 Declarative Copilots context를 이해함 Custom Copilot을 어떻게 만드는가 custom knowledge 추가 (confluence 등) 운영계 DB등을 등록 특정 agent 만든다고 할때, 기 등록된 knowledge 중 필요한 것 선택 action 선택 (결제, 팀즈 알람 등) deploy to channel Add Trigger (어떤 이벤트 발생 시 먼저 말을 거는 것도 가능) Agent Building, 프롬프트 엔지니어링 Prompt &amp; Agent, Rule base 분기 RAG, Image + Policy 연동 분기 지원 Multiple Agent RAG, Agent, FunctionCall 에이전트를 만드는 노코드 툴 AutoGen Document AI &amp; RAG 표 인식 가능 문서를 OCR로 가져올 수 있음 필기체도 가져올 수 있음 AI Security SovereigntyLLM에서의 새로운 보안 위협Prompt shields prompt define시 앞뒤로 define 하는 것 jailbreak 공격 block 및 모니터링 Purview / Defender DRM 걸린 파일들 권한 관리 등등 어떤식으로 핸들링하는지에 대한 SDK File SDK 엑셀파일 특정 칼럼 암호화 등 Respoinsible AI - AI Red Teaming Code level Github 주소 있음 Input Filter Output Filter Azure AI Content Safety Groundeness detection Evaluation &amp; Monitoring Data Monitoringm Incidents monitoring [결론]Apple / Microsoft의 AI 전략에서 바라보는 시사점 Multiple LLM으로 근본적인 안정성 보장 Server Big LLM과 Task sLM의 조화 직원용 PC, Kiosk, ATM, Edge Device … 주지할 것들 : 바깥 세상의 기술의 속도에 발 맞추어, 홀로 경쟁이 아닌, 그들과 발 맞추고 올라타야 하는 시점opensource 활용, partner Eco 활용 자체 AI Product + Total AI MSP","link":"/2024/07/14/Microsoft-Build/"},{"title":"FastAPI를 이용하여 llm 모델 서빙하는 서비스 EC2에 배포하기","text":"fastAPI란 파이썬 3.6부터 제공되는 트랜디하고 높은 성능을 가진 파이썬 프레임워크이다. t5-small 모델 서빙하는 서버 테스트 server.py12345678910111213141516171819202122232425262728293031323334353637383940from fastapi import FastAPIfrom fastapi.responses import RedirectResponsefrom langserve import add_routesfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipelinefrom transformers import pipelinefrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom starlette.requests import Requestfrom starlette.responses import JSONResponse, Responsefrom dotenv import load_dotenvfrom transformers import AutoTokenizer, pipelineimport torchimport requestsapp = FastAPI()@app.get(&quot;/&quot;)async def redirect_root_to_docs(): return RedirectResponse(&quot;/docs&quot;)# T5 모델 사용pipe = pipeline(&quot;text2text-generation&quot;, model=&quot;t5-small&quot;)model = HuggingFacePipeline(pipeline=pipe)add_routes( app, model, path=&quot;/t5-small&quot;,)@app.get(&quot;/plugin/test&quot;)async def test_plugin(): return &quot;success&quot;if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=5002) Dockerfile123456789101112131415161718192021222324252627# 베이스 이미지 설정FROM python:3.11-slim# 작업 디렉토리 설정WORKDIR /app# 필요한 시스템 패키지 설치RUN apt-get clean &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# requirements 파일들 및 미리 다운로드한 패키지 복사COPY requirements1.txt .COPY requirements2.txt .COPY packages /app/packages# 첫 번째 requirements 파일 설치RUN pip install --no-cache-dir -r requirements1.txt --default-timeout=300 -i https://pypi.tuna.tsinghua.edu.cn/simple# 두 번째 requirements 파일 설치 (미리 다운로드한 패키지 포함)RUN pip install --no-cache-dir -r requirements2.txt --find-links=/app/packages --default-timeout=300 --extra-index-url https://pypi.nvidia.com# 애플리케이션 소스 코드 복사COPY . .# FastAPI 서버 실행CMD [&quot;uvicorn&quot;, &quot;app.server:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;5002&quot;] Requirements1.txt123456789fastapiuvicornlangservelangchain-communitylangchain-openailangchain-coretransformerspython-dotenvrequests Requirements2.txt123torchnvidia-cudnn-cu12sse_starlette 이미지 빌드하기1% docker build -t lm-test-server:20240724 . 도커로 실행하기123456789101112131415161718192021222324252627% docker run --name lm-test-server -p 5002:5002 lm-test-server:20240724/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`. warn_deprecated(INFO: Started server process [1]INFO: Waiting for application startup.INFO: Application startup complete.INFO: Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit) __ ___ .__ __. _______ _______. _______ .______ ____ ____ _______| | / \\ | \\ | | / _____| / || ____|| _ \\ \\ \\ / / | ____|| | / ^ \\ | \\| | | | __ | (----`| |__ | |_) | \\ \\/ / | |__| | / /_\\ \\ | . ` | | | |_ | \\ \\ | __| | / \\ / | __|| `----./ _____ \\ | |\\ | | |__| | .----) | | |____ | |\\ \\----. \\ / | |____|_______/__/ \\__\\ |__| \\__| \\______| |_______/ |_______|| _| `._____| \\__/ |_______|LANGSERVE: Playground for chain &quot;/openai/&quot; is live at:LANGSERVE: │LANGSERVE: └──&gt; /openai/playground/LANGSERVE:LANGSERVE: Playground for chain &quot;/t5-small/&quot; is live at:LANGSERVE: │LANGSERVE: └──&gt; /t5-small/playground/LANGSERVE:LANGSERVE: See all available routes at /docs/LANGSERVE: ⚠️ Using pydantic 2.8.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details. 브라우저로 접속하여 테스트하기http://localhost:5002/docs로 접속하여 테스트한다. EC2에 서비스 올리기EC2에 도커 설치12345678# sudo yum update -y# amazon-linux-extras install docker -y# service docker startRedirecting to /bin/systemctl start docker.service# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.# usermod -aG docker $USER# newgrp docker 이미지 옮기기ECR에 도커이미지를 push 한다. 도커 로그인123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 도커 이미지 푸시1234# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latestdocker push &lt;your-dockerhub-username&gt;/my-project:latest EC2에서 이미지 pull 하기1234567$ sudo yum install aws-cli -y$ aws configure$ export AWS_ACCESS_KEY_ID=[access_key_id]$ export AWS_SECRET_ACCESS_KEY=[aws_secret_access_key]$ export AWS_SESSION_TOKEN=[aws_session_token]$ aws ecr get-login-password --region [region] | docker login --username AWS --password-stdin [id].dkr.ecr.[region].amazonaws.com$ docker pull [id].dkr.ecr.[region].amazonaws.com/t5-small-fastapi:20240727 배포하는 EC2에 외부 인터넷이 연동되지 않기에 모델을 다운로드받아 이미지 업로드download_model.py 12345678910111213# download_model.pyfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizermodel_name = &quot;t5-small&quot;# 모델과 토크나이저 다운로드model = AutoModelForSeq2SeqLM.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)# 모델 저장model.save_pretrained(&quot;./model&quot;)tokenizer.save_pretrained(&quot;./model&quot;) 1% brew install transformers 도커 배포 시 참고 사항start.sh 스크립트에 Python 애플리케이션을 백그라운드에서 실행하도록 설정하면 Docker 컨테이너가 종료되는 원인이 될 수 있다. 따라서, 애플리케이션이 포그라운드에서 실행되도록 수정해야 한다. 수정된 start.sh1234567#!/bin/sh# Install dependenciespoetry install# Run the application in the foregroundpoetry run python3 src/main.py Docker 빌드 중에 “No space left on device” 오류Docker 빌드 중에 “No space left on device” 오류가 발생하는 경우, 이는 Docker 데몬이 실행되고 있는 호스트 머신의 디스크 공간이 부족하기 때문에 발생하는 문제 불필요한 Docker 이미지 및 컨테이너 삭제1234567891011# 중지된 모든 컨테이너 삭제docker container prune -f# 사용되지 않는 모든 이미지 삭제docker image prune -a -f# 사용되지 않는 모든 네트워크 삭제docker network prune -f# 사용되지 않는 모든 볼륨 삭제docker volume prune -f IPv4 바인딩 에러12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455Installing dependencies from lock fileNo dependencies to install or updateInstalling the current project: orchestrator (0.1.0)/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/langchain/chat_models/__init__.py:31: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:`from langchain_community.chat_models import ChatOpenAI`.To install langchain-community run `pip install -U langchain-community`. warnings.warn(2024-07-29 11:42:49,494 - WARNING - WARNING! max_length is not default parameter. max_length was transferred to model_kwargs. Please make sure that max_length is what you intended.2024-07-29 11:42:49,504 - DEBUG - Starting new HTTPS connection (1): huggingface.co:4432024-07-29 11:42:49,738 - DEBUG - https://huggingface.co:443 &quot;GET /api/whoami-v2 HTTP/11&quot; 200 7572024-07-29 11:42:49,740 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False2024-07-29 11:42:49,741 - DEBUG - load_verify_locations cafile='/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/certifi/cacert.pem'2024-07-29 11:42:49,771 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False2024-07-29 11:42:49,772 - DEBUG - load_verify_locations cafile='/root/.cache/pypoetry/virtualenvs/orchestrator-9TtSrW0h-py3.10/lib/python3.10/site-packages/certifi/cacert.pem'INFO: Started server process [24]INFO: Waiting for application startup.INFO: Application startup complete.ERROR: [Errno 99] error while attempting to bind on address ('::1', 8002, 0, 0): cannot assign requested addressINFO: Waiting for application shutdown.INFO: Application shutdown complete.The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: fineGrained).Your token has been saved to /root/.cache/huggingface/tokenLogin successful main.py 수정src/main.py 파일에서 서버를 실행하는 부분을 확인하고, host 매개변수를 0.0.0.0으로 설정","link":"/2024/07/24/fastAPI/"},{"title":"Amazon Cognito 이용하여 인증기능 구현하기.","text":"AWS Cognito는 AWS에서 제공하는 인증 및 권한 부여 서비스이다. AWS Cognito를 사용하면 애플리케이션의 사용자 인증, 사용자 데이터 동기화 및 액세스 제어를 쉽게 관리할 수 있다. 이를 통해 개발자는 사용자 등록, 로그인, 비밀번호 복구 등과 같은 기능을 간단하게 구현할 수 있다. AWS Cognito는 다음과 같은 주요 기능을 제공한다. 사용자 풀(User Pools) : 사용자 풀은 사용자 등록, 로그인 및 계정 관리와 관련된 기능을 제공한다. 이를 통해 사용자가 애플리케이션에 둥록하고 로그인할 수 있으며, 소셜 로그인을 포함한 다양한 인증 방법을 지원한다. 아이덴티티 풀(Identity Pools) : 사용자 풀에 저장된 정보를 바탕으로 로그인 또는 회원가입에 성공한 사용자에게 AWS 인프라의 여러 서비스에 대한 권한을 부여할 수 있는 서비스. 연동 소셜 로그인 : AWS Cognito는 페이스북, 구글, 애플 등과 같은 소셜 로그인 기능을 제원하여 사용자가 소셜 미디어 계정으로 로그인할 수 있게 한다. AWS Cognito를 사용하면 사용자 인증 및 권한 관리를 보다 쉽게 구현할 수 있으며, 이와 관련된 보안 문제를 AWS에서 관리해주기 때문에 개발자는 애플리케이션 로직에 더 집중할 수 있다. Cognito Token 이용하기출처 : https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html 유저가 로그인에 성공하면, Cognito는 세션을 생성하고 인증된 사용자에게 ID token, access token, refresh token을 리턴한다.토큰은 API Gateway나 자체 구성된 server-side 리소스에 인증 목적으로 사용할 수 있다.Cognito Identity Pool을 이용해서 이 토큰들을 API Gateway가 아닌 AWS 서비스에 접근하기 위해 임시 AWS Credentials로 바꿀 수도 있다. 종류 ID token (자격 증명 토큰) : 로그인한 사용자의 자격 증명 클레임을 기반으로 API 호출 권한을 부여하는데 사용. Access token (액세스 토큰) : 지정된 액세스 보호 리소스의 사용자 지정 범위를 기반으로 API 호출 권한을 부여하는 데 사용. Refresh token (리프레스 토큰) : 신규 ID/액세스 토큰을 발급받는다. 리프레시 토큰의 default 만료 기간은 30일이며, 60분~10년 사이로 설정 가능하다. 특징 Cognito에서 발행하는 토큰은 클레임 기반의 토큰이다. Access/ID 토큰은 모두 cognito:groups라는 클레임을 포함한다. token : 유저를 인증하고, 리소스에 접근을 허용한다. clain : 토큰에 포함된 유저 관련 정보. 주체가 무엇인지 표현하는 이름과 값의 쌍. Cognito는 Base64 인코딩 된 string 값으로 토큰 발행한다. Cognito ID 또는 access token을 Base64로부터 plaintext JSON으로 디코딩 가능하다. regresh token 암호화되었으며 Cognito administrator나 유저로부터 읽힐 수 없다. Architecture 인증(Authentication) : API를 호출하는 클라이언트에 대한 Identity(신분)을 확인해주는 기능. 인가(Authorization) : 클라이언트가 API를 호출할 수 있는 권한이 있는지 확인해주는 기능. Cognito User Pool사용자 풀은 사용자에 대한 정보를 가지고 있는 저장소와 같은 역할.위에서 언급했듯이 여러가지 방법의 로그인 또는 회원가입을 지원. 성공적으로 사용자 인증 과정이 완료되면, Cognito는 JSON 형식의 웹 토큰(JWT)를 발행하며, 이 토큰을 사용해 특정 API에 대한 접근 보안 등 자격 증명을 수행하거나 AWS의 자격 증명으로 교환. 또한 사용자 풀의 모든 사용자는 그들 각각의 프로필을 가지고 있으며, SDK(javascript, Android, iOS)를 통해 프로필에 접근할 수 있다. 사용자 풀이 제공하는 기능가입 및 로그인수정 가능한 사용자 로그인을 위한 웹 UIFacebook, Google, Amazon, Apple을 통한 소셜 로그인 및 사용자 풀의 SAML 자격 증명 공급자를 통한 로그인사용자 관리 및 사용자 프로필멀티 팩터 인증(MFA, 2중 인증), 이상 자격 증명 확인, 계정 탈취 보호, 전화 및 이메일 확인과 같은 보안 기능AWS Lambda 트리거를 이용한 Cognito의 인증 과정 등의 커스터마이징 사용자 풀의 인증 flow현대식 인증 과정에는 사용자 인증을 위해 단순히 아이디, 암호 인증 외에도 여러가지 챌린지가 통합되어 있다.크게 인증은 두가지 단계로 일반화 할 수 있으며, 이들은 각각 InitiateAuth와 RespondToAuthChallenge API를 통해 구현된다. 인증이 실패하여 종료하거나 인증이 완료되어 토큰이 발행될 때까지 사용자는 순차적으로 사전에 정의된 챌린지들을 수행하게 된다. 챌린지는 만들고자 하는 앱에서 필요한 만큼 반복이 가능.이는 Cognito에서 개발자가 요구하는 복잡한 인증과정도 구현이 가능하게 한다. 자격 증명 풀(Identity Pools)자격 증명 풀은 특정 사용자의 고유한 자격 증명을 만들고 사용자에게 AWS 인프라에 대한 접근권한을 부여할 수 있다. Cognito user pool 사용자 Facebook, Google, Apple, SAML 인증 공급자로 인증된 사용자 기존의 인증 프로세스(서비스의 자체 인증 등)를 통해 인증된 사용자 자격 증명 풀을 이용하면 다른 AWS 서비스에 직접 접근하거나 API Gateway를 통해 서비스에 접근하도록 정의하는 권한을 가진 임시 AWS 자격 증명을 생성할 수 있다. 자격 증명 풀 인증 flow자격 증명 풀의 인증 flow는 외부 소셜 로그인, 기존의 인증 프로세스를 통한 사용자에게 발급되는 자격 증명에 대한 인증 flow이며, 크게 4가지가 있다. 외부 공급자 인증 flow2가지의 인증 방식이 존재하며, 이들 각각을 향상된 인증 흐름, 기본 인증 흐름으로 부른다. 향상된 인증 flow2단계로 자격 증명을 발급받을 수 있는 flow이며, GetId GetCredentialsForIdentity두번의 통신(디바이스와 Cognito간 통신을 의미)으로 자격 증명을 발급받을 수 있는 flow이다. 기본 인증 flow단계로 자격 증명을 발급받을 수 있는 flow이며, GetId GetOpenIdToken AssumeRoleWithWebIdentity세번의 통신(디바이스와 Cognito간 통신을 의미)으로 자격 증명을 발급받을 수 있는 flow. 개발자 인증 자격 증명 인증 flow자체 인증 시스템에서 인증된 사용자를 위한 자격 증명 인증 flow.2가지의 인증 방식이 존재하며, 이들 각각을 향상된 인증 흐름, 기본 인증 흐름으로 부른다. 2단계로 자격 증명을 발급받을 수 있는 flow이며, 자체 시스템에서 로그인 자체 시스템에서 로그인 검증 GetOpenIdTokenForDeveloperIdentity GetCredentialsForIdentity과정으로 구성되어 있다. 참고https://hyeon-joo.tistory.com/33https://velog.io/@w1nu/%EC%89%BD%EA%B2%8C-%ED%92%80%EC%96%B4%EC%93%B4-AWS-Cognito-%EA%B8%B0%EC%B4%88-%EC%9D%B4%EB%A1%A0https://velog.io/@jy3026/OAuth2.0%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80","link":"/2024/08/05/Adding-Authentication-to-API-Gateway-Using-AWS-Cognito/"},{"title":"PostgreSQL-pgvector를 AWS Aurora에서 사용하기","text":"AWS의 Aurora를 이용하여 pgvector를 설치하고 사용하겠다. RDS 대시보드에서 데이터베이스 생성 표준 생성, Aurora (PostgreSQL Compatible), Aurora PostgreSQL (Compatible with PostgreSQL 15.4 - 메이저 버전 15의 기본겂)을 선택. 기본값을 사용하기 위해 프로덕션 템플릿을 선택 마스터 사용자 이름은 postgres로, 자격 증명 관리는 AWS Secrets Manager에서 관리로 선택. 암호화 키는 기본값인 aws/secretsmanager로 선택. 구성 옵션도 기본값인 Aurora Standard. 인스턴스는 메모리 최적화 클래스(r 클래스 포함), db.r5.large로 선택. 다중 AZ 배포 (다른 AZ에 Aurora 복제본/리더 노드 생성)/ 연결은 EC2 컴퓨팅 리소스에 연결, 네트워크는 IPv4 VPC 보안 그룹은 기존 항목 선택 (default). 예상되는 월 사용 금액은 255.62 USD ElasticCache 클러스터나 RDS 프록시가 필요할까?","link":"/2024/07/31/Building-GenAI-Apps-with-AWS-Aurora-PostgreSQL-pgvector/"},{"title":"Route53을 이용해 도메인과 서버(EC2) 연결하기","text":"AWS Route53을 이용하여 웹 애플리케이션에 사용할 새 도메인 이름을 등록하고, DNS를 통해 해당 도메인 이름을 ALB가 제공하는 실행중인 웹 애플리케이션에 연결해보겠다. Route53에 접속하여, Dashboard에 등록하여","link":"/2024/07/30/Register-Domain-with-Route53/"},{"title":"API Gateway와 EC2 연결하기.","text":"API Gateway를 활용하여, EC2 인스턴스에 프록시로서 연결하는 방법에 대해 알아보겠다. 보통 AWS Lambda의 API를 만들때 API Gateway를 활용하곤 한다. 그런데, 몇몇 경우에는 EC2에 Proxy를 만들어서 사용해야하는 경우가 있다. EC2 인스턴스에 FastAPI 서버 하나를 돌리고 있으니, “http://x.x.x.x:5000/“ 라는 서버에 API Gateway를 연결해보도록 하겠다. API Gateway - 제공 API 유형API Gateway에서 제공하는 API는 대표적으로 3종류가 있다. HTTP API : API 프록시 기능정도만 필요할 때 적합. 단순 / 저렴하고 빠르다. REST API : API 관리 기능, 요청/응답에 대한 제어가 필요할 경우 적합, 복잡 / 비싸고 느리다. WebSocket API : 웹소켓 용도. 실시간 애플리케이션에서 주로 사용한다. 출처: https://inpa.tistory.com/entry/AWS-📚-API-Gateway-개념-기본-사용법-정리#rest_api [Inpa Dev 👨‍💻:티스토리] HTTP API HTTP를 통신 방식으로 사용하는 API를 HTTP API라고 한다. HTTP API는 Endpoint를 API gateway로 활용하여 HTTP 요청을 통해서 서버에 접근할 수 있도록 만들어준다. HTTP API는 데이터만 주고 받고 UI 화면이 필요하면 클라이언트가 별도로 처리한다. 대게 앱/웹/서버 to 서버에서 사용된다. 대부분의 Web API가 HTTP API로 이루어지고 있다. REST API REST API는 HTTP API에 여러가지 제약 조건이 추가된 형태이다. 자원의 식별 메시지를 통한 리소스 조작 자기서술적 메세지 애플리케이션의 상태에 대한 엔진으로서 하이퍼미디어 REST는 웹 서비스의 구조를 만드는데 활용되는 패턴이며 위의 4가지 제약조건을 만족해야 RESTFUL 하다라고 말할 수 있다. 대표적으로 CRUD 메서드 동작을 일컫는다. CREATE(post), READ(get), UPDATE(put), DELETE(delete) 그런데 이런 부분을 완벽하게 지키면서 개발하는 것은 현실적으로 어렵고, 또 추가 개발 비용대비 효과가 있는 것도 아니어서, 이미 많은 사람들이 해당 조건을 지키지 않아도 REST API라고 하기 때문에, HTTP API나 REST API를 거의 같은 의미로 사용하고 있는 현실이다. (물론 엄격하게는 다르다) WEBSOCKET API 요청을 받고 응답하는 REST API와 달리 WebSocket API는 클라이언트 앱과 백엔드 간의 양방향 통신을 지원한다. 웹 소켓은 사용자의 브라우저와 서버 사이의 인터액티브 통신 세션을 설정할 수 있게 하는 고급 기술 이다. 채팅 앱 및 스트리밍 대시보드와 같은 실시간 양방향 통신 애플리케이션을 구축하여 백엔드 서비스와 클라이언트 간의 메시지 전송을 처리하기위해 지속적인 연결을 유지한다. API GatewayAPI Gateway는 람다와 같이 서버리스 서비스이며, 수신한 API 호출에 대해서만 지불한다.다만 HTTP API / REST API / WEBSOCKET API 각각 모두 요금대가 다르다. API Gateway 중 REST API 사용REST API 게이트웨이 생성API Gateway 콘솔에서 API 생성 &gt; REST API 생성을 클릭한다. API 세부정보 새 API 엔드포인트 유형 - 지역 엔드포인트 유형 지역 : 특정 리전 안에서 사용 최적화된 에지 : CloudFront 사용(일반적인 인터넷 상) 프라이빗 : AWS내 VPC에서만 접근 가능 REST API 게이트웨이 경로 설정API를 생성하면 HTTP API와는 달리 REST API는 구성 화면이 약간 복잡하게 되어있다. (HTTP API보다 제공 기능이 많아서 그렇다) 리소스 메뉴를 눌러 리소스 생성을 한다. 리소스는 실제 api를 호출하는 api url을 정의한다라고 보면 된다. 리소스 이름과 리소스 경로를 설정해준다. 리소스 경로 쪽에 {userid}와 같이 중괄호로 묶어주면 경로 파라미터로서 사용할수 있다. 경로 파라미터란, 예를들어 아래 URL 경로와 같이 http://localhost/hello/12http://localhost/hello/667http://localhost/hello/55 서비스 특성상 뒤의 경로가 고정되어있지 않고, 여러개의 경로값을 사용할경우 이들을 묶는 일종의 변수 역할이라고 보면 된다. 두개의 경로를 설정해줬으니 이제 요청할 메서드 설정을 해준다. 통합 유형으로 HTTP를 선택하고, 엔드포인트 URL에는 요청보낼 목적지를 설정한다. https://api.github.com/users/{userid} 위 url은 깃헙에서 무료로 제공하는 api로서, {userid} 부분에 깃헙 프로필 닉네임을 적고 요청하면 해당 깃헙 프로필 정보를 json으로 반환해준다. 메서드 유형 : HTTP 엔드포인트 URL : 콘텐츠 처리 : 패스스루 REST API 게이트웨이 테스트 및 배포메서드 등록이 완료되면, 메서드 실행 환경으로 접근된다. {useri} 경로 파라미터에 닉네임 아무거나 적어서 테스트를 한다. 테스트까지 완료되면 완성된 REST API를 API Gateway에 정식 배포한다. 작업 &gt; API 배포를 선택하여 API를 스테이지에 등록한다. EC2의 네트워크 이슈위 방법대로 api gateway를 배포했는데 504 에러가 난다. ec2 끼리 ping 테스트도 안되는것을 보니 네트워크 문제가 있는듯하다. 보안 그룹 설정 확인인바운드 규칙: 두 인스턴스의 보안 그룹에서 ICMP 프로토콜(핑 요청)을 허용하고 있는지 확인하세요. ICMP를 허용하려면:AWS 콘솔에서 보안 그룹으로 이동하여 두 인스턴스에 적용된 보안 그룹을 선택합니다.인바운드 규칙에서 Custom ICMP(사용자 지정 ICMP - IPv4) - Echo Request(에코 응답) 유형을 추가하고, 소스에 허용할 IP 범위(예: 0.0.0.0/0 또는 특정 CIDR 블록, Anywhere-IPv4)를 지정합니다. 또는 HTTP를 허용해줘야 한다. 아웃바운드 규칙: 대부분의 경우 기본적으로 모든 아웃바운드 트래픽이 허용되지만, 이를 확인하여 ICMP 프로토콜이 아웃바운드 트래픽에서도 허용되는지 점검합니다. 네트워크 ACL 설정 확인네트워크 ACL: 인스턴스가 위치한 서브넷의 네트워크 ACL이 ICMP 트래픽을 허용하는지 확인합니다.네트워크 ACL에서 인바운드 및 아웃바운드 트래픽 모두에 대해 ICMP(코드 0, 유형 8) 규칙이 설정되어 있는지 확인합니다.네트워크 ACL은 상태 비저장(Stateless)이므로, 인바운드와 아웃바운드 규칙이 모두 필요합니다. 위의 내용들을 참고하여 설정해주면 같은 서브넷 내의 EC2 인스턴스끼리는 통신이 된다. (private IP로) 그런데 API Gateway에서는 EC2 호출이 되지 않고, 2번 EC2에서 1번 EC2 호출 시 public IP로는 실패한다. 하지만 퍼블릭 IPv4 DNS로 접근하면 된다… 아마 퍼블릭 IPv4 주소의 문제이지 않을까 싶다. EC2끼리 통신은 해결했으니 이제 API Gateway -&gt; EC2 통신이 잘 되도록 해결해야한다. 서브넷의 라우팅테이블 확인EC2가 속해있는 VPC 서브넷의 라우팅 테이블이 0.0.0.0/0 -&gt; IGW가 아니라 0.0.0.0/0 -&gt; Transit Gateway 인것으로 보아 해당 서브넷이 퍼블릭 서브넷이 아님을 의미하며, 모든 트래픽이 Transit Gateway를 통해 라우팅된다는 것을 나타낸다. Transit Gateway가 대상일 때의 의미: Transit Gateway 사용: Transit Gateway는 여러 VPC, 온프레미스 데이터 센터, 그리고 AWS의 다른 리전 간의 네트워크 연결을 중앙집중식으로 관리하는 서비스입니다. 이 설정은 모든 트래픽(특히 인터넷 트래픽이 아닌 트래픽)이 Transit Gateway를 통해 라우팅되어 다른 네트워크로 전달된다는 것을 의미합니다. 퍼블릭 서브넷이 아님: 인터넷 게이트웨이(Internet Gateway)가 대상이 아니기 때문에, 이 서브넷은 퍼블릭 서브넷이 아닙니다. 즉, 이 서브넷의 인스턴스들은 직접적으로 인터넷에 접근할 수 없습니다. 대신, 트래픽은 Transit Gateway를 통해 다른 네트워크(예: 다른 VPC, 온프레미스 네트워크)로 전달됩니다. 프라이빗 서브넷의 특성: 이 설정은 주로 프라이빗 서브넷에서 사용되며, 특정 리소스(예: 데이터베이스 서버, 내부 서비스 등)가 외부 인터넷이 아닌, 내부 네트워크나 다른 VPC로만 접근이 필요할 때 유용합니다. 실질적인 사용 사례: 기업 네트워크 통합: 여러 VPC나 온프레미스 네트워크를 AWS Transit Gateway를 통해 연결하고, 중앙집중식으로 네트워크를 관리하고자 할 때. 보안 강화: 인터넷에 직접 노출되지 않도록 하고, 모든 트래픽을 보안 정책이 적용된 내부 네트워크로만 전달하고자 할 때. 결론:해당 서브넷은 퍼블릭 서브넷이 아니며, 외부 인터넷과의 통신은 불가능합니다. 모든 트래픽은 Transit Gateway를 통해 다른 VPC나 온프레미스 네트워크로 전달됩니다. 따라서, 인터넷을 통해 외부와 직접 통신하기 위해서는 별도의 퍼블릭 서브넷과 인터넷 게이트웨이를 사용하는 것이 필요합니다. API Gateway가 VPC 리소스(EC2 인스턴스)와 상호작용하도록 VPC Link 설정하기API Gateway는 일반적으로 AWS의 전역 서비스로 동작하며, 특정 VPC에 속하지 않습니다. 대신, API Gateway가 VPC 리소스(예: EC2 인스턴스)와 상호작용하도록 설정할 수 있는 방법이 있습니다. 이 설정은 **VPC 링크(VPC Link)**를 통해 이루어지며, 이 경우 API Gateway는 특정 VPC와 연결됩니다. API Gateway를 통해 VPC 내의 리소스(예: EC2, NLB, ECS)와 통신해야 하는 경우, VPC 링크를 사용하여 안전하게 연결할 수 있습니다.VPC 링크는 API Gateway와 VPC 내의 리소스 간의 프라이빗 통신을 가능하게 합니다. API Gateway가 VPC 링크를 사용하지 않고 설정되어 있다면, API Gateway는 AWS의 관리형 인프라에서 실행되며, 특정 VPC에 속하지 않습니다.API Gateway와 VPC 리소스 간의 연결이 필요하지 않은 경우에는 VPC 링크를 사용하지 않아도 됩니다. VPC 링크 생성하기먼저, VPC 링크를 생성합니다. API Gateway 콘솔에 로그인: AWS 관리 콘솔에서 API Gateway로 이동합니다. VPC 링크 생성: 왼쪽 메뉴에서 VPC Links를 선택한 후 Create 버튼을 클릭합니다. VPC 링크 이름을 지정하고, 연결할 네트워크 로드 밸런서(NLB)를 선택합니다. 필요한 경우, 태그를 추가합니다. Create 버튼을 눌러 VPC 링크를 생성합니다. API Gateway에 VPC 링크 설정하기이제, VPC 링크를 API Gateway의 특정 API 엔드포인트에 설정합니다. API 선택: API Gateway 콘솔에서 VPC 링크를 설정할 API를 선택합니다. 리소스 및 메서드 선택: 설정할 리소스 및 메서드를 선택합니다. 예를 들어, /plugin/test 리소스의 GET 메서드를 선택합니다. 통합 유형 선택: Integration Request 섹션으로 이동합니다. **통합 유형(Integration type)**에서 VPC Link를 선택합니다. VPC 링크 및 로드 밸런서 선택: 이전 단계에서 생성한 VPC 링크를 선택합니다. Endpoint URL 필드에 연결할 VPC 리소스의 URL을 입력합니다. 이 URL은 네트워크 로드 밸런서(NLB)의 DNS 이름과 해당 포트를 포함해야 합니다. 예시: http://:/ 저장 및 배포: 설정을 저장하고 API를 배포합니다. 참고https://inpa.tistory.com/entry/AWS-%F0%9F%93%9A-API-Gateway-%EA%B0%9C%EB%85%90-%EA%B8%B0%EB%B3%B8-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%A0%95%EB%A6%AC#rest_api","link":"/2024/08/05/API-Gateway-EC2/"},{"title":"Lambda로 간단한 api 만들기","text":"AWS Lambda에 테스트용 API를 만드는 방법을 단계별로 적어보겠다. 이 과정에서는 AWS Lambda를 사용하여 간단한 HTTP API를 설정힌다. 1. AWS Lambda 함수 생성 Lambda 함수 생성: Create function 버튼을 클릭. Author from scratch를 선택. 함수 이름(Function name)을 입력하고, 런타임(Runtime)으로 사용하려는 언어를 선택(예: Python, Node.js 등). 역할(Role)을 선택. 새로운 역할을 생성하거나 기존 역할을 사용할 수 있음. Create function 버튼을 눌러 함수를 생성. Lambda 함수 코드 작성: 함수가 생성되면, 기본 코드 편집기에서 코드를 작성할 수 있다. 예를 들어, Node.js를 사용하는 경우 다음과 같은 간단한 코드를 사용할 수 있다:12345def lambda_handler(event, context): return { 'statusCode': 200, 'body': 'Hello from Lambda!' } Lambda 함수 저장: 코드를 작성한 후, Deploy 버튼을 눌러 함수를 저장하고 배포. 2. API Gateway에 Lambda 통합 설정 API Gateway 서비스로 이동: AWS 관리 콘솔에서 API Gateway 서비스로 이동. API 생성: Create API 버튼을 클릭. HTTP API 또는 REST API 중 하나를 선택. Build를 클릭하여 API 생성을 시작. 통합 설정: Add integration을 클릭하고, Lambda를 선택. 앞서 생성한 Lambda 함수를 선택. Next를 클릭하여 진행. 경로 및 메서드 설정: Resource 경로를 설정. 예를 들어, /test 경로를 설정할 수 있음. 메서드를 GET, POST 등 원하는 메서드로 설정. Next를 클릭하여 API 생성 프로세스를 완료. API 배포: Deploy 버튼을 클릭하여 API를 배포. API Gateway는 API 엔드포인트 URL을 제공. 이 URL을 통해 Lambda 함수와 통합된 API에 접근할 수 있음. 3. API 테스트 브라우저나 Postman, cURL 등을 사용하여 생성된 API 엔드포인트를 호출. 예시:1GET https://&lt;api-id&gt;.execute-api.&lt;region&gt;.amazonaws.com/test 이 호출은 Lambda 함수와 연결된 API Gateway를 통해 Lambda 함수로 요청을 전달하고, 그 응답을 반환. 로그 확인 (옵션) API Gateway와 Lambda 함수는 AWS CloudWatch에 로그를 기록. 로그를 확인하려면 CloudWatch 서비스로 이동하여 Lambda와 API Gateway의 로그를 조회할 수 있음. 이렇게 설정하면, AWS Lambda를 활용한 간단한 테스트용 API가 생성된다.이 API는 API Gateway를 통해 HTTP 요청을 수신하고, Lambda 함수에서 정의된 코드를 실행하여 응답을 반환하게 된다.","link":"/2024/08/06/Deploy-Simple-API-with-Lambda/"},{"title":"AWS API Gateway에 API키 등록하기","text":"API에 특정 API Key를 가지고 있는 요청만 접근 가능하도록, Lambda로 간단한 api 만들기에서 만든 REST API에 API 키릉 등록해보겠다. 메서드 요청 설정쪽에서 API 키가 필요함을 True로 바꿔준다. 요청 검사기도 넣어준다. 다음으로 API 키를 만들고, 사용량 계획과 연결해주어야한다. 먼저 API 키를 생성한다. 그리고 사용량 계획을 만든다. 만들면서 API를 배포했을때의 스테이지를 연결해준다. 키 값을 확인하고, postman으로 요청을 테스트해본다. 키 값을 적지 않고 그냥 요청했을 때는 forbidden이 뜨고, 값을 확인할 수 없다. header에 key : x-api-key, value : 키 값을 적어서 요청 보내면 정상적으로 응답이 나온다.","link":"/2024/08/08/AWS-Register-API-Key-in-API-Gateway/"},{"title":"Netflix-No-Rules-Rules","text":"이 글은 넷플릭스 창업주이자 CEO인 리드 헤이스팅스의 인터뷰가 담긴 책, &lt;규칙없음:넷플릭스, 지구상 가장 빠르고 유연한 기업의비밀&gt;의 요약 내용이다. 넷플릭스의 철학은 절차보다 사람을 소중히 여기고, 능률보다 혁신을 강조, 통제를 최대한 자제 인재밀도 (talent density)를 중시 Culture Deck : 넷플릭스 사내용 127개 슬라이드 - 정직성 중시적당한 성과를 내는 직원은 두둑한 퇴직금을 주어 해고하고, 새로운 스타를 맞이한다. 회사분위기가 편안하고 안전하게 바뀔수록 혁신은 더욱 활발해진다. 손실 회피(loss aversion) : 인간은 새로운 어떤 것을 얻으려는 욕구 이상으로 이미 소유하고 있는 것을 잃지 않으려고 애쓴다. 휴가 기회가 사라질 수 있으니 무슨 수를 써서든 기회를 잃지 않으려 휴가를 사용 애초에 정해진 휴가 기간이 없다면 휴가를 잃을 걱정이 없으므로 휴가를 아예 쓰지 않음 인재 밀도를 구축 일반적인 회사들이 규정과 통제 절차를 마련하는 이유는, 일 처리가 미숙하고 프로답지 못하거나 무책임한 직원들을 다루기 위함. 애초에 이런 사람들을 채용하지 않거나 내보낸다면 그런 규정은 필요 없음 인재 밀도가 높을수록 지원들에게 허용되는 자유는 더욱 커짐 솔직성을 키우기 재능 있는 직원들은 서로에게서 많은 것을 배움 재능 있는 직원들이 피드백을 습관처럼 서로 주고받게 되면 일을 더 잘하게 되고 동시에 서로 책임질 수 있는 행동을 하게 되어, 통제는 크게 필요하지 않게 됨. 통제를 줄여라 출장 규정, 지출 규정, 휴가 규정 등은 없앨 수 있는 것들 몇 가지 가이드라인만 주면 됨. 가이드라인 : 매니저에게는 ‘통제가 아닌 맥락으로 이끌 것’, 평사원에게는 ‘상사의 비위를 맞추려 들지 말 것’ 재능 있는 사람들은 서로 능률을 높인다.팀에 평범한 사람이 1~2명 섞여 있으면 팀 전체의 성과가 떨어짐 그룹 토의의 질을 떨어뜨려 팀 전반적 IQ를 낮춤 사람들이 싫어할 일을 하게 만들어 능률 떨어뜨림 남보다 탁월한 능력을 발휘하고 싶은 직원을 회사에서 나가게 만듬 평범한 사람도 받아준다는 사실을 보여줌으로써 문제를 복잡하게 만듬 -&gt; 빠르고 혁신적인 직장은 소위 말하는 ‘비범한 동료들’로 구성된다. 높은 성과 + 사심 없는 솔직함 = 대단히 높은 성과솔직한 문화가 조성되면 일을 잘하는 사람을 더욱 탁월한 인재로 만들 수 있다. 솔직한 피드백이 잦아지면 팀과 회사의 업무 속도와 능률이 기하급수적으로 증가한다. 4A 피드백 지침피드백을 줄 때 Aim to Assist (도움을 주겠다는 생각으로 하라) Actionable (실질적인 조치를 포함하라) 피드백을 받을 때 Appreciate(감사하라) Accept or Discard(받아들이거나 거부하라) 똑똑한 왕재수는 사절솔직한 문화라고 해서 다른 사람에게 미치는 영향을 고려하지 않고 아무렇게나 말해도 좋다는 뜻은 아니다.오히려 솔직하게 말하려면 4A 피드백 지침을 소중히 지켜야 한다.상대방의 기분도 헤아려야 한다. 그러려면 피드백을 주기 전에 꼼꼼히 따져보고 준비도 단단히 해야할 뿐 아니라, 책임을 맡은 사람들로부터 모니터링과 코치도 받아야 한다. 똑똑한 왕재수가 되지 않으려면 자신에게 물어라 회사 분위기를 안 좋게 만들 생각인가? 좀 더 품위있는 방법으로 문제를 처리할 수는 없는가? 1명의 슈퍼스타가 2명의 평범한 직원보다 낫다 항상 회사의 이익이 되는 방향으로 행동한다. 다른 사람의 목표 달성을 어렵게 하는 행위는 하지 않는다. 자신의 목표를 성취하기 위해 최선을 다한다. 이를 지킨다면 휴가기간을 정하는 문제는 각자 하고 싶은대로 해도 된다고 한다. 책임질 자유넷플릭스는 성과를 많이 내는 직원들이 스스로 생활을 통제할 수 있게 하는 한 가지 방법을 알아냈다.그리고 그런 통제가 오히려 모두를 더 자유롭게 해준다는 사실도 확인했다.인재 밀도가 높았기에, 직원들은 이미 양심과 책임 의식을 가지고 행동하고 있었다.솔직한 문화가 정착되었기에, 누군가가 제도를 역이용하거나 주어진 자유를 남용하기라도 하면, 주변 사람이 이를 지적해 상황을 바로 잡았다. 그와 동시에 직원들의 주인의식이 한층 높아졌다는 걸 피부로 느끼게 되었다고 한다.직원들에게 자유를 주면, 회사 일을 자기 일처럼 여기게 되어 더욱더 책임 있게 행동한다.자유는 책임의 대립 개념이 아니다. 오히려 자유는 책임을 향해가는 통로이다. 전 단계 맥락 (Context at the front end)선택이","link":"/2024/08/10/Netflix-No-Rules-Rules/"},{"title":"API Gateway에서 EC2의 application 호출","text":"회사에서 보안문제로, 외부망을 호출할 수 없게하여 EC2 에 다른 포트에 서빙중이었던 서비스에 접근하지 못한다고 proxy 역할을 할 다른 서버를 구축하고자 했다. 그래서 API-Gateway 로 EC2 를 직접 호출하게 했다. API Gateway를 REST API 유형으로 API 엔드포인트유형을 private으로 생성하기대상 그룹 생성 EC2에 서비스를 배포해놓았기 때문에, 대상 유형은 인스턴스로 선택 대상 그룹 이름과 프로토콜, VPC 등을 선택한다. 대상은 일단 EC2 앱들을 선택한다. 대상 그룹은 아래와 같이 하는것으로 변경했다둘 모두 같은 1번 ec2이며80포트에는 fastapi-server로 /plugin/test 요청 보내면 success를 돌려주는 api가8002포트에는 /chat?input=hi 이런식으로 보내면 llm 모델 응답을 보내주는 api가 올라가 있다 Network 로드 밸런서 생성 EC2에 가서, 로드밸런서 생성을 선택 기본구성을 해준다. 네트워크 매핑을 해준다, EC2가 있는 VPC를 선택해준다. -&gt; 주의사항 : 위 캡쳐처럼 app subnet으로 하는것이 아니라 elb를 선택해줘야 한다. 리스너 및 라우팅 설정을 해준다. 대상 그룹으로 연결해준다. VPC 링크 생성 API Gateway 대시보드로 가서 VPC 링크를 생성한다. API를 생성한다VPC 링크 연동 방식으로 API를 생성한다. -&gt; 주의사항 : 여기서 URL은 NLB의 DNS를 넣어줘야 한다. 주의사항 - NLB의 보안 그룹NLB의 보안그룹 편집에서, 보안설정 관련하여 PrivateLink 트래픽에 인바운드 규칙 허용을 선택하면 안된다. 메서드 생성 시 vpcLinkId와 vpcNLB 스테이지 변수 활용하기 배포 전 테스트 배포 후 테스트스테이지에 배포 후 url을 이용하여 외부 접근 테스트를 진행한다 chat api이번엔 EC2 8002번 포트에 /chat?input=hi 이런식으로 보내면 llm 응답을 돌려주는 api를 호출하도록 API Gateway에 새로운 메서드를 추가해보겠다. 앞의 다른 메서드와 유사하게 만들어준다. /chat?input=hi 이런식으로 쿼리 문자열 파라미터가 필요하므로 추가해준다 대상 그룹 등록 여부에 따른 호출 결과현재 8002포트의 /chat api는 에러가 있는데 대상그룹 등록여부에 따라 호출 결과가 다르다 대상 등록 시 대상 등록 취소 시 외부에서 호출1234567891011% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=baseball'{&quot;query&quot;:&quot;baseball&quot;,&quot;answer&quot;:&quot;\\&quot; baseball은 19세기 미국에서 발전한 야구 게임으로, 현재까지도 많은 사람들에게 사랑받고 있습니다. 그것은 미국에서 가장 인기 있는 스포츠 중 하나이며, 많은 선수들이 활동하고 있습니다.\\&quot;&quot;,&quot;ner&quot;:null}% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=football'{&quot;query&quot;:&quot;football&quot;,&quot;answer&quot;:&quot;검색 결과가 없습니다.&quot;,&quot;ner&quot;:null} % curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=LA' {&quot;query&quot;:&quot;LA&quot;,&quot;answer&quot;:&quot;LA는 5G 서비스 이용약관 계약의 성립에 대한 고객의 제출 서류입니다.&quot;,&quot;ner&quot;:null}% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/chat?input=summary' {&quot;query&quot;:&quot;summary&quot;,&quot;answer&quot;:&quot;통화 기록을 요약한 결과를 제공합니다.\\n\\n summary = \\&quot;주말 점심 약속 조정\\&quot;\\n summary_detail = \\&quot;\\n - 오랜만에 연락하여 근황을 나눔\\n - 주말 점심 약속 제안 및 시간 조정\\n - 강남역 스타벅스에서 만나기로 장소 확정\\&quot;\\n \\&quot;event\\&quot;={\\n \\&quot;날짜\\&quot;: \\&quot;2024년 06월 01일\\&quot;,\\n \\&quot;시간\\&quot;: \\&quot;오후 1시\\&quot;,\\n \\&quot;장소\\&quot;: \\&quot;강남역 스타벅스\\&quot;,\\n \\&quot;대상\\&quot;: \\&quot;영수\\&quot;\\n }&quot;,&quot;ner&quot;:null} ReferencesAmzzon API Gateway 기반 VPC Link 활용 방법AWS API Gateway 에서 EC2 에 서빙중인 application HTTP 로 호출하기EC2 Instance Connect Endpoint를 통해 private EC2 접속하기","link":"/2024/08/11/AWS-Call-Application-hosted-on-EC2-from-AWS-API-Gateway/"},{"title":"파인튜닝한 bert 모델 서빙서버 EC2에 배포하기","text":"FastApi 프레임워크를 사용하여 웹 애플리케이션을 구축해보겠다.주요 기능으로는 텍스트 처리 및 AI 모델을 활용한 다양한 응답을 제공하는 API 엔드포인트 정의이다. FastAPI 코드1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import osfrom fastapi import FastAPIfrom fastapi.responses import RedirectResponsefrom langserve import add_routesfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipelinefrom transformers import AutoModel, AutoTokenizer, pipelinefrom pydantic import BaseModelfrom fastapi import FastAPI, HTTPExceptionapp = FastAPI()# 현재 파일의 위치를 기준으로 상대 경로 설정current_dir = os.path.dirname(os.path.abspath('./model'))model_path = os.path.join(current_dir, 'model')# 모델과 토크나이저 로드model = AutoModel.from_pretrained(model_path)tokenizer = AutoTokenizer.from_pretrained(model_path)pipe = pipeline(&quot;text2text-generation&quot;, model=model, tokenizer=tokenizer)huggingface_pipeline = HuggingFacePipeline(pipeline=pipe)# FastAPI 앱에 라우트 추가add_routes( app, huggingface_pipeline, path=&quot;/model&quot;)@app.get(&quot;/&quot;)async def redirect_root_to_docs(): return RedirectResponse(&quot;/docs&quot;)class TestRequest(BaseModel): text: str@app.post(&quot;/test&quot;)async def test_endpoint(request: TestRequest): try: inputs = tokenizer(request.text, return_tensors=&quot;pt&quot;)['input_ids'] output = model(inputs) logits = output[&quot;logits&quot;] logits_2 = output[&quot;logits2&quot;] intent = output[&quot;intent&quot;] ner = output[&quot;ner&quot;] predictions = torch.argmax( torch.FloatTensor(torch.softmax(logits, dim=1).tolist()), dim=1, ) predictions_ner = logits_2.argmax(-1) test_predict = predictions_ner[0][1:len(predictions_ner)-2] # 출력 예제 return { &quot;input&quot;: request.text, &quot;intent&quot;: label2intent(predictions.tolist()), &quot;tokens&quot;: tokenizer.tokenize(request.text), &quot;slot_labels&quot;: label2slot(test_predict), &quot;intent_raw&quot;: intent.tolist(), &quot;ner_raw&quot;: ner.tolist() } except Exception as e: raise HTTPException(status_code=500, detail=str(e))if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=5005) Dockerfile 코드1234567891011121314151617181920212223FROM python:3.11-slimRUN pip install poetry==1.6.1RUN poetry config virtualenvs.create falseWORKDIR /codeCOPY ./pyproject.toml ./README.md ./poetry.lock* ./COPY ./package[s] ./packagesRUN poetry install --no-interaction --no-ansi --no-rootCOPY ./app ./appCOPY ./model ./modelRUN poetry install --no-interaction --no-ansiEXPOSE 5004CMD exec uvicorn app.server:app --host 0.0.0.0 --port 5004 로컬에서 테스트해보기123456789 % curl --location 'http://127.0.0.1:5004/predict' \\--header 'Content-Type: application/json' \\--data '{ &quot;input&quot;: &quot;This is a test sentence.&quot;}'{&quot;input&quot;:&quot;This is a test sentence.&quot;,&quot;output&quot;:[[-1.1625256538391113,-0.7818309664726257,-1.2470957040786743,-0.07029110193252563,-1.252798318862915,1.1913777589797974,0.5518047213554382,...,-0.08304034173488617,0.22596704959869385]]} ECR에 도커이미지 push123% aws configure% aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account-id&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.comLogin Succeeded AWS 자격 증명 파일(~/.aws/credentials)에 아래 것들이 있어야함 123aws_access_key_id=...aws_secret_access_key=...aws_session_token=... 도커 이미지 푸시 1234# Docker 이미지 태그docker tag my-project:latest &lt;your-dockerhub-username&gt;/my-project:latestdocker push &lt;your-dockerhub-username&gt;/my-project:latest EC2에서 이미지 pull 하기1234567$ sudo yum install aws-cli -y$ aws configure$ export AWS_ACCESS_KEY_ID=[access_key_id]$ export AWS_SECRET_ACCESS_KEY=[aws_secret_access_key]$ export AWS_SESSION_TOKEN=[aws_session_token]$ aws ecr get-login-password --region [region] | docker login --username AWS --password-stdin [id].dkr.ecr.[region].amazonaws.com$ docker pull [id].dkr.ecr.[region].amazonaws.com/[project_name]:tag EC2에서 배포하기123456% docker run --name [app name] -p 5004:5004 [id].dkr.ecr.[region].amazonaws.com/[app name]:[tag]INFO: Started server process [1]INFO: Waiting for application startup.INFO: Application startup complete.INFO: Uvicorn running on http://0.0.0.0:5004 (Press CTRL+C to quit)INFO: 10.71.176.76:31790 - &quot;POST /predict HTTP/1.1&quot; 200 OK 테스트123$ curl --location 'http://[ec2_ip].compute.amazonaws.com:5004/predict' --header 'Content-Type: application/json' --data '{&quot;input&quot;:&quot;This is a test sentence.&quot;}'{&quot;input&quot;:&quot;This is a test sentence.&quot;,&quot;output&quot;:[[-1.1625256538391113,-0.7818318009376526,...,-0.08304007351398468,0.22596575319766998]]} API Gateway 연동하기API Gateway에서 메서드를 만들고 HTTP Request Header를 넣어준다 배포전에 테스트를 해준다 테스트 결과는 아래와 같이 나온다. 배포한 API 로컬 PC에서 호출해보기1234567% curl --location 'https://wfz4ol6u28.execute-api.ap-northeast-2.amazonaws.com/dev/predict' \\--header 'accept: application/json' \\--header 'Content-Type: application/json' \\--data '{ &quot;text&quot;: &quot;거실 조명을 좀 더 아늑한 느낌으로&quot;}'{&quot;input&quot;:&quot;거실 조명을 좀 더 아늑한 느낌으로&quot;,&quot;intent&quot;:&quot;조명따뜻하게설정&quot;,&quot;ner&quot;:[&quot;home&quot;,&quot;device&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;],&quot;raw_intent&quot;:&quot;조명따뜻하게설정&quot;,&quot;test_predict&quot;:[3,4,0,0,0,0,0],&quot;raw_ner&quot;:[&quot;home&quot;,&quot;device&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;],&quot;tokenized_text&quot;:[&quot;거실&quot;,&quot;조명&quot;,&quot;##을&quot;,&quot;좀&quot;,&quot;더&quot;,&quot;아늑한&quot;,&quot;느낌으로&quot;]","link":"/2024/08/12/AWS-Deploy-Fine-Tuned-BERT-Model-Serving-Server-on-EC2/"},{"title":"AWS-Session-Manager-Error","text":"SSM 에이전트가 온라인 상태가 아닙니다 에러 권한이 업데이트 되면서 연결이 안 된것 같음. 권한을 제거했다가 붙여야 함.","link":"/2024/08/19/AWS-Session-Manager-Error/"},{"title":"LiteLLM","text":"LiteLLM은 다양한 대형 언어 모델(LLM) 제공자의 API를 간편하게 통합하고 관리할 수 있도록 설계된 라이브러리이다.OpenAI와 호환되는 API를 사용하여 이러한 모델들과의 상호작용을 표준화한다. 여러 LLM을 사용하는 프로젝트에서 LiteLLM은 통합, 관리, 그리고 비용 및 사용량 모니터링을 간편하게 만들어준다. LiteLLM의 주요 기능에는 Completions, Embeddings, Image generation을 지원하는 여러 provider들 간의 일관된 요청 처리가 포함된다.Completions : LLM이 주어진 입력(프롬프트)에 대한 응답으로 텍스트를 생성하는 작업Embeddings : 자연어 처리에서 텍스트를 수치화된 벡터로 변환하는 방법Image generation : 텍스트 입력을 기반으로 이미지를 생성하는 기술 OpenAI, Azure, AWS, HuggingFace 등 다양한 provider들을 지원하며, 로드 밸런싱, 속도 제한, 비용 추적 등의 기능도 제공하여 대규모 AI 애플리케이션 배포에 적합하다. LiteLLM의 두 가지 사용법LiteLLM Proxy Server - 100개 이상의 LLM을 호출하고, 로드 밸런싱하고, 프로젝트 전체에서 비용을 추적하는 서버(LLM 게이트웨이) LiteLLM은 다양한 LLM으로의 요청을 관리하고 라우팅할 수 있는 프록시 서버를 제공하며, 이 프록시는 개발자에게 통합된 인터페이스를 제공한다. 이 프록시는 로깅 및 가시성 기능도 제공하여 대규모 AI 배포를 제어하는데 중요한 역할을 한다. 일반적으로 Gen AI Enablement/ML 플랫폼에서 사용 LiteLLM 서버 (LLM 게이트웨이)는 아래의 것들을 관리한다.통합 인터페이스여러 LLM 제공자들의 API와 기능을 하나의 표준화된 인터페이스로 통합하여 제공하는 기능각각의 고유한 api를 따로 학습하거나 처리할 필요 없이, 하나의 일관된 방법으로 모든 모델에 접근하고 사용 가능OpenAI의 ChatCompletions &amp; Completions 포맷으로 Huggingface/Bedrock/TogetherAI 등의 100개 이상의 LLM 호출 Cost tracking여러 LLM 제공자와 상호작용할 때 발생하는 비용을 모니터링하고 관리할 수 있도록 도와주는 기능각 모델 사용 시 소모되는 토큰 수, 호출 빈도, 그리고 이에 따른 비용 추적, 전체적인 사용 비용을 투명하게 파악 가능 Open image-20240827-080122.pngimage-20240827-080122.png 스트리밍 비용, 사용량 import litellm track_cost_callbackdef track_cost_callback( kwargs, # kwargs to completion completion_response, # response from completion start_time, end_time # start/end time): try: response_cost = kwargs.get(“response_cost”, 0) print(“streaming response_cost”, response_cost) except: pass set callbacklitellm.success_callback = [track_cost_callback] # set custom callback function litellm.completion() callresponse = completion( model=”gpt-3.5-turbo”, messages=[ { “role”: “user”, “content”: “Hi 👋 - i’m openai” } ], stream=True) 로드밸런싱 : 여러 모델, 동일 모델의 여러 서버 배포 간 초당 1.5k 이상의 요청 처리 가능config 예시 : 요청은 model=gpt-3.5-turbo 여러 인스턴스로 라우팅 됨. model_list: model_name: gpt-3.5-turbolitellm_params: model: azure/ api_base: api_key: rpm: 6 # Rate limit for this deployment: in requests per minute (rpm) model_name: gpt-3.5-turbolitellm_params: model: azure/gpt-turbo-small-ca api_base: https://my-endpoint-canada-berri992.openai.azure.com/ api_key: rpm: 6 model_name: gpt-3.5-turbolitellm_params: model: azure/gpt-turbo-large api_base: https://openai-france-1234.openai.azure.com/ api_key: rpm: 1440routing_strategy: simple-shuffle # Literal[“simple-shuffle”, “least-busy”, “usage-based-routing”,”latency-based-routing”], default=”simple-shuffle” model_group_alias: {“gpt-4”: “gpt-3.5-turbo”} # all requests with gpt-4 will be routed to models with gpt-3.5-turbo num_retries: 2 timeout: 30 # 30 seconds redis_host: # set this when using multiple litellm proxy deployments, load balancing state stored in redis redis_password: redis_port: 1992 로깅 관찰성 : LLM 입력/출력 로딩LiteLLM은 Lunary, Langfuse, Helicone, Promptlayer, Traceloop, Slack에 데이터를 전송하기 위해 미리 정의된 콜백을 제공 from litellm import completion set env variables for logging toolsos.environ[“HELICONE_API_KEY”] = “your-helicone-key”os.environ[“LANGFUSE_PUBLIC_KEY”] = “”os.environ[“LANGFUSE_SECRET_KEY”] = “”os.environ[“LUNARY_PUBLIC_KEY”] = “your-lunary-public-key”os.environ[“OPENAI_API_KEY”] set callbackslitellm.success_callback = [“lunary”, “langfuse”, “helicone”] # log input/output to lunary, langfuse, supabase, helicone#openai callresponse = completion(model=”gpt-3.5-turbo”, messages=[{“role”: “user”, “content”: “Hi 👋 - i’m openai”}]) 가드레일, 캐싱을 사용자 정의LiteLLM은 다음을 지원한다메모리 캐시레디스 캐시Quadrant 의미 캐시Redis 의미론적 캐시S3 버킷 캐시 Proxy 엔드포인트프록시에 ChatCompletions 요청을 만들기 import openai # openai v1.0.0+client = openai.OpenAI(api_key=”anything”,base_url=”http://0.0.0.0:8000“) # set proxy to base_url request sent to model set on litellm proxy, litellm --modelresponse = client.chat.completions.create(model=”gpt-3.5-turbo”, messages = [ { “role”: “user”, “content”: “this is a test request, write a short poem” }])print(response) Swagger Open swagger.pngswagger.png LiteLLM python SDKLiteLLM Python SDK란?100개 이상의 LLM을 호출하고, 로드 밸런싱 및 비용 추적을 지원하는 Python 클라이언트OpenAI, Azure OpenAI, Anthropic, Cohere, Replicate, Bedrock, Vertex AI 등Python 코드에서 LiteLLM을 사용하여 여러 LLM에 액세스할 수 있는 통합 인터페이스 제공Azure, OpenAI 등 여러 배포 환경에서 Retry/fallback 로직을 적용하여 안정성 확보오픈소스 : GitHub에서 무료로 제공되어 개발자가 자신의 환경에서 배포하고 사용가능litellm/litellm at main · BerriAI/litellm가격(1) 오픈 소스 (무료)(2) 엔터프라이즈 기본 (월 $250)(3) 엔터프라이즈 프리미엄 (월 $1000)라우터 기능로드 밸런싱 및 fallback: 여러 배포 간 로드 밸런싱과 요청 우선 처리기본 안정성 로직: 쿨다운, fallback, 타임아웃, Retry(fixed + exponential backoff)프로덕션 환경 지원: Redis를 사용해 쿨다운 서버, 사용량 추적, tpm/rpm 제한 관리Callback: API 호출에 사용된 키, 엔드포인트, 모델 등을 추적모델 캐싱: Azure와 OpenAI 같은 다른 모델 그룹 간 캐싱이벤트 알림: LLM API 예외, 느린 응답 등의 이벤트를 Slack/웹훅 URL로 알림비용 및 배포 추적: 배포 비용 추적, 라우터 배포 및 디버깅기타 기능커스텀 설정: API 키, API 베이스, 버전, 타입, 프로젝트, 위치, 토큰 등 사용자 지정 가능완료 토큰 사용: 모든 완료 요청에서 토큰 사용량 반환사용자 정의 가격 측정: SageMaker, Azure 등의 가격 모델 지원비동기 임베딩 함수: embedding.aembedding 비동기 임베딩 지원조정 엔드포인트: OpenAI의 조정 엔드포인트 지원Budget Manager예산 관리: LLM API 호출 시 예산 초과를 방지.글로벌 예산 설정: litellm.max_budget으로 최대 예산(USD) 설정, 초과 시 BudgetExceededError 발생.BudgetManager 클래스: 사용자별 예산 설정 및 비용 관리LiteLLM Proxy Server: OpenAI 호환 엔드포인트로 LLM 호출, 예산 관리, 지출 추적, 부하 분산캐싱캐시 초기화: 메모리, Redis, S3 버킷, 디스크 캐시 등 다양한 캐시 옵션캐시 제어: no-cache, no-store, ttl, s-maxage 등 캐시 설정 가능캐시 컨텍스트 관리자: 캐시 활성화, 비활성화 및 매개변수 업데이트사용자 정의 캐시: 필요에 따라 캐시 설정을 커스터마이즈통합 기능Langchain: ChatLiteLLM()을 통한 통합Instructor: Function calling 기능 지원 우리꺼에서 어떻게 사용하는지, 유사 기술과 스택과 비교LangChain: LangChain은 LLM을 활용한 애플리케이션 개발을 위한 프레임워크로, 다양한 LLM 제공자와의 통합을 지원한다.LangChain은 특히 자연어 처리(NLP) 파이프라인을 구성하고, LLM의 출력을 후처리(post-processing)하거나 다양한 모델 간의 조합을 가능하게 하는 도구들을 제공한다.Haystack: Haystack은 주로 검색 엔진과 NLP 애플리케이션을 구축하는 데 사용되는 오픈소스 프레임워크이다.다양한 LLM을 포함한 모델을 손쉽게 통합할 수 있으며, 문서 검색, 질의응답, 텍스트 생성 등 다양한 기능을 제공한다.Haystack은 Elasticsearch, OpenSearch, Hugging Face 모델과의 통합을 통해 다양한 데이터 소스와 LLM을 활용할 수 있다.Rasa: Rasa는 주로 대화형 AI, 챗봇을 구축하기 위한 오픈소스 프레임워크로, 다양한 NLP 모델과 통합이 가능한다.Rasa는 자연어 이해(NLU)와 대화 관리(Dialogue Management)를 처리하며, 사용자 정의 가능한 파이프라인을 통해 여러 LLM과의 연동을 지원한다.OpenAI’s API: OpenAI는 자사의 API를 통해 GPT 시리즈 모델을 제공하며, 이를 다른 애플리케이션과 통합할 수 있는 다양한 도구들을 제공한다.여러 언어 모델을 사용한 텍스트 생성, 자연어 이해, 번역 등의 작업을 API를 통해 쉽게 수행할 수 있다.Hugging Face’s Transformers: Hugging Face는 다양한 LLM 모델을 포함한 Transformers 라이브러리를 제공하며, 여러 NLP 작업에 사용할 수 있는 모델들을 쉽게 통합하고 관리할 수 있는 도구를 제공한다.이 플랫폼은 모델 허브(Model Hub)와 함께 제공되어, 다양한 LLM 모델을 탐색하고 활용할 수 있다. Langchain과의 비교LangChain과 LiteLLM은 모두 LLM(대형 언어 모델)을 효과적으로 통합하고 활용하는 것을 목표로 하는 도구이지만, 그 목적과 사용 사례에 있어 차이가 있다.목적 및 사용 사례LangChain: 주로 LLM을 기반으로 한 복잡한 NLP 파이프라인 및 애플리케이션을 구축하는 데 중점을 둔다.LangChain은 데이터 소스 통합, 문서 검색, 질의응답, 대화형 에이전트와 같은 고급 애플리케이션에 강점을 가지고 있다.특히, 여러 모듈과 LLM을 조합하여 복잡한 워크플로우를 생성하는 데 유용하다​LiteLLM: 여러 LLM 제공자 간의 API 통합을 단순화하는 데 초점을 맞추고 있다.LiteLLM은 다양한 LLM 모델을 하나의 표준화된 API로 호출할 수 있게 하여, 여러 모델 간의 전환을 용이하게 한다.비용 추적, 로드 밸런싱, 요청 최적화 등 여러 모델을 효율적으로 관리할 수 있는 기능을 제공하며, 다양한 LLM을 사용하는 프로젝트에서 일관된 인터페이스를 제공하는 것이 주요 목적이다​.기능 및 특징LangChain:모듈성: LangChain은 여러 모듈로 구성되어 있으며, 각 모듈을 자유롭게 조합할 수 있다. 이로 인해 복잡한 대화형 애플리케이션이나 데이터 파이프라인을 구축하는 데 적합하다.다양한 작업 지원: 텍스트 생성, 문서 검색, 데이터 클러스터링 등 다양한 NLP 작업을 지원한다.데이터 소스와의 통합: LangChain은 다양한 데이터 소스와 통합할 수 있으며, 이를 통해 LLM을 활용한 고급 검색 및 정보 처리 기능을 구현할 수 있다.LiteLLM:통합 인터페이스: 여러 LLM 제공자의 API를 하나의 표준화된 인터페이스로 통합하여, 사용자가 다양한 LLM을 쉽게 전환하고 사용할 수 있도록 힌다.비용 추적 및 관리: 각 모델 사용에 따른 비용을 추적하고 관리할 수 있는 기능을 제공하여, 예산 관리 및 비용 최적화를 돕는다.간소화된 사용: 복잡한 설정 없이 여러 LLM을 일관되게 호출할 수 있어, 빠르고 쉽게 프로젝트에 통합할 수 있다.사용 편의성LangChain: 다소 복잡한 설정과 구성을 요구할 수 있으며, 다양한 모듈과 기능을 잘 이해하고 조합해야 한다.따라서, 강력한 커스터마이징이 필요하거나 복잡한 워크플로우를 구축하려는 사용자의 요구에 더 적합하다.LiteLLM: 비교적 간단한 설정과 사용을 제공하며, 여러 LLM을 쉽게 통합하고 관리할 수 있는 단순한 인터페이스를 제공힌다. 여러 모델을 효율적으로 관리하고자 하는 사용자에게 적합히다​결론:LangChain은 복잡한 NLP 애플리케이션을 구축하려는 사용자에게 더 적합하며, 다양한 모듈과 데이터를 통합하는 데 강점을 가지고 있다.반면, LiteLLM은 여러 LLM 제공자를 사용하는 프로젝트에서 통합과 비용 관리를 단순화하려는 사용자에게 더 유용하다. LiteLLM Python SDKAWS 예시 조합(다른 기능 조합 가능) : Lambda + API Gateway + Sagemaker + Step FunctionGoogle Cloud 예시 조합 : Functions + API Gateway + Vertex AIAzure 예시 조합 : Functions + API Management + Azure ML기능 / 솔루션LiteLLMAWSGoogle CloudAzureOpenAI API + LangChain + Redis + Prometheus/Grafana로드 밸런싱 및 폴백다중 배포 간 로드 밸런싱 및 폴백 제공Lambda와 API Gateway로 자동 처리Cloud Functions와 API Gateway로 지원Functions와 API Management로 지원LangChain에서 다양한 모델에 대한 로드 밸런싱 및 폴백 처리 가능기본 안정성 로직 (쿨다운, 타임아웃, 재시도)지원 (쿨다운, 타임아웃, Retry)Lambda + Step Functions로 워크플로우 제어 가능Functions + API Gateway로 제어 가능Functions + API Management로 제어 가능LangChain 및 Custom Logic으로 구현 가능프로덕션 환경 지원Redis로 쿨다운 및 사용량 추적 관리Redis, CloudWatch, Step Functions로 지원Cloud Memorystore, Cloud Monitoring으로 지원Azure Cache, Azure Monitor로 지원Redis, Prometheus/Grafana로 쿨다운 및 사용량 추적 가능Callback 기능지원SNS, SQS, Step Functions를 통한 호출Pub/Sub, Cloud Functions로 지원Event Grid, Logic Apps로 지원Webhooks 또는 Custom Logic으로 구현 가능모델 캐싱Redis 등 다양한 옵션 지원ElastiCache (Redis) 및 S3 버킷을 통해 캐싱Cloud Memorystore (Redis), Cloud Storage로 캐싱Azure Cache for Redis, Blob Storage로 캐싱Redis로 캐싱 가능이벤트 알림Slack/웹훅 URL로 알림 지원SNS, CloudWatch, Step Functions, Lambda로 알림 설정 가능Cloud Monitoring, Pub/Sub으로 알림 설정 가능Azure Monitor, Logic Apps로 알림 설정 가능Prometheus Alerts를 통해 Slack, 이메일 등으로 알림 가능비용 및 배포 추적비용 추적 및 라우터 배포 지원AWS Cost Explorer, Budgets, CloudWatch로 비용 추적 및 관리Google Cloud Billing, Budgets로 비용 추적 및 관리Azure Cost Management로 비용 추적 및 관리Prometheus/Grafana로 비용 추적 및 모니터링 가능커스텀 설정 (API 키, 토큰 등)사용자 지정 가능AWS Parameter Store, Secrets Manager로 설정 관리Secret Manager, Config Connector로 설정 관리Azure Key Vault, Configurations로 설정 관리환경 변수 및 Custom Scripts로 설정 관리완료 토큰 사용기본 지원Lambda 및 Sagemaker에서 API 호출 시 사용Cloud Functions 및 Vertex AI에서 API 호출 시 사용Functions 및 Azure ML에서 API 호출 시 사용OpenAI API, LangChain에서 기본 지원사용자 정의 가격 측정다양한 클라우드 가격 모델 지원AWS Cost Explorer로 사용자 정의 비용 분석 가능Google Cloud Billing에서 사용자 정의 비용 분석 가능Azure Cost Management에서 사용자 정의 비용 분석 가능Custom Scripts, Prometheus/Grafana로 비용 측정 가능비동기 임베딩 함수지원Lambda + Sagemaker에서 비동기 임베딩 함수 구현 가능Cloud Functions + Vertex AI에서 비동기 임베딩 함수 구현 가능Functions + Azure ML에서 비동기 임베딩 함수 구현 가능LangChain + OpenAI API에서 비동기 임베딩 함수 구현 가능조정 엔드포인트OpenAI Moderation API 지원Sagemaker 또는 Comprehend로 조정 기능 구현 가능Vertex AI 또는 Cloud Natural Language API로 구현 가능Azure Cognitive Services로 구현 가능OpenAI Moderation API로 직접 지원Budget Manager예산 관리 및 비용 추적 기능 지원AWS Budgets 및 Cost Explorer로 예산 관리 가능Google Cloud Budgets 및 Billing으로 예산 관리 가능Azure Budgets 및 Cost Management로 예산 관리 가능Custom Scripts, Prometheus로 예산 관리 가능캐싱 초기화 및 제어메모리, Redis, S3 등 지원Redis, S3, ElastiCache로 캐싱 초기화 및 제어 가능Cloud Memorystore, Cloud Storage로 캐싱 초기화 및 제어 가능Azure Cache for Redis, Blob Storage로 캐싱 초기화 및 제어 가능Redis, Local Disk Caching 등으로 초기화 및 제어 가능통합 기능 (Langchain, Instructor 등)통합 지원AWS Lambda, Sagemaker, API Gateway로 통합 가능Google Cloud Functions, Vertex AI, API Gateway로 통합 가능Azure Functions, Azure ML, API Management로 통합 가능LangChain, OpenAI API, Hugging Face로 통합 가능LiteLLM Python SDK 장점:다양한 LLM API 통합 관리로드 밸런싱 및 쿨다운, 재시도 로직 지원모델 캐싱, 예산 관리, 이벤트 알림 기능 제공유연한 커스터마이징 옵션LiteLLM Python SDK 단점:특정 LLM 제공업체에 의존추가 설정 및 구성이 필요함제공업체별 기능에 대한 제한된 사용자 정의 참고자료Simplifying Multi-Model LLM Development: A Developer’s Guide to LiteLLM and DatabricksLiteLLM | Technology Radar | Thoughtworks United StatesProjects built on LiteLLM | liteLLMLiteLLM - Getting Started | liteLLM💥 LiteLLM Proxy Server (LLM Gateway) | liteLLMRouter - Load Balancing, Fallbacks | liteLLM","link":"/2024/08/27/LiteLLM/"},{"title":"AWS EC2 배포 중  Cannot allocate memory (12) 에러","text":"AWS EC2에 배포 중 아래와 같은 에러가 났다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344452024-09-11 06:04:15,674 - DEBUG - Attempting to release lock 140279398944864 on /root/.cache/huggingface/hub/.locks/models--Dongjin-kr--ko-reranker/002b874aa6e3b367da62d14acba828ca8d1f4bf50830747ef922d45bf71daaa2.lock2024-09-11 06:04:15,674 - DEBUG - Lock 140279398944864 released on /root/.cache/huggingface/hub/.locks/models--Dongjin-kr--ko-reranker/002b874aa6e3b367da62d14acba828ca8d1f4bf50830747ef922d45bf71daaa2.lockThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: read).Your token has been saved to /root/.cache/huggingface/tokenLogin successfulThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.Token is valid (permission: read).Your token has been saved to /root/.cache/huggingface/tokenLogin successfulTraceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import utils as utils File &quot;/app/src/utils.py&quot;, line 5, in &lt;module&gt; import controller.rag_retriever_plugin_controller as rag_retriever_plugin_controller File &quot;/app/src/controller/rag_retriever_plugin_controller.py&quot;, line 13, in &lt;module&gt; plugin = RagRetrieverPlugin(documents) File &quot;/app/src/plugins/rag_retriever_plugin.py&quot;, line 30, in __init__ super().__init__(name, model, retriever, rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/Plugin/providers/plugin_retriever.py&quot;, line 21, in __init__ self.rerank = Rerank(rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/data/retriever.py&quot;, line 66, in __init__ model = AutoModelForSequenceClassification.from_pretrained(model_path) File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;, line 564, in from_pretrained return model_class.from_pretrained( File &quot;/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3735, in from_pretrained with safe_open(resolved_archive_file, framework=&quot;pt&quot;) as f:RuntimeError: unable to mmap 2239614572 bytes from file &lt;/root/.cache/huggingface/hub/models--Dongjin-kr--ko-reranker/snapshots/5f3bb02f3baa05dea3e3c6653ada191f2cc20d91/model.safetensors&gt;: Cannot allocate memory (12)Traceback (most recent call last): File &quot;/app/src/main.py&quot;, line 3, in &lt;module&gt; import utils as utils File &quot;/app/src/utils.py&quot;, line 5, in &lt;module&gt; import controller.rag_retriever_plugin_controller as rag_retriever_plugin_controller File &quot;/app/src/controller/rag_retriever_plugin_controller.py&quot;, line 13, in &lt;module&gt; plugin = RagRetrieverPlugin(documents) File &quot;/app/src/plugins/rag_retriever_plugin.py&quot;, line 30, in __init__ super().__init__(name, model, retriever, rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/Plugin/providers/plugin_retriever.py&quot;, line 21, in __init__ self.rerank = Rerank(rerank_model) File &quot;/usr/local/lib/python3.10/site-packages/aipin/data/retriever.py&quot;, line 66, in __init__ model = AutoModelForSequenceClassification.from_pretrained(model_path) File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;, line 564, in from_pretrained return model_class.from_pretrained( File &quot;/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3735, in from_pretrained with safe_open(resolved_archive_file, framework=&quot;pt&quot;) as f: 이 오류는 Hugging Face에서 큰 모델 파일을 로드하려고 할 때 시스템 메모리가 부족해서 발생한 것이다.unable to mmap 2239614572 bytes from file 오류는 메모리 부족을 의미하며, “Cannot allocate memory (12)”라는 메시지가 이를 명확히 보여준다. 1. 메모리 사용량 줄이기 스왑 공간 증가: 시스템에 RAM이 부족하다면 스왑 공간을 늘려보는 것이 좋다. 스왑 공간은 메모리가 부족할 때 디스크를 임시 메모리로 사용하는 방법이다. 성능은 다소 느려질 수 있지만 메모리 문제를 해결하는 데 도움이 될 수 있다. Linux 시스템에서 스왑 공간을 생성하고 활성화하는 방법123456[root@ip-10-71-176-76 ~]# fallocate -l 4G /swapfile[root@ip-10-71-176-76 ~]# sudo chmod 600 /swapfile[root@ip-10-71-176-76 ~]# sudo mkswap /swapfileSetting up swapspace version 1, size = 4 GiB (4294963200 bytes)no label, UUID=0ec98152-ae9d-4bf9-ab06-b697b5c99799[root@ip-10-71-176-76 ~]# sudo swapon /swapfile","link":"/2024/09/11/AWS-EC2-Cannot-allocate-memory/"},{"title":"Batch Server 구성하기","text":"로그에 쌓여있는 대화 데이터들을 일정한 주기마다 읽어(아마 InstanceID 기준)와 요약하여 (프롬프트 이용) Redis나 NAS와 같은 저장소에 저장할 수 있도록 배치 서버를 구성하는데 필요한 Batch Server 기술 스택을 조사한다. 배치 애플리케이션이란?배치(Batch)는 일괄처리이다. 매일 전날의 데이터를 집계 해야한다고 가정할 때처럼 큰 데이터를 읽고, 가공하고, 저장한다면 해당 서버는 순식간에 CPU, I/O 등의 자원을 다 써버려서 다른 Request를 처리하지 못하게 됨. 또한 이런 집계 기능은 자주 사용되지 않음, 그러므로 이를 위해 API를 구성하는 것은 낭비임. 추가로 데이터가 너무 많아서 처리중에 실패가 난다면, 5만번째에서 실패했다면, 5만 1번째부터 다시 실행할 수 있다면 좋음 또, 같은 파라미터로 같은 함수를 실행할 경우 이미 실행한 적 있어 실패하는 기능을 지원하면 좋음이럴때 단발성으로 대용량의 데이터를 처리하는 애플리케이션이 배치 애플리케이션. Batch Server 구성 기술스택Java Spring + CronJob 프로젝트 구성 Java: 데이터를 읽고 가공하는 로직을 작성. Spring: Spring Boot를 사용하여 간편하게 프로젝트를 구성하고, 배치 작업을 위한 스케줄링. Redis: Java에서 Redis와 상호작용하기 위해 Redis 클라이언트를 사용. CronJob: 주기적으로 Java 프로그램을 실행하기 위한 크론 작업을 설정. Redis 클라이언트 Java에서 Redis에 연결하기 위해 다음과 같은 Redis 클라이언트를 사용: Jedis: 간단하게 사용 가능한 Redis 클라이언트. Lettuce: 비동기 작업과 고성능을 요구할 때 사용하는 클라이언트. CronJob 설정 먼저 리눅스의 crontab에 Java 프로그램을 실행하도록 설정. 이를 위해 .jar 파일을 실행하는 명령어를 설정. 예를 들어, Java 프로젝트를 Maven이나 Gradle로 빌드한 후 log_to_redis.jar 파일을 얻었다고 가정하면, 다음과 같이 설정 가능: 1crontab -e 그리고 crontab에 다음과 같이 입력하여 매일 특정 시간마다 Java 애플리케이션을 실행하도록 설정 10 * * * * java -jar /path/to/log_to_redis.jar 위 명령은 매일 정각에 log_to_redis.jar를 실행 Java 코드 작성 로그 파일에서 데이터를 읽고 편집 후 Redis에 저장하는 로직을 작성. 로그 파일 읽기: BufferedReader 등을 이용하여 로그 파일을 읽음. 데이터 편집: 데이터를 가공하거나 필요한 형식으로 변환. Redis 저장: Jedis나 Lettuce를 이용하여 Redis에 데이터를 저장. 예시코드 123456789101112131415161718192021222324252627282930313233343536import redis.clients.jedis.Jedis;import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;public class LogToRedis { public static void main(String[] args) { String logFilePath = &quot;/path/to/logfile.log&quot;; String redisKey = &quot;log_data&quot;; // Redis 연결 try (Jedis jedis = new Jedis(&quot;localhost&quot;)) { // 로그 파일 읽기 try (BufferedReader br = new BufferedReader(new FileReader(logFilePath))) { String line; StringBuilder logData = new StringBuilder(); while ((line = br.readLine()) != null) { // 로그 데이터 가공 logData.append(processLogLine(line)).append(&quot;\\n&quot;); } // Redis에 저장 jedis.set(redisKey, logData.toString()); System.out.println(&quot;Log data saved to Redis under key: &quot; + redisKey); } catch (IOException e) { System.err.println(&quot;Error reading log file: &quot; + e.getMessage()); } } } private static String processLogLine(String line) { // 로그 데이터를 가공하는 로직 (필요에 맞게 구현) return line.toUpperCase(); // 예: 로그 데이터를 대문자로 변환 }} CronJob과 Java 연동 CronJob은 위에서 설정한 것처럼 crontab에서 특정 시간마다 Java 프로그램을 실행하도록 설정됨. 주기적인 실행은 cron이 관리하고, 로그를 처리하는 부분은 Java 코드가 처리. CronJob이란특정시간 또는 일정한 주기마다 작업을 자동으로 실행하기 위해 사용하는 유닉스 계열 시스템의 스케줄러 주요 개념 cron : 유닉스 기반 시스템에서 주기적으로 명령을 실행하는 데 사용하는 데몬(백그라운드 프로세스) crontab : cron 작업을 설정하고 관리하는 테이블 파일 또는 유틸리티로, 사용자가 명령어를 입력해 작업을 정의 cronjob : 주기적으로 실행되는 프로그램들 Command sudo systemctl enable cron : cron을 활성화 시킴. crontab -l : 현재 등록되어 있는 cron job을 보여줌 crontab -e : cron job을 등록할 수 있음. 첫 실행 시에 editor(nano, vim 등)를 선택하는 선택지가 출력 crontab -r : cron job을 지울 수 있음 cronjob 설정 방법cron job 등록 방법에 대해서. 1minute | hour | dom(day of month) | month | dow(day of week) | command 순서는 이렇게 된다. command 같은 것들은 “cd ~/my_project &amp;&amp; bash run.sh” 이렇게도 가능하고, 단순하게 bash run.sh 하나만 작성해도 괜찮다. 예시 crontab -e 로 crontab 편집기를 연다. 0 5 * * “sudo systemctl restart mongod” 를 문서의 맨 아래에 삽입한다. 매월/매일 05:00am에 MongoDB가 재시작된다. Spring Batch(Java)Spring Batch는 대용량 데이터를 처리하기에 적합하며, 주기적으로 실행되는 배치 작업을 쉽게 관리할 수 있다. 프로젝트 설정 Spring Batch 프로젝트를 설정하려면 Spring Initializr 또는 Maven/Gradle을 사용하여 기본 프로젝트를 구성한다. Redis와 Spring Batch를 사용하기 위해 다음 의존성을 추가한다. Gradle 의존성 예시: 12345dependencies { implementation 'org.springframework.boot:spring-boot-starter-batch' implementation 'org.springframework.boot:spring-boot-starter-data-redis' implementation 'redis.clients:jedis'} Spring Batch 구성 Spring Batch는 Job, Step, Reader, Processor, Writer 등으로 구성됩니다. 로그 파일에서 데이터를 읽고, 가공하고, Redis에 저장하기 위해 각각의 구성 요소를 설정. 주요 구성: Job: 배치 작업의 전체적인 단위. Step: Job을 이루는 각각의 처리 단계. ItemReader: 데이터를 읽어오는 역할. ItemProcessor: 데이터를 가공하는 역할. ItemWriter: 가공된 데이터를 Redis에 저장하는 역할. 로그 파일 읽기 (ItemReader) 로그 파일을 읽기 위한 FlatFileItemReader를 설정합니다. 이 리더는 CSV 또는 텍스트 파일의 각 줄을 읽는다. 1234567891011121314151617181920@Beanpublic FlatFileItemReader&lt;String&gt; reader() { FlatFileItemReader&lt;String&gt; reader = new FlatFileItemReader&lt;&gt;(); reader.setResource(new FileSystemResource(&quot;/path/to/logfile.log&quot;)); reader.setLineMapper(new DefaultLineMapper&lt;String&gt;() { { setLineTokenizer(new DelimitedLineTokenizer() { { setNames(&quot;logLine&quot;); } }); setFieldSetMapper(new BeanWrapperFieldSetMapper&lt;String&gt;() { { setTargetType(String.class); } }); } }); return reader;} 로그 데이터 가공 (ItemProcessor) 로그 데이터를 가공하는 ItemProcessor를 작성한다. 이 프로세서는 입력된 로그 데이터를 가공하거나 변환한다. 12345678910111213141516171819202122@Beanpublic ItemProcessor&lt;String, String&gt; processor() { return logLine -&gt; { // 로그 데이터를 처리 (예: 대문자로 변환) return logLine.toUpperCase(); };}~~~java5. Redis 저장 (ItemWriter)ItemWriter는 가공된 데이터를 Redis에 저장하는 역할을 한다. Jedis나 Spring Data Redis를 이용하여 데이터를 Redis에 저장.~~~java@Beanpublic ItemWriter&lt;String&gt; writer(Jedis jedis) { return logLines -&gt; { for (String logLine : logLines) { jedis.set(&quot;log_data_key&quot;, logLine); // Redis에 저장 } };} Spring Batch Job 및 Step 구성 이제 Reader, Processor, Writer를 사용하여 Job과 Step을 정의한다. Step은 로그 데이터를 읽고 가공한 후 Redis에 저장하는 단위 작업 1234567891011121314151617@Beanpublic Job logFileToRedisJob(JobBuilderFactory jobBuilderFactory, Step step1) { return jobBuilderFactory.get(&quot;logFileToRedisJob&quot;) .incrementer(new RunIdIncrementer()) .start(step1) .build();}@Beanpublic Step step1(StepBuilderFactory stepBuilderFactory, FlatFileItemReader&lt;String&gt; reader, ItemProcessor&lt;String, String&gt; processor, ItemWriter&lt;String&gt; writer) { return stepBuilderFactory.get(&quot;step1&quot;) .&lt;String, String&gt;chunk(10) // 한 번에 처리할 데이터 크기 .reader(reader) .processor(processor) .writer(writer) .build();} 스케줄링 설정 Spring Batch를 일정 시간마다 실행하려면 Spring의 @EnableScheduling과 @Scheduled를 사용하여 스케줄링을 설정할 수 있다. 123456789101112131415161718192021@EnableScheduling@Configurationpublic class BatchScheduler { private final JobLauncher jobLauncher; private final Job job; @Autowired public BatchScheduler(JobLauncher jobLauncher, Job job) { this.jobLauncher = jobLauncher; this.job = job; } @Scheduled(cron = &quot;0 0 * * * ?&quot;) // 매 시간 정각에 실행 public void runBatchJob() throws Exception { JobParameters params = new JobParametersBuilder() .addString(&quot;time&quot;, String.valueOf(System.currentTimeMillis())) .toJobParameters(); jobLauncher.run(job, params); }} 위의 코드는 크론 표현식을 사용하여 배치 작업을 매 시간마다 실행하게 설정한 예이다. @Scheduled의 크론 표현식을 조정하여 원하는 주기에 맞게 실행할 수 있다. Redis 설정 Spring Boot의 application.yml 파일을 통해 Redis 설정을 추가한다. 1234spring: redis: host: localhost port: 6379 또는 Jedis를 직접 사용하려면 Jedis 인스턴스를 빈으로 등록해 사용할 수 있다. 1234@Beanpublic Jedis jedis() { return new Jedis(&quot;localhost&quot;, 6379);} 전체 흐름: Spring Batch가 일정 시간마다 Job을 실행힌다. ItemReader가 로그 파일에서 데이터를 읽어온다. ItemProcessor가 데이터를 가공한다. ItemWriter가 Redis에 데이터를 저장한다.","link":"/2024/09/11/BatchServer/"},{"title":"CronJob 이란","text":"특정시간 또는 일정한 주기마다 작업을 자동으로 실행하기 위해 사용하는 유닉스 계열 시스템의 스케줄러 주요 개념 cron : 유닉스 기반 시스템에서 주기적으로 명령을 실행하는 데 사용하는 데몬(백그라운드 프로세스) crontab : cron 작업을 설정하고 관리하는 테이블 파일 또는 유틸리티로, 사용자가 명령어를 입력해 작업을 정의 cronjob : 주기적으로 실행되는 프로그램들 크론 작업 추가 crontab 명령어로 Cron을 사용할 수 있다. 공통 설정(Common Settings)— 일반적으로 사용되는 간격을 선택. 시스템은 분, 시간, 일, 월과 평일 (Minute, Hour, Day, Month, and Weekday) 텍스트 상자에서 적절한 설정을 구성. 분(Minute)— 크론 작업을 실행할 각 시간의 분 또는 크론 작업이 실행되는 매 시간 사이의 분 수 시간(Hour)— 크론 작업을 실행할 각 날짜의 시간 또는 크론 작업이 실행되는 매 시간 사이의 시간 수 일(Day)— 크론 작업을 실행할 달의 날짜 또는 크론 작업이 실행되는 매 시간 사이의 일 수 월(Month)— 크론 작업을 실행할 년도의 월 또는 크론 작업이 실행되는 매 시간 사이의 개월 수 평일(Weekday)— 크론 작업을 실행할 주의 평일 명령(Command)텍스트 상자에, 시스템에서 실행할 명령을 입력. Command sudo systemctl enable cron : cron을 활성화 시킴. crontab -l : 현재 등록되어 있는 cron job을 보여줌 crontab -e : cron job을 등록할 수 있음. 첫 실행 시에 editor(nano, vim 등)를 선택하는 선택지가 출력 crontab -r : cron job을 지울 수 있음 cronjob 설정 방법cron job 등록 방법에 대해서. 1minute | hour | dom(day of month) | month | dow(day of week) | command 순서는 이렇게 된다. command 같은 것들은 “cd ~/my_project &amp;&amp; bash run.sh” 이렇게도 가능하고, 단순하게 bash run.sh 하나만 작성해도 괜찮다. 예시crontab -e 로 crontab 편집기를 연다. 0 5 * * “sudo systemctl restart mongod” 를 문서의 맨 아래에 삽입한다. 매월/매일 05:00am에 MongoDB가 재시작된다.","link":"/2024/09/11/CronJob/"},{"title":"LangSmith란","text":"LangSmith란 LLM 애플리케이션을 디버깅, 테스트, 평가, 모니터링할 수 있는 개발자 플랫폼이다. 프로젝트나 LangChain 학습을 시작한다면, LangSmith를 설정 후 진행하는 것이 좋다고 한다. LangSmith는 굳이 분류하자면 LLMOps 도구이다.DevOps의 LLM 버전이라고 이해하면 된다.LLM 어플리케이션을 잘 만들고 운영하기 위한 도구이고, 아래와 같은 기능들을 지원한다. 현재 사용자가 어떻게 채팅을 하고 있는지 추적하고, 피드백을 수집한다. 비용은 얼마나 차징이 되고 있는지, 응답 시간은 얼마나 걸렸는지를 실시간으로 추적한다. 데이터셋을 구성하여 LLM 어플리케이션을 자동 평가하고, 개발하는 일들을 도와준다. 이러한 핵심 기능들을 토대로 LLMOps라고 분류할 수 있으나, 개발사인 랭체인에서는 “Developer Platform”이라고 칭하고 LLMOps라는 용어를 사용하지는 않는다. LangSmith는 기본적으로 LLM을 넘어 멀티모달리티를 지원하고, LLM 자체를 잘 만드는 것 보다 LLM을 잘 사용하는 방법에 더 중점을 두고 있어서, LLMOps라는 용어가 좋은 용어라고 보기는 어렵다고 생각한다. 1. 추적과 디버깅 랭스미스에서는 위와 같이 각 중간 과정의 in/out 결과를 보여준다.Retrieve가 잘 안되었는지, 올바른 근거 자료를 찾아 줬음에도 LLM이 대답을 못하고 있는지, 추론이 가능하다. LLM 모델을 바꾸거나, Prompt를 바꾸거나, 다양하게 모두 돌린 후 간단히 비교하기도 좋다.비용도 나오고, 수행시간도 나와서 어디서 비용이 많이 드는지, 너무 느리지 않는지 모두 확인 가능하다. 개선을 진행할수록 소수의 질문에서만 문제가 생기는데, 이를 체계화해서 로깅을 남기기보단, 그 시간을 단축시켜주는 도구로서 유용하다. 2. 데이터 수집과 평가LLM 어플리케이션의 알파이자 오메가는 사실 데이터셋이다.데이터셋을 구성하고 사용하기 좋게 잘 구성이 되어있다. 허깅페이스처럼 연구/개발자가 직접 데이터셋을 올릴 수 있다. 연동된 서비스의 유저 로그를 데이터셋으로 자동 수집 가능하다. 연동된 서비스의 유저 피드백을 같이 수집 가능하다. 자동으로 데이터를 가공해서 추가수집 가능하다. 챗 서비스를 서빙한다고 하면, 유저의 질문, 시스템의 답변, 유저의 만족도 조사결과까지 맞물려 데이터 수집이 되니 매우 유용하다.모아다가 강화학습을 할 수도 있고, A/B 테스트를 할 수도 있고, 불만족스러웠던 데이터만 따로 백테스팅을 할 수도 있고, 사용하기 나름이다. 어떤 방식으로든 모아진 데이터는 평가에 활용을 하기 좋게 구성되어 있다. 아래와 같이 평가 로직을 등록할 수 있는데, API Key와 함께 프롬프트를 써주면, LLM이 알아서 평가를 해준다.아래 그림은 답변이 도움이 되었는지를 수치화하는 평가로직인데, 프롬프트를 템플릿화해서 다 만들어놔서 편하다. 평가로직을 등록만 해두면, 서비스에서 발생하는 내 답변들을 자동으로 평가해 데이터화 해준다.. LLM 어플리케이션은 “평가”가 어려운 경우가 많다. 정석적이기 때문이다. 이 부분을 해결하기 위해 양질읠 데이터, LLM을 다시 평가 판사 (LLM as judge)로 사용하기, 외 기타 여러가지 방법이 있다.이를 잘 지원하는 도구로서 유용하다. 3. 협업도메인 전문가들이 데이터를 분석하고, 평가해서 데이터 라벨링을 해주고, 의견을 주고 해야하는데, 협업하기가 쉽지 않다. 데이터를 어떤 형태로 뽑아야 할지, 도메인 전문가들이 어떻게 라벨링을 달아서 시스템에 다시 올려야할지, 이 부분을 잘 도와준다. 데이터들을 Annotation queue에 넣어서 사람을 지정해 줄 수가 있다. 그러면 도메인전문가들이 데이터를 보고 정성평가를 달아주고, 데이터셋에 연동이 된다. 모두 웹 인터페이스로 사용이 가능해서, 개발이 익숙하지 않은 사람들과 협업이 매우 편하다. 4. 사용법4.1. LangSmith 설치1pip install -U langsmith 4.2. API Key 발급4.3. 환경 설정12345export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=&lt;your-api-key&gt;# The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=&lt;your-openai-api-key&gt; 4.4. trace를 기록하기1234567891011121314151617import openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str): result = client.chat.completions.create( messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}], model=&quot;gpt-3.5-turbo&quot; ) return result.choices[0].message.contentpipeline(&quot;Hello, world!&quot;)# Out: Hello there! How can I assist you today? 4.5. Evaluation 실행하기12345678910111213141516171819202122232425262728293031323334from langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = &quot;Sample Dataset&quot;dataset = client.create_dataset(dataset_name, description=&quot;A sample dataset in LangSmith.&quot;)client.create_examples( inputs=[ {&quot;postfix&quot;: &quot;to LangSmith&quot;}, {&quot;postfix&quot;: &quot;to Evaluations in LangSmith&quot;}, ], outputs=[ {&quot;output&quot;: &quot;Welcome to LangSmith&quot;}, {&quot;output&quot;: &quot;Welcome to Evaluations in LangSmith&quot;}, ], dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example): return {&quot;score&quot;: run.outputs[&quot;output&quot;] == example.outputs[&quot;output&quot;]}experiment_results = evaluate( lambda input: &quot;Welcome &quot; + input['postfix'], # Your AI system goes here data=dataset_name, # The data to predict and grade over evaluators=[exact_match], # The evaluators to score the results experiment_prefix=&quot;sample-experiment&quot;, # The name of the experiment metadata={ &quot;version&quot;: &quot;1.0.0&quot;, &quot;revision_id&quot;: &quot;beta&quot; },) AWS에 Langsmith를 배포하여 사용 가능한가?Langsmith-Pricing 위 링크에 의하면, Enterprise 플랜을 사용하는 경우 데이터가 환경을 벗어나지 않도록 AWS, GCP 또는 Azure의 K8S 클러스터에서 실행되도록 Langsmith 제공 가능하다. 셀프호스팅 Architecture Overview 랭스미스는 사용자가 제어하는 클라우드 환경에서 Kubernetes(권장) 또는 Docker를 통해 실행할 수 있다. LangSmith 애플리케이션은 5개의 LangSmith 서버와 3개의 상태 저장 서비스로 구성된다. 랭스미스 프런트엔드 랭스미스 백엔드 랭스미스 플랫폼 백엔드 LangSmith Playground LangSmith Queue clickhouse db postgres redis 각각에 대한 설명은 Architectural overview를 참고 1. Kubernetes에 설치하기Self-hosting LangSmith on Kubernetes 2. Docker에 설치하기Self-hosting LangSmith with Docker Docker 설치, 최소 4개의 vCPU, 16GB 메모리, 충분한 디스크공간 필요 LangSmith 라이센스 키 필요 Docker compose를 사용하여 실행, 프로덕션에서는 k8s 권장 docker-compose.yaml 배포 검증12curl localhost:1980/info{&quot;version&quot;:&quot;0.5.7&quot;,&quot;license_expiration_time&quot;:&quot;2033-05-20T20:08:06&quot;,&quot;batch_ingest_config&quot;:{&quot;scale_up_qsize_trigger&quot;:1000,&quot;scale_up_nthreads_limit&quot;:16,&quot;scale_down_nempty_trigger&quot;:4,&quot;size_limit&quot;:100,&quot;size_limit_bytes&quot;:20971520}} 장단점 사용하기 매우 편하다, 문서도 잘 되어있고, 코드 구현도 쉽다. LangChain과 함께 사용하면 더 쉽고, 잘 연동 된다. 오픈소스가 아니라서 상업 수준으로 사용하려면, 비용을 내야한다. ReferencesLangSmith Quick StartLangSmith,사용후기","link":"/2024/09/25/LangSmith/"},{"title":"Langchain이란?","text":"랭체인은 언어모델 기반 앱을 매우 쉽고 빠게 구축할 수 있도록 미리 만들어진 수많은 구성 요소와 모듈이 포함된 프레임워크이다. GPT 앱을 구축하려면 OpenAI 파이썬 패키지를 사용하는 것만으로는 충분하지 않다. LangChain은 많은 Module을 가지고 있고, 광범위해서 다양한 것을 할 수 있다. 예를들면 LangChain을 사용하지 않고 OpenAI API의 GPT4만을 사용한 것과 비교해서, LangChain에는 Memory를 위한 Module이 있지만, GPT(Open AI API)에는 없기 때문에 우리가 직접 구현해야 한다. 하지만 LangChain을 사용하면 6가지 종류의 Memory를 공짜로 얻을 수 있다. 그저 연결하기만 하면 작동한다. LangChain에는 Document를 위한 Module과, Embedding을 위한 Module이 있다. Prompt를 위한 Module도 LLM을 위한 Module도, Chat model을 위한 Module까지 LangChain에는 많은 것들이 있다. 또한 LangChain은 어플리케이션의 Component를 바꿀 수 있게 해준다. LangChain이 LLM 어플리케이션을 만들기 위한 모든 필요 요소들의 호환을 위한 계층이 되는 것이다. 또한 LangSmith라는 LLM 어플리케이션을 위한 Debugger도 있다.","link":"/2024/09/25/Langchain/"},{"title":"LangChain의 메모리 관리","text":"랭체인에는 5가지 정도 종류의 메모리가 있는데 각자 저장방식도 다르고, 각자만의 장단점이 있다. 챗봇에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 잆다. 오픈 AI에서 제공하는 기본 API는 랭체인 없이 사용 사용할 수 있는데, 메모리를 지원하지 않는다 (stateless) 그렇기에, 오늘은 랭체인에서 제공하는 메모리 사용법에 대해 적어보고자 한다. GPT는 어떻게 메모리 관리를 하는가세션 메모리와 단기 컨텍스트 관리 GPT는 단기 컨텍스트만 유지. 이전 대화의 일부가 필요할 때 최근 메시지를 기반으로 적응하지만, 세션이 종료되면 맥락이 초기화 됨. 캐시와 세션 관리 Redis 또는 Memcached와 같은 인메모리 데이터베이스를 사용해, 실시간 대화 상태를 빠르게 유지하고 관리하는 것으로 보임. 이러한 캐시는 짧은 시간동안만 상태를 유지하여 빠른 응답을 지원한다. 대화가 끝나면 캐시된 데이터는 자동으로 소멸한다. 장기 데이터 저장소 (비활성화된 기능) 만약 장기적 사용자 메모리 기능이 활성화된다면, 이를 위헤 데이터베이스(PostgreSQL, MongoDB)나 클라우드 오브젝트 스토리지 (AWS S3 등)를 사용할것으로 추정한다. ChatGPT는 주로 Transformer 모델, 토큰화 시스템, 그리고 Redis와 같은 캐시를 활용해 실시간 대화의 상태를 유지하는것으로 보인다.장기적인 메모리 기능은 아직 실험 단계에 있으며, 현재는 세션 단위의 단기 메모리만 지원한다.OpenAI는 내부 시스템 구성에 대한 구체적인 기술 스택을 직접 공개하지 않고 있다. LangChain의 메모리 관리LangChain에서 크게 5가지 정도의 메모리를 사용할 수 있다. ConversationBufferMemory ConversationBufferWindow ConversationSummaryMemory ConversationSummaryBufferMemory ConversationKGMemoryOpen AI에서 제공하는 기본 API는 LangChain 없이 사용할 수 있는데, 메모리를 지원하지 않는다 (stateless) Conversational Buffer 메모리 단순히 이전 대화 내용 전체를 저장하는 메모리 단점 : 대화 내용이 길어질수록 메모리도 계속 커지니까 비효율적이다. 모델 자체에는 메모리가 없다, 그래서 우리가 모델에게 요청을 보낼때, 이전 대화 기록 전체를 같이 보내야한다. 유저와 대화가 길어질수록, 모델에게 매번 보내야 될 대화 기록이 길어진다. → 비효율적 예시 1234from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory()memory.save_context({&quot;input&quot;: &quot;Hi&quot;}, {&quot;output&quot;: &quot;How are you?&quot;})memory.load_memory_variables({}) 1{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]} text completion 할 때 유용하다. (에측을 해야할 때, 텍스트를 자동완성하고 싶을때) 그러나 만약 chat model과 작업을 하면 AI 메시지와 Human 메시지가 다 필요하다. 메모리 종류와 무관하게 API는 다 똑같다. 모든 메모리는 save_context, load_memory_variables라는 함수를 가지고 있다. 이 메모리는 대화 내용 전체를 저장하는 메모리이다. 대화가 길어질수록 메모리에 수많은 내용이 계속 쌓이게 되어 비효율적이다. ConversationBufferWindow (대화 버퍼 윈도우)대화의 특정 부분(가장 최근부분)만을 저장하는 메모리. 예를들어 최근 5번째까지의 메시지를 저장한다고 했을 때, 6번째 메시지가 추가 됐을때 가장 오래된 메시지는 버려지는 방식. 저장 범위는 직접 설정 가능 메모리를 특정 크기로 유지할 수 있다는 것이 이 메모리의 큰 장점. 단점 : 챗봇이 전체 대화가 아니라 최근 대화에만 집중. 예시 12345678910111213from langchain.memory import ConversationBufferWindowMemorymemory = ConversationBufferWindowMemory( return_messages=True, k=4)def add_message(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})add_message('1', '1')add_message('2', '2')add_message('3', '3')add_message('4', '4')print(memory.load_memory_variables({})) 최근에 일어난 대화에만 집중한다. output 12345678{'history': [HumanMessage(content='1'),AIMessage(content='1'),HumanMessage(content='2'),AIMessage(content='2'),HumanMessage(content='3'),AIMessage(content='3'),HumanMessage(content='4'),AIMessage(content='4')]} ConversationSummaryMemory ConversationSummaryMemory는 llm을 사용한다. ConversationSummaryMemory는 message를 그대로 저장하는 것이 아니라, conversation의 요약을 자체적으로 해준다. 초반에는 이전의 메모리들보다 더 많은 토큰과 저장공간을 차지하게 된다. 예시 12345678910from langchain.memory import ConversationSummaryMemoryfrom langchain.chat_models import ChatOpenAIllm = Chat OpenAI(temperature=0.1)memory = ConversationSummaryMemory(llm=llm)def add_memory(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})def get_history(): return memory.load_memory_variables({})add_message(&quot;Hi I'm hamin, I'm in Brooklyn.&quot;, &quot;Wow that is so cool!&quot;)get_history() ConversationKnowledge Graph Memory LLM을 사용하는 memory class 대화 중의 엔티티의 knowledge graph를 만든다. 가장 중요한 것들만 뽑아내는 요약본 같은 것이다. 요약을 하지만 대화에서 entity를 뽑아내는 것이다. Entity를 뽑아낸다는 것은 사람, 장소, 날짜, 사건 등과 같은 의미 있는 개체를 추출하고 Knowledge graph를 구성한다는 것이다. 용도 맥락 유지 “그는 지금도 테슬라를 운영하고 있나요” → 그를 일론 머스크로 연결 정보 간 탐색 “스페이스X와 테슬라 사이에는 어떤 관계가 있나요” 대화에 따른 실시간 지식 업데이트 “테슬라는 최근 사이버트럭을 출시했어” 예시 1234567891011from langchain.memory import ConversationKGMemoryfrom langchain.chat_models import ChatOpenAIllm = Chat OpenAI(temperature=0.1)memory = ConversationKGMemory( llm=llm, return_messages=True,)def add_memory(input, output): memory.save_context({&quot;input&quot;: input}, {&quot;output&quot;: output})add_message(&quot;Hi I'm hamin, I'm in Brooklyn.&quot;, &quot;Wow that is so cool!&quot;)memory.load_memory_variables({&quot;input&quot;: &quot;who is hamin&quot;}) 단순 대화 이상의 구조화된 지식을 구성하고 더 나은 응답을 제공 맥락 유지와 다중 개체간의 관계 탐색에 유용 LLM chainoff-the-shelf(일반적인 목적을 가진 chain을 의미) chain으로 빠르게 시작할 수 있게해서 좋지만, 프레임워크를 다루느라 머리 싸매거나 off-the-shelf chain을 커스텀하기보다 직접 만들고 싶을 때, langchain expression 언어를 활용해서 어플리케이션을 만들 수 있다. 1234567891011121314151617181920212223242526from langchain.memory import ConversationSummaryBufferMemoryfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import LLMChainfrom langchain.prompts import PromptTemplatellm = ChatOpenAI(temperature=0.1)memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=120, memory_key=&quot;chat_history&quot;)template = &quot;&quot;&quot; You are a helpful AI talking to a human. {chat_history} Human:{question} You:&quot;&quot;&quot;chain = LLMChain( llm=llm, memory=memory, prompt=PromptTemplate.from_template(template), verbose=True,)chain.predict(question=&quot;My name is hamin&quot;)memory.load_memory_variables({})chain.predict(question=&quot;I live in brooklyn&quot;)chain.predict(question=&quot;What is my name?&quot;) interaction 토큰 수가 120개보다 많으면 가장 오래된 interaction을 요약해줌 최신 내용을 그대로 유지하고 대화 기록을 요약하기 시작함. verbose를 이용하여 프롬프트 디버깅이 가능하다. Chat based Memorymemory 클래스는 memory를 두가지 방식으로 출력할 수 있다. 문자열 형태, message 형태 대화기반의 채팅으로 바꾸고 싶다면 return_message=True를 넣어줘야한다 123456789101112 from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=120, memory_key=&quot;chat_history&quot;, return_messages=True)prompt = ChatPromptTempate.from_messsages([ (&quot;system&quot;, &quot;You are a helpful AI talking to a human&quot;), MessagePlaceholder(variable_name=&quot;chat_history&quot;), (&quot;human&quot;, &quot;{question}&quot;)]) 이렇게 넣어주면 문자열기반 템플릿 대신 ChatPromptTemplate을 불러올것이다. ConversationBufferMemory로부터 요약본을 받아올 때, 시스템 메시지도 추가된다. MessagesPlaceholder Caching캐싱을 사용하면 LM(언어모델)의 응답을 저장할 수 있다. 예) 채팅 봇이 있고 그 채팅 봇이 항상 똑같은 질문을 받는다면 계속 답변을 생성하지 않고, 이미 답변한 답을 캐싱을 이용하여 재사용한다. InMemoryCache123456from langchain.globals import set_llm_cache, set_debugfrom langchain.cache import InMemoryCacheset_llm_cache(InMemoryCache())set_debug(True)chat.predict(&quot;How do you make italian pasta&quot;)chat.predict(&quot;How do you make italian pasta&quot;) 모든 response가 메모리에 저장된다. 첫 predict와 두번째 predict의 응답속도가 다르다. set_debug는 프롬프트로 표시되고 있는 모든 것들을 보여준다. 데이터베이스 캐싱 123from langchain.globals import set_llm_cache, set_debugfrom langchain.cache import SQLiteCacheset_llm_cache(SQLiteCache(&quot;cache.db&quot;)) 실행시키면 자동으로 sqlite 데이터베이스가 생성된다. How to cache LLM responses | 🦜️🔗 LangChain ChatOpenAI의 streaming=True 데이터베이스에 히스토리 저장하기langchain docs의 integration을 보면 Memory를 백업할 수 있는 다양한 DB들을 확인할 수 있다.Message histories | 🦜️🔗 LangChainex) MongoDB, Redis, PostgresQL Redis Chat Message HistoryRedisChatMessageHistory를 이용하여 read/write에 대한 low-latency로 chat message를 저장하고 관리할 수 있다. Setup1% pip install -qU langchain-redis langchain-openai redis 1docker run -d -p 6379:6379 redis:latest Importing Required Libraries123456from langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.messages import AIMessage, HumanMessagefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_openai import ChatOpenAIfrom langchain_redis import RedisChatMessageHistory RedisChatMessageHistory 사용 예시 123456789# RedisChatMessageHistory 초기화history = RedisChatMessageHistory(session_id=&quot;user_123&quot;, redis_url=REDIS_URL)# Add messages to the historyhistory.add_user_message(&quot;Hello, AI assistant!&quot;)history.add_ai_message(&quot;Hello! How can I assist you today?&quot;)# Retrieve messagesprint(&quot;Chat History:&quot;)for message in history.messages: print(f&quot;{type(message).__name__}: {message.content}&quot;) output 123Chat History:HumanMessage: Hello, AI assistant!AIMessage: Hello! How can I assist you today? Redis Chat Message History | 🦜️🔗 LangChain MongoDB Chat Message History1pip install -U --quiet langchain-mongodb 12# os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# os.environ[&quot;LANGCHAIN_API_KEY&quot;] = getpass.getpass() 123456789from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistorychat_message_history = MongoDBChatMessageHistory( session_id=&quot;test_session&quot;, connection_string=&quot;mongodb://mongo_user:password123@mongo:27017&quot;, database_name=&quot;my_db&quot;, collection_name=&quot;chat_histories&quot;,)chat_message_history.add_user_message(&quot;Hello&quot;)chat_message_history.add_ai_message(&quot;Hi&quot;) 1chat_message_history.messages 1[HumanMessage(content='Hello'), AIMessage(content='Hi')]","link":"/2024/10/22/LangChain-Memory/"},{"title":"멀티턴, 대화 요약를 위한 DB 설계","text":"멀티턴, summary를 지원할 수 있도록 DB 설계를 해보도록 하겠다. DB 종류 선정 SQLite : 간단하고 빠르며, 챗봇 애플리케이션의 대화 기록을 관리하는 데 적합. 설치 및 설정이 간단 : 파일 기반 DB이므로 별도의 서버 설정이 필요 없고, 파일 하나로 데이터베이스를 다룰 수 있어 빠르게 사용 가능. 경량화 된 구조 : 작은 규모의 데이터베이스에 적합하며, 챗봇 대화 로그와 같이 비교적 단순한 데이터 구조를 다룰 때 효과적 빠른 성능 : 파일 기반 데이터베이스로, 챗봇 애플리케이션의 빠른 응답에 적합. 호환성 : 대부분의 언어와 플랫폼에서 기본으로 지원 결론 : 챗봇 대화 로그나 요약을 저장하는 정도의 용도라면 SQLite가 효율적, 만약 데이터베이스의 확장성과 분산 처리가 필요하다면 PostgreSQL 또는 MySQL 같은 데이터베이스로 전환하는 것이 좋다.09. SQLite 에 대화내용 저장 Redis 고속 데이터 처리 : 메모리 기반, I/O 빠르다, 대화 데이터의 실시간 처리가 중요한 챗봇 애플리케이션에 적합. TTL 기능 : 데이터 만료시간 설정 가능 경량 데이터 처리 : 데이터 구조가 간단하여 작은 규모의 데이터를 빠르게 저장하고 접근 가능. 데이터 영구성 부족 : 인메모리DB, 서버 재기동시 데이터 손실 → 스냅샷과 AOF 설정 추가 필요, but 완전한 영구 저장 솔루션보다는 다소 제한적. 대규모 데이터 저장에 적합하지 않음 : 인메모리DB, 메모리, 비용문제 발생 가능성 데이터 일관성 문제 : 데이터일관성 보다는 속도에 중점 → 상관 없을것 같기도 함.. PostgreSQL 확장성 및 안정성 : 대규모 데이터를 안정적으로 처리할 수 있는 확장성이 뛰어난 데이터베이스. 트랜잭선 지원 : 트랜잭션 처리 및 동시성 제어에 강력한 기능 제공. 데이터의 일관성과 무결성 보장. 인덱싱및 고급 쿼리 지원 : 요약이나 특정 대화만 빠르게 조회해야 할 때 성능을 높이는 데 유리. 설치 및 관리의 복잡성 : 서버 설치가 필요하고 구성도 다소 복잡함. 메모리 및 디스크 요구사항 : 시스템 리소스를 더 많이 요구, 소규모 프로젝트나 단순한 대화 기록 저장 용도로는 다소 과함. 결론 : 챗봇 데이터가 많아지고 검색이나 통계 분석이 중요한 경우 매우 적합한 선택. 다만 프로젝트 초기 개발 단계이거나 간단한 챗봇 로그 용도라면 PostgreSQL의 장점 활용 못할수도 있음. 1안 : SQLite 사용테이블 정의서한 턴의 대화마다 요약을 한다고 가정 했을 때. Table 명 : interactions No Column Name Attribute Name Type Null Keys Description 1 conversation_id transaction_id INTEGER NN PK Sequence 2 user_uuid user_id TEXT NN user id 3 user_input message TEXT NN user 발화 4 agent_response response TEXT NN agent 응답 5 summary summary TEXT NN 대화내용 요약 6 timestamp timestamp TIMESTAMP NN agent 응답 Redis + PostgreSQL? Redis로 실시간 대화 기록 처리 최신 대화 기록과 같은 실시간 데이터를 Redis에 저장하여 빠르게 읽고 쓰기 가능. 대화 중에는 Redis에서 데이터 관리. PostgreSQL로 영구 데이터 저장 주요 대화 기록이나 요약, 사용자 정보 등의 영구적인 데이터를 PostgreSQL에 저장해 장기적으로 유지 Redis에서 PostgreSQL로 데이터마이그레이션 Redis에 저장된 데이터를 주기적으로 PostgreSQL로 이전하는 스케줄러 사용. (배치 작업 등) 단점 시스템 복잡성 증가 각 데이터베이스의 설정, 관리, 모니터링이 필요하며, 장애 대응이나 성능 최적화도 각각 별도로 수행해야 한다. 데이터 동기화가 실패하거나 지연되면 Redis와 PostgreSQL 간의 데이터 불일치 문제가 발생할 수 있어, 이를 해결하기 위한 모니터링과 에러 핸들링이 필요 메모리 비용 증가 Redis는 대량의 데이터를 저장할 경우 메모리 비용 높다. Redis의 메모리 사용량을 줄이기 위해 TTL을 설정 -&gt; 필요한 경우 모든 데이터를 PostgreSQL에 영구 저장해야 하므로 Redis와 PostgreSQL의 데이터 저장 비용이 중복 두 시스템의 상이한 데이터 모델 Redis와 PostgreSQL은 서로 다른 데이터 모델과 쿼리 방식을 사용 (키-값 vs SQL) Langchain.memory의 ConversationSummaryBufferMemory 등이 Redis를 대체 가능한가?ConversationSummaryMemory는? 대화 요약에 특화 : 요약을 기반으로 챗봇의 기억을 관리하는 것이 목적 메모리 기반 : 임시 메모리에 저장. 서버 재기동 하거나 메모리 초기화 시 데이터 소실 → Redis는 스냅샷과 AOF 설정으로 반 영구 저장 가능 속도 및 접근성 : conversationSummaryMemory는 단일 프로세스 내의 임시 메모리로 사용되기 때문에 Redis처럼 여러 클라이언트가 접근하는 다중 프로세스 환경에서 사용하기는 제한적. → ConversationSummaryBufferMemory는 대화 내용을 요약하고 간단히 참조하는 메모리 역할에는 유용하지만 Redis를 대체할 수 있는 완전한 데이터 저장 솔루션은 아님. → 영구적이고 구조화된 대화 데이터를 저장하려면 Redis와 같은 외부 영구 저장소가 필요하며, ConversationSummaryBufferMemory는 Redis를 보완하는 임시 요약 메모리로 활용 가능. RunnableWithMessageHistory + RedisRunnableWithMessageHistory : 대화 기록과 맥락을 효과적으로 관리하기 위해 LangChain에서 제공하는 클래스 이전 메시지의 맥락을 유지해야 할 때 유용. 메시지 기록이 시간 순으로 관리되므로, 대화의 특정 메시지를 빠르게 조회가능 RunnableWithMessageHistory와 함께 대화 요약 모델(예: GPT-3, GPT-4 등)을 연결하여 요약 기능을 구현. 이 클래스는 요약이 필요한 메시지를 정리하여 전달하고, 요약된 응답을 받아 메시지 히스토리에 추가하는 방식으로 동작 12345678910111213141516171819from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 메시지 기록을 효과적으로 관리하려면 두 가지 요소 필요 Runnable : 주로 Retriever, Chain과 같이 BaseChatMessageHistory와 상호작용하는 runnable 객체 BaseChatMessageHistory의 인스턴스를 반환하는 호출 가능한 객체 (callable) 메시지 기록을 관리하기 위한 객체 메시지 기록을 저장, 검색, 업데이트하는 데 사용. 무엇을 이용해 요약을 하는가? 언어 모델을 사용한 요약 RunnableWithMessageHistory를 특정 언어 모델(예: OpenAI의 GPT)과 함께 설정하면, 대화 내용을 요약하는 요청을 모델에 전달 LangChain의 요약 도구 LangChain에서는 특정 요약 알고리즘을 사용하거나, SummaryMemory 클래스를 추가하여 반복적인 요약을 통해 핵심 내용을 추려나가는 구조를 제공 요약할 시점 커스터마이징RunnableWithMessageHistory와 ConversationSummaryMemory를 사용할 때, 특정 조건에 따라 요약을 트리거할 수 있도록 커스터마이징 가능 대화의 길이, 특정 이벤트 발생, 주기적인 타이밍 등 요약 시점을 유연하게 조정 가능 메시지 수에 따른 요약 트리거12if len(runnable.message_history) &gt;= 10: summary_memory.update_summary(runnable.message_history) 특정 키워드 또는 이벤트에 따라 요약123user_input = &quot;요약해줘&quot;if &quot;요약&quot; in user_input: summary_memory.update_summary(runnable.message_history) 주기적 요약 (타이머 기반)12345import timedef periodic_summary(): while True: time.sleep(300) # 5분마다 요약 summary_memory.update_summary(runnable.message_history) 대화의 특정 단계에서 요약12def on_conversation_end(): summary_memory.update_summary(runnable.message_history) 저장소 옵션 인메모리 ChatMessageHistory 사용 메모리 내 메시지 기록 관리 빠른 I/O 재기동 시 기록 삭제 RedisChatMessageHistory 사용 Redis 사용, 메시지 기록 영구 저장 가능 장점 고속 데이터 처리와 실시간 응답성 Redis는 인메모리 데이터 저장소, 대화 데이터 I/O 빠름. RunnableWithMessageHistory와 결합하여 사용자의 메시지를 실시간으로 Redis에 기록 대화 흐름 관리와 일관성 유지 RunnableWithMessageHistory는 대화의 흐름을 자연스럽게 관리 Redis에 기록된 이전 메시지들을 참조할 수 있어 대화의 일관성을 유지한다. TTL을 통한 효율적 메모리 관리 Redis에서는 대화 데이터를 일정 시간 동안만 유지하는 TTL(Time to Live) 설정이 가능. 오래된 대화 데이터를 자동으로 삭제하고, 최신 대화에 필요한 정보만 유지할 수 있어 메모리 사용을 최적화. RunnableWithMessageHistory와 함께 활용하면 최신 대화 기록만을 기반으로 챗봇이 응답하도록 구성 가능. 주기적인 요약 저장으로 메모리 절약 RunnableWithMessageHistory를 사용하여 대화 흐름이 길어지면 요약을 주기적으로 생성해 Redis에 저장 불필요한 세부 메시지를 줄이고 요약본을 중심으로 대화 맥락을 유지. 확장성과 유연성 Redis는 분산 환경에서 확장이 용이하므로, 대규모 사용자와의 대화 기록을 효과적으로 처리. RunnableWithMessageHistory를 통해 각 대화의 맥락을 관리하면서, Redis에 저장된 대화 기록을 다른 인스턴스나 마이크로서비스에서도 참조할 수 있어 유연한 확장이 가능. 대화 기록과 요약에 대한 손쉬운 접근성 Redis는 다양한 데이터 구조(List, Hash, Set 등)를 지원하므로, RunnableWithMessageHistory와 함께 대화 기록을 효율적으로 구조화하여 저장 가능. 2안: RunnableWithMessageHistory + Redis 대화 기록 저장: RunnableWithMessageHistory의 메시지를 받아서 Redis에 저장한다. 요약 저장: 주기적으로 요약을 업데이트하거나, 대화 종료 시 요약을 Redis에 저장한다. Redis 데이터 구조 설계 대화별 메시지 리스트 Key: conversation:{conversation_id}:messages Type: List 설명 특정 대화의 메시지를 순서대로 저장. 각 메시지는 JSON 형식으로 저장, sender, message, timestamp 정보 기록 예시 123RPUSH conversation:conv_001:messages '{&quot;sender&quot;: &quot;user&quot;, &quot;message&quot;: &quot;Hello!&quot;, &quot;timestamp&quot;: &quot;2024-10-30T10:00:00&quot;}' '{&quot;sender&quot;: &quot;bot&quot;, &quot;message&quot;: &quot;Hi! How can I help you?&quot;, &quot;timestamp&quot;: &quot;2024-10-30T10:00:01&quot;}' 2.대화 요약 Key: conversation:{conversation_id}:summary Type: Hash 설명 대화의 요약을 저장합니다. 요약을 주기적으로 업데이트. 예시 123HMSET conversation:conv_001:summary &quot;summary&quot; &quot;User asked about product details&quot; &quot;timestamp&quot; &quot;2024-10-30T10:15:00&quot; 사용자별 대화 목록 Key: user:{user_id}:conversations Type: List 설명 사용자의 대화 ID를 시간순으로 저장. 예시 - 사용자가 conv_001, conv_002, conv_003 대화를 순서대로 진행했다면1LPUSH user:123:conversations conv_003 conv_002 conv_001 user:123:conversations 리스트를 조회하면 사용자가 진행한 대화 목록을 시간 순서대로 볼 수 있다. 이 리스트는 사용자가 이전에 진행했던 대화 기록을 관리하거나, 특정 사용자의 과거 대화 요약을 불러올 때 유용 전체 워크플로우 예시 사용자가 메시지를 보내면 RunnableWithMessageHistory는 해당 메시지를 conversation:{conversation_id}:messages 리스트에 저장. 이때 각 메시지는 sender, message, timestamp와 함께 JSON 형식으로 저장. 대화가 일정 수준으로 진행되면 요약을 업데이트: 주기적으로 대화의 요약을 생성하여 conversation:{conversation_id}:summary 해시에 업데이트. 요약은 전체 대화의 주요 내용이나 주제를 간략하게 표현하며, 대화가 종료될 때, 일정수준으로 길어지면 업데이트 사용자별 대화 기록 관리 각 대화가 시작될 때마다 user:{user_id}:conversations 리스트에 해당 대화 ID를 추가. 이를 통해 사용자의 전체 대화이력을 관리, 특정 사용자의 과거 대화 조회 ReferencesRunnableWithMessageHistoryProviders|LangChain","link":"/2024/11/03/LangChain-Memory-1/"},{"title":"LangChain의 RunnableWithMessage와 Redis 활용하여 대화내용 저장하기","text":"LangChain의 RunnableWithMessage는 LangChain에서 제공하는 유틸리티 클래스로, 주로 챗봇 애플리케이션 개발 시 이전의 대화 히스토리와 상호작용을 관리하기 위해 사용된다.이 클래스는 주로 대화 기록을 저장하고 관리하여 대화 흐름을 유지하거나 개인화된 응답을 생성하는데 유용하다. 기본적으로 RunnableWithHistory 클래스는 대화 세션 내에서 사용자와 AI간의 대화 히스토리를 지속적으로 기록할 수 있도록 해주며, 필요한 경우 특정 조건에 따라 히스토리를 요약하거나 일부 삭제할 수 있는 기능도 제공한다.이를통해 챗봇이 각 세션마다 대화 컨텍스트를 유지할 수 있어 더욱 자연스러운 대화 경험을 제공한다. 123from dotenv import load_dotenvload_dotenv(dotenv_path=&quot;../.env&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 12345678910111213141516from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages( [ ( &quot;system&quot;, &quot;당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요&quot;, ), # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨 MessagesPlaceholder(variable_name=&quot;history&quot;), (&quot;human&quot;, &quot;{input}&quot;), # 사용자 입력을 변수로 사용 ])runnable = prompt | model # 프롬프트와 모델을 연결하여 runnable 객체 생성 123456789101112131415161718192021222324from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 영구 저장소 (Persistent Storage)영구 저장소(Persistent Storage)는 프로그램이 종료되거나 시스템이 재부팅되더라도 데이터를 유지하는 저장 메커니즘을 말한다.이는 데이터베이스, 파일시스템, 또는 기타 비휘발성 저장 장치를 통해 구현될 수 있다. 영구 저장소는 애플리케이션의 상태를 저장하고, 사용자 설정을 유지하며, 장기간 데이터를 보존하는 데 필수적이다.이를 통해 프로그램은 이전 실행에서 중단된 지점부터 다시 시작할 수 있으며, 사용자는 데이터 손실 없이 작업을 계속 할 수 있다. RunnableWithMessageHistory는 get_session_history 호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적이다. Redis 설치Redis가 설치되어 있지 않다면 먼저 설치해야 한다. 폐쇄망 EC2이기 때문에, 인터넷이 연결된 로컬 컴퓨터에서 EC2 서버에 설치할 패키지를 다운로드 한다. 123456% pip download redisCollecting redis Using cached redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)Using cached redis-5.2.0-py3-none-any.whl (261 kB)Saved ./redis-5.2.0-py3-none-any.whlSuccessfully downloaded redis S3에 redis whl 파일을 업로드한다.그리고 EC2에 S3에서 whl 파일을 가져온다. 12345# aws configure# export AWS_ACCESS_KEY_ID=......# aws s3 cp s3://aipin-bucket/redis-5.2.0-py3-none-any.whl /rootdownload: s3://aipin-bucket/redis-5.2.0-py3-none-any.whl to ./redis-5.2.0-py3-none-any.whl 다운로드한 파일들을 EC2에서 설치한다. 12345# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Package 'redis' requires a different Python: 3.7.16 not in '&gt;=3.8' –no-index 옵션을 사용하여 외부 PyPI에 접속하지 않고, 지정된 디렉토리(–find-links .)에서만 패키지를 찾도록 한다. redis 패키지가 Python3.8 이상의 버전을 요구하고 현재 시스템에 설치된 Python 버전이 3.7.16이기 때문에, Python 3.8이상으로 업그레이드 해야한다. Python 버전 업그레이드 방법 (Amazon Linux 환경) Python 3.8 설치123sudo yum install -y amazon-linux-extrassudo amazon-linux-extras enable python3.8sudo yum install -y python3.8 기본 Python 버전 변경 python 3.8을 python3로 대체하려면 아래와 같이 심볼릭 링크를 업데이트 할 수 있음1sudo ln -sf /usr/bin/python3.8 /usr/bin/python3 pip3 역시 Python 3.8에 맞는 버전으로 변경1sudo ln -sf /usr/bin/pip3.8 /usr/bin/pip3 Python 버전 확인1python3 --version 이제 Redis 패키지 설치 재시도를 한다. 1234567# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Could not find a version that satisfies the requirement async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; (from redis)ERROR: No matching distribution found for async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; 이번엔 이런 에러가 나는데, redis 패키지가 async-timeout 패키지의 특정 버전 (&gt;=4.0.3)을 필요로 하는데, 현재 환경에서는 해당 버전을 찾을 수 없어서 발생한 것이다.pip download가 기본적으로 상위 패키지만 다운로드하고, 해당 패키지의 모든 의존성을 포함하지 않기 때문이다.모든 의존성 패키지를 포함해 다운로드 하려면 pip에 추가 옵션을 지정해주어야 한다. 1pip download --no-binary=:all: redis 위와 같이 –no-binary 옵션을 사용해 의존성 패키지까지 함께 다운로드 할 수 있다. redis와 모든 의존성 패키지들이 .whl 또는 .tar.gz 파일로 함께 다운로드 될 것이다. tar.gz 파일을 S3를 통해 EC2로 전송하여 설치를 진행한다. 1234# aws s3 cp s3://aipin-bucket/redis-5.2.0.tar.gz /root# tar -xzf async-timeout-4.0.3.tar.gz# cd redis-5.2.0# python3 setup.py install 그런데 또 에러가 난다. 12345678910111213141516171819# python3 setup.py install/usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type' warnings.warn(msg)running installerror: can't create or remove files in install directoryThe following error occurred while trying to add or remove files in theinstallation directory: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/site-packages/test-easy-install-2904.write-test'The installation directory you specified (via --install-dir, --prefix, orthe distutils default setting) was: /usr/local/lib/python3.8/site-packages/This directory does not currently exist. Please create it and try again, orchoose a different installation directory (using the -d or --install-diroption). 이 오류는 /usr/local/lib/python3.8/site-packages/ 디렉토리가 존재하지 않아서 발생한 것이다. -&gt; 디렉토리 생성 후 설치 시도 12sudo mkdir -p /usr/local/lib/python3.8/site-packages/sudo python3 setup.py install 이번엔 setup.py를 사용하여 설치할 때 발생하는 marshal 모듈 관련 에러가 난다. 12345678910111213141516171819202122232425262728Traceback (most recent call last): File &quot;setup.py&quot;, line 4, in &lt;module&gt; setup( File &quot;/usr/lib/python3.8/site-packages/setuptools/__init__.py&quot;, line 129, in setup return distutils.core.setup(**attrs) File &quot;/usr/lib64/python3.8/distutils/core.py&quot;, line 148, in setup dist.run_commands() File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 966, in run_commands self.run_command(cmd) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 67, in run self.do_egg_install() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 109, in do_egg_install self.run_command('bdist_egg') File &quot;/usr/lib64/python3.8/distutils/cmd.py&quot;, line 313, in run_command self.distribution.run_command(command) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 218, in run os.path.join(archive_root, 'EGG-INFO'), self.zip_safe() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 269, in zip_safe return analyze_egg(self.bdist_dir, self.stubs) File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 379, in analyze_egg safe = scan_module(egg_dir, base, name, stubs) and safe File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 416, in scan_module code = marshal.load(f)ValueError: bad marshal data (unknown type code) 아예 방법을 바꿔서 Redis 서버를 설치하겠다. Redis 서버 설치Redis 서버를 설치하여 EC2 인스턴스에서 직접 Redis 서버를 운영하겠다. 인터넷이 연결된 환경에서 Redis 소스파일을 다운로드 한다.1wget http://download.redis.io/redis-stable.tar.gz 다운로드한 redis-stable.tar.gz 파일을 EC2 인스턴스로 전송한다. EC2에서 Redis 압축을 푼다.12tar -xzf redis-stable.tar.gzcd redis-stable Redis를 컴파일하고 설치한다.12makesudo make install Redis 서버를 실행하여 설치가 완료되었는지 확인한다.1234567891011121314151617181920212223242526redis-server11366:C 07 Nov 2024 18:11:49.254 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then rebootor run the command 'sysctl vm.overcommit_memory=1' for this to take effect.11366:C 07 Nov 2024 18:11:49.254 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo11366:C 07 Nov 2024 18:11:49.254 * Redis version=7.4.1, bits=64, commit=00000000, modified=0, pid=11366, just started11366:C 07 Nov 2024 18:11:49.254 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf11366:M 07 Nov 2024 18:11:49.254 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis Community Edition .-`` .-```. ```\\/ _.,_ ''-._ 7.4.1 (00000000/0) 64 bit ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 11366 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'11366:M 07 Nov 2024 18:11:49.255 * Server initialized11366:M 07 Nov 2024 18:11:49.255 * Ready to accept connections tcp 12# Redis 서버의 URL을 지정합니다.REDIS_URL = &quot;redis://localhost:6379/0&quot; 123456789from dotenv import load_dotenvimport osload_dotenv()# LANGCHAIN_TRACING_V2 환경 변수를 &quot;true&quot;로 설정합니다.os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# LANGCHAIN_PROJECT 설정os.environ[&quot;LANGCHAIN_PROJECT&quot;] = &quot;RunnableWithMessageHistory&quot; 1234567891011121314from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -&gt; RedisChatMessageHistory: # 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다. return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory( runnable, # 실행 가능한 객체 get_message_history, # 메시지 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키) 123456with_message_history.invoke( # 수학 관련 질문 &quot;코사인의 의미는 무엇인가요?&quot;를 입력으로 전달합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;What does cosine mean?&quot;}, # 설정 옵션으로 세션 ID를 &quot;redis123&quot; 로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;foobar&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;redis456&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis456&quot;}},) Redis 컨테이너에서…123docker exec -it redis-container shredis-clikeys * 1234567891011get message_store:redis456get message_store:redis123LRANGE message_store:redis456LRANGE message_store:redis123hgetall message_store:redis456hgetall message_store:redis123type message_store:redis456type message_store:redis123 Redis 조회 시1234567891011121314151617181920212223127.0.0.1:6379&gt; keys *1) &quot;message_store:redis456&quot;2) &quot;message_store:redis123&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis456 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 103, \\&quot;total_tokens\\&quot;: 120, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-1507d5ba-7bce-458d-b9ff-b6c93252dab3-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 103, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 120, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 60, \\&quot;total_tokens\\&quot;: 77, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-c7ce147f-2ac9-4068-8405-6bc91669a52e-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 60, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 77, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis123 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Shohei Ohtani is a Japanese professional baseball player who plays for the Los Angeles Angels in Major League Baseball (MLB). He is known for his exceptional skills as both a pitcher and a hitter.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 42, \\&quot;prompt_tokens\\&quot;: 185, \\&quot;total_tokens\\&quot;: 227, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-8ba62e18-27de-438b-8a88-a85b399951a6-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 185, \\&quot;output_tokens\\&quot;: 42, \\&quot;total_tokens\\&quot;: 227, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Who ohtani.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 26, \\&quot;prompt_tokens\\&quot;: 146, \\&quot;total_tokens\\&quot;: 172, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-56f41ea1-25d2-4071-aa8e-24693cc3dead-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 146, \\&quot;output_tokens\\&quot;: 26, \\&quot;total_tokens\\&quot;: 172, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;5) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\ucf54\\\\uc0ac\\\\uc778\\\\uc740 \\\\uc9c1\\\\uac01 \\\\uc0bc\\\\uac01\\\\ud615\\\\uc5d0\\\\uc11c \\\\uc778\\\\uc811\\\\ubcc0\\\\uacfc \\\\ube57\\\\ubcc0\\\\uc758 \\\\ube44\\\\uc728\\\\uc744 \\\\ub098\\\\ud0c0\\\\ub0c5\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 41, \\&quot;prompt_tokens\\&quot;: 92, \\&quot;total_tokens\\&quot;: 133, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-986633f9-22a6-4c16-89d9-3918780b585d-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 92, \\&quot;output_tokens\\&quot;: 41, \\&quot;total_tokens\\&quot;: 133, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;6) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;7) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 19, \\&quot;prompt_tokens\\&quot;: 47, \\&quot;total_tokens\\&quot;: 66, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-146afcbf-7301-4673-b3d4-0bf6ae2c9191-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 47, \\&quot;output_tokens\\&quot;: 19, \\&quot;total_tokens\\&quot;: 66, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;8) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot; 프로젝트에 Redis 저장 및 로드 코드 추가아래와 같이 Redis 설정을 추가하고 1234567# Redis 설정redis_client = redis.StrictRedis( host=os.getenv('REDIS_HOST', 'localhost'), port=int(os.getenv('REDIS_PORT', 6379)), db=0, decode_responses=True) 대화 히스토리 저장용 함수를 추가하고 1234567891011121314151617# 대화 히스토리 저장용 함수def save_chat_history_to_redis(user_id: str, message: str, response: str): timestamp = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) user_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message } assistant_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response } # 사용자 메시지와 응답을 각각 저장 redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(user_chat_data)) redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(assistant_chat_data)) print(&quot;대화 내용이 저장되었습니다&quot;) 저장된 히스토리를 불러오는 함수도 추가하였다. 12345def get_session_history(user_id: str) -&gt; List[ChatFormat]: # Redis에서 user_id에 해당하는 대화 내역을 가져오는 로직 작성 history_data = redis_client.lrange(f&quot;chat_history:{user_id}&quot;, 0, -1) history = [ChatFormat(**json.loads(item)) for item in history_data] return history 그리고 Agent 호출 로직에 history를 load 하여 호출하도록 추가하였다. 12345678910111213# Redis에서 사용자 대화 기록 가져와 history 대체하기history = get_session_history(user_id)# GIS AGENT 호출response = call_gis_agent(message, history, summary)# Redis에 대화 내용 저장save_chat_history_to_redis(user_id, message, response)# 대화 내역 출력 (optional)print(print_session_history(history))return response 그리고 비슷하게 summary를 저장하는 로직도 추가하였다. 실행하게 되면 Redis에서 아래와 같이 두 가지 Redis key가 사용자 별로 생성된다. 123127.0.0.1:6379&gt; keys *1) &quot;summary_list:default_user&quot;2) &quot;chat_history:default_user&quot; 한글로 채팅이 오고 가기 때문에 redis에서 조회해도 다 깨져있고, 로그에서 잘 저장되고 불러오는지 확인해보겠다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151대화 내용이 저장되었습니다저장된 대화 내용:Role: userContent: 효창공원역 맛집 추천해줘----------Role: assistantContent: 효창공원역 주변의 맛집을 추천해드릴게요:1. **창성옥** - **주소**: 서울특별시 용산구 새창로 124-10 (용문동 25-16) - **전화**: 02-718-2878 - **카테고리**: 한식 - **메뉴**: - 뼈전골 (소): 27,000원 - 뼈전골 (중): 36,000원 - 해장국: 10,000원 - **영업시간**: 24시간 운영 - **예약 가능 여부**: 가능2. **효창동짜장우동** - **주소**: 서울특별시 용산구 백범로 283 (효창동 81-1) - **전화**: 02-703-5287 - **카테고리**: 중식 - **메뉴**: - 짜장: 4,000원 - 김치우동: 4,000원 - 우동: 4,000원3. **박명도봉평메밀막국수** - **주소**: 서울특별시 용산구 원효로 184-1 (원효로2가 43) - **전화**: 02-717-7711 - **카테고리**: 국수전문 - **메뉴**: - 들기름막국수: 11,000원 - 물막국수: 11,000원 - 비빔막국수: 11,000원 - **영업시간**: 10:00 - 22:004. **용문해장국** - **주소**: 서울특별시 용산구 효창원로 110 (용문동 8-95) - **전화**: 02-712-6290 - **카테고리**: 한식 - **메뉴**: - 해장국: 7,000원 - 해장국 (2인분): 14,000원 - 뼈전골 (중): 30,000원 - **예약 가능 여부**: 가능, 주차 가능5. **아성녹두빈대떡** - **주소**: 서울특별시 용산구 효창원로48길 3 (용문동 8-113) - **전화**: 02-706-8238 - **카테고리**: 기타 - **메뉴**: - 녹두빈대떡: 15,000원 - 해물파전: 12,000원 - 감자전: 12,000원이 맛집들이 효창공원역 근처에서 좋은 선택이 될 것입니다!----------Role: userContent: 우리집은 반포자이 114동인데, 우리집에서 창성옥까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 창성옥까지의 소요 시간은 약 24분 30초이며, 거리는 약 12,291 미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 서울역까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 서울역까지의 소요 시간은 약 33분이며, 거리는 약 9.5킬로미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 아까 알려준 효창공원 맛집 중 첫번째가 어디였지?----------Role: assistantContent: 아까 알려드린 효창공원역 맛집 중 첫 번째는 **창성옥**입니다. 주소는 서울특별시 용산구 새창로 124-10 (용문동 25-16)이고, 전화번호는 02-718-2878입니다. 한식을 제공하며, 뼈전골과 해장국이 주요 메뉴입니다. 24시간 운영하며 예약도 가능합니다.----------Role: userContent: 아까 추천해준 효창공원 맛집 중 4번째는 어디였지?----------Role: assistantContent: 아까 추천해드린 효창공원역 맛집 중 네 번째는 **용문해장국**입니다. 주소는 서울특별시 용산구 효창원로 110 (용문동 8-95)이고, 전화번호는 02-712-6290입니다. 한식을 제공하며, 해장국과 뼈전골이 주요 메뉴입니다. 예약이 가능하고 주차도 가능합니다.----------Role: userContent: 우리 부모님집은 도원삼성래미안 아파트야----------Role: assistantContent: 도원삼성래미안 아파트에서 서울역까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 17분 (1074초)- **거리**: 약 3.965킬로미터 (3965미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 우리부모님집까지 얼마나 걸려----------Role: assistantContent: 반포자이 114동에서 도원삼성래미안 아파트까지의 소요 시간은 약 28분이며, 거리는 약 13,026미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 KT 광화문 West 빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 광화문 West 빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 29분 (1766초)- **거리**: 약 9.3킬로미터 (9307미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 19분 (1144초)- **거리**: 약 14.47킬로미터 (14470미터)안전 운전하세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간:** 약 19분 17초 (1157초)- **거리:** 약 14,464 미터안전 운전하시기 바랍니다!----------Role: userContent: 우리부모님집에서 KT 대관령 수련관 얼마나 걸려?----------Role: assistantContent: 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 2시간 34분 (9280초)- **거리**: 약 197.6킬로미터이 정보를 바탕으로 여행을 계획하실 수 있습니다. 추가로 도움이 필요하시면 말씀해 주세요!----------None/Users/leehamin/app/GIS-Agent/gis-ai-agent-be/agent_api/models/chat_models.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead. chain = LLMChain(prompt=get_summary_prompt_template(), llm=llm)Summary list가 저장되었습니다.answer_data: {'answer': '반포자이 114동에서 KT위즈파크까지의 경로 안내는 이미 완료되었습니다. 경로는 반포자이 114동에서 출발하여 신반포로를 따라 남쪽으로 이동한 후, 반포대교를 건너 수원 방향으로 계속 이동하여 경수대로를 따라 KT위즈파크에 도착하는 것입니다. 안전한 여행 되세요!', 'chat_status': 'C1001'}answer_summary_list : ['사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, 그리고 KT 판교빌딩까지의 소요 시간과 거리를 문의하여 답변을 받았습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 또한, 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보를 확인했습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 사용자는 효창공원역 맛집 추천을 다시 요청했고, 첫 번째와 네 번째 맛집의 정보를 다시 확인했습니다. 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보도 확인했으며, 반포자이 114동에서 KT위즈파크까지의 소요 시간과 경로를 안내받았습니다.']INFO: 127.0.0.1:51812 - &quot;POST /api/v1/chat HTTP/1.1&quot; 200 OK 위와 같이 대화 내용과 summary_list가 모두 잘 저장되고 load되며 멀티턴 또한 잘 진행된다. User 별로 대화가 저장되도록 user_uuid 적용더미로 default_user라는 user_id로 통일하던 것을유저별로 대화 history와 summary가 적용될 수 있도록, 식별자인 user_uuid를 적용하였다. 헤더에 X-User-UUID를 달고 가는 방식이다. 12345678def generate_or_get_uuid(user_uuid: Optional[str]) -&gt; UUID: if user_uuid: try: # UUID가 유효한지 검사 return UUID(user_uuid) except ValueError: raise_error(&quot;E1006&quot;) else: # UUID가 없으면 새 UUID 생성 return uuid4() 유저 아이디에 맞게 redis key가 생성된다. 12345127.0.0.1:6379&gt; keys *1) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;2) &quot;summary_list:default_user&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;chat_history:default_user&quot; 테스트 세션1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;네, 판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n- 위도: 37.3933\\n- 경도: 127.1144\\n\\n더 필요한 정보가 있으면 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot; ]} 테스트 세션2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;효창공원 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;효창공원 주변의 맛집 정보는 다음과 같습니다:\\n\\n1. **효창원** \\n - **주소**: 서울 용산구 효창원로 97\\n - **특징**: 전통 한식과 다양한 메뉴를 제공하는 식당입니다.\\n\\n2. **카페 드 파리** \\n - **주소**: 서울 용산구 효창원로 85\\n - **특징**: 디저트와 커피를 전문으로 하는 아늑한 카페입니다.\\n\\n3. **고기리 막창** \\n - **주소**: 서울 용산구 효창원로 95\\n - **특징**: 고기 요리를 전문으로 하는 맛집으로, 막창이 유명합니다.\\n\\n이 외에도 더 많은 맛집이 있으니, 필요하시면 추가 정보를 요청해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot; ]} 테스트 세션3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;첫번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 49분입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot; ]} 테스트 세션4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원으로 이동한 후, 효창원에서 김포공항까지 이동하는 데 걸리는 총 소요 시간은 다음과 같습니다:\\n\\n- **판교 그랑블 아파트 → 효창원**: 약 41분 38초\\n- **효창원 → 김포공항**: 약 56분 4초\\n\\n따라서, 총 소요 시간은 약 1시간 37분 42초입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot; ]} 테스트 세션5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 1시간 10분이며, 효창원에서 인천공항까지의 소요 시간은 약 47분입니다. 따라서, 총 소요 시간은 약 1시간 57분입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다. 사용자가 인천공항으로 가는 시간을 묻자, 어시스턴트는 총 소요 시간이 약 1시간 57분이라고 안내합니다.&quot; ]} 서버에 배포프로젝트에서 requirements.txt 파일을 사용하고 있기 때문에, 이 파일에 redis 패키지를 추가했다. 1redis Redis가 실행되고 있는 호스트의 IP로 바꾸었다 12# host=os.getenv('REDIS_HOST', 'localhost'),host=os.getenv('REDIS_HOST', '10.71.176.93'), 1234# EC2 환경redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://10.71.176.93:6379/0&quot;)# 로컬 환경# redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://localhost:6379/0&quot;) Redis 서버가 외부 연결을 허용하는지 확인Redis 설정파일 위치 확인12# Redis 설정 파일 위치 확인find / -name &quot;redis.conf&quot; Redis 설정 파일 수정123456# 이렇게 있던 것을bind 127.0.0.1 -::1# 이렇게bind 0.0.0.0protected-mode no Redis 재시작123456789redis-cli shutdown# Redis 서버의 프로세스 ID 확인ps aux | grep redis-server# 프로세스 ID를 이용해 종료 (예: PID가 1234인 경우)kill 1234# Redis 서버 재시작redis-server /path/to/redis.conf 여전히 protected mode가 실행중으로 나타난다1234[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379-DENIED Redis is running in protected mode because protected mode is enabled and no password is set for the default user. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Set up an authentication password for the default user. NOTE:You only need to do one of the above things in order for the server to start accepting connections from the outside. Redis CLI를 통해 임시로 protected-mode 비활성화Redis 서버에 접근 가능한 경우, Redis CLI를 사용하여 protected-mode를 비활성화할 수 있다. 123456789# Redis CLI 접속redis-cli# protected-mode 비활성화CONFIG SET protected-mode no[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379 이번엔 연결이 되는 것 같다 서버 테스트테스트 1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n\\n- **위도**: 37.393299557105\\n- **경도**: 127.11436915566647\\n\\n이 정보가 도움이 되길 바랍니다! 추가적인 질문이 있으면 언제든지 말씀해 주세요.&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot; ]} 테스트 2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;포천 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;포천의 맛집 정보를 제공해드렸습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot; ]} 테스트 3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;영종도 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415161718192021222324252627282930313233343536{ &quot;chat&quot;: { &quot;answer&quot;: &quot;영종도에서 추천하는 맛집 목록입니다:\\n\\n1. **선녀풍**\\n - 주소: 인천광역시 중구 을왕동 용유서로 272, 689-1\\n - 전화: 032-751-2121\\n - 주메뉴: 해물파전(15,000원), 선녀물회(20,000원), 낙지물회(28,000원)\\n - 영업시간: 매일 12:00 - 22:00\\n - 주차: 가능\\n - 좌표: [37.4444, 126.3787]\\n\\n2. **황해해물칼국수**\\n - 주소: 인천광역시 중구 덕교동 마시란로 37, 128-56\\n - 전화: 032-752-3017\\n - 주메뉴: 산낙지(15,000원), 전복 (4마리)(16,000원), 해물칼국수(10,000원)\\n - 영업시간: 매일 10:00 - 20:00\\n - 주차: 가능\\n - 좌표: [37.4262, 126.4212]\\n\\n3. **동해막국수**\\n - 주소: 인천광역시 중구 을왕동 용유서로479번길 16, 859-3\\n - 전화: 032-746-5522\\n - 영업시간: 매일 11:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4616, 126.3705]\\n\\n4. **미애네칼국수**\\n - 주소: 인천광역시 중구 덕교동 용유로21번길 51, 80-14\\n - 전화: 032-746-3838\\n - 주메뉴: 산낙지(18,000원), 전복회(15,000원), 바다속칼국수 (소)(35,000원)\\n - 영업시간: 매일 09:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4300, 126.4242]\\n\\n5. **을항**\\n - 주소: 인천광역시 중구 을왕동 선녀바위로55번길 39, 686-5\\n - 전화: 032-752-2227\\n - 주메뉴: 물회(1인)(25,000원), 물회(대)(75,000원), 물회(중)(55,000원)\\n - 좌표: [37.4436, 126.3782]\\n\\n맛집 선택에 도움이 되길 바랍니다!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot; ]}---title: LangChain의 RunnableWithMessage와 Redis 활용하여 대화내용 저장하기date: 2024-11-03 22:37:01tags: - LangChain - LLM - Redis - [LLM, LangChain]cover: /gallery/langChain.png---LangChain의 RunnableWithMessage는 LangChain에서 제공하는 유틸리티 클래스로, 주로 챗봇 애플리케이션 개발 시 이전의 대화 히스토리와 상호작용을 관리하기 위해 사용된다.이 클래스는 주로 대화 기록을 저장하고 관리하여 대화 흐름을 유지하거나 개인화된 응답을 생성하는데 유용하다.기본적으로 RunnableWithHistory 클래스는 대화 세션 내에서 사용자와 AI간의 대화 히스토리를 지속적으로 기록할 수 있도록 해주며, 필요한 경우 특정 조건에 따라 히스토리를 요약하거나 일부 삭제할 수 있는 기능도 제공한다.이를통해 챗봇이 각 세션마다 대화 컨텍스트를 유지할 수 있어 더욱 자연스러운 대화 경험을 제공한다.&lt;!--more--&gt;~~~pythonfrom dotenv import load_dotenvload_dotenv(dotenv_path=&quot;../.env&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 123456# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install langchain-teddynotefrom langchain_teddynote import logging# 프로젝트 이름을 입력합니다.logging.langsmith(&quot;langchain-Memory&quot;) 12345678910111213141516from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages( [ ( &quot;system&quot;, &quot;당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요&quot;, ), # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨 MessagesPlaceholder(variable_name=&quot;history&quot;), (&quot;human&quot;, &quot;{input}&quot;), # 사용자 입력을 변수로 사용 ])runnable = prompt | model # 프롬프트와 모델을 연결하여 runnable 객체 생성 123456789101112131415161718192021222324from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {} # 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수def get_session_history(session_ids: str) -&gt; BaseChatMessageHistory: print(session_ids) if session_ids not in store: # 세션 ID가 store에 없는 경우 # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장 store[session_ids] = ChatMessageHistory() return store[session_ids] # 해당 세션 ID에 대한 세션 기록 반환with_message_history = ( RunnableWithMessageHistory( # RunnableWithMessageHistory 객체 생성 runnable, # 실행할 Runnable 객체 get_session_history, # 세션 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키 )) 영구 저장소 (Persistent Storage)영구 저장소(Persistent Storage)는 프로그램이 종료되거나 시스템이 재부팅되더라도 데이터를 유지하는 저장 메커니즘을 말한다.이는 데이터베이스, 파일시스템, 또는 기타 비휘발성 저장 장치를 통해 구현될 수 있다. 영구 저장소는 애플리케이션의 상태를 저장하고, 사용자 설정을 유지하며, 장기간 데이터를 보존하는 데 필수적이다.이를 통해 프로그램은 이전 실행에서 중단된 지점부터 다시 시작할 수 있으며, 사용자는 데이터 손실 없이 작업을 계속 할 수 있다. RunnableWithMessageHistory는 get_session_history 호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적이다. Redis 설치Redis가 설치되어 있지 않다면 먼저 설치해야 한다. 폐쇄망 EC2이기 때문에, 인터넷이 연결된 로컬 컴퓨터에서 EC2 서버에 설치할 패키지를 다운로드 한다. 123456% pip download redisCollecting redis Using cached redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)Using cached redis-5.2.0-py3-none-any.whl (261 kB)Saved ./redis-5.2.0-py3-none-any.whlSuccessfully downloaded redis S3에 redis whl 파일을 업로드한다.그리고 EC2에 S3에서 whl 파일을 가져온다. 12345# aws configure# export AWS_ACCESS_KEY_ID=......# aws s3 cp s3://aipin-bucket/redis-5.2.0-py3-none-any.whl /rootdownload: s3://aipin-bucket/redis-5.2.0-py3-none-any.whl to ./redis-5.2.0-py3-none-any.whl 다운로드한 파일들을 EC2에서 설치한다. 12345# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Package 'redis' requires a different Python: 3.7.16 not in '&gt;=3.8' –no-index 옵션을 사용하여 외부 PyPI에 접속하지 않고, 지정된 디렉토리(–find-links .)에서만 패키지를 찾도록 한다. redis 패키지가 Python3.8 이상의 버전을 요구하고 현재 시스템에 설치된 Python 버전이 3.7.16이기 때문에, Python 3.8이상으로 업그레이드 해야한다. Python 버전 업그레이드 방법 (Amazon Linux 환경) Python 3.8 설치123sudo yum install -y amazon-linux-extrassudo amazon-linux-extras enable python3.8sudo yum install -y python3.8 기본 Python 버전 변경 python 3.8을 python3로 대체하려면 아래와 같이 심볼릭 링크를 업데이트 할 수 있음1sudo ln -sf /usr/bin/python3.8 /usr/bin/python3 pip3 역시 Python 3.8에 맞는 버전으로 변경1sudo ln -sf /usr/bin/pip3.8 /usr/bin/pip3 Python 버전 확인1python3 --version 이제 Redis 패키지 설치 재시도를 한다. 1234567# pip3 install redis-5.2.0-py3-none-any.whl --no-index --find-links .WARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.Looking in links: .Processing ./redis-5.2.0-py3-none-any.whlERROR: Could not find a version that satisfies the requirement async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; (from redis)ERROR: No matching distribution found for async-timeout&gt;=4.0.3; python_full_version &lt; &quot;3.11.3&quot; 이번엔 이런 에러가 나는데, redis 패키지가 async-timeout 패키지의 특정 버전 (&gt;=4.0.3)을 필요로 하는데, 현재 환경에서는 해당 버전을 찾을 수 없어서 발생한 것이다.pip download가 기본적으로 상위 패키지만 다운로드하고, 해당 패키지의 모든 의존성을 포함하지 않기 때문이다.모든 의존성 패키지를 포함해 다운로드 하려면 pip에 추가 옵션을 지정해주어야 한다. 1pip download --no-binary=:all: redis 위와 같이 –no-binary 옵션을 사용해 의존성 패키지까지 함께 다운로드 할 수 있다. redis와 모든 의존성 패키지들이 .whl 또는 .tar.gz 파일로 함께 다운로드 될 것이다. tar.gz 파일을 S3를 통해 EC2로 전송하여 설치를 진행한다. 1234# aws s3 cp s3://aipin-bucket/redis-5.2.0.tar.gz /root# tar -xzf async-timeout-4.0.3.tar.gz# cd redis-5.2.0# python3 setup.py install 그런데 또 에러가 난다. 12345678910111213141516171819# python3 setup.py install/usr/lib64/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type' warnings.warn(msg)running installerror: can't create or remove files in install directoryThe following error occurred while trying to add or remove files in theinstallation directory: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/site-packages/test-easy-install-2904.write-test'The installation directory you specified (via --install-dir, --prefix, orthe distutils default setting) was: /usr/local/lib/python3.8/site-packages/This directory does not currently exist. Please create it and try again, orchoose a different installation directory (using the -d or --install-diroption). 이 오류는 /usr/local/lib/python3.8/site-packages/ 디렉토리가 존재하지 않아서 발생한 것이다. -&gt; 디렉토리 생성 후 설치 시도 12sudo mkdir -p /usr/local/lib/python3.8/site-packages/sudo python3 setup.py install 이번엔 setup.py를 사용하여 설치할 때 발생하는 marshal 모듈 관련 에러가 난다. 12345678910111213141516171819202122232425262728Traceback (most recent call last): File &quot;setup.py&quot;, line 4, in &lt;module&gt; setup( File &quot;/usr/lib/python3.8/site-packages/setuptools/__init__.py&quot;, line 129, in setup return distutils.core.setup(**attrs) File &quot;/usr/lib64/python3.8/distutils/core.py&quot;, line 148, in setup dist.run_commands() File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 966, in run_commands self.run_command(cmd) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 67, in run self.do_egg_install() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/install.py&quot;, line 109, in do_egg_install self.run_command('bdist_egg') File &quot;/usr/lib64/python3.8/distutils/cmd.py&quot;, line 313, in run_command self.distribution.run_command(command) File &quot;/usr/lib64/python3.8/distutils/dist.py&quot;, line 985, in run_command cmd_obj.run() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 218, in run os.path.join(archive_root, 'EGG-INFO'), self.zip_safe() File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 269, in zip_safe return analyze_egg(self.bdist_dir, self.stubs) File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 379, in analyze_egg safe = scan_module(egg_dir, base, name, stubs) and safe File &quot;/usr/lib/python3.8/site-packages/setuptools/command/bdist_egg.py&quot;, line 416, in scan_module code = marshal.load(f)ValueError: bad marshal data (unknown type code) 아예 방법을 바꿔서 Redis 서버를 설치하겠다. Redis 서버 설치Redis 서버를 설치하여 EC2 인스턴스에서 직접 Redis 서버를 운영하겠다. 인터넷이 연결된 환경에서 Redis 소스파일을 다운로드 한다.1wget http://download.redis.io/redis-stable.tar.gz 다운로드한 redis-stable.tar.gz 파일을 EC2 인스턴스로 전송한다. EC2에서 Redis 압축을 푼다.12tar -xzf redis-stable.tar.gzcd redis-stable Redis를 컴파일하고 설치한다.12makesudo make install Redis 서버를 실행하여 설치가 완료되었는지 확인한다.1234567891011121314151617181920212223242526redis-server11366:C 07 Nov 2024 18:11:49.254 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then rebootor run the command 'sysctl vm.overcommit_memory=1' for this to take effect.11366:C 07 Nov 2024 18:11:49.254 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo11366:C 07 Nov 2024 18:11:49.254 * Redis version=7.4.1, bits=64, commit=00000000, modified=0, pid=11366, just started11366:C 07 Nov 2024 18:11:49.254 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf11366:M 07 Nov 2024 18:11:49.254 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis Community Edition .-`` .-```. ```\\/ _.,_ ''-._ 7.4.1 (00000000/0) 64 bit ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 11366 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'11366:M 07 Nov 2024 18:11:49.255 * Server initialized11366:M 07 Nov 2024 18:11:49.255 * Ready to accept connections tcp 12# Redis 서버의 URL을 지정합니다.REDIS_URL = &quot;redis://localhost:6379/0&quot; 123456789from dotenv import load_dotenvimport osload_dotenv()# LANGCHAIN_TRACING_V2 환경 변수를 &quot;true&quot;로 설정합니다.os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;# LANGCHAIN_PROJECT 설정os.environ[&quot;LANGCHAIN_PROJECT&quot;] = &quot;RunnableWithMessageHistory&quot; 1234567891011121314from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -&gt; RedisChatMessageHistory: # 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다. return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory( runnable, # 실행 가능한 객체 get_message_history, # 메시지 기록을 가져오는 함수 input_messages_key=&quot;input&quot;, # 입력 메시지의 키 history_messages_key=&quot;history&quot;, # 기록 메시지의 키) 123456with_message_history.invoke( # 수학 관련 질문 &quot;코사인의 의미는 무엇인가요?&quot;를 입력으로 전달합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;What does cosine mean?&quot;}, # 설정 옵션으로 세션 ID를 &quot;redis123&quot; 로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;foobar&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis123&quot;}},) 123456with_message_history.invoke( # 이전 답변에 대한 한글 번역을 요청합니다. {&quot;ability&quot;: &quot;math&quot;, &quot;input&quot;: &quot;이전의 답변을 한글로 번역해 주세요.&quot;}, # 설정 값으로 세션 ID를 &quot;redis456&quot;로 지정합니다. config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;redis456&quot;}},) Redis 컨테이너에서…123docker exec -it redis-container shredis-clikeys * 1234567891011get message_store:redis456get message_store:redis123LRANGE message_store:redis456LRANGE message_store:redis123hgetall message_store:redis456hgetall message_store:redis123type message_store:redis456type message_store:redis123 Redis 조회 시1234567891011121314151617181920212223127.0.0.1:6379&gt; keys *1) &quot;message_store:redis456&quot;2) &quot;message_store:redis123&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis456 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 103, \\&quot;total_tokens\\&quot;: 120, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-1507d5ba-7bce-458d-b9ff-b6c93252dab3-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 103, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 120, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc218\\\\ud559\\\\uc5d0 \\\\ub2a5\\\\uc219\\\\ud55c \\\\ub3c4\\\\uc6b0\\\\ubbf8\\\\uc785\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 17, \\&quot;prompt_tokens\\&quot;: 60, \\&quot;total_tokens\\&quot;: 77, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-c7ce147f-2ac9-4068-8405-6bc91669a52e-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 60, \\&quot;output_tokens\\&quot;: 17, \\&quot;total_tokens\\&quot;: 77, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; 127.0.0.1:6379&gt; lrange message_store:redis123 0 -11) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Shohei Ohtani is a Japanese professional baseball player who plays for the Los Angeles Angels in Major League Baseball (MLB). He is known for his exceptional skills as both a pitcher and a hitter.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 42, \\&quot;prompt_tokens\\&quot;: 185, \\&quot;total_tokens\\&quot;: 227, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-8ba62e18-27de-438b-8a88-a85b399951a6-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 185, \\&quot;output_tokens\\&quot;: 42, \\&quot;total_tokens\\&quot;: 227, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;2) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Who ohtani.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;3) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 26, \\&quot;prompt_tokens\\&quot;: 146, \\&quot;total_tokens\\&quot;: 172, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-56f41ea1-25d2-4071-aa8e-24693cc3dead-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 146, \\&quot;output_tokens\\&quot;: 26, \\&quot;total_tokens\\&quot;: 172, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;4) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;5) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\ucf54\\\\uc0ac\\\\uc778\\\\uc740 \\\\uc9c1\\\\uac01 \\\\uc0bc\\\\uac01\\\\ud615\\\\uc5d0\\\\uc11c \\\\uc778\\\\uc811\\\\ubcc0\\\\uacfc \\\\ube57\\\\ubcc0\\\\uc758 \\\\ube44\\\\uc728\\\\uc744 \\\\ub098\\\\ud0c0\\\\ub0c5\\\\ub2c8\\\\ub2e4.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 41, \\&quot;prompt_tokens\\&quot;: 92, \\&quot;total_tokens\\&quot;: 133, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-986633f9-22a6-4c16-89d9-3918780b585d-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 92, \\&quot;output_tokens\\&quot;: 41, \\&quot;total_tokens\\&quot;: 133, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;6) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;\\\\uc774\\\\uc804\\\\uc758 \\\\ub2f5\\\\ubcc0\\\\uc744 \\\\ud55c\\\\uae00\\\\ub85c \\\\ubc88\\\\uc5ed\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot;7) &quot;{\\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\&quot;, \\&quot;additional_kwargs\\&quot;: {\\&quot;refusal\\&quot;: null}, \\&quot;response_metadata\\&quot;: {\\&quot;token_usage\\&quot;: {\\&quot;completion_tokens\\&quot;: 19, \\&quot;prompt_tokens\\&quot;: 47, \\&quot;total_tokens\\&quot;: 66, \\&quot;completion_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;reasoning_tokens\\&quot;: 0}, \\&quot;prompt_tokens_details\\&quot;: {\\&quot;audio_tokens\\&quot;: null, \\&quot;cached_tokens\\&quot;: 0}}, \\&quot;model_name\\&quot;: \\&quot;gpt-3.5-turbo-0125\\&quot;, \\&quot;system_fingerprint\\&quot;: null, \\&quot;finish_reason\\&quot;: \\&quot;stop\\&quot;, \\&quot;logprobs\\&quot;: null}, \\&quot;type\\&quot;: \\&quot;ai\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: \\&quot;run-146afcbf-7301-4673-b3d4-0bf6ae2c9191-0\\&quot;, \\&quot;example\\&quot;: false, \\&quot;tool_calls\\&quot;: [], \\&quot;invalid_tool_calls\\&quot;: [], \\&quot;usage_metadata\\&quot;: {\\&quot;input_tokens\\&quot;: 47, \\&quot;output_tokens\\&quot;: 19, \\&quot;total_tokens\\&quot;: 66, \\&quot;input_token_details\\&quot;: {\\&quot;cache_read\\&quot;: 0}, \\&quot;output_token_details\\&quot;: {\\&quot;reasoning\\&quot;: 0}}}}&quot;8) &quot;{\\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;data\\&quot;: {\\&quot;content\\&quot;: \\&quot;What does cosine mean?\\&quot;, \\&quot;additional_kwargs\\&quot;: {}, \\&quot;response_metadata\\&quot;: {}, \\&quot;type\\&quot;: \\&quot;human\\&quot;, \\&quot;name\\&quot;: null, \\&quot;id\\&quot;: null, \\&quot;example\\&quot;: false}}&quot; 프로젝트에 Redis 저장 및 로드 코드 추가아래와 같이 Redis 설정을 추가하고 1234567# Redis 설정redis_client = redis.StrictRedis( host=os.getenv('REDIS_HOST', 'localhost'), port=int(os.getenv('REDIS_PORT', 6379)), db=0, decode_responses=True) 대화 히스토리 저장용 함수를 추가하고 1234567891011121314151617# 대화 히스토리 저장용 함수def save_chat_history_to_redis(user_id: str, message: str, response: str): timestamp = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) user_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message } assistant_chat_data = { &quot;timestamp&quot;: timestamp, &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response } # 사용자 메시지와 응답을 각각 저장 redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(user_chat_data)) redis_client.rpush(f&quot;chat_history:{user_id}&quot;, json.dumps(assistant_chat_data)) print(&quot;대화 내용이 저장되었습니다&quot;) 저장된 히스토리를 불러오는 함수도 추가하였다. 12345def get_session_history(user_id: str) -&gt; List[ChatFormat]: # Redis에서 user_id에 해당하는 대화 내역을 가져오는 로직 작성 history_data = redis_client.lrange(f&quot;chat_history:{user_id}&quot;, 0, -1) history = [ChatFormat(**json.loads(item)) for item in history_data] return history 그리고 Agent 호출 로직에 history를 load 하여 호출하도록 추가하였다. 12345678910111213# Redis에서 사용자 대화 기록 가져와 history 대체하기history = get_session_history(user_id)# GIS AGENT 호출response = call_gis_agent(message, history, summary)# Redis에 대화 내용 저장save_chat_history_to_redis(user_id, message, response)# 대화 내역 출력 (optional)print(print_session_history(history))return response 그리고 비슷하게 summary를 저장하는 로직도 추가하였다. 실행하게 되면 Redis에서 아래와 같이 두 가지 Redis key가 사용자 별로 생성된다. 123127.0.0.1:6379&gt; keys *1) &quot;summary_list:default_user&quot;2) &quot;chat_history:default_user&quot; 한글로 채팅이 오고 가기 때문에 redis에서 조회해도 다 깨져있고, 로그에서 잘 저장되고 불러오는지 확인해보겠다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151대화 내용이 저장되었습니다저장된 대화 내용:Role: userContent: 효창공원역 맛집 추천해줘----------Role: assistantContent: 효창공원역 주변의 맛집을 추천해드릴게요:1. **창성옥** - **주소**: 서울특별시 용산구 새창로 124-10 (용문동 25-16) - **전화**: 02-718-2878 - **카테고리**: 한식 - **메뉴**: - 뼈전골 (소): 27,000원 - 뼈전골 (중): 36,000원 - 해장국: 10,000원 - **영업시간**: 24시간 운영 - **예약 가능 여부**: 가능2. **효창동짜장우동** - **주소**: 서울특별시 용산구 백범로 283 (효창동 81-1) - **전화**: 02-703-5287 - **카테고리**: 중식 - **메뉴**: - 짜장: 4,000원 - 김치우동: 4,000원 - 우동: 4,000원3. **박명도봉평메밀막국수** - **주소**: 서울특별시 용산구 원효로 184-1 (원효로2가 43) - **전화**: 02-717-7711 - **카테고리**: 국수전문 - **메뉴**: - 들기름막국수: 11,000원 - 물막국수: 11,000원 - 비빔막국수: 11,000원 - **영업시간**: 10:00 - 22:004. **용문해장국** - **주소**: 서울특별시 용산구 효창원로 110 (용문동 8-95) - **전화**: 02-712-6290 - **카테고리**: 한식 - **메뉴**: - 해장국: 7,000원 - 해장국 (2인분): 14,000원 - 뼈전골 (중): 30,000원 - **예약 가능 여부**: 가능, 주차 가능5. **아성녹두빈대떡** - **주소**: 서울특별시 용산구 효창원로48길 3 (용문동 8-113) - **전화**: 02-706-8238 - **카테고리**: 기타 - **메뉴**: - 녹두빈대떡: 15,000원 - 해물파전: 12,000원 - 감자전: 12,000원이 맛집들이 효창공원역 근처에서 좋은 선택이 될 것입니다!----------Role: userContent: 우리집은 반포자이 114동인데, 우리집에서 창성옥까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 창성옥까지의 소요 시간은 약 24분 30초이며, 거리는 약 12,291 미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 서울역까지 얼마나걸려?----------Role: assistantContent: 반포자이 114동에서 서울역까지의 소요 시간은 약 33분이며, 거리는 약 9.5킬로미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 아까 알려준 효창공원 맛집 중 첫번째가 어디였지?----------Role: assistantContent: 아까 알려드린 효창공원역 맛집 중 첫 번째는 **창성옥**입니다. 주소는 서울특별시 용산구 새창로 124-10 (용문동 25-16)이고, 전화번호는 02-718-2878입니다. 한식을 제공하며, 뼈전골과 해장국이 주요 메뉴입니다. 24시간 운영하며 예약도 가능합니다.----------Role: userContent: 아까 추천해준 효창공원 맛집 중 4번째는 어디였지?----------Role: assistantContent: 아까 추천해드린 효창공원역 맛집 중 네 번째는 **용문해장국**입니다. 주소는 서울특별시 용산구 효창원로 110 (용문동 8-95)이고, 전화번호는 02-712-6290입니다. 한식을 제공하며, 해장국과 뼈전골이 주요 메뉴입니다. 예약이 가능하고 주차도 가능합니다.----------Role: userContent: 우리 부모님집은 도원삼성래미안 아파트야----------Role: assistantContent: 도원삼성래미안 아파트에서 서울역까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 17분 (1074초)- **거리**: 약 3.965킬로미터 (3965미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 우리부모님집까지 얼마나 걸려----------Role: assistantContent: 반포자이 114동에서 도원삼성래미안 아파트까지의 소요 시간은 약 28분이며, 거리는 약 13,026미터입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!----------Role: userContent: 우리집에서 KT 광화문 West 빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 광화문 West 빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 29분 (1766초)- **거리**: 약 9.3킬로미터 (9307미터)안전한 운전 되세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 19분 (1144초)- **거리**: 약 14.47킬로미터 (14470미터)안전 운전하세요!----------Role: userContent: 우리집에서 KT 판교빌딩까지 얼마나 걸려?----------Role: assistantContent: 반포자이 114동에서 KT 판교빌딩까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간:** 약 19분 17초 (1157초)- **거리:** 약 14,464 미터안전 운전하시기 바랍니다!----------Role: userContent: 우리부모님집에서 KT 대관령 수련관 얼마나 걸려?----------Role: assistantContent: 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 안내 결과는 다음과 같습니다:- **소요 시간**: 약 2시간 34분 (9280초)- **거리**: 약 197.6킬로미터이 정보를 바탕으로 여행을 계획하실 수 있습니다. 추가로 도움이 필요하시면 말씀해 주세요!----------None/Users/leehamin/app/GIS-Agent/gis-ai-agent-be/agent_api/models/chat_models.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead. chain = LLMChain(prompt=get_summary_prompt_template(), llm=llm)Summary list가 저장되었습니다.answer_data: {'answer': '반포자이 114동에서 KT위즈파크까지의 경로 안내는 이미 완료되었습니다. 경로는 반포자이 114동에서 출발하여 신반포로를 따라 남쪽으로 이동한 후, 반포대교를 건너 수원 방향으로 계속 이동하여 경수대로를 따라 KT위즈파크에 도착하는 것입니다. 안전한 여행 되세요!', 'chat_status': 'C1001'}answer_summary_list : ['사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, 그리고 KT 판교빌딩까지의 소요 시간과 거리를 문의하여 답변을 받았습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 또한, 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보를 확인했습니다.', '사용자가 효창공원역 근처 맛집을 추천받고, 그 중 첫 번째와 네 번째 맛집에 대한 정보를 확인했습니다. 또한, 반포자이 114동에서 창성옥, 서울역, 도원삼성래미안 아파트, KT 광화문 West 빌딩, KT 판교빌딩, 그리고 KT 대관령 수련관까지의 소요 시간과 거리를 문의하여 답변을 받았습니다. 사용자는 효창공원역 맛집 추천을 다시 요청했고, 첫 번째와 네 번째 맛집의 정보를 다시 확인했습니다. 도원삼성래미안 아파트에서 KT 대관령 수련관까지의 경로 정보도 확인했으며, 반포자이 114동에서 KT위즈파크까지의 소요 시간과 경로를 안내받았습니다.']INFO: 127.0.0.1:51812 - &quot;POST /api/v1/chat HTTP/1.1&quot; 200 OK 위와 같이 대화 내용과 summary_list가 모두 잘 저장되고 load되며 멀티턴 또한 잘 진행된다. User 별로 대화가 저장되도록 user_uuid 적용더미로 default_user라는 user_id로 통일하던 것을유저별로 대화 history와 summary가 적용될 수 있도록, 식별자인 user_uuid를 적용하였다. 헤더에 X-User-UUID를 달고 가는 방식이다. 12345678def generate_or_get_uuid(user_uuid: Optional[str]) -&gt; UUID: if user_uuid: try: # UUID가 유효한지 검사 return UUID(user_uuid) except ValueError: raise_error(&quot;E1006&quot;) else: # UUID가 없으면 새 UUID 생성 return uuid4() 유저 아이디에 맞게 redis key가 생성된다. 12345127.0.0.1:6379&gt; keys *1) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;2) &quot;summary_list:default_user&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;chat_history:default_user&quot; 테스트 세션1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;네, 판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n- 위도: 37.3933\\n- 경도: 127.1144\\n\\n더 필요한 정보가 있으면 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot; ]} 테스트 세션2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;효창공원 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;효창공원 주변의 맛집 정보는 다음과 같습니다:\\n\\n1. **효창원** \\n - **주소**: 서울 용산구 효창원로 97\\n - **특징**: 전통 한식과 다양한 메뉴를 제공하는 식당입니다.\\n\\n2. **카페 드 파리** \\n - **주소**: 서울 용산구 효창원로 85\\n - **특징**: 디저트와 커피를 전문으로 하는 아늑한 카페입니다.\\n\\n3. **고기리 막창** \\n - **주소**: 서울 용산구 효창원로 95\\n - **특징**: 고기 요리를 전문으로 하는 맛집으로, 막창이 유명합니다.\\n\\n이 외에도 더 많은 맛집이 있으니, 필요하시면 추가 정보를 요청해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot; ]} 테스트 세션3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;첫번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 49분입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot; ]} 테스트 세션4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원으로 이동한 후, 효창원에서 김포공항까지 이동하는 데 걸리는 총 소요 시간은 다음과 같습니다:\\n\\n- **판교 그랑블 아파트 → 효창원**: 약 41분 38초\\n- **효창원 → 김포공항**: 약 56분 4초\\n\\n따라서, 총 소요 시간은 약 1시간 37분 42초입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot; ]} 테스트 세션5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 효창원까지의 소요 시간은 약 1시간 10분이며, 효창원에서 인천공항까지의 소요 시간은 약 47분입니다. 따라서, 총 소요 시간은 약 1시간 57분입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 더 필요한 정보가 있는지 묻습니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다.&quot;, &quot;사용자가 자신의 집이 판교 그랑블 아파트라고 말하며, 집의 좌표를 알고 있는지 물어봅니다. 어시스턴트는 위도 37.3933, 경도 127.1144의 좌표를 제공합니다. 사용자가 효창공원 맛집을 추천해달라고 하자, 어시스턴트는 효창원, 카페 드 파리, 고기리 막창 등의 맛집 정보를 제공합니다. 사용자가 효창원을 선택하며, 집에서 그곳까지 얼마나 걸리는지 물어보자, 어시스턴트는 약 49분이 걸린다고 답변합니다. 사용자가 효창원에서 식사를 마치고 김포공항으로 가는 시간을 묻자, 어시스턴트는 판교 그랑블 아파트에서 효창원까지 약 41분 38초, 효창원에서 김포공항까지 약 56분 4초로 총 소요 시간이 약 1시간 37분 42초라고 안내합니다. 사용자가 인천공항으로 가는 시간을 묻자, 어시스턴트는 총 소요 시간이 약 1시간 57분이라고 안내합니다.&quot; ]} 서버에 배포프로젝트에서 requirements.txt 파일을 사용하고 있기 때문에, 이 파일에 redis 패키지를 추가했다. 1redis Redis가 실행되고 있는 호스트의 IP로 바꾸었다 12# host=os.getenv('REDIS_HOST', 'localhost'),host=os.getenv('REDIS_HOST', '10.71.176.93'), 1234# EC2 환경redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://10.71.176.93:6379/0&quot;)# 로컬 환경# redis_url = os.getenv(&quot;REDIS_URL&quot;, &quot;redis://localhost:6379/0&quot;) Redis 서버가 외부 연결을 허용하는지 확인Redis 설정파일 위치 확인12# Redis 설정 파일 위치 확인find / -name &quot;redis.conf&quot; Redis 설정 파일 수정123456# 이렇게 있던 것을bind 127.0.0.1 -::1# 이렇게bind 0.0.0.0protected-mode no Redis 재시작123456789redis-cli shutdown# Redis 서버의 프로세스 ID 확인ps aux | grep redis-server# 프로세스 ID를 이용해 종료 (예: PID가 1234인 경우)kill 1234# Redis 서버 재시작redis-server /path/to/redis.conf 여전히 protected mode가 실행중으로 나타난다1234[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379-DENIED Redis is running in protected mode because protected mode is enabled and no password is set for the default user. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Set up an authentication password for the default user. NOTE:You only need to do one of the above things in order for the server to start accepting connections from the outside. Redis CLI를 통해 임시로 protected-mode 비활성화Redis 서버에 접근 가능한 경우, Redis CLI를 사용하여 protected-mode를 비활성화할 수 있다. 123456789# Redis CLI 접속redis-cli# protected-mode 비활성화CONFIG SET protected-mode no[root@ec2-ct01-dev-slm-app-01 ~]# curl -v telnet://10.71.176.93:6379* Trying 10.71.176.93:6379...* Connected to 10.71.176.93 (10.71.176.93) port 6379 이번엔 연결이 되는 것 같다 서버 테스트테스트 1Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;우리집은 판교 그랑블 아파트야, 우리집 좌표 알고 있니?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트의 좌표는 다음과 같습니다:\\n\\n- **위도**: 37.393299557105\\n- **경도**: 127.11436915566647\\n\\n이 정보가 도움이 되길 바랍니다! 추가적인 질문이 있으면 언제든지 말씀해 주세요.&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot; ]} 테스트 2Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;포천 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213{ &quot;chat&quot;: { &quot;answer&quot;: &quot;포천의 맛집 정보를 제공해드렸습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot; ]} 테스트 3Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;영종도 맛집 알려줘&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response1234567891011121314{ &quot;chat&quot;: { &quot;answer&quot;: &quot;영종도에서 추천하는 맛집 목록입니다:\\n\\n1. **선녀풍**\\n - 주소: 인천광역시 중구 을왕동 용유서로 272, 689-1\\n - 전화: 032-751-2121\\n - 주메뉴: 해물파전(15,000원), 선녀물회(20,000원), 낙지물회(28,000원)\\n - 영업시간: 매일 12:00 - 22:00\\n - 주차: 가능\\n - 좌표: [37.4444, 126.3787]\\n\\n2. **황해해물칼국수**\\n - 주소: 인천광역시 중구 덕교동 마시란로 37, 128-56\\n - 전화: 032-752-3017\\n - 주메뉴: 산낙지(15,000원), 전복 (4마리)(16,000원), 해물칼국수(10,000원)\\n - 영업시간: 매일 10:00 - 20:00\\n - 주차: 가능\\n - 좌표: [37.4262, 126.4212]\\n\\n3. **동해막국수**\\n - 주소: 인천광역시 중구 을왕동 용유서로479번길 16, 859-3\\n - 전화: 032-746-5522\\n - 영업시간: 매일 11:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4616, 126.3705]\\n\\n4. **미애네칼국수**\\n - 주소: 인천광역시 중구 덕교동 용유로21번길 51, 80-14\\n - 전화: 032-746-3838\\n - 주메뉴: 산낙지(18,000원), 전복회(15,000원), 바다속칼국수 (소)(35,000원)\\n - 영업시간: 매일 09:00 - 21:00\\n - 주차: 가능\\n - 좌표: [37.4300, 126.4242]\\n\\n5. **을항**\\n - 주소: 인천광역시 중구 을왕동 선녀바위로55번길 39, 686-5\\n - 전화: 032-752-2227\\n - 주메뉴: 물회(1인)(25,000원), 물회(대)(75,000원), 물회(중)(55,000원)\\n - 좌표: [37.4436, 126.3782]\\n\\n맛집 선택에 도움이 되길 바랍니다!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot; ]} 테스트 4Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;세번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 영종도의 세 번째 맛집인 동해막국수까지의 경로는 약 1시간 5분이 소요되며, 거리는 약 78.54 km입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot; ]} 테스트 5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 김포국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 김포국제공항**\\n - 소요 시간: 약 40분\\n - 거리: 약 45.51 km\\n\\n따라서 전체 소요 시간은 약 1시간 45분이며, 총 거리는 약 124.05 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot; ]} 테스트 6Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Request1234567891011121314151617{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 인천국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 인천국제공항**\\n - 소요 시간: 약 19분\\n - 거리: 약 13.88 km\\n\\n따라서 전체 소요 시간은 약 1시간 24분이며, 총 거리는 약 92.42 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트, 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다. 마지막으로 사용자가 인천공항으로 가는 경로를 묻자, AI가 소요 시간과 거리를 안내하였다.&quot; ]} Redis에서도 해당 사용자 id에 맞게 history와 summary가 잘 저장된다.123456789[root@ec2-ct01-dev-slm-app-01 ~]# redis-cli127.0.0.1:6379&gt; keys *1) &quot;summary_list:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;2) &quot;chat_history:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;127.0.0.1:6379&gt; lrange chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77 0 -11) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;user\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\uc6b0\\\\ub9ac\\\\uc9d1\\\\uc740 \\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc57c, \\\\uc6b0\\\\ub9ac\\\\uc9d1 \\\\uc88c\\\\ud45c \\\\uc54c\\\\uace0 \\\\uc788\\\\ub2c8?\\&quot;}&quot;2) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;assistant\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc758 \\\\uc88c\\\\ud45c\\\\ub294 \\\\ub2e4\\\\uc74c\\\\uacfc \\\\uac19\\\\uc2b5\\\\ub2c8\\\\ub2e4:\\\\n\\\\n- **\\\\uc704\\\\ub3c4**: 37.393299557105\\\\n- **\\\\uacbd\\\\ub3c4**: 127.11436915566647\\\\n\\\\n\\\\uc774 \\\\uc815\\\\ubcf4\\\\uac00 \\\\ub3c4\\\\uc6c0\\\\uc774 \\\\ub418\\\\uae38 \\\\ubc14\\\\ub78d\\\\ub2c8\\\\ub2e4! \\\\ucd94\\\\uac00\\\\uc801\\\\uc778 \\\\uc9c8\\\\ubb38\\\\uc774 \\\\uc788\\\\uc73c\\\\uba74 \\\\uc5b8\\\\uc81c\\\\ub4e0\\\\uc9c0 \\\\ub9d0\\\\uc500\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;}&quot; 123456789101112## 테스트 4### Request~~~json{ &quot;chat&quot;: { &quot;message&quot;: &quot;세번째 것으로 할래! 우리집에서 거기까지 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response123456789101112131415{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 영종도의 세 번째 맛집인 동해막국수까지의 경로는 약 1시간 5분이 소요되며, 거리는 약 78.54 km입니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot; ]} 테스트 5Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그럼 집에서 출발해서 거기서 밥먹고, 김포공항 가면 얼마나 걸릴까?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Response12345678910111213141516{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 김포국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 김포국제공항**\\n - 소요 시간: 약 40분\\n - 거리: 약 45.51 km\\n\\n따라서 전체 소요 시간은 약 1시간 45분이며, 총 거리는 약 124.05 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot; ]} 테스트 6Request1234567{ &quot;chat&quot;: { &quot;message&quot;: &quot;그러면 김포공항 대신 인천공항으로 가면 얼마나 걸려?&quot;, &quot;history&quot;: [] }, &quot;summary_list&quot;:[]} Request1234567891011121314151617{ &quot;chat&quot;: { &quot;answer&quot;: &quot;판교 그랑블 아파트에서 출발하여 동해막국수에서 식사를 하고, 인천국제공항까지 가는 전체 경로는 다음과 같습니다:\\n\\n1. **판교 그랑블 아파트 → 동해막국수 (인천)**\\n - 소요 시간: 약 1시간 5분\\n - 거리: 약 78.54 km\\n\\n2. **동해막국수 (인천) → 인천국제공항**\\n - 소요 시간: 약 19분\\n - 거리: 약 13.88 km\\n\\n따라서 전체 소요 시간은 약 1시간 24분이며, 총 거리는 약 92.42 km입니다. 안전하고 즐거운 여행 되세요!&quot;, &quot;status_info&quot;: { &quot;chat_status&quot;: &quot;C1001&quot;, &quot;summary_status&quot;: &quot;S1001&quot; } }, &quot;summary_list&quot;: [ &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다.&quot;, &quot;사용자가 판교 그랑블 아파트에 대해 묻자, AI가 아파트의 좌표를 제공하며 도움이 되길 바란다고 답변했다. 사용자가 포천 맛집을 묻자, AI가 포천의 맛집 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 영종도 맛집을 묻자, AI는 영종도에서 추천하는 다양한 맛집 목록과 상세 정보를 제공하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트와 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한, 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포국제공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다.&quot;, &quot;사용자가 판교 그랑블 아파트, 포천 및 영종도의 맛집에 대해 묻자, AI가 각각의 좌표와 맛집 정보를 제공하며 도움이 되길 바란다고 답변했다. 이후 사용자가 자신의 집인 판교 그랑블 아파트의 좌표를 알고 있는지 묻자, AI는 정확한 좌표를 제공하였다. 또한 사용자가 영종도의 맛집 중 세 번째 맛집인 동해막국수에 가고 싶다고 하자, AI는 판교 그랑블 아파트에서 해당 맛집까지의 소요 시간과 거리를 안내하였다. 이어서 사용자가 동해막국수에서 식사를 하고 김포공항까지의 경로를 묻자, AI는 전체 소요 시간과 거리를 상세히 안내하였다. 마지막으로 사용자가 인천공항으로 가는 경로를 묻자, AI가 소요 시간과 거리를 안내하였다.&quot; ]} Redis에서도 해당 사용자 id에 맞게 history와 summary가 잘 저장된다.123456789[root@ec2-ct01-dev-slm-app-01 ~]# redis-cli127.0.0.1:6379&gt; keys *1) &quot;summary_list:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;2) &quot;chat_history:b4d4e144-b8af-47d9-953b-ca80e65a474b&quot;3) &quot;chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;4) &quot;summary_list:6b622f61-fd80-4906-bc50-e9aee6c72e77&quot;127.0.0.1:6379&gt; lrange chat_history:6b622f61-fd80-4906-bc50-e9aee6c72e77 0 -11) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;user\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\uc6b0\\\\ub9ac\\\\uc9d1\\\\uc740 \\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc57c, \\\\uc6b0\\\\ub9ac\\\\uc9d1 \\\\uc88c\\\\ud45c \\\\uc54c\\\\uace0 \\\\uc788\\\\ub2c8?\\&quot;}&quot;2) &quot;{\\&quot;timestamp\\&quot;: \\&quot;2024-11-11 08:36:48\\&quot;, \\&quot;role\\&quot;: \\&quot;assistant\\&quot;, \\&quot;content\\&quot;: \\&quot;\\\\ud310\\\\uad50 \\\\uadf8\\\\ub791\\\\ube14 \\\\uc544\\\\ud30c\\\\ud2b8\\\\uc758 \\\\uc88c\\\\ud45c\\\\ub294 \\\\ub2e4\\\\uc74c\\\\uacfc \\\\uac19\\\\uc2b5\\\\ub2c8\\\\ub2e4:\\\\n\\\\n- **\\\\uc704\\\\ub3c4**: 37.393299557105\\\\n- **\\\\uacbd\\\\ub3c4**: 127.11436915566647\\\\n\\\\n\\\\uc774 \\\\uc815\\\\ubcf4\\\\uac00 \\\\ub3c4\\\\uc6c0\\\\uc774 \\\\ub418\\\\uae38 \\\\ubc14\\\\ub78d\\\\ub2c8\\\\ub2e4! \\\\ucd94\\\\uac00\\\\uc801\\\\uc778 \\\\uc9c8\\\\ubb38\\\\uc774 \\\\uc788\\\\uc73c\\\\uba74 \\\\uc5b8\\\\uc81c\\\\ub4e0\\\\uc9c0 \\\\ub9d0\\\\uc500\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\&quot;}&quot;","link":"/2024/11/03/LangChain-RunnableWithMessage/"},{"title":"SK AI SUMMIT 2024 후기","text":"2024년 Coex에서 열린 SK AI SUMMIT 1일차 세션들을 듣고 후기를 남긴다. ON device AI @에이닷 전화 강연 목적 및 핵심 주제 온디바이스 AI의 개념과 구현 사례를 통해 개인정보 보호 및 비용 절감을 위해 STT 서버를 온디바이스에서 처리하는 방법을 소개 핵심 내용 온디바이스 STT 서버 처리: 음성 파일 전송 없이 단말에서 음성 인식을 처리하여 개인정보 보호 및 서버 연산 비용 절감을 도모. 기존 서버 트래픽은 하루 약 천만 콜, 피크 시간대에는 한 시간당 120만 콜에 이르며, 이를 AWS에서 처리하면서 GPU 비용 문제가 발생 iOS와 Android의 차이: iOS는 2채널 통화 녹음을 지원해 모델 연산이 두 배로 필요하지만, Android는 화자 구별 작업이 추가로 필요 모델 학습 및 경량화 과정 Scratch Training → Knowledge Distillation → TensorFlow 모델 생성 → TF Lite &amp; 양자화 → Beamsearch → On-Device ASR의 단계로 최적화 모델 크기를 130MB로 줄이면서도 성능을 유지하며, Transformer에서 시작해 Conformer 및 Branchformer로 발전해 디코더 레이어를 줄여 속도를 약 20% 개선 지식 증류: 부모 모델의 특정 레이어를 선택하여 적용하며 선택 방식에 따라 성능 차이 발생 모델 성능 및 특성: 최종 모델은 0.15B, 133MB 크기로 서버용 LAS 구조와 유사하지만, On-device용으로 조정된 구조로서 상용 인식기보다 높은 인식 성능을 보임 장점 및 단점 온디바이스 모델 추론과 경량화 과정을 이해할 수 있음 데이터를 서버에 저장하지 않고 단말에 저장하기 때문에 개인 정보보호 문제 벗어날 수 있고 연산비용 절감가능 STT 관련 용어가 많아 초보자에게는 어려울 수 있음 주요 인사이트 온디바이스 모델로 개인정보 보호와 비용 절감이 가능 모델을 클라이언트에 탑재함으로써 서버 유지비용 감소할 것으로 보임 iOS와 Android 간의 처리 차이를 고려해야 함 On-Device 국방 경계 솔루션 혁신 강연 목적 및 핵심 주제 국방 도메인에서 실시간 비디오 온디바이스 처리 및 MLOps 파이프라인을 통한 국방 경계 솔루션의 혁신 사례 핵심 내용 On-Device AI 제약사항 : Transformer는 못 쓰고, YOLO, Resnet18로 사용해야 한다. 국방 경계이기 때문에 사람 놓치면 큰일남, 탐지 정확도 높아야 한다. 하지만 짙은 안개, 수풀 흔들림, 동물 등 오탐지 많음 전략 DET Inference Time Augmentation 입력해상도 1920 * 1080 Large Model로 딱 한번 추론하기 vs 입력해상도 640 * 640 Medium Model 분할 배치 추론하기 관심 영역(이전 프레임의 결과, 모션 등) 재추론 등 정책 결정, 경계 부분의 검출 고려한 Customized NMS 적용 필요 DET + CLS Model Ensemble Detector 모델로 전체 Recall을 올리고, Classification으로 전체 오탐지 수 줄인다. 학습 데이터를 더 많이 구하고, 만든다 국방 경계지역은 군사 보안구역이기 때문에, 학습데이터 구하기 힘듬 직접 촬영 + 생성 모델로 합성 데이터 만들기 똑똑한 과외 선생님 도움아래 못난 제자 없다. 똑똑한 과외 선생님 (Knowledge Distillation : Classification 성능 개선) 사용 학습 데이터 취득의 양과 질 확보가 어렵고, 가벼운 모델을 학습하는 경우에 매우 효과적 모든 Classification 모델과 다양한 국방 Test DB에서 정확도 개선효과 검증 성능 최적화 H/W 디코더가 S/W 디코더 대비 2.6배 빠르게 처리하며, Knowledge Distillation을 활용해 경량 모델의 성능을 개선하여 최종 93% 정확도를 달성. 장점 및 단점 Data Transfer Latency가 최소화 됨 도서 산간 어디든지 설치 가능한 솔루션 이미지 처리 관련 분야로 기술 도입 난이도가 높고, 세부 정보 부족 주요 인사이트 온디바이스 이미지 처리 기술이 국방 및 보안 분야에서 데이터 보안 및 실시간 처리가 가능함을 확인 향후 국방 VLM을 접목하여 긴급 상황에 자동 공지 시스템 개발 가능성을 제시 하드웨어를 이해하는 AI 모델 최적화 기술과 그 성공 사례 강연 목적 및 핵심 주제 NotaAI의 AI 모델 최적화 기술과 하드웨어 명령어 최적화를 통한 성능 개선 사례를 설명 핵심 내용 NotaAI는 LLM, GenAI, Computer Vision 등 AI 기술과 CPU, GPU, NPU, Memory 등 Semiconductor 간의 최적화를 돕는다. 칩셋 맞춤형 최적화 필요성 : HW는 2년에 2배정도 발전을 이루는데, AI Model Size는 2년에 410배 증가 추세 성능 향상 사례: 최신 모델이라도 특정 칩셋의 명령어가 최적화되지 않으면 성능 저하가 발생할 수 있으며, 이를 하드웨어 명령어 최적화를 통해 성능 저하 문제를 해결 모델 성능 향상 요소: 신경망 구조, 데이터셋, 하드웨어에 따라 성능이 달라지며, 하드웨어에 맞춘 최적화로 효율을 극대화할 수 있음 장점 및 단점 AI Model 최적화를 통해 한정된 HW 자원을 효율적으로 사용 가능 하드웨어 명령어 최적화의 중요성을 인식할 수 있음 기술적 설명보다 회사 소개에 다소 치중된 내용 주요 인사이트 HW 개발 속도가 AI Model 사이즈 증가속도를 따라가지 못하기에 모델 최적화는 중요해보임 AI 모델을 특정 하드웨어에 맞춰 최적화하면 성능이 향상될 수 있으며, 실무에서 다양한 칩셋에 맞춘 최적화가 필요함을 확인 어디 갈까? TMAP 데이터로 장소 추천 받기! 강연 목적 및 핵심 주제 티맵 주행 정보를 활용한 장소 추천 서비스. 핵심 내용 주행 목적지 데이터를 바탕으로 다양한 주행 패턴 중 여행 세션을 정의하고, 주행 데이터를 효과적으로 모델링하기 위해 활용한 그래프 구조 기반의 딥러닝 방식 소개 딥러닝한 모델을 실제 서비스에 적용하기 위한 MLOps 파이브라인 설명 추천의 개인화 및 고도화를 위한 LLM 적용 추천 서비스 방향성 여행이라는 패턴은 하루 내에 여행 관광과 관련된 장소를 3곳 이상 들렀고, 이 중 하나는 숙소에 들렀던 목적지의 시퀀스로 정의 Graph 구조를 활용한 딥러닝 - 모델링 주안점 시간적, 공간적 Locality를 위한 Graph 기반의 모델 사용 시간에 대해 고려하기 위해서 시간 임베딩을 추가 고차 연관성 학습을 위한 Multi-granularity Graph 사용 Popularity bias 완화를 위한 L2-norm 적용 시간대 별 이동거리를 고려한 장소 추천 이른 시간일수록 더 먼 장소 추천 시간대별 적합한 카테고리의 장소 추천 점심시간대에는 식당을 추천 저녁 시간대에는 호텔이나 캠핑장 추천 Light-weight 한 모델이 잘 working 한 이유 충분한 티맵의 데이터 그래프 모델의 Locality 학습 우위 빠른 서비스 배포를 위한 MLOps NEUF-CLI (Custom SDK) 사용 Jenkins 파이프라인 활용 장점 및 단점 시간적, 공간적 Locality를 고려하여 Graph 기반의 모델을 사용하였기에 추천의 퀄리티가 좋아보였음. 가벼운 모델을 사용했지만 데이터가 충분히 방대하여 괜찮은 추천 결과를 나타내 줌 아직 개인의 취향에 따라 추천해주는 부분은 없어서 아쉬움 주요 인사이트 검색한 POI를 기준으로 하면, 검색만 하고 실제로 해당 위치에 가지 않을수도 있기에, 실제로 이동하여 해당 위치에 간 경우만 데이터로 활용한 점이 효과적이라고 생각. 데이터 전처리 및 분석이 중요한 요소임을 확인 POI 검색 기준으로 실제 이동 데이터만 활용한 것이 효과적 장소를 추천하기 위해 모든 주행 데이터를 사용하는 것은 다양한 패턴이 존재하여 모델에 혼선을 유발하므로 조정해야 함 소규모 모델도 충분한 데이터를 활용해 높은 성능을 낼 수 있음 우리집 TV에도 AI가?! Btv 미디어 Agent 강연 목적 및 핵심 주제 AI가 접목된 TV의 개인화된 미디어 환경 핵심 내용 LLM 기반 검색 개선: 기존 키워드 기반 검색에서 LLM 기반 검색으로 개선 에이닷을 접목하여 미디어에 관련된 정보를 검색 키워드만 추출하는 NUGU SDK와 다르게 맥락을 추출하여 반영하고 검색 지식 그래프를 활용하여 컨텐츠간의 유사도 파악 임베딩 모델 어떤것을 사용했는지는 공개 불가 장점 및 단점 개인의 취향과 이력 패턴은 따로 저장하고 있지 않고, 사용할 때 취향과 맥락이 잘 반영되도록 입력해야 함 단순히 키워드를 추출하는 방식보다는 맥락이 반영되어 검색의 만족도가 높음 LLM 사용 이유와 구체적인 성능 설명 부족 주요 인사이트 개인화 이력 패턴은 개인정보 문제로 반영되지 못함 컨텐츠 메타정보는 컨텐츠 정보를 판매 회사에서 구매하고, 유튜브 댓글 등을 활용하여 임베딩을 통해 구성함 AI 여행 파트너, TGO: 일정부터 혜택까지 Agent가 설계하는 스마트 여행 강연 목적 및 핵심 주제 고객의 취향, 예산, 일정을 분석하여 최적의 여행 계획을 수립해주는 AI 여행파트너 TGO 핵심 내용 그래프 DB 기반 RAG를 활용하여 고객의 취향, 예산, 일정을 입력하면 데이터를 분석하여 최적의 여행 계획을 수립해 줌 MBTI도 입력하면 이를 반영하여 여행계획 수립 다중 에이전트 협력: 여러 에이전트가 협력하여 여행 일정을 설계하며 토큰 발생과 처리 시간 소요 문제를 인지 장점 및 단점 개인의 취향을 저장하고 있다가 이를 반영하지는 못하고, 분석 요청시 데이터를 입력해야 함 다중 에이전트 협력 구조를 이해할 수 있음 주요 인사이트 벡터 서치를 위한 데이터는 웹 크롤링을 통해 DB에 축적하며, POI 검색 고도화를 위한 웹서치 진행함 여러 에이전트의 응답시간은 여행 일정 수립에 중점을 두어 고려를 하지 않아 오래걸림","link":"/2024/11/05/SK-AI-SUMMIT-2024/"},{"title":"Azure-AZ204","text":"MS AZ204 자격증 취득을 위한 공부 과정 및 방법을 기록해보겠다. 시험 신청방법AZ-204 소개 및 접수시험 접수 방법 공부순서 및 공부전략 MS 러닝센터 -&gt; 마이크로소프트 교육 및 실습 -&gt; 덤프문제 풀이 시간을 단축시켜서 공부를 하고 싶다면 마이크로소프트 러닝센터의 AZ204 부분의 대분류와 그 아래 소제목들을 정리한 다음 덤프문제를 먼저 풀고 덤프 문제 풀이를 하면서 대제목 별로 문제들을 구분해서 암기를 해야할 부분을 정리하면서 공부를 하고 그 후에 러닝센터 AZ204의 실습파트를 하나하나씩 실제로 해보면 하루 6시간정도 공부를 한다고 했을 때 2주면 딸수 있지 않을까 하는 행복회로를 돌려본다. 정리하자면 AZ204 대분류 in MS 러닝센터 AZ204 -&gt; dump 문제풀이 -&gt; dump 문제를 대분류에 맞게 노트정리 -&gt; 노트정리한 것 암기 -&gt; 덤프문제 2~3회 풀기 시험문제 출제 비중 Category Ratio Develop Azure Compute Solution 25-30% Develop for Azure Storage 10-15% Implement Azure Security 15-20% Monitor, troubelshoot, and optimize Azure solutions 10-15% Connect to and consume Azure servicies and third-party service 25-30% 시험범위 연습문제 학습자료 연습문제 시험 당일에 대하여 시험시간 : 210분… 매우 기가 빨릴 것으로 보인다. 시험 자료..시크릿노트덤프문제집","link":"/2024/11/05/Azure-AZ204/"},{"title":"Azure EntraID란?","text":"Microsoft Entra ID는 클라우드 기반의 identity and access management 서비스이다. Microsoft Entra ID 란?Entra ID를 사용하면, Microsoft365, Azure portal 및 그 외 수많은 SaaS 어플리케이션과 같은 외부 리소스들에 접근을 승인하고 사용할 수 있게 해준다. 또한, 사용자가 속한 기업의 인트라넷에 있는 내부 리소스들과 사용자의 조직에서 사용하는 다른 클라우드 어플리케이션에도 접근할 수 있도록 해준다.","link":"/2024/11/15/Azure-EntraID/"},{"title":"Document-Knowledge-Mining-Solution-Accelerator","text":"Azure OpenAI Service와 Azure AI Document Intelligence를 기반으로 구축된, 비정형, 다중모드 문서에서 요약, 엔터티, 메타데이터를 처리하고 추출하여 데이터를 검색하고 채팅할 수 있는 솔루션. 아키텍처 그림 실제 엔터티 추출 : 사람, 제품, 이벤트 장소 또는 행동과 같은 고유한 정보를 처리하고 추출 채팅 기반 Insight discovery : 모든 인덱싱 된 assets, 단일 assets, 선택한 assets 세트 또는 사용자 주도 키워드 검색을 기반으로 생성된 asset 목록과 채팅 가능 텍스트 및 문서 데이터 분석 : 문서, 손글씨 텍스트, 차트, 그래프, 표 및 양식 필드를 포함한 다중 모드 문서의 내용을 분석, 비교 및 요약하여 심층적인 통찰력을 제공 프롬프트 제안 가이드 : 프롬프트 문의를 기반으로 다음 질문 세트를 제안 다중 모드 정보 처리 : 여러 콘텐츠 유형과 다양한 형식의 지식을 처리하고 추출 대량의 데이터를 신속하게 분석하고, 관련 제안을 생성하여 빠르고 쉽게 추론할 수 있도록 도와준다. PrerequisitesPowershell 설치12% brew install powershell/tap/powershell% pwsh macOS에 Azure CLI 설치123456789101112131415161718192021$ brew update &amp;&amp; brew install azure-cli% az --versionazure-cli 2.67.0core 2.67.0telemetry 1.1.0Dependencies:msal 1.31.0azure-mgmt-resource 23.1.1Python location '/usr/local/Cellar/azure-cli/2.67.0_1/libexec/bin/python'Extensions directory '/Users/leehamin/.azure/cliextensions'Python (Darwin) 3.12.8 (main, Dec 3 2024, 18:42:41) [Clang 16.0.0 (clang-1600.0.26.4)]Legal docs and information: aka.ms/AzureCliLegalYour CLI is up-to-date. kubectl 설치12345678910% sudo az aks install-cliPassword:The detected architecture of current device is &quot;x86_64&quot;, and the binary for &quot;amd64&quot; will be downloaded. If the detection is wrong, please download and install the binary corresponding to the appropriate architecture.No version specified, will get the latest version of kubectl from &quot;https://dl.k8s.io/release/stable.txt&quot;Downloading client to &quot;/usr/local/bin/kubectl&quot; from &quot;https://dl.k8s.io/release/v1.32.0/bin/darwin/amd64/kubectl&quot;Please ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.No version specified, will get the latest version of kubelogin from &quot;https://api.github.com/repos/Azure/kubelogin/releases/latest&quot;Downloading client to &quot;/tmp/tmpdiem7i3s/kubelogin.zip&quot; from &quot;https://github.com/Azure/kubelogin/releases/download/v0.1.6/kubelogin.zip&quot;Moving binary to &quot;/usr/local/bin/kubelogin&quot; from &quot;/tmp/tmpdiem7i3s/bin/darwin_amd64/kubelogin&quot;Please ensure that /usr/local/bin is in your search PATH, so the `kubelogin` command can be found. aks-preview 설치Azure CLI의 extension으로서 AKS를 운영할 수 있다. 1234567891011121314151617 % az extension add --name aks-previewNo stable version of 'aks-preview' to install. Preview versions allowed.The installed extension 'aks-preview' is in preview.% az extension list/usr/local/Cellar/azure-cli/2.67.0_1/libexec/lib/python3.12/site-packages/azure/batch/models/_models_py3.py:4839: SyntaxWarning: invalid escape sequence '\\s' &quot;&quot;&quot;A Job Preparation Task to run before any Tasks of the Job on any given[ { &quot;experimental&quot;: false, &quot;extensionType&quot;: &quot;whl&quot;, &quot;name&quot;: &quot;aks-preview&quot;, &quot;path&quot;: &quot;/Users/leehamin/.azure/cliextensions/aks-preview&quot;, &quot;preview&quot;: true, &quot;version&quot;: &quot;13.0.0b2&quot; }] Helm 설치k8s의 패키지 매니저이다. 123% brew install helm% helm versionversion.BuildInfo{Version:&quot;v3.16.4&quot;, GitCommit:&quot;7877b45b63f95635153b29a42c0c2f4273ec45ca&quot;, GitTreeState:&quot;dirty&quot;, GoVersion:&quot;go1.23.4&quot;} Docker Desktop 설치서비스를 컨테이너화 하고 Azure Container Registry로 배포하기 위하여 도커 데스크탑을 설치한다.Deploy script를 실행하기 전에 도커 데스크탑이 실행중인지 확인 필요하다. Azure Accesssubscription-level이 Owner나 User Access Administrator role로 필요하다. Regional AvailabilityAzure 서비스와 모델이 지역별로 제공 여부가 제한된다. Azure Open AI (GPT-4o mini) 사용 모델 : 이 솔루션은 GPT-4o mini와 text-embedding-3-large 모델을 사용한다. 제공 지역 : 해당 모델들은 현재 다음 지역에서만 사용 가능 West US3 East US East US2 SwedenCentral Azure AI Document Intelligence 사용 API 버전 : 이 솔루션은 2023-10-31-preview 또는 그 이후 버전의 Document Intelligence를 사용한다. 제공 지역 : 현재 해당 API는 East US 지역에서만 사용 가능하다. 배포 지역 제한 : 이 모델은 반드시 East US 지역에 배포 되어야 한다. Deployment자동화 된 배포 단계 Azure 리소스 배포 Azure 리소스에서 Secrets 가져오기 애플리케이션 Config 파일에 Secrets 업데이트 Azure App Configuration에 Application Config 설정 애플리케이션 컴파일, 이미지 빌드 및 Azire Container Registry에 푸시 k8s 클러스터 인프라 구성 k8s 구성 파일 업데이트 인증서, 인그레스 컨트롤러, 애플리케이션 이미지를 ACR에서 배포 Deployment Script 실행12345% git clone https://github.com/microsoft/Document-Knowledge-Mining-Solution-Accelerator.git# Powershell에서% cd Document-Knowledge-Mining-Solution-Accelerator&gt; .\\resourcedeployment.ps1 실행 시 에러 발생1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 _____ _ | __ \\ | | | | | | ___ ___ _ _ _ __ ___ ___ _ __ | |_ | | | |/ _ \\ / __| | | | '_ _ \\ / _ \\ '_ \\| __| | |__| | (_) | (__| |_| | | | | | | __/ | | | |_ |_____/ \\___/ \\___|\\__,_|_| |_| |_|\\___|_| |_|\\__| __ __ _ _ | |/ / | | | | | \\/ (_) (_) | ' / _ __ _____ _| | ___ __| | __ _ ___ | \\ / |_ _ __ _ _ __ __ _ | &lt; | '_ \\ / _ \\ \\ /\\ / / |/ _ \\/ _ |/ _ |/ _ \\ | |\\/| | | '_ \\| | '_ \\ / _ | | . \\| | | | (_) \\ V V /| | __/ (_| | (_| | __/ | | | | | | | | | | | | (_| | |_|\\_\\_| |_|\\___/ \\_/\\_/ |_|\\___|\\__,_|\\__, |\\___| |_| |_|_|_| |_|_|_| |_|\\__, | _____ _ _ _ __/ | _ __/ | / ____| | | | | (_) |___/ /\\ | | |___/ | | (___ ___ | |_ _| |_ _ ___ _ __ / \\ ___ ___ ___| | ___ _ __ __ _| |_ ___ _ __ \\___ \\ / _ \\| | | | | __| |/ _ \\| '_ \\ / /\\ \\ / __/ __/ _ \\ |/ _ \\ '__/ _ | __/ _ \\| '__| ____) | (_) | | |_| | |_| | (_) | | | | / ____ \\ (_| (_| __/ | __/ | | (_| | || (_) | | |_____/ \\___/|_|\\__,_|\\__|_|\\___/|_| |_| /_/ \\_\\___\\___\\___|_|\\___|_| \\__,_|\\__\\___/|_| Please enter your Azure subscription ID to deploy your resources&gt; : ???Please enter the Azure Data Center Region to deploy your resourcesAvailable regions are:EastUS, EastUS2, WestUS, WestUS2, WestUS3, CentralUS, NorthCentralUS, SouthCentralUS, WestEurope, NorthEurope, SoutheastAsia, EastAsia, JapanEast, JapanWest, AustraliaEast, AustraliaSoutheast, CentralIndia, SouthIndia, CanadaCentral, CanadaEast, UKSouth, UKWest, FranceCentral, FranceSouth, KoreaCentral, KoreaSouth, GermanyWestCentral, GermanyNorth, NorwayWest, NorwayEast, SwitzerlandNorth, SwitzerlandWest, UAENorth, UAECentral, SouthAfricaNorth, SouthAfricaWest, BrazilSouth, BrazilSoutheast, QatarCentral, ChinaNorth, ChinaEast, ChinaNorth2, ChinaEast2&gt; : KoreaCentralPlease enter the Azure Data Center Region to deploy your GPT modelAvailable regions are:EastUS, EastUS2, SwedenCentral, WestUS3&gt; : EastUSPlease enter your email address for certificate management&gt; : hamin.lee@kt.com************************************************ Step 1 : Deploy Azure resources ************************************************Log in to Azure.....A web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.AADSTS53003: Access has been blocked by Conditional Access policies. The access policy does not allow token issuance. Trace ID: 4b59632c-1d82-4588-85b4-abc86f4f1000 Correlation ID: e4938e47-8a82-462e-80a7-e83facbe56a5 Timestamp: 2024-12-18 08:38:47ZInteractive authentication is needed. Please run:az loginThe subscription of '80083cf5-0434-4e94-b9a4-9f8ba244207e' doesn't exist in cloud 'AzureCloud'.Switched subscription to '80083cf5-0434-4e94-b9a4-9f8ba244207e' Deploying Azure resources in KoreaCentral region.....Started Deploying Knowledge Mining Solution Accelerator Service Azure resources.....Evaluating Deployment resource availabilities to preview changes...ERROR: Error while attempting to retrieve the latest Bicep version: HTTPSConnectionPool(host='aka.ms', port=443): Max retries exceeded with url: /BicepLatestRelease (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)'))).There might be something wrong with your deployment. ReferenceKnowledge Mining/Conversation knowledge mining solution accelerator","link":"/2024/12/17/Document-Knowledge-Mining-Solution-Accelerator/"},{"title":"Conversation-Knowledge-Mining-Solution-Accelerator","text":"대량의 대화 데이터를 보유한 고객이 생성형 AI를 사용하여 주요 구문을 surface(분석)하고 운영 지표와 함께 중요한 Insight를 얻을 수 있도록 하는 솔루션. 아키텍처 그림 실제 엔터티 추출 : 사람, 제품, 이벤트 장소 또는 행동과 같은 고유한 정보를 처리하고 추출 채팅 기반 Insight discovery : 모든 인덱싱 된 assets, 단일 assets, 선택한 assets 세트 또는 사용자 주도 키워드 검색을 기반으로 생성된 asset 목록과 채팅 가능 텍스트 및 문서 데이터 분석 : 문서, 손글씨 텍스트, 차트, 그래프, 표 및 양식 필드를 포함한 다중 모드 문서의 내용을 분석, 비교 및 요약하여 심층적인 통찰력을 제공 프롬프트 제안 가이드 : 프롬프트 문의를 기반으로 다음 질문 세트를 제안 다중 모드 정보 처리 : 여러 콘텐츠 유형과 다양한 형식의 지식을 처리하고 추출 대량의 데이터를 신속하게 분석하고, 관련 제안을 생성하여 빠르고 쉽게 추론할 수 있도록 도와준다. 1. Simple Deploy1-1. roleAssignment 정책상 부여 불가1234{ &quot;code&quot;: &quot;InvalidTemplateDeployment&quot;, &quot;message&quot;: &quot;The template deployment failed with error: 'Authorization failed for template resource '64f2b582-8f7c-560f-807d-a3f84df8679b' of type 'Microsoft.Authorization/roleAssignments'. The client 'hyejoo.jung@kt.com' with object id '349020e1-be53-4e57-bdb6-40240f0037be' does not have permission to perform action 'Microsoft.Authorization/roleAssignments/write' at scope '/subscriptions/80083cf5-0434-4e94-b9a4-9f8ba244207e/resourceGroups/BC-RA-joo/providers/Microsoft.Authorization/roleAssignments/64f2b582-8f7c-560f-807d-a3f84df8679b'.'.&quot;} Microsoft.Authorization/roleAssignments write 권한 이슈 아래와 같이 roleAssignment 관련 주석 처리123456789101112131415161718192021222324// {// &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;,// &quot;apiVersion&quot;: &quot;2022-04-01&quot;,// &quot;name&quot;: &quot;[guid(resourceGroup().id, resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635'))]&quot;,// &quot;properties&quot;: {// &quot;principalId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;,// &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635')]&quot;,// &quot;principalType&quot;: &quot;ServicePrincipal&quot;// },// &quot;dependsOn&quot;: [// &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot;// ]// }...// {// &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;,// &quot;apiVersion&quot;: &quot;2022-04-01&quot;,// &quot;name&quot;: &quot;[guid(resourceGroup().id, parameters('managedIdentityObjectId'), resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483'))]&quot;,// &quot;properties&quot;: {// &quot;principalId&quot;: &quot;[parameters('managedIdentityObjectId')]&quot;,// &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483')]&quot;,// &quot;principalType&quot;: &quot;ServicePrincipal&quot;// }// }, 1-2. OpenAI 모델 지원 이슈 sku 변경 1234567891011121314151617181920{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15}...&quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;GlobalStandard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; }} sku name 필드를 Standard → GlobalStandard로 변경 배포 후 역할 정상 작동하는지 확인 필요 12345678910111213141516171819202122232425262728293031323334353637383940resource accounts_ckm_openai_name_gpt_4 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = { parent: accounts_ckm_openai_name_resource name: 'gpt-4o-mini' sku: { name: 'GlobalStandard' capacity: 15 } properties: { model: { format: 'OpenAI' name: 'gpt-4o-mini' version: '2024-07-18' } versionUpgradeOption: 'OnceNewDefaultVersionAvailable' raiPolicyName: 'Microsoft.Default' } //dependsOn:[accounts_ckm_openai_name_resource]}{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ]} 최종 템플릿123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708{ &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;8858477527853866674&quot; } }, &quot;parameters&quot;: { &quot;solutionPrefix&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 6, &quot;metadata&quot;: { &quot;description&quot;: &quot;Prefix Name&quot; } }, &quot;location&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[resourceGroup().location]&quot; } }, &quot;variables&quot;: { &quot;resourceGroupLocation&quot;: &quot;[parameters('location')]&quot;, &quot;solutionLocation&quot;: &quot;[variables('resourceGroupLocation')]&quot; }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_managed_identity&quot;, &quot;resourceGroup&quot;: &quot;[resourceGroup().name]&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;14133192615685065374&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Location&quot; } }, &quot;miName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-managed-identity', parameters('solutionName'))]&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Name&quot; } } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.ManagedIdentity/userAssignedIdentities&quot;, &quot;apiVersion&quot;: &quot;2023-01-31&quot;, &quot;name&quot;: &quot;[parameters('miName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;tags&quot;: { &quot;app&quot;: &quot;[parameters('solutionName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot; } } // { // &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;, // &quot;apiVersion&quot;: &quot;2022-04-01&quot;, // &quot;name&quot;: &quot;[guid(resourceGroup().id, resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635'))]&quot;, // &quot;properties&quot;: { // &quot;principalId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;, // &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '8e3af657-a8ff-443c-a75c-2fe8c4bcb635')]&quot;, // &quot;principalType&quot;: &quot;ServicePrincipal&quot; // }, // &quot;dependsOn&quot;: [ // &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot; // ] // } ], &quot;outputs&quot;: { &quot;managedIdentityOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;id&quot;: &quot;[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName'))]&quot;, &quot;objectId&quot;: &quot;[reference(resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', parameters('miName')), '2023-01-31').principalId]&quot;, &quot;name&quot;: &quot;[parameters('miName')]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_azure_ai_service&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;8537007800307151650&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;accounts_byc_cogser_name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-cogser', parameters('solutionName'))]&quot; } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;S0&quot; }, &quot;kind&quot;: &quot;CognitiveServices&quot;, &quot;identity&quot;: { &quot;type&quot;: &quot;None&quot; }, &quot;properties&quot;: { &quot;apiProperties&quot;: {}, &quot;customSubDomainName&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;networkAcls&quot;: { &quot;defaultAction&quot;: &quot;Allow&quot;, &quot;virtualNetworkRules&quot;: [], &quot;ipRules&quot;: [] }, &quot;publicNetworkAccess&quot;: &quot;Enabled&quot; } } ], &quot;outputs&quot;: { &quot;cogSearchOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;cogServiceName&quot;: &quot;[parameters('accounts_byc_cogser_name')]&quot;, &quot;cogServiceKey&quot;: &quot;[listKeys(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01').key1]&quot;, &quot;cogServiceEndpoint&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01').endpoint]&quot;, &quot;cogServiceRegion&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_byc_cogser_name')), '2023-05-01', 'full').location]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_azure_open_ai&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;16531009550718652696&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;accounts_ckm_openai_name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-openai', parameters('solutionName'))]&quot; } }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;S0&quot; }, &quot;kind&quot;: &quot;OpenAI&quot;, &quot;properties&quot;: { &quot;customSubDomainName&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot;, &quot;networkAcls&quot;: { &quot;defaultAction&quot;: &quot;Allow&quot;, &quot;virtualNetworkRules&quot;: [], &quot;ipRules&quot;: [] }, &quot;publicNetworkAccess&quot;: &quot;Enabled&quot; } }, { &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ] } ], &quot;outputs&quot;: { &quot;openAIOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;openAPIKey&quot;: &quot;[listKeys(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name')), '2023-05-01').key1]&quot;, &quot;openAPIVersion&quot;: &quot;2023-05-01&quot;, &quot;openAPIEndpoint&quot;: &quot;[reference(resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name')), '2023-05-01').endpoint]&quot;, &quot;openAIAccountName&quot;: &quot;[parameters('accounts_ckm_openai_name')]&quot; } } } } } }, { &quot;type&quot;: &quot;Microsoft.Resources/deployments&quot;, &quot;apiVersion&quot;: &quot;2022-09-01&quot;, &quot;name&quot;: &quot;deploy_keyvault&quot;, &quot;resourceGroup&quot;: &quot;[resourceGroup().name]&quot;, &quot;properties&quot;: { &quot;expressionEvaluationOptions&quot;: { &quot;scope&quot;: &quot;inner&quot; }, &quot;mode&quot;: &quot;Incremental&quot;, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;value&quot;: &quot;[parameters('solutionPrefix')]&quot; }, &quot;solutionLocation&quot;: { &quot;value&quot;: &quot;[variables('solutionLocation')]&quot; }, &quot;objectId&quot;: { &quot;value&quot;: &quot;[reference(extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity'), '2022-09-01').outputs.managedIdentityOutput.value.objectId]&quot; }, &quot;tenantId&quot;: { &quot;value&quot;: &quot;[subscription().tenantId]&quot; }, &quot;managedIdentityObjectId&quot;: { &quot;value&quot;: &quot;[reference(extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity'), '2022-09-01').outputs.managedIdentityOutput.value.objectId]&quot; }, &quot;azureOpenAIApiKey&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai'), '2022-09-01').outputs.openAIOutput.value.openAPIKey]&quot; }, &quot;azureOpenAIApiVersion&quot;: { &quot;value&quot;: &quot;2023-07-01-preview&quot; }, &quot;azureOpenAIEndpoint&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai'), '2022-09-01').outputs.openAIOutput.value.openAPIEndpoint]&quot; }, &quot;cogServiceEndpoint&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceEndpoint]&quot; }, &quot;cogServiceName&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceName]&quot; }, &quot;cogServiceKey&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceKey]&quot; }, &quot;cogServiceRegion&quot;: { &quot;value&quot;: &quot;[reference(resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service'), '2022-09-01').outputs.cogSearchOutput.value.cogServiceRegion]&quot; }, &quot;enableSoftDelete&quot;: { &quot;value&quot;: false } }, &quot;template&quot;: { &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;metadata&quot;: { &quot;_generator&quot;: { &quot;name&quot;: &quot;bicep&quot;, &quot;version&quot;: &quot;0.28.1.47646&quot;, &quot;templateHash&quot;: &quot;12149961923112846409&quot; } }, &quot;parameters&quot;: { &quot;solutionName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;minLength&quot;: 3, &quot;maxLength&quot;: 15, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Name&quot; } }, &quot;solutionLocation&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Solution Location&quot; } }, &quot;utc&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[utcNow()]&quot; }, &quot;kvName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;[format('{0}-kv-{1}', parameters('solutionName'), uniqueString(parameters('utc')))]&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Name&quot; } }, &quot;objectId&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Object Id. The object ID of a user, service principal or security group in the Azure Active Directory tenant for the vault.&quot; } }, &quot;createMode&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;default&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Create Mode&quot; } }, &quot;enableForDeployment&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Deployment. Property to specify whether Azure Virtual Machines are permitted to retrieve certificates stored as secrets from the key vault.&quot; } }, &quot;enableForDiskEncryption&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Disk Encryption. Property to specify whether Azure Disk Encryption is permitted to retrieve secrets from the vault and unwrap keys.&quot; } }, &quot;enableForTemplateDeployment&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enabled For Template Deployment. Property to specify whether Azure Resource Manager is permitted to retrieve secrets from the key vault.&quot; } }, &quot;enablePurgeProtection&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable Purge Protection. Property specifying whether protection against purge is enabled for this vault.&quot; } }, &quot;enableRBACAuthorization&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: true, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable RBAC Authorization. Property that controls how data actions are authorized.&quot; } }, &quot;enableSoftDelete&quot;: { &quot;type&quot;: &quot;bool&quot;, &quot;defaultValue&quot;: false, &quot;metadata&quot;: { &quot;description&quot;: &quot;Enable Soft Delete. Property to specify whether the \\&quot;soft delete\\&quot; functionality is enabled for this key vault.&quot; } }, &quot;softDeleteRetentionInDays&quot;: { &quot;type&quot;: &quot;int&quot;, &quot;defaultValue&quot;: 30, &quot;metadata&quot;: { &quot;description&quot;: &quot;Soft Delete Retention in Days. softDelete data retention days. It accepts &gt;=7 and &lt;=90.&quot; } }, &quot;publicNetworkAccess&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;enabled&quot;, &quot;allowedValues&quot;: [ &quot;enabled&quot;, &quot;disabled&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;Public Network Access, Property to specify whether the vault will accept traffic from public internet.&quot; } }, &quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;GlobalStandard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; } }, &quot;tenantId&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: { &quot;description&quot;: &quot;Tenant Id&quot; } }, &quot;managedIdentityObjectId&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;azureOpenAIApiKey&quot;: { &quot;type&quot;: &quot;securestring&quot; }, &quot;azureOpenAIApiVersion&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;azureOpenAIEndpoint&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceEndpoint&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceKey&quot;: { &quot;type&quot;: &quot;securestring&quot; }, &quot;cogServiceName&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;cogServiceRegion&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;variables&quot;: { &quot;vaultUri&quot;: &quot;[format('https://{0}.vault.azure.net/', parameters('kvName'))]&quot; }, &quot;resources&quot;: [ { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults&quot;, &quot;apiVersion&quot;: &quot;2022-07-01&quot;, &quot;name&quot;: &quot;[parameters('kvName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot;, &quot;tags&quot;: { &quot;app&quot;: &quot;[parameters('solutionName')]&quot;, &quot;location&quot;: &quot;[parameters('solutionLocation')]&quot; }, &quot;properties&quot;: { &quot;accessPolicies&quot;: [ { &quot;objectId&quot;: &quot;[parameters('objectId')]&quot;, &quot;permissions&quot;: { &quot;certificates&quot;: [ &quot;all&quot; ], &quot;keys&quot;: [ &quot;all&quot; ], &quot;secrets&quot;: [ &quot;all&quot; ], &quot;storage&quot;: [ &quot;all&quot; ] }, &quot;tenantId&quot;: &quot;[parameters('tenantId')]&quot; } ], &quot;createMode&quot;: &quot;[parameters('createMode')]&quot;, &quot;enabledForDeployment&quot;: &quot;[parameters('enableForDeployment')]&quot;, &quot;enabledForDiskEncryption&quot;: &quot;[parameters('enableForDiskEncryption')]&quot;, &quot;enabledForTemplateDeployment&quot;: &quot;[parameters('enableForTemplateDeployment')]&quot;, &quot;enablePurgeProtection&quot;: &quot;[parameters('enablePurgeProtection')]&quot;, &quot;enableRbacAuthorization&quot;: &quot;[parameters('enableRBACAuthorization')]&quot;, &quot;enableSoftDelete&quot;: &quot;[parameters('enableSoftDelete')]&quot;, &quot;softDeleteRetentionInDays&quot;: &quot;[parameters('softDeleteRetentionInDays')]&quot;, &quot;provisioningState&quot;: &quot;RegisteringDns&quot;, &quot;publicNetworkAccess&quot;: &quot;[parameters('publicNetworkAccess')]&quot;, &quot;sku&quot;: { &quot;family&quot;: &quot;A&quot;, &quot;name&quot;: &quot;[parameters('sku')]&quot; }, &quot;tenantId&quot;: &quot;[parameters('tenantId')]&quot;, &quot;vaultUri&quot;: &quot;[variables('vaultUri')]&quot; } }, // { // &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;, // &quot;apiVersion&quot;: &quot;2022-04-01&quot;, // &quot;name&quot;: &quot;[guid(resourceGroup().id, parameters('managedIdentityObjectId'), resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483'))]&quot;, // &quot;properties&quot;: { // &quot;principalId&quot;: &quot;[parameters('managedIdentityObjectId')]&quot;, // &quot;roleDefinitionId&quot;: &quot;[resourceId('Microsoft.Authorization/roleDefinitions', '00482a5a-887f-4fb3-b363-3b7fe8e74483')]&quot;, // &quot;principalType&quot;: &quot;ServicePrincipal&quot; // } // }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'TENANT-ID')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('tenantId')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-KEY')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIApiKey')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-VERSION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIApiVersion')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-OPENAI-ENDPOINT')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('azureOpenAIEndpoint')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-ENDPOINT')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceEndpoint')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-KEY')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceKey')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-NAME')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceName')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'COG-SERVICES-REGION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('cogServiceRegion')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-SUBSCRIPTION-ID')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[subscription().subscriptionId]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-RESOURCE-GROUP')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[resourceGroup().name]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] }, { &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/secrets&quot;, &quot;apiVersion&quot;: &quot;2021-11-01-preview&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('kvName'), 'AZURE-LOCATION')]&quot;, &quot;properties&quot;: { &quot;value&quot;: &quot;[parameters('solutionLocation')]&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot; ] } ], &quot;outputs&quot;: { &quot;keyvaultOutput&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;value&quot;: { &quot;id&quot;: &quot;[resourceId('Microsoft.KeyVault/vaults', parameters('kvName'))]&quot;, &quot;name&quot;: &quot;[parameters('kvName')]&quot;, &quot;uri&quot;: &quot;[variables('vaultUri')]&quot;, &quot;resource&quot;: &quot;[reference(resourceId('Microsoft.KeyVault/vaults', parameters('kvName')), '2022-07-01', 'full')]&quot; } } } } }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.Resources/deployments', 'deploy_azure_ai_service')]&quot;, &quot;[resourceId('Microsoft.Resources/deployments', 'deploy_azure_open_ai')]&quot;, &quot;[extensionResourceId(format('/subscriptions/{0}/resourceGroups/{1}', subscription().subscriptionId, resourceGroup().name), 'Microsoft.Resources/deployments', 'deploy_managed_identity')]&quot; ] } ]} 리소스 그룹 : rg-ckm-1219-v0 Region : Korea Central Solution Prefix : ConKM 배포 실패 에러 내용123456789101112131415161718192021{ &quot;code&quot;: &quot;DeploymentFailed&quot;, &quot;target&quot;: &quot;/subscriptions/80083cf5-0434-4e94-b9a4-9f8ba244207e/resourceGroups/rg-ckm-1219-v0/providers/Microsoft.Resources/deployments/Microsoft.Template-20241219202047&quot;, &quot;message&quot;: &quot;At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-deployment-operations for usage details.&quot;, &quot;details&quot;: [ { &quot;code&quot;: &quot;InvalidTemplate&quot;, &quot;message&quot;: &quot;Deployment template validation failed: 'The provided value for the template parameter 'sku' is not valid. The value 'GlobalStandard' is not part of the allowed value(s): 'standard,premium'.'.&quot;, &quot;additionalInfo&quot;: [ { &quot;type&quot;: &quot;TemplateViolation&quot;, &quot;info&quot;: { &quot;lineNumber&quot;: 1, &quot;linePosition&quot;: 3520, &quot;path&quot;: &quot;properties.template.parameters.sku.allowedValues&quot; } } ] } ]} sku 파라미터의 값이 유요하지 않다고 한다 GlobalStandard가 아니라 standard, premium만 사용 가능하다고 한다. 123456789101112131415161718192021222324252627282930313233{ &quot;type&quot;: &quot;Microsoft.CognitiveServices/accounts/deployments&quot;, &quot;apiVersion&quot;: &quot;2023-05-01&quot;, &quot;name&quot;: &quot;[format('{0}/{1}', parameters('accounts_ckm_openai_name'), 'gpt-4o-mini')]&quot;, &quot;sku&quot;: { &quot;name&quot;: &quot;GlobalStandard&quot;, &quot;capacity&quot;: 15 }, &quot;properties&quot;: { &quot;model&quot;: { &quot;format&quot;: &quot;OpenAI&quot;, &quot;name&quot;: &quot;gpt-4o-mini&quot;, &quot;version&quot;: &quot;2024-07-18&quot; }, &quot;versionUpgradeOption&quot;: &quot;OnceNewDefaultVersionAvailable&quot;, &quot;raiPolicyName&quot;: &quot;Microsoft.Default&quot; }, &quot;dependsOn&quot;: [ &quot;[resourceId('Microsoft.CognitiveServices/accounts', parameters('accounts_ckm_openai_name'))]&quot; ]}...&quot;sku&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;: &quot;standard&quot;, &quot;allowedValues&quot;: [ &quot;standard&quot;, &quot;premium&quot; ], &quot;metadata&quot;: { &quot;description&quot;: &quot;SKU&quot; }}, 다시 위와 같이 수정 다시 배포 해보니 배포가 성공적으로 마무리 되었다. 리소스는 위와 같이 나타난다. 2. Fabric workspace 생성 Fabric Workspace로 이동한다. 왼쪽 내비게이션에서 + 워크스페이스 버튼을 클릭하여 워크스페이스를 생성한다. URL에서 Workspace ID를 확인한다. 워크스페이스 ID 확인 방법Fabric 사이트에서 워크스페이스의 항목 URL을 통해 가장 쉽게 워크스페이스 ID를 찾을 수 있다. Power BI와 마찬가지로, Fabric URL에는 워크스페이스 ID가 포함되어 있으며, 이는 URL에서 /groups/ 다음에 나오는 고유 식별자이다.예를 들어:https://powerbi.com/groups/11aa111-a11a-1111-1abc-aa1111aaaa/… 또는, Power BI 관리자 포털의 설정에서 워크스페이스 이름 옆에 있는 **세부정보(Details)**를 선택하여 워크스페이스 ID를 확인할 수도 있다. 3. Fabric 리소스 및 아티팩트 배포 Azure Portal로 이동 Azure Portal에 접속. Azure Cloud Shell 열기 내비게이션 메뉴의 오른쪽 상단에서 Azure Cloud Shell을 클릭 다음 명령 실행 아래 명령어를 Azure Cloud Shell에서 실행한다. 12345678910111213141516az login ***Azure Cloud Shell의 로그인 지침을 따라 로그인을 완료.rm -rf ./Customer-Service-Conversational-Insights-with-Azure-OpenAI-Servicesgit clone https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Servicescd ./Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services/Deployment/scripts/fabric_scriptssh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param- keyvault_param: 1단계에서 생성한 Key Vault의 이름 ConKM-kv-pm7k3nx4pijba - 키볼트 확인하기 az keyvault list --resource-group &lt;Resource-Group-Name&gt; --query &quot;[].{Name:name}&quot; -o table- workspaceid_param: 2단계에서 생성한 Workspace ID bfccf397-49f8-4822-a23e-19b0187256c7- solutionprefix_param: Lakehouse 생성 시 추가되는 접두사 conkmsh ./run_fabric_items_scripts.sh ConKM-kv-pm7k3nx4pijba bfccf397-49f8-4822-a23e-19b0187256c7 conkm Lakehouse 만들기 Fabric 구독 하지만… Fabric Lakehouse 연결 세부 정보 가져오기 배포가 완료되면 Fabric Workspace로 이동 워크스페이스에서 Lakehouse를 찾는다. (예: lakehouse_solutionprefix_param) Lakehouse 이름 옆의 …을 클릭. SQL Analytics Endpoint를 선택. 팝업 창에서 SQL 연결 문자열 복사(Copy SQL connection string) 버튼을 클릭. 데이터 파이프라인 처리 완료 대기 데이터 파이프라인 처리가 완료될 때까지 10~15분 정도 기다린 후 다음 단계로 진행.","link":"/2024/12/19/Conversation-Knowledge-Mining-Solution-Accelerator/"},{"title":"Azure-AZ104","text":"MS AZ104 자격증 취득을 위한 공부 과정 및 방법을 기록해보겠다. 시험 개요 최소 6개월 이상의 Azure 관리 실무 경험을 권장 Azure의 핵심 서비스, Azure 워크로드, 보안 및 거버넌스에 대한 탄탄한 이해가 필요 기본적인 지식이 부족하다면 AZ-900 과정을 참고 PowerShell, 명령줄 인터페이스(Command Line Interface), Azure Portal 및 ARM 템플릿 사용 경험이 있어야함. 시험 정보 응시료: 시험 비용은 165 미국 달러 점수: Microsoft 자격증 시험은 1000점 만점으로 채점되며, 700점 이상을 획득해야 AZ-104 인증을 받고 Azure Administrator 배지를 획득할 수 있다. 갱신 주기: AZ-104 시험은 매년 갱신해야 합니다. Microsoft는 주기적으로 인증을 폐지하거나 시험 내용을 대폭 변경하면 시험 번호가 변경될 수도 있음 문항 및 시간: 시험은 약 65문항으로 구성되며, 이를 해결할 3시간이 주어진다. 랩 없음: 작성 시점 기준으로, 시험에는 랩(실습 과제)이 포함되지 않는다. 시험 구성 Azure 아이덴티티 및 거버넌스 관리 (15~20%) 스토리지 구현 및 관리 (10~15%) Azure 컴퓨팅 리소스 배포 및 관리 (25~30%) 가상 네트워킹 구성 및 관리 (30~35%) Azure 리소스 모니터링 및 백업 (10~15%) 섹션별 구성총 3가지 섹션으로 구성, 해당 섹션이 마무리되면 다시 돌아갈 수 없음. 토픽 기반 문제 5개(?) 페이지당 하나의 주제에 대한 질문 Y/N 고르기 (3문제) 빈칸 체우기 (select box) 순서 맞추기 5지 선답 (두 세개씩 고르는 것도 있음) 세션2의 리뷰는 세션 3종료 후 가능 같은 이슈에 대한 솔루션을 제공하고 제시된 솔루션으로 이슈 해결 가능한지 각 문제단위로 답을 제출하고 나면 답 수정 불가 Y/N 고르는 방식 시험 후기 한국어 시험으로 신청하면, 영어로 전환하여 보는것 가능. 덤프를 외워갔으나 사진찍듯이 외워 갔기에, 시험 문제의 포맷이 다소 달랐기에 같은 내용의 문제더라도 파악하는데 다소 어려움이 있었음. 덤프에서 나오는 비중이 높으나, 제대로 외우지 못했거나, 덤프에 없는 문제가 나오면 기본지식과 지문을 잘 숙지하여 풀면 됨. Azure와 네트워크 관련 기본 지식이 있으면 푸는데 도움이 됨.","link":"/2024/12/19/Azure-AZ104/"},{"title":"Azure WebApp(App Service)으로 API 배포하기","text":"요즘 AWS/Azure/GCP 등 클라우드 환경의 일반화로 굳이 서버/네트워크 장비 등 자체구매/운영 (온프레미스) 하지 않아도 된다. 그렇기에 최소한의 노력을 들여 PaaS 서비스 또는 Container를 지원하는 서비스를 사용하여 쉽게 배포/운영 할 수 있다. Azure에서는 App Service라는 웹 어플리케이션 용 PaaS를 제공한다. Java, Node.JS, Python 등 대부분의 웹프레임워크를 지원한다. 이 서비스를 이용하면 굳이 클라우드 환경에서 WEB/WAS/DB 등 서버를 별도 생성/구축하지 않고 웹 서비스를 구축할 수 있다. 클라우드 환경에서 VM을 활용해서 구축한다면, VM을 생성하고, OS 설치/운영하고, OS에 필요 라이브러리를 설치/구동하고, 개발 모듈을 올려서 연동하는 등 다수의 구축작업을 해야한다. Azure App Service, Azure Functions을 활용하면 이러한 부분을 간소화해서 구축/운영할 수 있다. 0. Azure 웹앱의 동작 원리Azure 웹앱은 기본적으로 Docker Container로 구동한다. 내부적으로 Oryx라는 Container 빌드 패키지를 사용한다. Oryx 상세 파라미터 Wep App 배포하기다음과 같은 순서로 진행하도록 한다. Python Web Source Code 확보 Azure App Service 생성 Source Code 배포 (Azure Upload) 구동 확인 Azure에서 제공하는 Sample을 Github에서 다운로드 받아 사용하도록 한다. 아래는 Python Web Framework인 Flask의 Hello World이다.MS Flask Web App 12$ git clone https://github.com/Azure-Samples/msdocs-python-flask-webapp-quickstart$ cd msdocs-python-fastapi-quickstart (소스코드 동작 확인) 로컬에서 정상 동작하는지 일단 확인을 해 보자.1234$ python3 -m venv .venv$ source .venv/bin/activate$ pip install -r requirements.txt$ flask run 1. Azure App Service 생성/설정Azure App Servie를 생성해준다. App Service를 생성하는 방법은 Azure Portal Azure CLI VS Code Azure SDK 필자는 Azure Portal에서 하곘다. Azure Portal 로그인 후 좌측 상단 메뉴에서 “리소스 만들기”를 클릭한다. 그 중에서 웹 앱을 클릭한다. 웹앱 만들기에서 설정이 필요한 부분들은 아래와 같다. 구독/리소스그룹 : 본인의 구독과 리소스 그룹을 선택 or 생성해 주면 된다. 웹앱 이름 : 겹치지 않도록 이름을 설정하자. 예) “azure-webapp-python-test” 이 이름은 웹앱 생성후, “https://&lt;웹앱이름&gt;.azurewebsites.net”로 azure 기본 제공하는 subdomain name이 된다. 게시 방식 : 코드, docker 컨테이너, 정적웹앱. 우리는 가장 단순하게 “코드”를 선택하도록 하자. 런타임 스택 : Python 3.9를 선택. 다른버전 선택도 가능하다. 운영체제 : Linux. (Windows를 선택해도 무방하나, 본 포스팅은 Linux를 기반으로 진행한다.) 지역 : 본인이 편한 위치를 선택하면 된다. 가격정책플랜 : 우리는 테스트 해보는 것이기 때문에 기본으로 사용하자. “검토 + 만들기 “ 버튼을 누른다. 유효성이 검토되면 설정에 대한 요약페이지가 출력되고, 최종적으로 하단에 “만들기” 버튼을 또 한번 눌러 주어야 한다. 이후. “…배포 진행 중”이라는 페이지가 출력 될 것이다. 2~3분 기다리면 “배포 성공” 메세지가 출력된다. “리소스 이동” 버튼을 눌러준다. 웹앱이 생성 완료되면, “리소스로 이동”하여 웹앱의 왼쪽 메뉴중 “개요”에서 전반적인 웹앱의 정보를 확인한다.우측 상단의 FTP/FTPS 등의 정보는, 배포 시 사용하는데, 민감정보이므로 노출되지 않도록 주의한다. 배포좌측에 “배포 센터”로 이동한다. 실제 상용에서는 Stage -&gt; Production으로 나누어서 지속배포(CI/CD)체계를 구축하나, 이 포스팅에서는 단순화하여 “로컬 Git”을 사용하여 배포하겠다. “소스 &gt; 로컬 Git”을 선택해준다. “저장” 버튼을 클릭해야 적용이 된다. 저장이 완료되면 “Git Clone URI” 정보가 출력된다. 이 URI를 통해 “git push”를 실행하므로 URI를 복사해 두도록한다. 여기서 중요한점 중 하나는 사용자 이름이다. 사용자 이름은 기본적으로 다음과 같이 표기되어 있다 &lt;웹앱 이름&gt;$&lt;웹앱 이름&gt;실제 Git Push 할 때 인증은 “$&lt;웹앱 이름&gt;” 부분만 사용된다. 예를 들어 표기된 사용자 이름이 “flask-test$flask-test”라면, 실제 사용되는 부분은 $를 포함한 “$flask-test” 부분이 해당된다. 이부분을 잘못 입력하면 인증실패로 git push가 실행되지 않는다. 패스워드의 경우 전체를 복사하여 사용하면 된다. 이제 Azure Portal에서 설정할 내용은 완료하였다. 다음은 Terminal 에서 Git push를 진행하면된다. 본 포스팅에서는 별도 빌드 또는 추가 설정이 필요없기 때문에 “설정/구성”을 추가로 진행하지 않았다. 그러나, 상세설정 들이 필요하다면 아래처럼 “구성” 메뉴에서 “application 설정”, “시작 명령어” 등을 상세 지정할 수 있다. “구성&gt;애플리케이션 설정”은 Application 내부에서 사용할 별도의 설정을 지정할 수 있다. 위와 같이 하면 FTP 기본 인증 게시가 비활성화되어 있다고 에러가 난다. 아래와 같이 바꿔준다. 설정 저장이 완료되면, 아래와 같이 페이지에 “Git Clone URI” 정보가 출력된다. 이 URI로 우리는 ‘git push’를 실행하게 된다. git push를 할 때 인증정보는 “FTP 자격증명” 메뉴에서 확인할 수 있다. 해당 탭에서 Git Clone URI를 다시 확인할 수 있으며, 사용자 이름/암호를 확인할 수 있다. 향후에는 “게시 프로필 관리”를 통해 인증이 관리되어야 한다. Local Git 소스 배포소스코드 폴더로 이동하여 Git Push를 진행하도록 하자. 이 때 는 Azure Portal의 “Local Git 자격증명”에서 확인한 Git Clone URI이다. 사용자명/패스워드는 이전 절에서 거론했던 내용을 사용한다. ztna를 끄고 push를 진행해야 한다. 12$ git remote add azure &lt;GIT URI&gt;$ git push -u azure main:master 이제 소스코드 배포를 완료 했다. 앞서 거론했든이 Python은 별다른 빌드를 하지 않기 때문에, 별다른 설정파일 없이 웹앱을 생성할 수 있다. 이제 웹앱의 “개요”페이지에서 확인할 수 있는 웹앱 URL을 확인하고, 웹브라우저로 접속해 보기 바란다. Reference[Azure] Azure 웹앱(App Service)으로 API 서버 만들기 - 1","link":"/2025/01/07/Azure-WebApp-Sample-Deploy/"},{"title":"Azure-API-Management","text":"Azure API Management를 이용하여 QoS, 인증, 서비스 구분, 로깅, Traffic 추적을 고려하여 서비스는 구축하는 방법에 대한 글 QoS란서비스 품질을 관리하고 보장하기 위한 개념, 네트워크와 애플리케이션의 안정성과 성능을 유지하는 데 사용. QoS는 시스템이 다양한 요청을 처리할 때 자원을 효율적으로 배분하여 성능 저하를 방지하고, 중요한 트래픽이 우선적으로 처리되도록 보장한다. 주요 요소 대역폭 관리 (Bandwidth Management) 네트워크 트래픽을 제어하여 특정 서비스나 애플리케이션에 필요한 대역폭 보장 우선순위 설정 (Prioritization) 특정 요청이나 트래픽 유형을 우선 처리 지연 최소화 (Latency Reduction) 요청 및 응답 간의 지연 시간을 최소화 패킷 손실 관리 (Packet Loss Management) 데이터 패킷 손실을 방지하고 신뢰성 높은 데이터 전송을 보장 스루풋 관리 (Throughput Management) 네트워크가 초당 처리할 수 있는 데이터량을 최대화하여 시스템 성능을 최적화 Azure에서 QoS 관련 도구 Azure API Management : API 요청량 제한, 속도 제한, 스로틀링 등을 설정 가능 Azure Traffic Manager : 글로벌 트래픽 라우팅과 서비스 우선순위 설정 Azure Load Balancer : 로드 밸런싱을 통해 네트워크 트래픽 분산 API Management와 App Service를 사용하여 구축하면…고려사항 QoS 인증 서비스 구분 (요청하는 출발지 서버 또는 클라이언트 구분) 로깅 Traffic 추적 API Management의 QoS (Quality of Service) Rate Limiting &amp; Throttling rate-limit-by-key 요청의 속도를 제한하거나 특정 시간 동안의 요청수 제한 가능 서비스의 오버로드를 방지하고 공정한 사용 보장 가능 ‘rate-limit’, ‘quota’ 정책으로 설정 가능 사용자 지정 키 기반, IP 주소 기반, 사용자 ID 기반, 클라이언트 기반 제한 가능 Caching 자주 사용되는 응답 데이터를 캐싱하여 성능을 향상시키고 백엔드 서비스의 부담을 줄인다. 설정된 캐시 기간 동안 동일 요청은 API Management에서 응답 처리 Request/Response Timeout Circuit Breaker 특정 서비스가 불안정하거나 응답 시간이 길 경우, 요청을 차단하고 대체 응답을 제공할 수 있다. Circuit Breaker가 작동하면 API Management는 정의된 시간동안 백엔드 서비스로의 요청을 전송하는 것을 중단하고 클라이언트에게 503 서비스를 사용할 수 없음을 반환 Bicep 예시1234567891011121314151617181920212223242526272829 resource symbolicname 'Microsoft.ApiManagement/service/backends@2023-09-01-preview' = { name: 'myAPIM/myBackend' properties: { url: 'https://mybackend.com' protocol: 'http' circuitBreaker: { rules: [ { failureCondition: { count: 3 errorReasons: [ 'Server errors' ] interval: 'PT1H' statusCodeRanges: [ { min: 500 max: 599 } ] } name: 'myBreakerRule' tripDuration: 'PT1H' acceptRetryAfter: true } ] } }} count : 실패로 간주되는 요청의 횟수, 3연속 실패 시 Circuit Breaker 작동 errorReasons : 실패로 간주할 에러 이유 목록, ‘Server errors’는 서버 쪽 에러 (HTTP 5xx)가 원인임을 나타냄 interval : 실패 조건을 평가하는 시간 간격, 1시간 (PT1H) 동안 요청 실패를 기준으로 계산 statusCodeRanges : 실패로 간주할 HTTP 상태 코드 범위, 500~599 상태 코드는 서버 에러로 간주 Circuit Breaker 동작 요약 트리거 조건: 지난 1시간 동안 500-599 상태 코드로 인해 3번의 실패가 발생했을 경우. 작동 방식: Circuit Breaker가 활성화되면 모든 요청을 차단하고 클라이언트에게 즉시 응답을 반환 차단 기간: 1시간 동안(PT1H) 요청 차단. 백엔드 재시도: 백엔드가 Retry-After 헤더를 보내는 경우, 해당 시점까지 재시도를 연기 인증 및 권한 관리OAuth 2.0은 웹 API와 같은 리소스에 대한 액세스를 보호하는 데 널리 사용되는 표준 권한 부여 프레임워크이다. OAuth 2.0은 사용자의 자격 증명을 공유하지 않고도 클라이언트 앱이 사용자를 대신하여 리소스에서 수행할 수 있는 작업을 제한한다. OAuth 2.0은 인증 프로토콜은 아니지만 사용자 인증 및 SSO 기능을 제공하여 OAuth 2.0을 확장하는 OIDC(OpenID Connect)와 함께 사용되는 경우가 많다 OAuth 흐름 클라이언트는 ID 공급자에 대한 자격 증명을 사용하여 인증 클라이언트는 ID 공급자의 권한 부여 서버에서 시간이 제한된 액세스 토큰(JSON 웹 토큰 또는 JWT)를 가져온다 ID 공급자(예: MS Entra ID)는 토큰의 발급자이며 토큰에는 리소스 서버(예: 백엔드 API 또는 API Management 게이트웨이 자체)에 대한 액세스를 승인하는 대상 그룹 클레임이 포함된다 클라이언트는 API를 호출하고 액세스 토큰을 제공한다 (예: 권한 부여 헤더) 리소스 서버는 엑세스 토큰의 유효성을 검사한다. 유효성 검사는 발급자 및 대상 그룹 클레임에 예상 값이 포함되어 있는지 유효서을 검사하는 복잡한 프로세스이다. 토큰 유효성 검사 조건에 따라 백엔드 API의 리소스에 대한 액세스 권한이 부여된다. 시나리오 예시 : 클라이언트 앱이 API Management에 직접 권한을 부여 API Management 서비스는 API를 대신하여 작동 액세스 토큰의 범위는 호출 애플리케이션과 API Management 게이트웨이 사이 API Management에서 게이트웨이가 요청을 백엔드에 전달하기 전에 토큰의 유효성을 검사하도록 정책(validate-jwt 또는 validate-azure-ad-token)을 구성 MS Entra ID는 권한 부여 공급자 ReferencesAzure API Management의 API에 대한 인증 및 권한 부여 Microsoft ID 플랫폼 앱 형식 및 인증 흐름 OAuth 2.0 및 OpenID Connect : Azure AD, Facebook, Google 등과 연동하여 인증 및 토큰 기반 권한 관리를 지원 OAuth 2.0 API Key 인증 : 각 클라이언트가 고유한 API 키로 인증을 수행할 수 있다. IP 제한 : 특정 IP 주소 또는 범위에서만 접근 가능하도록 설정 가능 서비스 구분 Route 설정: API Gateway에서 경로(Route)를 정의하여 특정 요청이 해당 App Service로 전달되도록 설정 가능. Versioning: API Gateway에서 버전 관리를 통해 클라이언트가 원하는 API 버전을 호출할 수 있도록 지원. Multi-Backend 지원: 하나의 API Gateway를 통해 여러 App Service나 Azure Function을 서비스 구분 없이 연결할 수 있다. Azure API Management의 가시성(?) 도구 유용한 분야 데이터 지연 보존 샘플링 데이터 종류 지원되는 배포 모델 API 검사기 테스트 및 디버깅 인스턴트 마지막 100개 추적 요청에 따라 튜닝됨 요청 추적 관리형, 자체 호스팅, Azure Arc, 작업 영역 기본 제공 분석 보고 및 모니터링 분 수명 100% 보고서 및 로그 관리형 Azure Monitor 메트릭 보고 및 모니터링 분 90일(연장하려면 업그레이드) 100% 메트릭 관리, 자체 호스팅2, Azure Arc Azure Monitor 로그 보고, 모니터링, 디버그 분 31일/5GB(연장하려면 업그레이드) 100%(조정 가능) 로그 관리1, 자체 호스팅3, Azure Arc3 Azure Application Insights 보고, 모니터링, 디버그 초 90일/5GB(연장하려면 업그레이드) 사용자 지정 로그, 메트릭 관리형1, 자체 호스팅1, Azure Arc1, 작업 영역1 Azure Event Hubs를 통해 로깅 사용자 지정 시나리오 초 관리되는 사용자 사용자 지정 사용자 지정 관리1, 자체 호스팅1, Azure Arc1 OpenTelemetry 모니터링 분 관리되는 사용자 100% 메트릭 Self-hosted2 References : https://learn.microsoft.com/ko-kr/azure/api-management/observability API Management 모니터링서비스를 모니터링하기 위해 데이터를 수집하는 방법과 수집된 데이터로 수행할 수 있는 작업 수집할 데이터 설명 데이터를 수집하고 라우팅하는 방법 데이터를 볼 수 있는 위치 지원되는 데이터 메트릭 데이터 메트릭은 시간상 특정 지점에서 시스템의 측면을 설명하는 숫자 값입니다. 메트릭은 다른 메트릭과 비교하여 알고리즘을 사용하여 집계하고 시간에 따른 추세를 분석할 수 있습니다. - 정기적으로 자동으로 수집됩니다.- 일부 플랫폼 메트릭을 Log Analytics 작업 영역으로 라우팅하여 다른 데이터를 쿼리할 수 있습니다. 각 메트릭에 대한 DS 내보내기 설정을 확인하여 진단 설정을 사용하여 메트릭 데이터를 라우팅할 수 있는지 확인합니다. 메트릭 탐색기 Azure Monitor에서 지원하는 Azure API Management 메트릭 리소스 로그 데이터 로그는 타임스탬프를 사용하여 기록된 시스템 이벤트입니다. 로그는 다양한 형식의 데이터를 포함할 수 있으며 구조화되거나 자유 형식의 텍스트일 수 있습니다. 쿼리 및 분석을 위해 리소스 로그 데이터를 Log Analytics 작업 영역으로 라우팅할 수 있습니다. 리소스 로그 데이터를 수집하고 라우팅하는 진단 설정을 만듭니다. Log Analytics Azure Monitor에서 지원하는 Azure API Management 리소스 로그 데이터 활동 로그 데이터 Azure Monitor 활동 로그는 구독 수준 이벤트에 대한 인사이트를 제공합니다. 활동 로그에는 리소스가 수정되거나 가상 머신이 시작될 때와 같은 정보가 포함됩니다. - 자동으로 수집됩니다.- 무료로 Log Analytics 작업 영역에 대한 진단 설정을 만듭니다. 활동 로그 API Management에 대한 기본 제공 모니터링Azure API Management API 분석 사용 API에 대한 분석을 제공하므로, API의 사용량과 성능을 분석할 수 있다. 분석을 사용하여 API에 대한 개괄적인 모니터링과 문제 해결을 수행할 수 있다. API Management는 Azure Monitor 기반 대시보드를 사용하여 분석을 제공 대시보드는 Azure Log Analytics 작업 영역의 데이터를 집계한다. API Management REST API를 사용하여 분석 데이터에 엑세스할 수 있다. Azure Monitor 기반 대시보드와 기본 제공 분석에도 매우 유사한 데이터가 표시된다. Grafana 대시보드를 사용한 API Management 모니터링 데이터 시각화Azure Managed Grafana를 사용하여 Log Analytics 작업 영역에 수집된 API Management 모니터링 데이터를 시각화할 수 있다. References https://learn.microsoft.com/ko-kr/azure/api-management/monitor-api-management Azure Managed Grafana란? References API Management 모니터링 게시된 API 모니터링 로깅Application Insights API Gateway와 App Service 양쪽에서 Application Insights를 통해 요청/응답 로깅, 에러 추적, 성능 모니터링 등을 설정할 수 있다. Azure Portal, REST API 또는 관련 Azure 도구를 사용하여 Application Insights와 API Management 간의 연결을 만들 수 있다. (API Management는 연결을 위한 로거 리소스 구성) Azure Monitor요청 수, 대기 시간, 성공/실패 상태 코드 등 다양한 로그를 실시간으로 수집하고 대시보드로 시각화 가능. Custom Logging필요에 따라 로그를 Blob Storage나 Event Hub로 전달해 장기 보관 및 분석을 할 수 있다. Slack에 경고 Azure API Management 서비스에서 외부 서비스 사용 외부 서비스가 일종의 중요한 이벤트 알림을 받을 수 있도록 설정 가능 조건을 만족하는 경우 send-one-way-request 정책을 사용하여 외부 HTTP 요청을 만들 수 있다. Hipchat 및 Slack, SendGrid 및 MailChimp와 같은 메일 API, PagerDuty와 같은 중요 자원 인시던트 가능 예시) HTTP 응답 상태 코드가 500 이상인 경우 Slack 대화방에 메시지를 보내는 방법 12345678910111213141516171819202122&lt;choose&gt; &lt;when condition=&quot;@(context.Response.StatusCode &gt;= 500)&quot;&gt; &lt;send-one-way-request mode=&quot;new&quot;&gt; &lt;set-url&gt;https://hooks.slack.com/services/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&lt;/set-url&gt; &lt;set-method&gt;POST&lt;/set-method&gt; &lt;set-body&gt;@{ return new JObject( new JProperty(&quot;username&quot;,&quot;APIM Alert&quot;), new JProperty(&quot;icon_emoji&quot;, &quot;:ghost:&quot;), new JProperty(&quot;text&quot;, String.Format(&quot;{0} {1}\\nHost: {2}\\n{3} {4}\\n User: {5}&quot;, context.Request.Method, context.Request.Url.Path + context.Request.Url.QueryString, context.Request.Url.Host, context.Response.StatusCode, context.Response.StatusReason, context.User.Email )) ).ToString(); }&lt;/set-body&gt; &lt;/send-one-way-request&gt; &lt;/when&gt;&lt;/choose&gt; ReferencesAzure Application Insights와 Azure API Management를 통합하는 방법 Event Hubs에 이벤트 기록References Azure API Management에서 Azure Event Hubs에 이벤트를 기록하는 방법 트래픽 추적 Distributed Tracing: Application Insights와 통합하여 API Gateway부터 App Service까지 트랜잭션을 추적 가능하다. Integration with Azure Front Door: 전 세계적으로 사용자 트래픽을 라우팅하고, 실시간 요청의 경로를 추적한다. Latency Monitoring: 각 구간의 응답 시간 및 병목현상을 모니터링할 수 있다.","link":"/2025/01/14/Azure-API-Management/"},{"title":"Azure-API-Management-Deploy","text":"이 글에서는 Azure API Management를 구축해보겠다. API Management에서 APIs에서 미리 만들어둔 App Service를 browse하여 Api를 만든다 생성된 api의 test 탭에 들어가면 그냥 url 만 복사하여 요청하면 1curl --location 'https://apimgmt-az01-og084501-dev-ktintelliagent-test-01.azure-api.net/test20250131' 아래와 같이 401 에러가 난다 1234{ &quot;statusCode&quot;: 401, &quot;message&quot;: &quot;Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.&quot;} Ocp-Apim0Subscription-Key가 없어서 그렇다 Ocp-Apim-Subscription-KeyAPI Management의 규격이다 Header에 넣어주면 된다 Ocp-Apim-Trace:c도 넣어야 할까? APIs에 Subscription에 Built-in all-access subscription 항목에 있는 키를 가져오면 된다 Pruduct(제품) 전용 키라고 생각하면 된다. API Management에 GET123curl --location 'https://apimgmt-az01-og084501-dev-ktintelliagent-test-01.azure-api.net/test20250131' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' API Management에 POST123456789101112131415161718192021curl --location 'https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/hello' \\--header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \\--header 'Accept-Language: ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7' \\--header 'Cache-Control: no-cache' \\--header 'Connection: keep-alive' \\--header 'Content-Type: application/x-www-form-urlencoded' \\--header 'Origin: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net' \\--header 'Pragma: no-cache' \\--header 'Referer: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/' \\--header 'Sec-Fetch-Dest: document' \\--header 'Sec-Fetch-Mode: navigate' \\--header 'Sec-Fetch-Site: same-origin' \\--header 'Sec-Fetch-User: ?1' \\--header 'Upgrade-Insecure-Requests: 1' \\--header 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36' \\--header 'sec-ch-ua: &quot;Google Chrome&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;' \\--header 'sec-ch-ua-mobile: ?0' \\--header 'sec-ch-ua-platform: &quot;macOS&quot;' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' \\--data-urlencode 'name=kahlua' 위 헤더들에서 줄여서 아래처럼 보내도 무방하다 1234567curl --location 'https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/hello' \\--header 'Origin: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net' \\--header 'Referer: https://appsvc-az01-dev-ktintelliagent-prototype-01-daeya8a3dwdcegfs.koreacentral-01.azurewebsites.net/' \\--header 'Ocp-Apim-Subscription-Key: f917567d3d2c4c46a5ddf04c33550ef8' \\--header 'Ocp-Apim-Trace: true' \\--header 'Content-Type: application/x-www-form-urlencoded' \\--data-urlencode 'name=kahlua' Azure API Management(APIM)에 OAuth2.0 인증 설정1. API Management에 OAuth 2.0 인증 설정Azure API Management에서 OAuth 2.0을 사용하여 액세스 토큰을 발급하고 API 요청을 인증하려면, 먼저 OAuth 2.0 제공자를 설정해야 합니다. (1) Azure Active Directory (Azure AD)에서 App 등록 Azure Portal에서 Azure Active Directory &gt; 앱 등록 &gt; 새 등록을 클릭 리디렉션 URI를 https://oauth.pstmn.io/v1/browser-callback 또는 API Management 엔드포인트로 설정 애플리케이션(클라이언트) ID 및 디렉터리(테넌트) ID 저장 (2) 클라이언트 자격 증명 생성 인증서 및 암호 &gt; 새 클라이언트 암호 생성 생성된 클라이언트 암호 값 저장 (3) API 권한 추가 API 권한 &gt; 권한 추가 Microsoft Graph 또는 사용할 API 선택 &gt; Application permissions 또는 Delegated permissions 선택 access_as_user 또는 필요한 권한 부여 2. API Management에서 OAuth 2.0 제공자 등록 Azure API Management 포털로 이동 OAuth 2.0 인증 제공자 &gt; 새 OAuth 2.0 제공자 추가 인증 엔드포인트 및 토큰 엔드포인트 입력 인증 엔드포인트: 1https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/authorize 토큰 엔드포인트: 1https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token 클라이언트 ID 및 클라이언트 암호 입력 권한 부여 방식으로 Authorization Code 또는 Client Credentials 선택 3. API Management 정책 설정 (액세스 토큰 인증)API 요청이 들어올 때 액세스 토큰을 검증하는 정책을 설정해야 합니다. (1) API Management 정책 추가 Azure API Management &gt; APIs &gt; 대상 API 선택 정책 편집기(Inbound Processing)에서 다음 정책 추가 12345678910&lt;inbound&gt; &lt;base /&gt; &lt;validate-jwt header-name=&quot;Authorization&quot; failed-validation-httpcode=&quot;401&quot; failed-validation-error-message=&quot;Invalid token.&quot;&gt; &lt;openid-config url=&quot;https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration&quot; /&gt; &lt;required-claims&gt; &lt;claim name=&quot;aud&quot;&gt;YOUR-CLIENT-ID&lt;/claim&gt; &lt;/required-claims&gt; &lt;/validate-jwt&gt;&lt;/inbound&gt; {tenant_id}: Azure AD 테넌트 ID YOUR-CLIENT-ID: 앱 등록한 클라이언트 ID 4. API 호출 테스트 액세스 토큰 발급 (OAuth 2.0 client_credentials 사용) 123curl -X POST &quot;https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token&quot; \\-H &quot;Content-Type: application/x-www-form-urlencoded&quot; \\-d &quot;grant_type=client_credentials&amp;client_id={client_id}&amp;client_secret={client_secret}&amp;scope=api://{client_id}/.default&quot; 응답 받은 액세스 토큰을 사용하여 API 호출 12curl -X GET &quot;https://{api-management-endpoint}/api/resource&quot; \\-H &quot;Authorization: Bearer {access_token}&quot;","link":"/2025/01/21/Azure-API-Management-Deploy/"},{"title":"Azure-EventHub","text":"EventHub는 짧은 대기 시간으로 초당 수백만개의 이벤트를 스트리밍할 수 있는 클라우드의 네이티브 데이터 스트리밍 서비스이다. Azure Stream Analytics를 사용하여 실시간 인사이트를 생성하여 이벤트 허브에서 데이터를 처리 Azure Data Explorer를 사용하여 스트리밍 데이터를 분석하고 탐색. Azure Event Hubs의 Apache Kafka AMQP(고급 메시지 큐 프롵토콜), Apache Kafka 및 HTTPS 프로토콜을 기본적으로 지원하는 다중 프로토콜 이벤트 스트리밍 엔진 Apache Kafka 용 Azure Event Hubs Event Hubs의 스키마 레지스트리 Event Hubs의 Azure 스키마 레지스트리는 이벤트 스트리밍 애플리케이션의 스키마를 관리하기 위한 중앙 집중식 리포지토리를 제공 스키마 레지스트리는 이벤트 생산자 및 소비자 간에 데이터 호환성 및 일관성을 보장. 스키마 레지스트리는 기존 Kafka 애플리케이션과 통합되며 Avro 및 JSON 를 비롯한 여러 스키마 형식을 지원한다 ReferenceEvent Hub의 Azure 스키마 레지스트리 Stream Analytics를 사용하여 스트리밍 이벤트 실시간 처리 Azure Steam Analytics와 통합하여 실시간 스트림 처리를 지원 SQL 기반 Stream Analytics 쿼리 언어를 사용하여 실시간 스트림 처리를 수행하고 스트리밍 데이터를 분석하기 위한 다양한 함수를 활용할 수 있다. Azure Stream Analytics 통합 섹션 Azure Data Explorer를 사용하여 스트리밍 데이터 탐색 거의 실시간으로 대량의 데이터를 분석할 수 있는 빅 데이터 분석을 위한 완전 관리형 플랫폼 Azure Data Explorer와 통합하면 스트리밍 데이터의 거의 실시간 분석 및 탐색 수행 가능 Event Hub에 데이터 연결 작동 방식Event Hubs는 이벤트 소비자와 이벤트 생산자를 분리하는 시간 보존 버퍼가 있는 통합 이벤트 스트리밍 플랫폼을 제공 Event Hubs의 주요 기능 구성 요소 생산자 애플리케이션: Event Hubs SDK 또는 Kafka 생산자 클라이언트를 사용하여 이벤트 허브에 데이터를 수집. 네임스페이스: 하나 이상의 이벤트 허브 또는 Kafka 토픽에 대한 관리 컨테이너.스트리밍 용량 할당, 네트워크 보안 구성, 지역 재해 복구 사용과 같은 관리 작업이 네임스페이스 수준에서 처리. Event Hubs/Kafka 토픽: Event Hubs에서 이벤트를 이벤트 허브 또는 Kafka 토픽으로 구성할 수 있다. 이는 하나 이상의 파티션을 구성할 수 있는 추가 전용 분산 로그이다. 파티션: 이벤트 허브의 크기를 조정하는 데 사용. 파티션은 고속도로의 차선 같다. 스트리밍 처리량이 더 필요한 경우 파티션을 더 추가할 수 있다. 소비자 애플리케이션: 이 애플리케이션은 이벤트 로그를 검색하고 소비자 오프셋을 유지 관리하여 데이터를 사용할 수 있다. 소비자는 Kafka 소비자 클라이언트 또는 Event Hubs SDK 클라이언트일 수 있다. 소비자 그룹: 이 논리적 소비자 인스턴스 그룹은 이벤트 허브 또는 Kafka 토픽에서 데이터를 읽는다. 이를 통해 여러 소비자가 각자의 속도와 자체 오프셋을 사용하여 이벤트 허브에서 동일한 스트리밍 데이터를 독립적으로 읽을 수 있다. 장점 확장성: 대량의 메시지를 처리할 수 있어 고부하 시스템에 적합. 내구성: 메시지를 안정적으로 저장하고, 소비자가 언제든 가져갈 수 있음. 분산 처리: 메시지 파티션을 활용해 다중 소비자가 병렬로 처리 가능. 통합 가능성: Azure 생태계(예: Azure Functions, Logic Apps)와 손쉽게 통합. 단점 실시간성: WebSocket이나 gRPC에 비해 레이턴시가 클 수 있음. 양방향 통신 부족: 주로 Publisher-Subscriber 모델로 동작하며, 클라이언트가 서버로 메시지를 보낼 때 직접적인 스트리밍 채널을 제공하지 않음. 초기 설정 복잡성: Event Hub 설정 및 인증 관리가 비교적 복잡. 적합한 사용 사례 높은 부하(대량의 사용자 또는 메시지)를 처리해야 하며, 메시지 손실을 최소화해야 하는 시스템. 비동기적으로 LLM 응답을 처리하는 애플리케이션. 데이터 분석 또는 아카이빙이 필요한 서비스 Azure Event Hubs: 네이티브 Apache Kafka 지원을 사용하는 실시간 데이터 스트리밍 플랫폼 Azure Eventhub에 있는 정보를 클라이언트가 바로 가져가는 것도 가능한가?Azure Event Hubs에서 정보를 클라이언트가 바로 가져가는 방식은 기본적으로 Event Hubs의 동작 방식과 설계에 따라 제한적. Event Hubs는 데이터 수집 및 분산 처리를 위해 설계된 이벤트 스트림 플랫폼으로, 직접 접근보다는 소비자 그룹(Consumer Group)을 통해 데이터를 처리하는 것이 일반적이다. 직접 클라이언트 접근 가능 여부기본적으로 불가능 Event Hubs는 메시지 브로커 역할을 하며, 데이터를 생산자(Producer)와 소비자(Consumer) 간에 비동기적으로 전달한다. 클라이언트가 Event Hubs에 직접 접근해서 데이터를 가져가도록 설계되어 있지 않다. Event Hubs는 데이터를 Offset 및 Partition 단위로 관리하여 소비자가 순서에 따라 데이터를 처리하도록 설계됨. 클라이언트가 바로 가져가려면 Event Hubs의 읽기 API를 구현해야 하고, 이 과정에서 메시지 순서나 Offset 관리가 필요. 가능한 대안 및 설계 방안1. Azure Functions or Logic Apps를 사용한 중계 Event Hubs의 데이터를 트리거로 읽고, 이를 클라이언트에게 전달하는 REST API 또는 WebSocket 인터페이스를 만든다. 클라이언트는 Event Hubs 대신 중계 서버를 통해 데이터를 요청. 2. Azure Stream Analytics와 실시간 데이터 제공 Event Hubs 데이터를 실시간으로 처리하여 결과를 클라이언트가 사용할 수 있도록 데이터베이스나 다른 서비스에 저장. 예: 처리된 데이터를 Azure Cosmos DB, SQL Database 또는 Blob Storage에 저장. 3. Azure SignalR Service와 WebSocket 통합 Event Hubs 데이터를 소비자가 실시간으로 처리한 뒤, SignalR을 사용해 데이터를 클라이언트로 푸시. SignalR은 클라이언트와 지속적인 WebSocket 연결을 유지하여 실시간 데이터 전송을 가능하게 한다. 4. Event Processor Host (SDK 기반 소비) 클라이언트가 직접 Event Hubs의 데이터를 가져가려면 Event Hubs SDK를 활용하여 Consumer로 동작. 이 경우, 클라이언트는 Event Hubs에 Consumer Group으로 등록되고, 데이터를 처리해야 한다. 하지만 이는 클라이언트가 직접 Offset 관리와 같은 복잡성을 처리해야 하므로 일반적으로 권장되지 않음. 추천 아키텍처 Event Hubs → Azure Functions → REST API Event Hubs 데이터를 Azure Functions로 처리하고, REST API로 제공. 클라이언트는 REST API를 통해 데이터를 요청. Event Hubs → Stream Analytics → Cosmos DB → 클라이언트 Stream Analytics로 데이터를 처리한 뒤, 클라이언트가 저장된 데이터를 DB에서 조회. Event Hubs → SignalR Service → WebSocket 클라이언트 실시간 데이터 제공이 필요한 경우 SignalR을 사용해 WebSocket으로 전송. 결론Event Hubs는 클라이언트가 직접 접근해서 데이터를 가져가도록 설계되지 않았다. 클라이언트가 데이터를 직접 요청하거나 실시간으로 수신해야 하는 경우, 중간 계층을 설계하는 것이 최적의 방법이다.중계 서버나 다른 Azure 서비스를 활용하여 데이터를 안정적으로 제공하도록 아키텍처를 구성하는 것을 권장한다. 설계 개요 채팅 응답 발행 (Producer) → Agent가 Event Hub로 응답 전송 채팅 응답 수신 (Consumer 서버) → Event Hub에서 메시지 수신 후 클라이언트로 HTTP 요청 전송 클라이언트로 응답 전달 → HTTP API 주요 기술 스택 역할 기술 설명 Producer FastAPI Agent 응답을 Event Hub에 발행 Event Hub Azure Event Hub 메시지 브로커 역할 Consumer Legacy 서버 Event Hub에서 메시지 읽기 클라이언트 응답 HTTP API 클라이언트에 직접 전송 상세 흐름 (예시)채팅 응답 발행 (Producer)12345678910from azure.eventhub import EventHubProducerClient, EventData# EVENT_HUB_CONNECTION_STRING : Azure Event Hub 인스턴스에 연결할 때 필요한 인증 정보producer = EventHubProducerClient.from_connection_string( &quot;EVENT_HUB_CONNECTION_STRING&quot;, eventhub_name=&quot;chat-responses&quot;)event_data = EventData('{&quot;user_id&quot;: &quot;123&quot;, &quot;message&quot;: &quot;Hello!&quot;}')producer.send_batch([event_data])producer.close() 채팅 응답 수신 (Consumer, Legacy 서버)서버에서 Event Hub 메시지를 읽고 즉시 클라이언트로 HTTP 요청 전송 123456789101112131415161718192021222324252627282930import asyncioimport aiohttpfrom azure.eventhub.aio import EventHubConsumerClientimport jsonasync def send_to_client(user_id, message): async with aiohttp.ClientSession() as session: async with session.post(f&quot;http://client-server.com/chat/{user_id}&quot;, json={&quot;message&quot;: message}) as resp: print(f&quot;Sent to client {user_id}, status: {resp.status}&quot;)async def process_event(event): data = json.loads(event.body_as_str()) user_id = data[&quot;user_id&quot;] message = data[&quot;message&quot;] # HTTP API로 클라이언트에 전송 await send_to_client(user_id, message)async def on_event(partition_context, event): await process_event(event) await partition_context.update_checkpoint(event)async def main(): client = EventHubConsumerClient.from_connection_string( &quot;EVENT_HUB_CONNECTION_STRING&quot;, consumer_group=&quot;$Default&quot;, eventhub_name=&quot;chat-responses&quot; ) async with client: await client.receive(on_event=on_event, starting_position=&quot;-1&quot;)asyncio.run(main()) user_id 또는 transaction_ID 등을 이용하여 클라이언트 구분 로직 필요 특징장점 구현이 단순: Redis와 WebSocket이 필요 없음 실시간 전송 가능: 서버에서 바로 클라이언트로 HTTP 요청 전송 확장성 높음: 여러 Consumer 서버가 메시지를 처리 가능 단점 클라이언트가 항상 온라인이어야 함 → 클라이언트가 오프라인이면 메시지 손실 가능 결론 클라이언트가 항상 온라인 상태라면 WebSocket 없이도 구현 가능 클라이언트가 메시지를 놓치지 않으려면 Redis나 DB로 클라이언트 별 메시지를 저장하고 있어야 함 (Legacy 서버) 출처1. Azure Event Hubs 📄 공식 문서: Azure Event Hubs 개요 Event Hubs Python SDK 사용법 📦 GitHub (Azure SDK for Python) Azure SDK for Python (eventhub) 2. Python을 사용한 Event Hub 소비자 (Consumer) 구현 Azure Event Hub에서 메시지 소비 (Python) Azure Event Hub Consumer Client 3. 기타 Azure 관련 문서 (배포 및 운영) Azure Monitor 및 Application Insights Azure Kubernetes Service(AKS)로 Event Hub 배포 Azure Event Hub Pub/Sub 하기 위한 인증 방법 이벤트허브 만들기","link":"/2025/01/28/Azure-EventHub/"},{"title":"왜 Hexo를 골랐는가?","text":"Why Hexo?Hexo framework 와 Git을 이용한 간단한 블로그 설정에 대한 포스팅입니다. Why select Hexo?다른 프레임워크간의 비교정적 페이지 블로그 프레임워크 중에서 대표적으로 Hexo, Hugo, Jekyll이 있다. Hexo Node.js 기반 한글 레퍼런스와 사용자 수가 많음 다양한 테마 지원 플러그인과 확장성이 좋음 주로 중국 개발자들이 많이 사용 Hugo Go 기반 빌드 속도가 빠름 문서화가 깔끔함 한글 레퍼런스가 거의 없음 어떤 점이 좋은지 애매함 Jekyll Ruby 기반 사용자 수가 가장 많음 한글 레퍼런스가 풍부함 속도가 느림 (10개 포스팅 시 5분 이상 소요됨) Windows 지원 이슈가 많음 결론적으로 Jekyll은 루비 기반이라 속도 문제와 윈도우 지원 이슈 때문에 포기했고, Hugo는 Go 언어를 다뤄야 하며 레퍼런스가 부족해서 최종적으로 Hexo를 선택했다.","link":"/2019/09/17/Why-Hexo/"},{"title":"Deep Learning for Self-Driving Cars","text":"Deep Learning for Self-Driving CarsMIT의 딥러닝 자율주행차 과정(Deep Learning for Self-Driving Cars) 중 DeepTraffic 프로젝트를 참고하여 작성한 글입니다. 고속도로에서 교통량이 많은 구간을 빠르게 주행할 수 있도록 신경망을 설계하는 것이 목표입니다. 프로젝트 개요자율주행차를 위한 딥러닝 모델을 학습시키는 과정은 다음과 같이 진행됩니다. 데이터 수집: 도로 환경을 시뮬레이션하여 다양한 주행 데이터를 확보 신경망 설계: 최적의 주행 경로를 찾을 수 있도록 CNN, RNN 등의 모델 구성 훈련 및 검증: 주행 성능을 평가하고 최적화 수행 시뮬레이션 테스트: 실제 도로 주행과 유사한 환경에서 모델 평가 사용된 기술 TensorFlow/Keras: 신경망 학습 및 모델 구현 Reinforcement Learning: 차량 주행 최적화를 위한 강화학습 기법 적용 Computer Vision: 도로 환경 인식을 위한 이미지 처리 기술 활용 12345678910import tensorflow as tffrom tensorflow import kerasmodel = keras.Sequential([ keras.layers.Dense(128, activation='relu', input_shape=(state_size,)), keras.layers.Dense(64, activation='relu'), keras.layers.Dense(action_size, activation='linear')])model.compile(optimizer='adam', loss='mse') 결과 및 평가모델을 학습시킨 후, 시뮬레이션을 실행하여 주행 속도 및 충돌 여부를 평가합니다. 평균 속도: 60km/h → 90km/h 향상 충돌 횟수 감소: 30% 감소 학습 속도: 50,000 에피소드 학습 시 안정적인 주행 가능 결론DeepTraffic 프로젝트를 통해 딥러닝을 활용한 자율주행 최적화 가능성을 확인할 수 있었습니다. 향후 연구 방향으로는 실제 차량 테스트 및 강화학습 개선이 필요할 것으로 보입니다.","link":"/2020/04/08/Deep-Learning-for-Self-Driving-Cars/"},{"title":"마이클 샌델의 The Case against Perfection을 읽고","text":"마이클 샌델의 The Case against Perfection을 읽고마이클 샌델의 『The Case against Perfection』을 읽고 정리한 내용입니다. 목차 강화의 윤리학 생체공학적 운동선수 맞춤 아기를 설계하는 부모 우생학의 어제와 오늘 정복과 선물 에필로그: 배아 윤리학과 줄기세포 논쟁 해제: 생명공학 시대와 마음의 습관 샌델이 말하고자 하는 바 윤리적 불편함의 근원 특정한 입장을 취할 때 느껴지는 윤리적 불안감은 어디에서 오는가? 유전공학을 적용했을 때, 그것이 가져올 사회적 변화와 장기적 결과를 고려해야 함. 잘못된 선택이 초래하는 결과 윤리적으로 옳지 않은 선택은 궁극적으로 나쁜 결과를 초래함. 기술의 단기적 성과에만 집착해서는 안 됨. 인간의 본래적 가치에 대한 성찰 책임, 연대, 사회적 협력이 인간 사회에서 중요한 이유. 기술이 이러한 본래적 가치를 유지하는 방향으로 발전해야 함. 책을 읽은 동기 마이클 샌델이 유전공학과 윤리에 대해 어떤 입장을 취하는지 알고 싶어서. 과학기술이 인간 사회에 미치는 영향을 철학적으로 고찰하기 위해. 대학 과제로 선정되어 읽게 됨. 3줄 요약 유전공학을 통한 신체 강화는 도덕적 불편함을 초래하며, 이는 단순한 수단의 문제가 아니라 목적 자체의 문제일 수 있음. 자유주의적 우생학은 국가가 강제하지 않더라도, 유전적 선택이 필연적으로 사회적 불평등을 심화할 가능성이 있음. 인간의 재능과 능력은 선물이며, 이를 조작하려는 시도는 인간의 본질에 대한 지나친 지배욕일 수 있음. 유전공학적 강화근육 강화: 안전성과 공정성 논란 유전자 치료가 안전하다면 금지할 이유가 있는가? 기존의 선천적 재능과 유전적 강화의 차이는 무엇인가? 스포츠에서의 공정성을 어디까지 인정해야 하는가? 기억력 및 신장 강화 후세대까지 영향을 미친다면 윤리적으로 문제가 되는가? 신체적 강화를 어디까지 허용해야 하는가? 경제적 불평등이 유전적 격차로 이어질 가능성은? 스포츠와 강화 운동 장비의 혁신도 일종의 강화인가? 과거에는 스테로이드 사용이 문제였지만, 지금은 영양과 트레이닝이 더 중요한 요소로 변화. 스포츠에서 ‘자연적 재능’의 의미는 무엇인가? 우생학과 유전공학과거 우생학 19세기 우생학의 창시자 프랜시스 골턴 → 유전적 열등함을 제거해야 한다는 주장. 아돌프 히틀러는 우생학을 바탕으로 인종 차별 정책을 시행. 우생학이 사회적 약자에게 가혹하게 적용됨. 자유시장 우생학 개인의 선택에 의해 우생학적 선택이 이루어진다면 윤리적으로 문제가 없는가? 유전적 강화가 새로운 계급 사회를 형성할 가능성은? 유전공학이 초래할 세 가지 재앙 개인의 책임 증폭: 유전공학이 발전할수록, 능력 부족은 개인의 책임으로 돌려짐. 사회적 연대감 붕괴: 성공이 유전자 선택의 결과라면, 타인의 어려움에 대한 공감이 약화될 가능성. 사회 불평등 심화: 유전적 강화가 부유층에게만 가능하다면, 불평등이 영구화될 위험. 마이클 샌델의 문제제기 유전공학의 무분별한 발전이 인간 본연의 가치를 해칠 수 있음. 기술 발전이 사회적 연대와 윤리적 가치에 미치는 영향을 고려해야 함. 결론『The Case against Perfection』은 유전공학과 생명윤리에 대한 깊은 고민을 던지는 책이다.샌델은 유전적 강화를 단순한 기술적 발전이 아니라, 인간 존재의 의미를 재고하는 철학적 문제로 바라본다.우리는 과연 완벽을 추구해야 하는가, 아니면 선물로 주어진 인간 본연의 모습을 받아들여야 하는가? 🤔","link":"/2020/04/30/Perfection/"},{"title":"인공지능 스피커를 이용한 음성인식 키오스크 플랫폼","text":"인공지능 스피커를 이용한 음성인식 키오스크 플랫폼SKT의 음성인식 스피커와 AWS EC2, S3, Node.js, React.js 를 이용하여 음성인식 키오스크 플랫폼을 개발하였습니다.기술에 소외된 분들이 키오스크를 이용하는 데 어려움을 겪는 것에 문제의식을 느껴 개발하게 되었습니다.또한 기존의 키오스크 플랫폼이 구축 비용이 큰 것에 비해 음성인식 키오스크는 구축 비용이 적습니다. 📌 프로젝트 개요AbstractVISQUIT는 인공지능 기반의 음성인식 기술로 구동되는 주문 서비스입니다.많은 상점들이 키오스크를 사용하여 주문과 결제 과정을 자동화하고 있지만, 키오스크 설치 비용이 여전히 비싸다는 문제가 있습니다.이를 해결하기 위해, SKT NUGU 스피커를 활용한 음성 인식 주문 시스템을 개발하였습니다. 📌 문제 정의 디지털 기술이 익숙하지 않은 노년층이 키오스크 사용에 어려움을 겪음 기존 키오스크의 UI가 복잡하여 접근성이 낮음 키오스크의 구축 비용이 높아 자영업자들에게 부담이 됨 🔹 솔루션 NUGU 스피커를 활용한 음성인식 기반 주문 시스템을 도입 간결한 UI 설계로 접근성 향상 기존 키오스크 대비 저렴한 비용으로 구축 가능 🎯 시스템 구조음성인식 키오스크 시스템은 다음과 같은 아키텍처로 구성됩니다: Frontend: React.js Backend: Node.js (Express.js) Cloud Services: AWS EC2, S3 AI &amp; Voice Recognition: SKT NUGU API CI/CD: Docker, Jenkins 🏗️ 기능 개요 음성인식 주문 시스템: 고객이 NUGU 스피커에 말하면 주문이 자동으로 입력됨 메뉴 탐색 및 선택: 음성을 통해 메뉴를 탐색하고 선택 가능 결제 시스템 연동: 주문 완료 후 결제 API와 연동 관리자 기능: 주문 내역 확인 및 메뉴 업데이트 가능 📌 UI 및 UX 개선 기존 키오스크 터치스크린 기반, UI가 복잡함 노년층 및 비숙련자에게 접근성이 낮음 VISQUIT 키오스크 음성만으로 조작 가능하여 접근성이 향상됨 UI가 단순하여 누구나 쉽게 사용 가능 🛠️ 기술 스택 영역 기술 스택 Frontend React.js, JavaScript Backend Node.js, Express.js AI SKT NUGU API Cloud AWS EC2, S3 CI/CD Docker, Jenkins 📎 프로젝트 자료 📂 Frontend Repository 📂 Backend Repository 📑 최종 발표 자료 (PDF) 🏆 프로젝트 기여자 cadenzah hamin7 hy06ix king10tech 🔚 결론이 프로젝트를 통해 음성인식 기술이 키오스크 시장에서 중요한 역할을 할 수 있음을 확인하였습니다.향후 연구 방향으로는 더 정교한 음성인식 기능 추가 및 다국어 지원을 고려하고 있습니다.","link":"/2019/12/07/VISQUIT/"},{"title":"인공지능 스피커를 이용한 스포츠 질문 서비스","text":"인공지능 스피커를 이용한 스포츠 질문 서비스이 프로젝트는 인공지능과 응용 수업에서 진행한 스포츠 매니아 프로젝트로, MLB, La Liga, EPL 등 다양한 스포츠 경기 정보를 제공하는 AI 기반 음성인식 서비스입니다.SKT Nugu 스피커와 AWS Lambda, Django를 활용하여 개발하였으며, 사용자가 음성으로 질문하면 실시간으로 스포츠 정보를 제공합니다. 🎯 프로젝트 개요🔹 문제 정의 스포츠 팬들이 다양한 경기 정보를 한 곳에서 쉽게 확인할 수 있는 플랫폼 부족 음성인식을 활용한 스포츠 정보 제공 서비스의 부재 모바일과 웹에서 제공되는 텍스트 기반 정보의 한계 🔹 솔루션 SKT Nugu AI 스피커를 활용하여 음성 기반 스포츠 정보 제공 AWS Lambda &amp; Django로 데이터 처리 및 응답 속도 최적화 다양한 리그(MLB, EPL, La Liga) 지원 및 실시간 경기 정보 제공 ⚙️ 시스템 아키텍처이 서비스는 다음과 같은 클라우드 기반 구조로 동작합니다: Frontend: SKT Nugu AI 스피커 Backend: Django (REST API) Cloud Services: AWS Lambda, DynamoDB Voice Recognition: SKT Nugu API 📌 주요 기능 음성인식 질문 응답: 사용자가 “오늘 MLB 경기 결과 알려줘”라고 말하면 최신 경기 결과 제공 리그별 경기 정보 제공: MLB, EPL, La Liga 경기 일정 및 결과 조회 실시간 데이터 업데이트: AWS Lambda를 활용한 경기 정보 자동 업데이트 🎥 시연 영상아래 링크에서 프로젝트의 데모 영상을 확인할 수 있습니다: 📎 프로젝트 자료 📂 프로젝트 홈페이지 📑 최종 보고서 🏆 프로젝트 기여자 hamin7 cadenzah hy06ix 🔚 결론이 프로젝트를 통해 음성인식 기술이 스포츠 정보 제공 서비스에 유용하게 활용될 수 있음을 확인하였습니다.향후 연구 방향으로는 더 다양한 스포츠 리그 지원 및 맞춤형 경기 정보 추천 기능을 추가할 계획입니다. 🚀","link":"/2019/12/08/sportsMania/"},{"title":"(프로젝트) Pomodoro 할 일 관리 어플","text":"(프로젝트) Pomodoro 할 일 관리 어플Pomodoro 기법을 활용하여 할 일을 효율적으로 관리할 수 있는 어플리케이션을 개발했습니다.React, Redux, Firebase 및 AWS Lambda를 활용하여 클라우드 기반의 빠르고 확장 가능한 서비스를 구축하였습니다. 📌 프로젝트 개요🔹 문제 정의 사용자가 집중력을 유지하며 작업할 수 있도록 Pomodoro 기법 적용 직관적인 UI/UX를 제공하여 생산성 향상 실시간 동기화를 지원하는 클라우드 기반의 서비스 제공 🔹 솔루션 React 및 Redux 기반의 SPA (Single Page Application) 구축 Firebase를 활용한 실시간 데이터 동기화 AWS Lambda를 활용한 백엔드 로직 실행 ⚙️ 기술 스택 영역 기술 스택 Frontend React.js, Redux, JavaScript Backend Firebase, AWS Lambda Database Firestore (NoSQL) CI/CD GitHub Actions, Firebase Hosting 🏗️ 주요 기능 할 일 등록 및 관리: 사용자가 할 일을 추가, 수정, 삭제 가능 Pomodoro 타이머 기능: 25분 집중 + 5분 휴식 타이머 지원 클라우드 동기화: Firebase 기반 실시간 데이터 저장 반응형 UI 지원: 모바일 및 데스크톱 환경 최적화 🎥 데모 및 실행 링크 📂 프로젝트 Repository 🚀 서비스 이용하기 📎 프로젝트 자료아래는 프로젝트의 주요 화면들입니다: 🔚 결론이 프로젝트를 통해 Pomodoro 기법을 활용한 할 일 관리 시스템의 효과를 확인하였습니다.향후 연구 방향으로는 사용자 맞춤형 알림 및 분석 기능 추가를 고려하고 있습니다. 🚀","link":"/2021/02/09/pomodoro/"},{"title":"동시성 처리","text":"공짜 점심은 끝났다. 소프트웨어는 동일한데 CPU가 빨라지면 덩달아 빨라지는것 Scale Up 성능 향상 특이점 이후의 CPU CPU는 더 이상 빨라지지 않고 더 많은 일(다중 코어)을 동시에 할 수 있게 발전 (-&gt; Scale Out) 동시성을 고려한 프로그램을 작성해야 한다. 확장성 (Scalability) 하나의 시스템이 성능 향상에 지장을 받지 않고 자원에 대한 수요읠 변화를 수용할 수 있는 정도 수요의 증가를 수용할 때 자원을 추가 투입하는 것은 당연하고 단지 그 비율이 선형적이거나 아니면 조금 더 효율적이길 바란다. 동시성 (Concurrency) 확장성을 위한 수단으로써, 하마디로 자원을 더 주면 프로그램이 알아서 잘 활용하는 것 자원을 잘 활용하기 위한 프로그램의 복잡도가 증가하지 않거나 완만하게 증가하기를 바란다. 객체지향 모델링 객체란 은닉한 상태에 대해 지켜야 하는 논리를 보호하는 안전한 동작들만 노출 시킴 시스템은 메서드 호출에 반응하여 내부의 상태를 수정하는, 즉 전체 상태를 진전시키는 동작 호출을 통해 서로와 통신하는 객체 인스턴스의 네트워크로 표현 멀티쓰레드 객체에서의 은닉이라는 것은 멀티쓰레드 환경을 고려하지 않았다면 이러한 동작들이 안전하지 않게 되며 내부 상태 오염에 이르게 됨 실제 멀티 쓰레드 환경에서의 시스템은 쓰레드들이 동작 호출을 통해 객체 인스턴스의 네트워크를 따라 이동하는 것 객체지향 모델링의 한계 상태의 은닉만으로는 동시에 동작을 호출하는 것에 대한 보호를 할 수 없음 getNext()는 세가지 동작으로 구성 value -&gt; 9 9 + 1 -&gt; 10 10 -&gt; value value -&gt; 10 10 + 1 -&gt; 11 11 -&gt; value 1234567public class UnsafeSequence { private int value' public int getNext() { return this.value++; }} 쓰레드를 제어해야 함 쓰레드 프로세스란 격리되어 있고 독립적으로 동작하는 OS의 관리 단위로 순차적인 실행과 OS의 I/O를 통한 외부와의 통신을 특징으로 함 쓰레드는 프로세스내에서 동시적인 실행과 메모리 공유를 통한 통신 처리량의 향상 메모리 공유라는 것은 비순차성을 의미하며 개발이 복잡해지고 추론이 어려워지는 것을 의미 충분히 동기화하지 않으면 멀티쓰레드에서의 동작들의 순서는 예측 불가 멀티쓰레드라는 것이 전체 성능은 올릴지라도 어느 정도의 런타임 부하를 수반 동시적 프로그래밍 공유하고 변경 가능한 상태에 대한 접근을 통제 멀티 쓰레드에 안전한 코드를 만든다는 것은 상태를 쓰레드간에 공유하지 않는다. 상태를 변경 불가능하게 한다. 상태를 접근할 때 동기화를 사용한다 하나 이상의 쓰레드가 상태에 접근하고 그 중 하나가 변경할 수 있다면 이 모든 쓰레드들은 동기화를 통해 해당 상태에 대한 접근을 순서대로 배열하여야 한다. 1234567public class StatelessFactorizer implements Servlet { public void service(ServiceRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); encodeIntoResponse(resp, factors); }} 소인수분해를 하는 서블릿 상태가 없으면 통제할 대상이 없으므로 항상 멀티쓰레드에 안전 12345678910111213141516171819import java.math.BigInteger;public clss UnsafeCachingFactorizer implementsservlet { private BigInteger lastNumber; private BigInteger[] lastFactos; public void service (ServletRequest req, ServletResponse resp){ BigInteger i = extractFromRequest(req); if (i.equals(this.lastNumber) encodingIntoResponse(reqp, this.lastFactors)); else{ BigInteger[] factors = factor(i); this.lastNumber = i; this.lastFactors = factors; encodeIntoResponse(resp, factors); } }} WIP…","link":"/2021/09/13/Concurrency/"},{"title":"Lagom Core Concepts","text":"Lagom Core Concepts이 글에서는 Lagom 프레임워크의 핵심 개념을 설명하고, 예제 코드로 MembershipService.scala를 분석합니다. 📌 Lagom 개요Lagom은 **마이크로서비스 아키텍처(MSA)**를 구축하기 위한 Scala 기반 프레임워크입니다. Lagom의 주요 특징 Service API 중심 개발: 서비스 인터페이스를 정의하고 구현을 분리할 수 있음. Event Sourcing &amp; CQRS 지원: 데이터 변경을 이벤트로 관리. 비동기, 반응형 시스템: Akka와 Play 기반으로 높은 확장성 제공. 개발 편의성: Lagom Dev Mode를 활용한 빠른 개발 사이클. 🏗 MembershipService 예제아래는 Lagom에서 제공하는 MembershipService의 정의입니다. 12345678910111213141516171819202122package com.example.membership.apiimport akka.{Done, NotUsed}import com.lightbend.lagom.scaladsl.api.transport.Methodimport com.lightbend.lagom.scaladsl.api.{Descriptor, Service, ServiceCall}trait MembershipService extends Service { def join(name: String): ServiceCall[NotUsed, Done] def leave(name: String): ServiceCall[NotUsed, Done] def get(name: String): ServiceCall[NotUsed, String] override final def descriptor: Descriptor = { import Service._ named(&quot;membership&quot;) .withCalls( restCall(Method.PUT, &quot;/membership/:name&quot;, join _), restCall(Method.DELETE, &quot;/membership/:name&quot;, leave _), restCall(Method.POST, &quot;/membership/:name&quot;, get _) ) .withAutoAcl(true) }} 🛠 코드 설명 ServiceCall[NotUsed, Done]: 요청과 응답을 정의하는 Lagom의 핵심 개념. join, leave, get: REST 엔드포인트를 제공하는 메서드. descriptor: 서비스의 API 정의. restCall: HTTP REST 엔드포인트 매핑. 🔗 참고 자료 Lagom 공식 문서 Akka &amp; Lagom 개념 🔚 결론Lagom은 Scala 기반의 마이크로서비스 프레임워크로, 서비스 API 중심으로 개발을 진행할 수 있도록 돕습니다. 이 글에서는 MembershipService 예제를 통해 Lagom의 기본적인 구조와 특징을 살펴보았습니다. 🚀","link":"/2022/01/17/lagomCoreConcepts/"}],"tags":[{"name":"CLI","slug":"CLI","link":"/tags/CLI/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Backend","slug":"Backend","link":"/tags/Backend/"},{"name":"Travel","slug":"Travel","link":"/tags/Travel/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"JupyterLab","slug":"JupyterLab","link":"/tags/JupyterLab/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Microsoft","slug":"Microsoft","link":"/tags/Microsoft/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Netflix","slug":"Netflix","link":"/tags/Netflix/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"},{"name":"LiteLLM","slug":"LiteLLM","link":"/tags/LiteLLM/"},{"name":"Batch Server","slug":"Batch-Server","link":"/tags/Batch-Server/"},{"name":"Cron","slug":"Cron","link":"/tags/Cron/"},{"name":"LangSmith","slug":"LangSmith","link":"/tags/LangSmith/"},{"name":"LangChain","slug":"LangChain","link":"/tags/LangChain/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"LLM,LangChain","slug":"LLM-LangChain","link":"/tags/LLM-LangChain/"},{"name":"SK","slug":"SK","link":"/tags/SK/"},{"name":"Azure","slug":"Azure","link":"/tags/Azure/"},{"name":"Microsoft,Azure","slug":"Microsoft-Azure","link":"/tags/Microsoft-Azure/"},{"name":"MS Azure","slug":"MS-Azure","link":"/tags/MS-Azure/"},{"name":"AZ204","slug":"AZ204","link":"/tags/AZ204/"},{"name":"Agent","slug":"Agent","link":"/tags/Agent/"},{"name":"AZ104","slug":"AZ104","link":"/tags/AZ104/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Self-Driving Cars","slug":"Self-Driving-Cars","link":"/tags/Self-Driving-Cars/"},{"name":"Books","slug":"Books","link":"/tags/Books/"},{"name":"Ethics","slug":"Ethics","link":"/tags/Ethics/"},{"name":"Biotechnology","slug":"Biotechnology","link":"/tags/Biotechnology/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"React.js","slug":"React-js","link":"/tags/React-js/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"Frontend","slug":"Frontend","link":"/tags/Frontend/"},{"name":"Firebase","slug":"Firebase","link":"/tags/Firebase/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"Redux","slug":"Redux","link":"/tags/Redux/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"Multi-threading","slug":"Multi-threading","link":"/tags/Multi-threading/"},{"name":"Scalability","slug":"Scalability","link":"/tags/Scalability/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Programming","slug":"Programming","link":"/tags/Programming/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Microservices","slug":"Microservices","link":"/tags/Microservices/"},{"name":"Lagom","slug":"Lagom","link":"/tags/Lagom/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"CLI","slug":"Linux/CLI","link":"/categories/Linux/CLI/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"MachineLearning","slug":"AI/MachineLearning","link":"/categories/AI/MachineLearning/"},{"name":"NLP","slug":"AI/NLP","link":"/categories/AI/NLP/"},{"name":"DeepLearning","slug":"AI/DeepLearning","link":"/categories/AI/DeepLearning/"},{"name":"Backend","slug":"Backend","link":"/categories/Backend/"},{"name":"AWS","slug":"Backend/AWS","link":"/categories/Backend/AWS/"},{"name":"Travel","slug":"Travel","link":"/categories/Travel/"},{"name":"Japan","slug":"Travel/Japan","link":"/categories/Travel/Japan/"},{"name":"JupyterLab","slug":"AI/JupyterLab","link":"/categories/AI/JupyterLab/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"SageMaker","slug":"AWS/SageMaker","link":"/categories/AWS/SageMaker/"},{"name":"EC2","slug":"AWS/EC2","link":"/categories/AWS/EC2/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Linux","slug":"Docker/Linux","link":"/categories/Docker/Linux/"},{"name":"EKS","slug":"AWS/EKS","link":"/categories/AWS/EKS/"},{"name":"Microsoft","slug":"Microsoft","link":"/categories/Microsoft/"},{"name":"AI","slug":"Microsoft/AI","link":"/categories/Microsoft/AI/"},{"name":"Framework","slug":"Framework","link":"/categories/Framework/"},{"name":"Python","slug":"Framework/Python","link":"/categories/Framework/Python/"},{"name":"Cognito","slug":"AWS/Cognito","link":"/categories/AWS/Cognito/"},{"name":"Aurora","slug":"AWS/Aurora","link":"/categories/AWS/Aurora/"},{"name":"Route53","slug":"AWS/Route53","link":"/categories/AWS/Route53/"},{"name":"API Gateway","slug":"AWS/API-Gateway","link":"/categories/AWS/API-Gateway/"},{"name":"Lambda","slug":"AWS/Lambda","link":"/categories/AWS/Lambda/"},{"name":"Netflix","slug":"Netflix","link":"/categories/Netflix/"},{"name":"Book Review","slug":"Book-Review","link":"/categories/Book-Review/"},{"name":"LLM","slug":"LLM","link":"/categories/LLM/"},{"name":"BERT","slug":"LLM/BERT","link":"/categories/LLM/BERT/"},{"name":"LiteLLM","slug":"LiteLLM","link":"/categories/LiteLLM/"},{"name":"Batch Server","slug":"Batch-Server","link":"/categories/Batch-Server/"},{"name":"Cron","slug":"Cron","link":"/categories/Cron/"},{"name":"LangSmith","slug":"LangSmith","link":"/categories/LangSmith/"},{"name":"LangSmith","slug":"LLM/LangSmith","link":"/categories/LLM/LangSmith/"},{"name":"LangChain","slug":"LLM/LangChain","link":"/categories/LLM/LangChain/"},{"name":"Agent","slug":"Agent","link":"/categories/Agent/"},{"name":"Document","slug":"Agent/Document","link":"/categories/Agent/Document/"},{"name":"Javascript","slug":"Javascript","link":"/categories/Javascript/"},{"name":"Node.js","slug":"Javascript/Node-js","link":"/categories/Javascript/Node-js/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Autonomous Vehicles","slug":"Machine-Learning/Autonomous-Vehicles","link":"/categories/Machine-Learning/Autonomous-Vehicles/"},{"name":"Philosophy","slug":"Philosophy","link":"/categories/Philosophy/"},{"name":"Toy Project","slug":"AI/Toy-Project","link":"/categories/AI/Toy-Project/"},{"name":"Bioethics","slug":"Philosophy/Bioethics","link":"/categories/Philosophy/Bioethics/"},{"name":"Projects","slug":"AI/Toy-Project/Projects","link":"/categories/AI/Toy-Project/Projects/"},{"name":"Backend","slug":"AI/Toy-Project/Projects/Backend","link":"/categories/AI/Toy-Project/Projects/Backend/"},{"name":"Node.js","slug":"AI/Toy-Project/Projects/Backend/Node-js","link":"/categories/AI/Toy-Project/Projects/Backend/Node-js/"},{"name":"Frontend","slug":"AI/Toy-Project/Projects/Backend/Node-js/Frontend","link":"/categories/AI/Toy-Project/Projects/Backend/Node-js/Frontend/"},{"name":"SKT","slug":"AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT","link":"/categories/AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT/"},{"name":"AWS","slug":"AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT/AWS","link":"/categories/AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT/AWS/"},{"name":"EC2","slug":"AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT/AWS/EC2","link":"/categories/AI/Toy-Project/Projects/Backend/Node-js/Frontend/SKT/AWS/EC2/"},{"name":"Projects","slug":"AI/Projects","link":"/categories/AI/Projects/"},{"name":"Backend","slug":"AI/Projects/Backend","link":"/categories/AI/Projects/Backend/"},{"name":"Django","slug":"AI/Projects/Backend/Django","link":"/categories/AI/Projects/Backend/Django/"},{"name":"SKT","slug":"AI/Projects/Backend/Django/SKT","link":"/categories/AI/Projects/Backend/Django/SKT/"},{"name":"AWS","slug":"AI/Projects/Backend/Django/SKT/AWS","link":"/categories/AI/Projects/Backend/Django/SKT/AWS/"},{"name":"Lambda","slug":"AI/Projects/Backend/Django/SKT/AWS/Lambda","link":"/categories/AI/Projects/Backend/Django/SKT/AWS/Lambda/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Backend","slug":"Projects/Backend","link":"/categories/Projects/Backend/"},{"name":"Frontend","slug":"Projects/Backend/Frontend","link":"/categories/Projects/Backend/Frontend/"},{"name":"React.js","slug":"Projects/Backend/Frontend/React-js","link":"/categories/Projects/Backend/Frontend/React-js/"},{"name":"AWS","slug":"Projects/Backend/Frontend/React-js/AWS","link":"/categories/Projects/Backend/Frontend/React-js/AWS/"},{"name":"Lambda","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/"},{"name":"Toy Projects","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/"},{"name":"Firebase","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/"},{"name":"KT","slug":"Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/KT","link":"/categories/Projects/Backend/Frontend/React-js/AWS/Lambda/Toy-Projects/Firebase/KT/"},{"name":"Software Engineering","slug":"Software-Engineering","link":"/categories/Software-Engineering/"},{"name":"Distributed Systems","slug":"Backend/Distributed-Systems","link":"/categories/Backend/Distributed-Systems/"},{"name":"Computer Science","slug":"Software-Engineering/Computer-Science","link":"/categories/Software-Engineering/Computer-Science/"},{"name":"Software Architecture","slug":"Backend/Distributed-Systems/Software-Architecture","link":"/categories/Backend/Distributed-Systems/Software-Architecture/"},{"name":"Performance Optimization","slug":"Software-Engineering/Computer-Science/Performance-Optimization","link":"/categories/Software-Engineering/Computer-Science/Performance-Optimization/"}],"pages":[{"title":"Who am I","text":"Backend Engineer What I Interested inProgramming Language &amp; Framework Scala, Lagom, Akka Java, SpringBoot, WebFlux Python, FastAPI, LangChain Javascript, React, Node.js Infra Docker &amp; Docker Compose Kubernetes AWS, Azure LLM LangChain, LangSmith pgvector RAG ETC Cassandra, Redis, PostgreSQL, PostGIS Kafka Jenkins Android ProjectsKT내비게이션 앱 내 챗봇 AI Agent 개발 기간 : 2024.6 ~ 2024.12 프로젝트 개요 멀티턴 대화를 지원하는 LLM 기반 챗봇 서비스 개발(PoC) 기여한 점 Master Agent 개발 경로탐색/ POI(Point Of Interest) Agent 등을 관리 및 발화 의도 분류 멀티턴 대화를 위한 대화 기록 및 대화요약 저장을 통한 개인화 (Redis + Langchain) pgvector 기반 개인화 고도화 GPU 서버 기반 JupyterLab 및 SageMaker 환경 구축하여 LLM 모델 학습 및 서빙 지원 LLM 서비스 API 제공을 위한 인프라 및 DevOps 체계 구축 활용 기술 Backend: Python, FastAPI, LangChain, Redis, pgvector Infra &amp; DevOps: AWS(Lambda, API Gateway, EC2, EKS), Jenkins 경로탐색엔진 성능 고도화 기간 : 2023.7 ~ 2024.6 프로젝트 개요 기존 내비게이션 엔진의 경로 탐색 속도를 서울역-제주공항 기준 7초 → 2초 미만으로 단축목표 양방향 다익스트라, Contraction Hierarchy(CCH) 알고리즘을 활용하여 효율적인 도로네트워크 그래프 모델을 구성하고, 이를 기반으로 빠른 속도의 경로탐색 구현 주요 역할 및 성과 Customizable Contraction Hierarchy(CCH) 알고리즘 적용(PoC) 상황 도로네트워크 링크 수 증가(2024년 3,757,898개)로 인한 경로탐색 속도 저하 문제 기존 양방향 다익스트라알고리즘에서는 모든 도로네트워크를 탐색해야 했음 기여한 점 Contraction Hierarchy 알고리즘을 통해 경로탐색 모델을 구축 그래프의 노드를 가중치(큰길우선, 무료도로우선 등) 우선순위에 따라 경로 탐색 시 불필요한 경로를 제외하는 전처리 작업을 통해 경로탐색 효율성 향상 성과 및 영향 경로탐색 속도 대폭 개선 (서울역 → 제주공항 / 7초 → 약 3초) 활용 기술 알고리즘: Bidirectional Dijkstra, Contraction Hierarchy(CCH) PostgreSQL(PostGIS), Java KT 내비게이션 서버 컨테이너화 기간 : 2023.1 ~ 2023.6 프로젝트 개요 기존 VM 기반으로 운영되던 서비스를 KT Cloud 컨테이너 환경으로 이전하여 리소스 최적화 주요 역할 및 성과 기존 VM 기반 서비스 → 컨테이너 기반 전환 상황 기존 서비스는 VM 환경에서 개별적으로 운영되어 서버 리소스 낭비가 발생, 배포 및 확장성 관리가 어려운 문제가 있었음 기여한 점 API Call 수 및 리소스 사용률 기반으로 필요 인프라 설계, 유관 부서들과 논의하여 인프라 구축 Docker &amp; k8s를 활용하여 기존 VM 기반 서비스를 컨테이너 환경으로 전환 필요 시 자동 확장(Auto Scaling)이 가능하도록 k8s Horizontal Pod Autoscaler(HPA) 적용 인프라 전환 시 높은 보안 수준을 달성하기 위해 보안단과 협업하여 보안성 검토 프로세스를 진행 성과 및 영향 VM 서버 30대 → VM 서버 8대(컨테이너 16대)로 축소, 운영 비용 절감 및 효율성 향상 k8s 기반으로 서비스 가용성이 99.99% 이상 유지 CI/CD 파이프라인 구축을 통한 개발 환경 생산성 증가 상황 기존 VM 환경에서는 수동 배포(커맨드) 방식으로 운영되어 배포 시간이 길고, 운영 중 장애 발생 시 롤백 절차가 복잡했음 기여한 점 Jenkins 기반 CI/CD 파이프라인 구축 (빌드 → 테스트 → 배포) 쿠버네티스 설정값 튜닝을 통한 배포 안정성 강화 성과 및 영향 배포시간 약 50% 단축, 장애 발생 시 롤백 시간 감소, 무중단 배포 활용기술 Docker, 쿠버네티스, Jenkins KT 내비게이션 통합로그 시스템 개발 기간 : 2022.9 ~ 2022.12 프로젝트 개요 내비게이션 앱에서 발생하는 GPS 정보, 사고제보, 그룹 주행 정보 등의 로그와, 관리자시스템에서 입력하는 유고 정보(사고 차량, 도로 통제 등) 를 효율적으로 수집하고 분석할 수 있도록 로그 집계 시스템 개발 기존에는 각 시스템에서 여러 유형의 로그를 모두 처리해야 했지만, 모든 로그를 집계하는 통합로그 시스템 구축을 통해 하나의 시스템에만 의존할 수 있도록 개선 기여한 점 관리자 시스템에서 유고정보 입력 및 데이터 흐름 개선 React 기반 관리자페이지에서 유고정보 입력 기능 개발 SpringBoot 서버에 유고정보 저장 및 Kafka 메시지 Pub 로직 추가 Kafka 기반의 통합로그 시스템 개발 내비게이션 앱에서 서버로 전송하는 GPS, 사고제보, 그룹주행 로그 등을 Kafka에 메시지 Pub Kafka 기반으로 유고정보를 메시지 Sub하고 내비게이션 앱으로 실시간 응답 성과 및 영향 로그 집계 및 메시징 시스템 도입으로 유지보수성 향상 로그의 원천으로부터 다른 시스템간의 결합도 감소 활용 기술 React, SpringBoot, Spring Webflux, Kafka KT 내비게이션 녹색교통 패턴 정책화 기간 : 2022.7 ~ 2022.8 프로젝트 개요 기존에는 변동이 거의 없는 녹색교통 정책이 Batch 방식으로 1시간마다 긴급정보 서버로부터 불필요하게 갱신 이를 개선하기 위해 KT내비게이션 관리자시스템에서 직접 녹색교통 정책을 등록 및 관리하고, 정책 변경이 발생할 때만 앱에 적용되도록 이벤트 기반으로 최적화. 기여한 점 관리자시스템에서 녹색교통 정책 입력 및 메시지 발행 기능 개발 React 기반 관리자페이지에서 녹색교통 입력 기능 개발 SpringBoot 서버에 녹색교통 저장 및 Kafka 메시지 Pub 로직 추가 Kafka 기반 메시징 시스템 구축 녹색교통 정책 Topic을 생성, 관리자시스템에서 Pub, 내비게이션 GW서버에서 Sub 내비게이션 GW서버의 2주치 정책 스케줄 생성 및 앱 활성화 시 정책 로딩 로직 개발 성과 및 영향 정책 변경 발생 시에만 유효한 데이터를 제공하여 불필요한 요청 제거 활용 기술 React, Spring Boot, Kafka, Scala, Lagom KT내비게이션 API GW 내재화 및 고도화 기간 : 2021.9 ~ 2022.6 프로젝트 개요 원내비 내재화 프로젝트에서 API Gateway 서버 개발 담당 내비게이션 서비스의 트래픽 처리 최적화, 사용자 데이터 관리 시스템 구축, 인증 시스템 구축 주요 역할 및 성과 동기 호출 개선 상황 출퇴근 시간과 명절과 같은 높은 트래픽 유입시 서비스 간 통신이 동기호출방식이 많아 병목 현상 발생, 장애 발생 시 서비스 전체가 영향 받음 기여한 점 비동기 프로세싱 적용 기존 동기 API 호출을 Akka Actor 기반 비동기 메시징 방식으로 변경, 서비스간 병목 현상 제거 Kafka를 활용, 일부 요청을 이벤트 기반 아키텍처로 변경 Circuit Breaker 패턴을 도입하여 특정 서비스 장애 발생 시 API GW의 정상 작동 보장 성과 일부 쓰기 작업에 대해 비동기 메시징 적용, API 응답속도 40% 단축 (평균 1.5초 -&gt; 900ms) Circuit Breaker 도입으로 장애 발생 시 자동 복구율 향상 데이터 정합성 개선 상황 기존시스템은 JPA, RDBMS를 사용하여 다중 인스턴스 환경에서 동일한 사용자 데이터가 여러 개의 노드에 존재, 특정 시점에서 다른 노드와 데이터 불일치 발생 기여한 점 Akka Persistence 및 이벤트 소싱 적용을 통한 데이터 일관성 및 확장성 강화 실시간 주행 기록, 상태 정보 등의 데이터를 이벤트 소싱으로 저장하여 최신 상태를 유지하고 트랜잭션 일관성 확보 데이터베이스 레벨에서 Read/Write 분리하고, 고속 데이터 쓰기/읽기 성능을 위해 Cassandra 도입 CQRS 패턴 도입하여 명령과 조회 로직을 분리하여 데이터 일관성 강화 성과 이벤트 소싱 적용 후 장애 발생 시 데이터 복구율 99.9% 데이터 정합성 문제로 인한 고객 문의 80% 감소 로드밸런싱 문제 개선 상황 기존시스템은 트래픽이 몰릴경우 특정 노드가 과부하 상태가 되는 문제 발생 로드밸런싱이 동기 방식 기반으로 적용되어 스케일링이 어려움 기여한 점 Akka의 Cluster Sharding 적용, 특정 노드에 부하가 집중되는 문제 해결, 인스턴스ID를 기반으로 노드 간 트래픽을 자동으로 분산 OpenResty, Kong을 활용하여 트래픽을 균등 분산 성과 서버 과부하 문제 해결 -&gt; 평균 CPU 사용률 5% 감소 서비스 다운타임 60% 감소 -&gt; SLA 개선 활용 기술 Scala, Lagom, Akka, Kafka, Cassandra, OpenResty 지니TV Android 네이티브 앱 전환 개발 기간 : 2020.12 ~ 2021.8 프로젝트 개요 웹뷰 기반으로 제공되던 기존 올레TV 서비스를 안드로이드 TV 플랫폼으로 전환 주요 역할 검색, 기기 설정, VOD 재생 화면 개발 및 서버 연동 상황 기존 올레TV 검색 기능은 웹뷰 기반이라 속도가 느리고 관련 VoC가 많았음 리모컨 포커스 손실 이슈, 예상치 못한 UI 비활성화 이슈 발생 기여한 점 안드로이드 TV 환경에 맞춰 Leanback 라이브러리 기반의 검색 UI 개발 AI스피커(기가지니)를 연동한 음성 검색기능 추가를 통한 사용자 편의성 강화 Leanback 라이브러리 및 커스텀 포커스 핸들링을 적용, Fragment 간 포커스 이동 경로 최적화 성과 및 영향 기존대비 UI 렌더링 속도 2배 향상, 속도관련 VoC 발생률 0%로 감소 검증팀 테스트 결과 포커스 사라지는 결함 0%로 감소 활용 기술 Android Studio, Java, Leanback Library","link":"/about/index.html"}]}